{"results": [{"createdAt": null, "postedAt": "2011-08-07T05:58:34.499Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] My Wild and Reckless Youth", "slug": "seq-rerun-my-wild-and-reckless-youth", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:01.541Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TnJau8m9EttRHBoGv/seq-rerun-my-wild-and-reckless-youth", "pageUrlRelative": "/posts/TnJau8m9EttRHBoGv/seq-rerun-my-wild-and-reckless-youth", "linkUrl": "https://www.lesswrong.com/posts/TnJau8m9EttRHBoGv/seq-rerun-my-wild-and-reckless-youth", "postedAtFormatted": "Sunday, August 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20My%20Wild%20and%20Reckless%20Youth&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20My%20Wild%20and%20Reckless%20Youth%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTnJau8m9EttRHBoGv%2Fseq-rerun-my-wild-and-reckless-youth%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20My%20Wild%20and%20Reckless%20Youth%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTnJau8m9EttRHBoGv%2Fseq-rerun-my-wild-and-reckless-youth", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTnJau8m9EttRHBoGv%2Fseq-rerun-my-wild-and-reckless-youth", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 210, "htmlBody": "<p>Today's post, <a href=\"/lw/iy/my_wild_and_reckless_youth/\">My Wild and Reckless Youth</a> was originally published on 30 August 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Traditional rationality (without Bayes' Theorem) allows you to formulate hypotheses without a reason to prefer them to the status quo, as long as they are falsifiable. Even following all the rules of traditional rationality, you can waste a lot of time. It takes a lot of rationality to avoid making mistakes; a moderate level of rationality will just lead you to make new and different mistakes.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/6zw/seq_rerun_say_not_complexity/\">Say Not \"Complexity\"</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TnJau8m9EttRHBoGv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 7.515301772878924e-07, "legacy": true, "legacyId": "9094", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DwtYPRuCxpXTrzG9m", "W8QhXPyiYf2t3HZTF", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-07T09:59:22.784Z", "modifiedAt": null, "url": null, "title": "Meetup : Hiking in Vancouver, Canada", "slug": "meetup-hiking-in-vancouver-canada", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:01.103Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "michaelkeenan", "createdAt": "2009-03-02T10:01:40.717Z", "isAdmin": false, "displayName": "michaelkeenan"}, "userId": "GBtCcsREHSDGbo9Dw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dz2nnYNPJcMQGQ788/meetup-hiking-in-vancouver-canada", "pageUrlRelative": "/posts/dz2nnYNPJcMQGQ788/meetup-hiking-in-vancouver-canada", "linkUrl": "https://www.lesswrong.com/posts/dz2nnYNPJcMQGQ788/meetup-hiking-in-vancouver-canada", "postedAtFormatted": "Sunday, August 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Hiking%20in%20Vancouver%2C%20Canada&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Hiking%20in%20Vancouver%2C%20Canada%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdz2nnYNPJcMQGQ788%2Fmeetup-hiking-in-vancouver-canada%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Hiking%20in%20Vancouver%2C%20Canada%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdz2nnYNPJcMQGQ788%2Fmeetup-hiking-in-vancouver-canada", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdz2nnYNPJcMQGQ788%2Fmeetup-hiking-in-vancouver-canada", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/24'>Hiking in Vancouver, Canada</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 August 2011 06:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Grouse Mountain, British Columbia, Canada</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're hiking at Grouse Mountain on Tuesday evening from 6:30pm. The best way to get there seems to be taking the seabus to North Vancouver and then catching the 236 from there.</p>\n\n<p>There is a free (if you pay for the mountain pass or have a annual pass) transit (shuttle bus) that goes from downtown to Grouse. \nPickup/drop-off points (for the shuttle bus) are Canada Place, the Hyatt Regency and the Blue Horizon Hotel.</p>\n\n<p>I'm going to be driving there from Joyce St in Collingwood from about 5:30pm, so get in touch (michael dot keenan AT gmail dot com or +1 650 283 9013) if you'd like to be picked up on the way.</p>\n\n<p>To hear more last minute details, join the <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/24'>Hiking in Vancouver, Canada</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dz2nnYNPJcMQGQ788", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.516064961795536e-07, "legacy": true, "legacyId": "9095", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Hiking_in_Vancouver__Canada\">Discussion article for the meetup : <a href=\"/meetups/24\">Hiking in Vancouver, Canada</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 August 2011 06:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Grouse Mountain, British Columbia, Canada</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're hiking at Grouse Mountain on Tuesday evening from 6:30pm. The best way to get there seems to be taking the seabus to North Vancouver and then catching the 236 from there.</p>\n\n<p>There is a free (if you pay for the mountain pass or have a annual pass) transit (shuttle bus) that goes from downtown to Grouse. \nPickup/drop-off points (for the shuttle bus) are Canada Place, the Hyatt Regency and the Blue Horizon Hotel.</p>\n\n<p>I'm going to be driving there from Joyce St in Collingwood from about 5:30pm, so get in touch (michael dot keenan AT gmail dot com or +1 650 283 9013) if you'd like to be picked up on the way.</p>\n\n<p>To hear more last minute details, join the <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Hiking_in_Vancouver__Canada1\">Discussion article for the meetup : <a href=\"/meetups/24\">Hiking in Vancouver, Canada</a></h2>", "sections": [{"title": "Discussion article for the meetup : Hiking in Vancouver, Canada", "anchor": "Discussion_article_for_the_meetup___Hiking_in_Vancouver__Canada", "level": 1}, {"title": "Discussion article for the meetup : Hiking in Vancouver, Canada", "anchor": "Discussion_article_for_the_meetup___Hiking_in_Vancouver__Canada1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-07T16:53:42.171Z", "modifiedAt": null, "url": null, "title": "Paper draft: Relative advantages of uploads, artificial general intelligences, and other digital minds", "slug": "paper-draft-relative-advantages-of-uploads-artificial", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:01.891Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SAWjWqPY8vCPR3TwM/paper-draft-relative-advantages-of-uploads-artificial", "pageUrlRelative": "/posts/SAWjWqPY8vCPR3TwM/paper-draft-relative-advantages-of-uploads-artificial", "linkUrl": "https://www.lesswrong.com/posts/SAWjWqPY8vCPR3TwM/paper-draft-relative-advantages-of-uploads-artificial", "postedAtFormatted": "Sunday, August 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Paper%20draft%3A%20Relative%20advantages%20of%20uploads%2C%20artificial%20general%20intelligences%2C%20and%20other%20digital%20minds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APaper%20draft%3A%20Relative%20advantages%20of%20uploads%2C%20artificial%20general%20intelligences%2C%20and%20other%20digital%20minds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSAWjWqPY8vCPR3TwM%2Fpaper-draft-relative-advantages-of-uploads-artificial%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Paper%20draft%3A%20Relative%20advantages%20of%20uploads%2C%20artificial%20general%20intelligences%2C%20and%20other%20digital%20minds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSAWjWqPY8vCPR3TwM%2Fpaper-draft-relative-advantages-of-uploads-artificial", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSAWjWqPY8vCPR3TwM%2Fpaper-draft-relative-advantages-of-uploads-artificial", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<p><a href=\"http://www.xuenay.net/Papers/DigitalAdvantages.pdf\">http://www.xuenay.net/Papers/DigitalAdvantages.pdf</a></p>\n<p><em>Abstract: I survey four categories of factors that might give a digital mind, such as an upload or an artificial general intelligence, an advantage over humans. The categories are hardware advantages, self-improvement advantages, co-operative advantages and human handicaps. The shape of hardware growth curves as well as the ease of modifying minds are found to be some of the core influences on how quickly a digital mind may take advantage of these factors.</em></p>\n<p>Still a bit of a rough draft (could use a bunch of tidying up, my references aren't in a consistent format, etc.), but I wanted to finally get this posted somewhere public so I could get further feedback.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SAWjWqPY8vCPR3TwM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 7.517378408186446e-07, "legacy": true, "legacyId": "9097", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-07T17:46:40.071Z", "modifiedAt": null, "url": null, "title": "IntelligenceExplosion.com", "slug": "intelligenceexplosion-com", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:02.257Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XQkFe4fNmgwQAiYia/intelligenceexplosion-com", "pageUrlRelative": "/posts/XQkFe4fNmgwQAiYia/intelligenceexplosion-com", "linkUrl": "https://www.lesswrong.com/posts/XQkFe4fNmgwQAiYia/intelligenceexplosion-com", "postedAtFormatted": "Sunday, August 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20IntelligenceExplosion.com&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligenceExplosion.com%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXQkFe4fNmgwQAiYia%2Fintelligenceexplosion-com%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=IntelligenceExplosion.com%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXQkFe4fNmgwQAiYia%2Fintelligenceexplosion-com", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXQkFe4fNmgwQAiYia%2Fintelligenceexplosion-com", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<p>I put together a 'landing page' for the intelligence explosion concept similar to Nick Bostrom's landing pages for <a href=\"http://www.anthropic-principle.com/\">anthropics</a>, the <a href=\"http://www.simulation-argument.com/\">simulation argument</a>, and <a href=\"http://www.existentialrisk.com/\">existential risk</a>. The new website is <a href=\"http://intelligenceexplosion.com/\">IntelligenceExplosion.com</a>. You can see I borrowed the CSS from Bostrom's anthropics page and then simplified it.</p>\n<p>Just as with the <a href=\"http://intelligence.org/singularityfaq\">Singularity FAQ</a>, I'll be keeping this website up to date, so please send me corrections or bibliography additions at luke [at] singinst [dot] org.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1be": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XQkFe4fNmgwQAiYia", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 17, "extendedScore": null, "score": 7.517546341702894e-07, "legacy": true, "legacyId": "9098", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-07T23:39:58.376Z", "modifiedAt": null, "url": null, "title": "Towards a New Decision Theory for Parallel Agents ", "slug": "towards-a-new-decision-theory-for-parallel-agents", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:03.674Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "potato", "createdAt": "2011-06-15T09:18:51.735Z", "isAdmin": false, "displayName": "Ronny"}, "userId": "kY5hs2WkacnSZd937", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yKKP3EgvGAAPwTmTs/towards-a-new-decision-theory-for-parallel-agents", "pageUrlRelative": "/posts/yKKP3EgvGAAPwTmTs/towards-a-new-decision-theory-for-parallel-agents", "linkUrl": "https://www.lesswrong.com/posts/yKKP3EgvGAAPwTmTs/towards-a-new-decision-theory-for-parallel-agents", "postedAtFormatted": "Sunday, August 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Towards%20a%20New%20Decision%20Theory%20for%20Parallel%20Agents%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATowards%20a%20New%20Decision%20Theory%20for%20Parallel%20Agents%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKKP3EgvGAAPwTmTs%2Ftowards-a-new-decision-theory-for-parallel-agents%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Towards%20a%20New%20Decision%20Theory%20for%20Parallel%20Agents%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKKP3EgvGAAPwTmTs%2Ftowards-a-new-decision-theory-for-parallel-agents", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKKP3EgvGAAPwTmTs%2Ftowards-a-new-decision-theory-for-parallel-agents", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1708, "htmlBody": "<p>A recent post: <a href=\"/lw/6yh/consistently_inconsistent/\" target=\"_blank\">Consistently Inconsistent</a>, raises some problems with the unitary view of the mind/brain, and presents the modular view of the mind as an alternate hypothesis. The parallel/modular view of the brain not only deals better with the apparent hypocritical and contradictory ways our desires, behaviors, and believes seem to work, but also makes many successful empirical predictions, as well as postdictions. Much of that work can be found in Dennett's 1991 book: \"Consciousness Explained\" which details both the empirical evidence against the unitary view, and the <em>intuition-fails </em>involved in retaining a unitary view after being presented with that evidence.</p>\n<p>The aim of this post is not to present further evidence in favor of the parallel view, nor to hammer any more nails in the the unitary view's coffin; the scientific and philosophical communities have done well enough in both departments to discard the intuitive hypothesis that there is some <em>executive of the mind </em>keeping things orderly. The dilemma I wish to raise is a question: \"How should we update our decision theories to deal with independent, and sometimes inconsistent, desires and believes being had by <em>one agent</em>?\"</p>\n<hr />\n<p>If we model one agent's <em>desires </em>by using one utility function, and this function orders the outcomes the agent <a href=\"/lw/rb/possibility_and_couldness/\" target=\"_blank\">can reach</a><em> </em>on one real axis, then it seems like we might be falling back into the intuitive view that there is some <em>me </em>in there with one definitive list of preferences. The picture given to us by Marvin Mimsky and Dennett involves a bunch of individually dumb agents, each with a unique set of specialized <em>abilities</em> and <em>desires, </em>interacting in such a way so as to produce one smart agent, with a diverse set of abilities and desires, but the smart agent only apears when viewed from the right <em>level of description.&nbsp; </em>For convenience, we will call those dumb-specialized agents \"subagents\", and the smart-diverse agent that emerges from their interaction \"the smart agent\". When one considers what it would be useful for a seeing-neural-unit to <em>want</em> to do, and contrasts it with what it would be useful for a <em>get that food</em>-neural-unit to want to do, e.g., examine that prey longer v.s. charge that prey, turn head v.s. keep running forward, stay attentive v.s. eat that food, etc. it becomes clear that cleverly managing which unit gets to have how much control, and when, is an essential part of the decision making process of the whole. Decision theory, as far as I can tell, does not model any part of that managing process; instead we treat the smart agent as having its own set of desires, and don't discuss how the subagents' goals are being managed to produce that global set of desires.</p>\n<p>It is possible that the many subagents in a brain act <em>isomorphically </em>to an agent with one utility function and a unique problem space, when they operate in concert. A trivial example of such an agent might have only two subagents \"A\" and \"B\", and <a href=\"/lw/rb/possibility_and_couldness/\" target=\"_blank\">possible</a> outcomes O<sub>1</sub> through O<sub>n</sub>. We can plot the utilities that each subagent gives to these outcomes on a two dimensional positive Cartesian graph; A's assigned utilities being represented by position in X, and B's utilities by position in Y. The method by which these subagents are managed to produce behavior might just be: go for the possible outcome furthest from (0,0); in, which case, the utility function of the whole agent&nbsp; U(O<sub>x</sub>) would just be the distance from (0,0) to (A's U(O<sub>x</sub>) , B's U(O<sub>x</sub>)).</p>\n<p>An agent which manages its subagents so as to be <em>isomorphic</em> to one utility function on one problem space is certainly mathematically describable, but also implausible. It is unlikely that the actual physical-neural subagents in a brain deal with the same problem spaces, i.e., they each have their own unique set of O<sub>1 </sub>through O<sub>n</sub>. It is not as if all the subagents are playing the same game, but each has a unique goal within that game &ndash; they each have their own unique set of <em>legal moves</em> too. This makes it problematic to model the global utility function of the smart agent as assigning one real number to every member of a set of possible outcomes, since there is no one set of possible outcomes for the smart agent as a whole. Each subagent has its own search space with its own format of representation for that problem space. The problem space and utility function of the smart agent are implicit in the interactions of the subagents; they emerge from the interactions of agents on a lower level; the smart agents utility function and problem space are never explicitly <em>written down</em>.</p>\n<p>A useful example is smokers that are quitting. Some part of their brains that can do complicated predictions doesn't <em>want</em> its body to smoke. This part of their brain <em>wants</em> to avoid death, i.e., will avoid death if it can, and <em>knows </em>that choosing the possible outcome of smoking puts its body at high risk for death. Another part of their brains <em>wants</em> nicotine, and <em>knows</em> that choosing the move of smoking gets it nicotine. The nicotine craving subagent doesn't <em>want</em> to die, it also doesn't <em>want</em> to stay alive, these outcomes aren't in the domain of the nicotine-subagent's utility function at all. The part of the brain responsible for predicting its bodies death if it continues to smoke, probably isn't significantly rewarded by nicotine in a parallel manner. If a cigarette is around and offered to the smart agent, these subagents must compete for control of the relevant parts of their body, e.g., nicotine-subagent might set off a global craving, while predict-the-future-subagent might set off a vocal response saying \"no thanks, I'm quitting.\" The overall desire to smoke or not smoke of the smart agent is just the result of this competition. Similar examples can be made with different desires, like a desire to over eat and a desire to look slim, or the desire to stay seated and the desire to eat a warm meal.</p>\n<p>We may call the algorithm which settles these internal power struggles the \"managing algorithm\", and we may call a decision theory which models managing algorithms a \"parallel decision theory\". It's not the businesses of decision theorists to discover the specifics of the human managing process, that's the business of empirical science. But certain parts of the human managing algorithm can be reasonably decided on. It is very unlikely that our managing algorithm is utilitarian for example, i.e., the smart agent doesn't do whatever gets the highest net utility for its subagents. Some subagents are more powerful than others; they have a higher prior chance of success than their competitors; some others are weak in a parallel fashion. The question of what counts as one subagent in the brain is another empirical question which is not the business of decision theorists either, but anything that we do consider a subagent in a parallel theory must solve its problem in the form of a <a href=\"/lw/174/decision_theory_why_we_need_to_reduce_could_would/\" target=\"_blank\">CSA</a>, i.e., it must internally represent its outcomes, know what outcomes it can get to from whatever outcome it is at, and assign a utility to each outcome. There are likely many neural units that fit that description in the brain. Many of them probably contain as parts <em>subsubagnets </em>which also fit this description, but eventually, if you divide the parts enough, you get to neurons which are not CSAs, and thus not subagents.</p>\n<p>If we want to understand how we make decisions, we should try to model a CSA, which is <em>made</em> out of more spcialized sub-CSAs competing and agreeing, which are made out of further specialized sub-sub-CSAs competing and agreeing, which are made out of, etc. which are made out of non-CSA algorithms. If we don't understand that, we don't understand how brains make decisions.</p>\n<hr />\n<p>I hope that the considerations above are enough to convince reductionists that we should develop a parallel decision theory if we&nbsp; want to reduce decision making to computing. I would like to add an axiomatic parallel decision theory to the LW arsenal, but I know that that is not a one man/woman job. So, if you think you might be of help in that endeavor, and are willing to devote yourself to some degree, please contact me at hastwoarms@gmail.com. Any team we assemble will likely not meet in person often, and will hopefully frequently meet on some private forum. We will need decision theorists, general mathematicians, people intimately familiar with the modular theory of mind, and people familiar with neural modeling. What follows are some suggestions for any team or individual that might pursue that goal independently:</p>\n<ul>\n<li>The specifics of the managing algorithm used in brains are mostly unknown. As such, any parallel decision theory should be built to handle as diverse a range of managing algorithms as possible.</li>\n<li>No composite agent should have any property that is not reducible to the interactions of the agents it is <em>made</em> out of. If you have a complete description of the subagents, and a complete description of the managing algorithm, you have a complete description of the smart agent. </li>\n<li>There is nothing wrong with treating the <em>lowest level </em>of CSAs as black boxes. The specifics of the non-CSA algorithms, which the lowest level CSAs are made out of are not relevant to parallel decision theory.&nbsp;</li>\n<li>Make sure that the theory can handle each subagent having its own unique set of possible outcomes, and its own unique method of representing those outcomes. </li>\n<li>Make sure that each CSA above the lowest level actually has \"could\", \"should\", and \"would\" labels on the nodes in its problem space, and make sure that those labels, their values, and the problem space itself can be reduced to the managing of the CSAs on the level below. </li>\n<li>Each level above the lowest should have CSAs dealing with more a more diverse range of problems than the ones on the level bellow. The lowest level should have the most specialized CSAs. </li>\n<li>If you've achieved the six goals above, try comparing your parallel decision theory to other decision theories; see how much predictive accuracy is gained by using a parallel decision theory instead of the classical theories. </li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yKKP3EgvGAAPwTmTs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 4, "extendedScore": null, "score": 7.518666719751586e-07, "legacy": true, "legacyId": "9089", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WnjGhcRb2c6CabK5d", "3buXtNiSK8gcRLMSG", "gxxpK3eiSQ3XG3DW7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-08T02:10:51.711Z", "modifiedAt": null, "url": null, "title": "Career choice for a utilitarian giver", "slug": "career-choice-for-a-utilitarian-giver", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:03.547Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "juliawise", "createdAt": "2011-07-18T13:52:30.717Z", "isAdmin": false, "displayName": "juliawise"}, "userId": "JtChJYGsjzgAh5Ag8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hFR59Wc9NwWWppJ7R/career-choice-for-a-utilitarian-giver", "pageUrlRelative": "/posts/hFR59Wc9NwWWppJ7R/career-choice-for-a-utilitarian-giver", "linkUrl": "https://www.lesswrong.com/posts/hFR59Wc9NwWWppJ7R/career-choice-for-a-utilitarian-giver", "postedAtFormatted": "Monday, August 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Career%20choice%20for%20a%20utilitarian%20giver&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACareer%20choice%20for%20a%20utilitarian%20giver%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhFR59Wc9NwWWppJ7R%2Fcareer-choice-for-a-utilitarian-giver%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Career%20choice%20for%20a%20utilitarian%20giver%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhFR59Wc9NwWWppJ7R%2Fcareer-choice-for-a-utilitarian-giver", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhFR59Wc9NwWWppJ7R%2Fcareer-choice-for-a-utilitarian-giver", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 817, "htmlBody": "<p class=\"MsoNormal\">I&rsquo;m a utilitarian contemplating a career change.<span>&nbsp; </span>I currently give all my income to international development (which is possible because my husband supports us both financially).<span>&nbsp; </span>I don&rsquo;t have any special gift for science, etc. that would help save the world, so I think donations are the best way I can help.</p>\n<p class=\"MsoNormal\">I&rsquo;m 26 and halfway through social work school.<span>&nbsp; </span>I enjoy social work and am reasonably good at it, but the most I&rsquo;ll ever earn is probably $80K/year.<span>&nbsp; </span>I&rsquo;m now thinking more about the moral imperative to earn more and thus give more.</p>\n<p class=\"MsoNormal\">Most high-earning careers are not ones I think I would enjoy.<span>&nbsp; </span>That means I would be fighting burnout for the rest of my career.<span>&nbsp; (I'm open to suggestions if you think otherwise.)&nbsp; </span>The exception is psychiatry, which I do think I would enjoy and be moderately good at.<span>&nbsp; </span>But I would need about nine years of school and residency to become a psychiatrist.</p>\n<p class=\"MsoNormal\">If I go to medical school and become an average psychiatrist, I&rsquo;d double my expected lifetime earnings compared to social work (even after paying for school).<span>&nbsp; </span>I could give about 2 million dollars more, which GiveWell thinks turns into about 2,500 lives saved.<span>&nbsp; </span>No amount of inconvenience on my part compares with that many lives.</p>\n<p class=\"MsoNormal\">So what I want to do is figure out whether I could be productive as a psychiatrist or some other profession, or whether there&rsquo;s a good reason I should stay on my current course.</p>\n<p class=\"MsoNormal\"><strong>Some considerations:</strong></p>\n<p class=\"MsoNormal\">I&rsquo;m fairly smart but not competitive-natured.<span>&nbsp; </span>I think this would make me bad at a lot of careers that pay well but don&rsquo;t require extra school, because there&rsquo;s more competition for those jobs.</p>\n<p class=\"MsoNormal\">I&rsquo;m not sure about my academic capabilities.<span>&nbsp; </span>I haven&rsquo;t taken a real science course since high school.<span>&nbsp; </span>It&rsquo;s also been a long time since I had to do the kind of rote memorization that I believe is needed in law or medical school.<span>&nbsp; </span>I&rsquo;m worried that I would get into one of these and then find I wasn&rsquo;t up to the work.</p>\n<p class=\"MsoNormal\">I have no interest in chemistry.<span>&nbsp; </span>Also, I don&rsquo;t do well when sleep-deprived.<span>&nbsp; </span>Both of these might make me a terrible med student.</p>\n<p class=\"MsoNormal\">I&rsquo;ve had bouts of depression in the past, but never ones that crippled my ability to study/work.<span>&nbsp; </span>If I were busier, they might cripple me more.</p>\n<p class=\"MsoNormal\">I would need at least a year of postbac science classes before I could go to medical school.<span>&nbsp; </span>This would bring the time to become a psychiatrist to nine years, plus at least a year to apply.<span>&nbsp; </span>That seems like forever, though I know when I&rsquo;m older it won&rsquo;t seem as long as it does now.</p>\n<p class=\"MsoNormal\">Investing that time in more school has an opportunity cost.<span>&nbsp; </span>If I stick with social work, I could start donating again in one year.<span>&nbsp; </span>If I become a psychiatrist, it would be more like twelve years before I could donate again.<span>&nbsp; </span>I don&rsquo;t know what effect that delay would have.<span>&nbsp; </span>Psychiatry earnings would overtake social work earnings about 18 years from now.</p>\n<p class=\"MsoNormal\">I know I should count my useless undergraduate major and one year of social work school as sunk costs.<span>&nbsp; </span>But adding a lot more school on top of the eighteen years I&rsquo;ve already done feels exhausting, and I think I&rsquo;m more likely to fail now than I would have been if I&rsquo;d started planning earlier.</p>\n<p class=\"MsoNormal\">Medical school would mean nine years of giving up many of the things I enjoy &ndash; spending time with my husband, cooking, gardening, reading.<span>&nbsp; </span>This gives me an incentive to burn out, because it would mean I could do those things again.</p>\n<p class=\"MsoNormal\">I&rsquo;m married.<span>&nbsp; </span>I don&rsquo;t want to believe it applies to us, but statistically, me going to medical school would increase our risk of divorce.<span>&nbsp; </span><a href=\"http://www.scienceblog.com/community/older/1997/A/199700590.html\">This study</a> says 51% of married psychiatry students divorce during or after medical school (about double our current statistical risk).<span>&nbsp; </span>I don&rsquo;t think my marriage is more important than 2,500 people&rsquo;s lives. But I do think seeing it die would make me much worse at school.<span>&nbsp; </span>Even if we didn&rsquo;t actually divorce, I would expect our relationship to be significantly stressed because I would be gone or busy so much of the time.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\">If I quit or fail out of medical school, I&rsquo;ve wasted a lot of time and money.</p>\n<p class=\"MsoNormal\">If my coworkers are high earners, convincing any of them to donate effectively would have a larger impact than convincing social workers to do the same.<span>&nbsp; </span>However, I&rsquo;ve had zero luck persuading anyone I know (except my husband), so this may be irrelevant.</p>\n<p class=\"MsoNormal\"><strong>The questions</strong></p>\n<p class=\"MsoNormal\">Do you have advice on powering through an unpleasant experience for a good cause?<span>&nbsp; </span>Is nine years too long to power through?<span>&nbsp; </span>Are there other careers I should be considering?</p>\n<p class=\"MsoNormal\"><em>Update, May 2012: I decided not to try medical school, because I thought I would hate it. &nbsp;I finished social work school and am looking for jobs in psychiatric social work, which I was doing this last year and really enjoyed.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4kQXps8dYsKJgaayN": 1, "qAvbtzdG2A2RBn7in": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hFR59Wc9NwWWppJ7R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 38, "extendedScore": null, "score": 8.2e-05, "legacy": true, "legacyId": "9100", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"MsoNormal\">I\u2019m a utilitarian contemplating a career change.<span>&nbsp; </span>I currently give all my income to international development (which is possible because my husband supports us both financially).<span>&nbsp; </span>I don\u2019t have any special gift for science, etc. that would help save the world, so I think donations are the best way I can help.</p>\n<p class=\"MsoNormal\">I\u2019m 26 and halfway through social work school.<span>&nbsp; </span>I enjoy social work and am reasonably good at it, but the most I\u2019ll ever earn is probably $80K/year.<span>&nbsp; </span>I\u2019m now thinking more about the moral imperative to earn more and thus give more.</p>\n<p class=\"MsoNormal\">Most high-earning careers are not ones I think I would enjoy.<span>&nbsp; </span>That means I would be fighting burnout for the rest of my career.<span>&nbsp; (I'm open to suggestions if you think otherwise.)&nbsp; </span>The exception is psychiatry, which I do think I would enjoy and be moderately good at.<span>&nbsp; </span>But I would need about nine years of school and residency to become a psychiatrist.</p>\n<p class=\"MsoNormal\">If I go to medical school and become an average psychiatrist, I\u2019d double my expected lifetime earnings compared to social work (even after paying for school).<span>&nbsp; </span>I could give about 2 million dollars more, which GiveWell thinks turns into about 2,500 lives saved.<span>&nbsp; </span>No amount of inconvenience on my part compares with that many lives.</p>\n<p class=\"MsoNormal\">So what I want to do is figure out whether I could be productive as a psychiatrist or some other profession, or whether there\u2019s a good reason I should stay on my current course.</p>\n<p class=\"MsoNormal\"><strong id=\"Some_considerations_\">Some considerations:</strong></p>\n<p class=\"MsoNormal\">I\u2019m fairly smart but not competitive-natured.<span>&nbsp; </span>I think this would make me bad at a lot of careers that pay well but don\u2019t require extra school, because there\u2019s more competition for those jobs.</p>\n<p class=\"MsoNormal\">I\u2019m not sure about my academic capabilities.<span>&nbsp; </span>I haven\u2019t taken a real science course since high school.<span>&nbsp; </span>It\u2019s also been a long time since I had to do the kind of rote memorization that I believe is needed in law or medical school.<span>&nbsp; </span>I\u2019m worried that I would get into one of these and then find I wasn\u2019t up to the work.</p>\n<p class=\"MsoNormal\">I have no interest in chemistry.<span>&nbsp; </span>Also, I don\u2019t do well when sleep-deprived.<span>&nbsp; </span>Both of these might make me a terrible med student.</p>\n<p class=\"MsoNormal\">I\u2019ve had bouts of depression in the past, but never ones that crippled my ability to study/work.<span>&nbsp; </span>If I were busier, they might cripple me more.</p>\n<p class=\"MsoNormal\">I would need at least a year of postbac science classes before I could go to medical school.<span>&nbsp; </span>This would bring the time to become a psychiatrist to nine years, plus at least a year to apply.<span>&nbsp; </span>That seems like forever, though I know when I\u2019m older it won\u2019t seem as long as it does now.</p>\n<p class=\"MsoNormal\">Investing that time in more school has an opportunity cost.<span>&nbsp; </span>If I stick with social work, I could start donating again in one year.<span>&nbsp; </span>If I become a psychiatrist, it would be more like twelve years before I could donate again.<span>&nbsp; </span>I don\u2019t know what effect that delay would have.<span>&nbsp; </span>Psychiatry earnings would overtake social work earnings about 18 years from now.</p>\n<p class=\"MsoNormal\">I know I should count my useless undergraduate major and one year of social work school as sunk costs.<span>&nbsp; </span>But adding a lot more school on top of the eighteen years I\u2019ve already done feels exhausting, and I think I\u2019m more likely to fail now than I would have been if I\u2019d started planning earlier.</p>\n<p class=\"MsoNormal\">Medical school would mean nine years of giving up many of the things I enjoy \u2013 spending time with my husband, cooking, gardening, reading.<span>&nbsp; </span>This gives me an incentive to burn out, because it would mean I could do those things again.</p>\n<p class=\"MsoNormal\">I\u2019m married.<span>&nbsp; </span>I don\u2019t want to believe it applies to us, but statistically, me going to medical school would increase our risk of divorce.<span>&nbsp; </span><a href=\"http://www.scienceblog.com/community/older/1997/A/199700590.html\">This study</a> says 51% of married psychiatry students divorce during or after medical school (about double our current statistical risk).<span>&nbsp; </span>I don\u2019t think my marriage is more important than 2,500 people\u2019s lives. But I do think seeing it die would make me much worse at school.<span>&nbsp; </span>Even if we didn\u2019t actually divorce, I would expect our relationship to be significantly stressed because I would be gone or busy so much of the time.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\">If I quit or fail out of medical school, I\u2019ve wasted a lot of time and money.</p>\n<p class=\"MsoNormal\">If my coworkers are high earners, convincing any of them to donate effectively would have a larger impact than convincing social workers to do the same.<span>&nbsp; </span>However, I\u2019ve had zero luck persuading anyone I know (except my husband), so this may be irrelevant.</p>\n<p class=\"MsoNormal\"><strong id=\"The_questions\">The questions</strong></p>\n<p class=\"MsoNormal\">Do you have advice on powering through an unpleasant experience for a good cause?<span>&nbsp; </span>Is nine years too long to power through?<span>&nbsp; </span>Are there other careers I should be considering?</p>\n<p class=\"MsoNormal\"><em>Update, May 2012: I decided not to try medical school, because I thought I would hate it. &nbsp;I finished social work school and am looking for jobs in psychiatric social work, which I was doing this last year and really enjoyed.</em></p>", "sections": [{"title": "Some considerations:", "anchor": "Some_considerations_", "level": 1}, {"title": "The questions", "anchor": "The_questions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "29 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-08T03:09:47.291Z", "modifiedAt": null, "url": null, "title": "Meetup : Orange County Atheist Meetup Wednesday August 10", "slug": "meetup-orange-county-atheist-meetup-wednesday-august-10", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:01.034Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JGWeissman", "createdAt": "2009-04-01T04:43:56.740Z", "isAdmin": false, "displayName": "JGWeissman"}, "userId": "Mw8rsM7m7E8nnEFEp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mJdeQJNSbh2SpQXCd/meetup-orange-county-atheist-meetup-wednesday-august-10", "pageUrlRelative": "/posts/mJdeQJNSbh2SpQXCd/meetup-orange-county-atheist-meetup-wednesday-august-10", "linkUrl": "https://www.lesswrong.com/posts/mJdeQJNSbh2SpQXCd/meetup-orange-county-atheist-meetup-wednesday-august-10", "postedAtFormatted": "Monday, August 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Orange%20County%20Atheist%20Meetup%20Wednesday%20August%2010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Orange%20County%20Atheist%20Meetup%20Wednesday%20August%2010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJdeQJNSbh2SpQXCd%2Fmeetup-orange-county-atheist-meetup-wednesday-august-10%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Orange%20County%20Atheist%20Meetup%20Wednesday%20August%2010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJdeQJNSbh2SpQXCd%2Fmeetup-orange-county-atheist-meetup-wednesday-august-10", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJdeQJNSbh2SpQXCd%2Fmeetup-orange-county-atheist-meetup-wednesday-august-10", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 144, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/25'>Orange County Atheist Meetup Wednesday August 10</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 August 2011 07:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">18542 MacArthur Blvd Irvine, CA 92612</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week we are going to do something a little different for the weekly Irvine meetup. There is an atheist group in Orange County that has monthly meetups, so let's join them for their <a href=\"http://www.ocatheists.com/archives/000112.shtml\" rel=\"nofollow\">August Meetup</a>. I attended the meetup last month, and found that several of the people there were interested in such topics as probability theory, heuristics and biases, and Singularity issues.</p>\n\n<p>The main event starts at 7:30 at the upstairs table at the IHOP. Some members also show up for drinks and appetizers at the El Torito across the street at 6:30. They would appreciate if you RSVP in the comments of the linked announcement.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/25'>Orange County Atheist Meetup Wednesday August 10</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mJdeQJNSbh2SpQXCd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 7.51933221161305e-07, "legacy": true, "legacyId": "9103", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Orange_County_Atheist_Meetup_Wednesday_August_10\">Discussion article for the meetup : <a href=\"/meetups/25\">Orange County Atheist Meetup Wednesday August 10</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 August 2011 07:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">18542 MacArthur Blvd Irvine, CA 92612</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week we are going to do something a little different for the weekly Irvine meetup. There is an atheist group in Orange County that has monthly meetups, so let's join them for their <a href=\"http://www.ocatheists.com/archives/000112.shtml\" rel=\"nofollow\">August Meetup</a>. I attended the meetup last month, and found that several of the people there were interested in such topics as probability theory, heuristics and biases, and Singularity issues.</p>\n\n<p>The main event starts at 7:30 at the upstairs table at the IHOP. Some members also show up for drinks and appetizers at the El Torito across the street at 6:30. They would appreciate if you RSVP in the comments of the linked announcement.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Orange_County_Atheist_Meetup_Wednesday_August_101\">Discussion article for the meetup : <a href=\"/meetups/25\">Orange County Atheist Meetup Wednesday August 10</a></h2>", "sections": [{"title": "Discussion article for the meetup : Orange County Atheist Meetup Wednesday August 10", "anchor": "Discussion_article_for_the_meetup___Orange_County_Atheist_Meetup_Wednesday_August_10", "level": 1}, {"title": "Discussion article for the meetup : Orange County Atheist Meetup Wednesday August 10", "anchor": "Discussion_article_for_the_meetup___Orange_County_Atheist_Meetup_Wednesday_August_101", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-08T04:02:39.234Z", "modifiedAt": null, "url": null, "title": "Rationality and Relationships", "slug": "rationality-and-relationships", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:32.405Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vejHgqSafKoKX6SD2/rationality-and-relationships", "pageUrlRelative": "/posts/vejHgqSafKoKX6SD2/rationality-and-relationships", "linkUrl": "https://www.lesswrong.com/posts/vejHgqSafKoKX6SD2/rationality-and-relationships", "postedAtFormatted": "Monday, August 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20and%20Relationships&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20and%20Relationships%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvejHgqSafKoKX6SD2%2Frationality-and-relationships%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20and%20Relationships%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvejHgqSafKoKX6SD2%2Frationality-and-relationships", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvejHgqSafKoKX6SD2%2Frationality-and-relationships", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 394, "htmlBody": "<p>Recently I asked for feedback on two versions of a new post, 'Rationality Lessons from Romance'. '<a href=\"/lw/6v5/new_post_version_2_please_read_this_only_if_your/\">Version 2</a>' was my original draft. '<a href=\"/lw/6pf/new_post_version_1_please_read_this_only_if_your/\">Version 1</a>' was a more recent draft edited in response to comments from a collaborator. We wanted to test whether my collaborator's comments genuinely improved the post. Version 2 now sits at 1 upvote and version 1 now sits at 16 upvotes, and I take this to be some evidence in favor of the hypothesis that my collaborator's comments improved the post.</p>\n<p>My thanks to those who participated in that experiment; we got the information we wanted!</p>\n<p>Thanks also to those of you who provided feedback on either version of the post. Most comments were critical, but this is often the case even with massively upvoted posts like <a href=\"/lw/58m/build_small_skills_in_the_right_order/\">Build Small Skills in the Right Order</a>. My hope is that relationships posts on Less Wrong merely need to be better and more sensitive to a wide variety of&nbsp;sensitivities than my first attempt was, not that sex and relationships are inherently&nbsp;<a href=\"/lw/gw/politics_is_the_mindkiller/\">mind-killing</a> topics. There is an amazing interplay between rationality and relationships, and I feel it would be a shame to leave them unexplored.</p>\n<p>So I'm trying to learn how LessWrongers can discuss relationships productively.</p>\n<p>One difficulty in extracting lessons from the feedback on 'Rationality Lessons from Romance' is that different people had different complaints.&nbsp;Some were '<a href=\"/lw/6v5/new_post_version_2_please_read_this_only_if_your/4kdu\">seriously skeeved-out</a>' by the overtly personal nature of the post, even though earlier posts of a similar personal nature <a href=\"http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age\">have</a> <a href=\"/lw/50p/reflections_on_rationality_a_year_out/\">fared</a> <a href=\"/lw/5dj/the_benefits_of_madness_a_positive_account_of/\">well</a> <a href=\"/lw/20l/ureshiku_naritai/\">on</a> Less Wrong, and even though <a href=\"/lw/6v5/new_post_version_2_please_read_this_only_if_your/4kia\">others</a> <a href=\"/lw/6v5/new_post_version_2_please_read_this_only_if_your/4kex\">didn't</a> mind the personal-ness of the post.&nbsp;<a href=\"/lw/6v5/new_post_version_2_please_read_this_only_if_your/4kdu\">Some</a> didn't like the promotion of polyamory, <a href=\"/lw/6v5/new_post_version_2_please_read_this_only_if_your/4kia\">others</a> didn't mind. Some thought my interactions with women were <a href=\"/lw/6v5/new_post_version_2_please_read_this_only_if_your/4kef\">unethical</a>, <a href=\"/lw/6v5/new_post_version_2_please_read_this_only_if_your/4kg7\">others</a> <a href=\"/lw/6v5/new_post_version_2_please_read_this_only_if_your/4kia\">didn't</a>. Some <a href=\"/lw/6v5/new_post_version_2_please_read_this_only_if_your/4kla\">disagreed</a> with a premise of the post, that sexual jealousy and monogamy are suboptimal for some people. Some felt the brief lessons pulled out from the stories were <a href=\"/lw/6v5/new_post_version_2_please_read_this_only_if_your/4luv\">effective</a>, others <a href=\"/lw/6v5/new_post_version_2_please_read_this_only_if_your/4kdv\">didn't</a>. <a href=\"/lw/6pf/new_post_version_1_please_read_this_only_if_your/4khe\">Some</a> thought I was, like Socrates, being skewered for looking at things with too much clarity, <a href=\"/lw/6pf/new_post_version_1_please_read_this_only_if_your/4ke6\">others</a> thought my post was weird and confusing.</p>\n<p>I may try to rewrite 'Rationality Lessons from Romance', incorporating as much of the feedback as I can. In the meantime, I'd like to ask the Less Wrong community to very different questions:</p>\n<p><ol>\n<li>What standard or non-standard rationality lessons can you draw from your own journeys in romance?</li>\n<li>How have your intimate relationships been affected by your implementation of rationality skills?</li>\n</ol></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vejHgqSafKoKX6SD2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 18, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "9106", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aYfjK9oHuTcYqwBvp", "CMSaT7sv96C4jWN2J", "qwdupkFd6kmeZHYXy", "9weLK2AJ9JEt2Tt8f", "SFG9Cm7mf5eP4juKs", "zPJE7MDtL25RpN7Cc", "xnPFYBuaGhpq869mY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-08T05:00:22.904Z", "modifiedAt": null, "url": null, "title": "Anki Library", "slug": "anki-library", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:01.292Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/32n7bCLDzFrtErMoN/anki-library", "pageUrlRelative": "/posts/32n7bCLDzFrtErMoN/anki-library", "linkUrl": "https://www.lesswrong.com/posts/32n7bCLDzFrtErMoN/anki-library", "postedAtFormatted": "Monday, August 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anki%20Library&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnki%20Library%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F32n7bCLDzFrtErMoN%2Fanki-library%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anki%20Library%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F32n7bCLDzFrtErMoN%2Fanki-library", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F32n7bCLDzFrtErMoN%2Fanki-library", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 92, "htmlBody": "<p><span style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px;\">I have a few anki decks that I like, and am making more based on books I'm reading (currently doing one on Good Calories, Bad Calories). They don't replace reading the book, could probably supplement a summary, but definitely helps recall on specific details (at least for me).<br /><br />Other people have some anki decks that I'm interested in sharing.<br /><br />Would people be interested in setting up a shared anki library? Or at least a list of decks so that people can find them in anki's search?</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "32n7bCLDzFrtErMoN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "9107", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-08T09:55:28.151Z", "modifiedAt": null, "url": null, "title": "Part 1 On What is a Self Discussion", "slug": "part-1-on-what-is-a-self-discussion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xF79z2ZfP34GNKHyC/part-1-on-what-is-a-self-discussion", "pageUrlRelative": "/posts/xF79z2ZfP34GNKHyC/part-1-on-what-is-a-self-discussion", "linkUrl": "https://www.lesswrong.com/posts/xF79z2ZfP34GNKHyC/part-1-on-what-is-a-self-discussion", "postedAtFormatted": "Monday, August 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Part%201%20On%20What%20is%20a%20Self%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APart%201%20On%20What%20is%20a%20Self%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxF79z2ZfP34GNKHyC%2Fpart-1-on-what-is-a-self-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Part%201%20On%20What%20is%20a%20Self%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxF79z2ZfP34GNKHyC%2Fpart-1-on-what-is-a-self-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxF79z2ZfP34GNKHyC%2Fpart-1-on-what-is-a-self-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1554, "htmlBody": "<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>In <a title=\"Nonperson Predicates\" href=\"/lw/x4/nonperson_predicates/\">Nonperson Predicates</a> Eliezer said:</p>\n<blockquote>\n<p>\"Build an AI?&nbsp; Sure!&nbsp; Make it Friendly?&nbsp; Now that you point it out,  sure!&nbsp; But trying to come up with a \"nonperson predicate\"?&nbsp; That's just  way above the difficulty level they signed up to handle.</p>\n<p>But a longtime <em>Overcoming Bias</em> reader will be aware that <a href=\"http://www.overcomingbias.com/2007/08/mysterious-answ.html\">a blank map does not correspond to a blank territory</a>.&nbsp; That <a href=\"http://www.overcomingbias.com/2008/03/wrong-questions.html\">impossible confusing questions correspond to places where your own thoughts are tangled</a>, not to places where the environment itself contains magic.&nbsp; That <a href=\"http://www.overcomingbias.com/2008/05/einsteins-super.html\">even difficult problems do not require an aura of destiny to solve</a>.&nbsp; And that the first step to solving one is <a href=\"http://www.overcomingbias.com/2008/10/try-persevere.html\">not running away from the problem like a frightened rabbit</a>, but instead sticking long enough to learn something.</p>\n<p>So I am not running away from this problem myself.\"</p>\n</blockquote>\n<p>Me neither. When entering the non-existent gates of bayesian Heaven, I don't want to have to admit that I have located a sufficiently small problem in problem-space that seems solvable and&nbsp; that unsolved constitutes an <a title=\"Bostrom 2011 Paper\" href=\"http://www.existentialrisk.com/concept.html\">existential risk</a>, that was not being tackled by anyone I met in the Singularity Institute, and I just ran away from it.</p>\n<p>So, would you mind helping me? In the course of writing my CEV text, I noticed that discussing what are people/selves was a necessary previous step. I've written the first part of that text, and would like to know what is excessive/unclear/improvable/vague.</p>\n<h2><span id=\"internal-source-marker_0.07708366916928255\" style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">On What Is a Self</span></h2>\n<p><br /><br /><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Selves and Persons</span><br /><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">In  the eight movement of your weekly chess game you do what feels same as  always: Reflect for a few seconds on the many layers of structure  underlying the current game-state, specially regarding changes from your  opponent&rsquo;s last move. It seems reasonable to eat his pawn with your  bishop. After moving you look at him and see the sequence of  expressions: Doubt &ldquo;Why did he do that?&rdquo;, distrust &ldquo;He must be seeing  something I don&rsquo;t&rdquo;, inquiry &ldquo;Let me double check this&rdquo;, Schadenfreud  &ldquo;No, he actually failed&rdquo; and finally joy &ldquo;Piece of cake, I&rsquo;ll win&rdquo;. He  takes your bishop with a horse that from your perspective could only be  coming from neverland. Still stunned, you resign. It is the second time  in a row you lose the game due to a simple mistake. The excuse bursts  naturally out of your mouth: &ldquo;I&rsquo;m not myself today&rdquo;</span><br /><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">The functional role (with plausibly evolutionary reasons) of this use of the concept of Self is easy to unscramble. </span><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">1) Do not hold your model of me as responsible for these mistakes</span><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">2)  &nbsp;Either (a) I sense something strange about the inner machinery of my  mind, the algorithm feels different from the inside. Or (b) at least my  now visible mistakes are realiable evidence of a difference which I  detected in hindsight. </span><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">3)  If there is a person watching this game, notice how my signaling and my  friend&rsquo;s not contesting it is reliable evidence I normally play chess  better than this</span><br /><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">A  few minutes later, you see your friend yelling histerically at someone  in the phone, you explain to the girl who was watching: &ldquo;He is not that  kind of person&rdquo;</span><br /><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Here  we have a situation where the analogous of 1 and 3 work, but there is  no way for you to tell what the algorithm feels from the inside. You  still know in hindsight that your friend doesn&rsquo;t usually yell like that.  Though 1, 2, and 3 still hold, 2(a) is not the case anymore. </span><br /><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">I  suggest the property of 2(a) that blocks interchangeability of the  concepts of Self and Person is &ldquo;having first person epistemic  information about X&rdquo;. Selves have that, people don&rsquo;t. We use the term  &lsquo;person&rsquo; when we want to talk only about the epistemically  intersubjective properties of someone. Self is reserved for a person&rsquo;s  perspective of herself, including, for instance, &nbsp;indexical facts. </span><br /><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Other  than that, Self and Person seem to be interchangeable concepts. This  generalization is useful because that means most of the problem of  personhood and selfhood can be collapsed into one thing. </span><br /><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Unfortunately, the Self/Person intersection is a concept that is itself a Mongrel Concept, so it has again to be split apart. </span><br /><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Mongrel and Cluster Concepts </span><br /></p>\n<p style=\"text-indent: 36pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">When  a concept seems to defy easy explanability, there are two interesting  possibilities of how to interact with it. The first would be to &nbsp;assume  that the disparate uses of the term &lsquo;Self&rsquo;&rsquo; in ordinary language and  science can be captured by a unique, all-encompassing notion of Self.  The second is to assume that different uses of &lsquo;Self&rsquo;&rsquo; reveal a  plurality of notions of Selfhood, each in need of a separate account. I  will endorse this second assumption: Self is a mongrel</span></p>\n<p><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">concept  in need of disambiguation. (to strenghten the analogy power of thinking  about mongrels, it may help to know that Information, Consciousness and  Health are thought to be mongrel concepts as well)</span><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;  Without using specific tags for the time being, let us assume that  there will be 4 kinds of Self, 1,2,3, and 4. To say that Self is a  concept that sometimes maps into 1, sometimes into 3 and so on is not to  exaustivelly frame the concept usage. That is because 1 and 2  themselves may be cluster concepts. </span><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;  The cluster concept shape is one of the most common shapes of concepts  in our mental vocabulary. Concepts are associational structures. Most of  the times, instead of drawing a clear line around a set in the world  inside of which all X fits, and outside of which none does, concepts  present a cluster like structure with nearly all core area members  belonging and nearly none in the far fetched radius belonging. &nbsp;Not all  of their typical features are logically necessary. &nbsp;The recognition of  features produces an activation, the strength of which depends not only  on the degree to which the feature is present but a weighting factor.  &nbsp;When the sum of the activations crosses a threshold, the concept  becomes active and the stimulus is said to belong to that category.</span><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;  Selves are mongrel concepts composed of different conceptual  intuitions, each of which is itself a cluster concept, thus Selves are  part of the most elusive, abstract, high-level entities entertained by  minds. Whereas this may be aesthetically pleasant, presenting us as  considerably complex entities, it is also a great ethical burden, for it  leaves the domain of ethics, highly dependant on the concepts of  Selfhood and Personhood, with a scattered slippery ground-level notion  from which to create the building blocks of ethical theories. </span><br /><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;  Several analogies have been used to convey the concept of Cluster  Concept, these convey images of star clusters, neural networks lighting  up, and sets of properties with a majority vote. A particularly well  known analogy used by Wittgenstein is the game analogy, in which  language games determine prescribe normative meanings which constrict a  word&rsquo;s meaning, without determining a clear cut case. Wittgenstein  defended that there was no clear set of necessary conditions that  determine what a game is. Bernard Suits came up with a refutation of  that claim, stating that there is such a definition (modified from &ldquo;What  is a game&rdquo; 1967, </span><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Philosophy of Science</span><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\"> Vol. 34, No. 2 [Jun., 1967], pp. 148-156):</span><br /></p>\n<blockquote>\n<p style=\"margin-left: 31.5pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\"> \"To play a game is to engage in activity designed to bring about a  specific state of affairs, using only means permitted by specific rules,  where the means permitted by the rules are more limited in scope than  they would be in the absence of such rules, and where the sole reason  for accepting the rules is to make possible such activity.\" </span></p>\n</blockquote>\n<p><br /><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">&nbsp;&nbsp;&nbsp; Can we hope for a similar soon to be found understanding of Self? Let us invoke: </span><br /></p>\n<p style=\"margin-left: 22.5pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">The Hidden Variable Hypothesis</span><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">:  There is a core essence which determines the class of selves from  non-selves, it is just not yet within our current state-of-knowledge  reach. &nbsp;</span></p>\n<p><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;  While desirable, there are various resons to be skeptical of The Hidden  Variable Hyphotesis: (1) Any plausible candidate core would have to be  able to disentangle selves from Organisms in general, Superorganisms  (i.e. insect societies) and institutions (2) We clearly entertain  different models of what selves are for different purposes, as shown  below in Section Varieties of Self-Systems Worth Having. (3) Design  consideration: Being evolved structures which encompass several  resources of a recently evolved mind, that came to being through a  complex dual-inheritance evolution of several hundred thousand  replicators belonging to two kinds (genes and memes), Selves are among  the most complex structures known and thus unlikely to possess a core  essence, due to causal design considerations independant of how  untractable it would be to detect and describe this essence. &nbsp;</span></p>\n<p style=\"text-indent: 36pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">From  now on then, I will be assuming as common ground that Selves are  Mongrel concepts, comprised of some yet indiscussed number of Cluster  Concepts. </span></p>\n<p>Not Yet Written Following Topics:<strong><br /></strong></p>\n<p><strong><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Organisms, Superorganisms, and Selves</span><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Selves and Sorites</span><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Selves Beyond Sorites</span><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Persons, the Evidence of Other Selfs</span><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Selves as Utility Increaser Unnatural Clusters</span><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">What do We Demand of Selves</span><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Varities of Self-Systems Worth Having</span><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Drescher: Personhood Is An Ethical Predicate</span><br /><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">What Matters About Selves?</span></strong></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xF79z2ZfP34GNKHyC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 2, "extendedScore": null, "score": 7.520619245774449e-07, "legacy": true, "legacyId": "9112", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>In <a title=\"Nonperson Predicates\" href=\"/lw/x4/nonperson_predicates/\">Nonperson Predicates</a> Eliezer said:</p>\n<blockquote>\n<p>\"Build an AI?&nbsp; Sure!&nbsp; Make it Friendly?&nbsp; Now that you point it out,  sure!&nbsp; But trying to come up with a \"nonperson predicate\"?&nbsp; That's just  way above the difficulty level they signed up to handle.</p>\n<p>But a longtime <em>Overcoming Bias</em> reader will be aware that <a href=\"http://www.overcomingbias.com/2007/08/mysterious-answ.html\">a blank map does not correspond to a blank territory</a>.&nbsp; That <a href=\"http://www.overcomingbias.com/2008/03/wrong-questions.html\">impossible confusing questions correspond to places where your own thoughts are tangled</a>, not to places where the environment itself contains magic.&nbsp; That <a href=\"http://www.overcomingbias.com/2008/05/einsteins-super.html\">even difficult problems do not require an aura of destiny to solve</a>.&nbsp; And that the first step to solving one is <a href=\"http://www.overcomingbias.com/2008/10/try-persevere.html\">not running away from the problem like a frightened rabbit</a>, but instead sticking long enough to learn something.</p>\n<p>So I am not running away from this problem myself.\"</p>\n</blockquote>\n<p>Me neither. When entering the non-existent gates of bayesian Heaven, I don't want to have to admit that I have located a sufficiently small problem in problem-space that seems solvable and&nbsp; that unsolved constitutes an <a title=\"Bostrom 2011 Paper\" href=\"http://www.existentialrisk.com/concept.html\">existential risk</a>, that was not being tackled by anyone I met in the Singularity Institute, and I just ran away from it.</p>\n<p>So, would you mind helping me? In the course of writing my CEV text, I noticed that discussing what are people/selves was a necessary previous step. I've written the first part of that text, and would like to know what is excessive/unclear/improvable/vague.</p>\n<h2 id=\"On_What_Is_a_Self\"><span id=\"internal-source-marker_0.07708366916928255\" style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">On What Is a Self</span></h2>\n<p><br><br><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Selves and Persons</span><br><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">In  the eight movement of your weekly chess game you do what feels same as  always: Reflect for a few seconds on the many layers of structure  underlying the current game-state, specially regarding changes from your  opponent\u2019s last move. It seems reasonable to eat his pawn with your  bishop. After moving you look at him and see the sequence of  expressions: Doubt \u201cWhy did he do that?\u201d, distrust \u201cHe must be seeing  something I don\u2019t\u201d, inquiry \u201cLet me double check this\u201d, Schadenfreud  \u201cNo, he actually failed\u201d and finally joy \u201cPiece of cake, I\u2019ll win\u201d. He  takes your bishop with a horse that from your perspective could only be  coming from neverland. Still stunned, you resign. It is the second time  in a row you lose the game due to a simple mistake. The excuse bursts  naturally out of your mouth: \u201cI\u2019m not myself today\u201d</span><br><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">The functional role (with plausibly evolutionary reasons) of this use of the concept of Self is easy to unscramble. </span><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">1) Do not hold your model of me as responsible for these mistakes</span><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">2)  &nbsp;Either (a) I sense something strange about the inner machinery of my  mind, the algorithm feels different from the inside. Or (b) at least my  now visible mistakes are realiable evidence of a difference which I  detected in hindsight. </span><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">3)  If there is a person watching this game, notice how my signaling and my  friend\u2019s not contesting it is reliable evidence I normally play chess  better than this</span><br><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">A  few minutes later, you see your friend yelling histerically at someone  in the phone, you explain to the girl who was watching: \u201cHe is not that  kind of person\u201d</span><br><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Here  we have a situation where the analogous of 1 and 3 work, but there is  no way for you to tell what the algorithm feels from the inside. You  still know in hindsight that your friend doesn\u2019t usually yell like that.  Though 1, 2, and 3 still hold, 2(a) is not the case anymore. </span><br><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">I  suggest the property of 2(a) that blocks interchangeability of the  concepts of Self and Person is \u201chaving first person epistemic  information about X\u201d. Selves have that, people don\u2019t. We use the term  \u2018person\u2019 when we want to talk only about the epistemically  intersubjective properties of someone. Self is reserved for a person\u2019s  perspective of herself, including, for instance, &nbsp;indexical facts. </span><br><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Other  than that, Self and Person seem to be interchangeable concepts. This  generalization is useful because that means most of the problem of  personhood and selfhood can be collapsed into one thing. </span><br><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Unfortunately, the Self/Person intersection is a concept that is itself a Mongrel Concept, so it has again to be split apart. </span><br><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Mongrel and Cluster Concepts </span><br></p>\n<p style=\"text-indent: 36pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">When  a concept seems to defy easy explanability, there are two interesting  possibilities of how to interact with it. The first would be to &nbsp;assume  that the disparate uses of the term \u2018Self\u2019\u2019 in ordinary language and  science can be captured by a unique, all-encompassing notion of Self.  The second is to assume that different uses of \u2018Self\u2019\u2019 reveal a  plurality of notions of Selfhood, each in need of a separate account. I  will endorse this second assumption: Self is a mongrel</span></p>\n<p><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">concept  in need of disambiguation. (to strenghten the analogy power of thinking  about mongrels, it may help to know that Information, Consciousness and  Health are thought to be mongrel concepts as well)</span><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;  Without using specific tags for the time being, let us assume that  there will be 4 kinds of Self, 1,2,3, and 4. To say that Self is a  concept that sometimes maps into 1, sometimes into 3 and so on is not to  exaustivelly frame the concept usage. That is because 1 and 2  themselves may be cluster concepts. </span><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;  The cluster concept shape is one of the most common shapes of concepts  in our mental vocabulary. Concepts are associational structures. Most of  the times, instead of drawing a clear line around a set in the world  inside of which all X fits, and outside of which none does, concepts  present a cluster like structure with nearly all core area members  belonging and nearly none in the far fetched radius belonging. &nbsp;Not all  of their typical features are logically necessary. &nbsp;The recognition of  features produces an activation, the strength of which depends not only  on the degree to which the feature is present but a weighting factor.  &nbsp;When the sum of the activations crosses a threshold, the concept  becomes active and the stimulus is said to belong to that category.</span><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;  Selves are mongrel concepts composed of different conceptual  intuitions, each of which is itself a cluster concept, thus Selves are  part of the most elusive, abstract, high-level entities entertained by  minds. Whereas this may be aesthetically pleasant, presenting us as  considerably complex entities, it is also a great ethical burden, for it  leaves the domain of ethics, highly dependant on the concepts of  Selfhood and Personhood, with a scattered slippery ground-level notion  from which to create the building blocks of ethical theories. </span><br><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;  Several analogies have been used to convey the concept of Cluster  Concept, these convey images of star clusters, neural networks lighting  up, and sets of properties with a majority vote. A particularly well  known analogy used by Wittgenstein is the game analogy, in which  language games determine prescribe normative meanings which constrict a  word\u2019s meaning, without determining a clear cut case. Wittgenstein  defended that there was no clear set of necessary conditions that  determine what a game is. Bernard Suits came up with a refutation of  that claim, stating that there is such a definition (modified from \u201cWhat  is a game\u201d 1967, </span><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Philosophy of Science</span><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\"> Vol. 34, No. 2 [Jun., 1967], pp. 148-156):</span><br></p>\n<blockquote>\n<p style=\"margin-left: 31.5pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\"> \"To play a game is to engage in activity designed to bring about a  specific state of affairs, using only means permitted by specific rules,  where the means permitted by the rules are more limited in scope than  they would be in the absence of such rules, and where the sole reason  for accepting the rules is to make possible such activity.\" </span></p>\n</blockquote>\n<p><br><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">&nbsp;&nbsp;&nbsp; Can we hope for a similar soon to be found understanding of Self? Let us invoke: </span><br></p>\n<p style=\"margin-left: 22.5pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">The Hidden Variable Hypothesis</span><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">:  There is a core essence which determines the class of selves from  non-selves, it is just not yet within our current state-of-knowledge  reach. &nbsp;</span></p>\n<p><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;  While desirable, there are various resons to be skeptical of The Hidden  Variable Hyphotesis: (1) Any plausible candidate core would have to be  able to disentangle selves from Organisms in general, Superorganisms  (i.e. insect societies) and institutions (2) We clearly entertain  different models of what selves are for different purposes, as shown  below in Section Varieties of Self-Systems Worth Having. (3) Design  consideration: Being evolved structures which encompass several  resources of a recently evolved mind, that came to being through a  complex dual-inheritance evolution of several hundred thousand  replicators belonging to two kinds (genes and memes), Selves are among  the most complex structures known and thus unlikely to possess a core  essence, due to causal design considerations independant of how  untractable it would be to detect and describe this essence. &nbsp;</span></p>\n<p style=\"text-indent: 36pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">From  now on then, I will be assuming as common ground that Selves are  Mongrel concepts, comprised of some yet indiscussed number of Cluster  Concepts. </span></p>\n<p>Not Yet Written Following Topics:<strong><br></strong></p>\n<p><strong id=\"Organisms__Superorganisms__and_SelvesSelves_and_SoritesSelves_Beyond_SoritesPersons__the_Evidence_of_Other_SelfsSelves_as_Utility_Increaser_Unnatural_ClustersWhat_do_We_Demand_of_SelvesVarities_of_Self_Systems_Worth_HavingDrescher__Personhood_Is_An_Ethical_PredicateWhat_Matters_About_Selves_\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Organisms, Superorganisms, and Selves</span><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Selves and Sorites</span><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Selves Beyond Sorites</span><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Persons, the Evidence of Other Selfs</span><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Selves as Utility Increaser Unnatural Clusters</span><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">What do We Demand of Selves</span><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Varities of Self-Systems Worth Having</span><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Drescher: Personhood Is An Ethical Predicate</span><br><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">What Matters About Selves?</span></strong></p>\n<p>&nbsp;</p>", "sections": [{"title": "On What Is a Self", "anchor": "On_What_Is_a_Self", "level": 1}, {"title": "Organisms, Superorganisms, and SelvesSelves and SoritesSelves Beyond SoritesPersons, the Evidence of Other SelfsSelves as Utility Increaser Unnatural ClustersWhat do We Demand of SelvesVarities of Self-Systems Worth HavingDrescher: Personhood Is An Ethical PredicateWhat Matters About Selves?", "anchor": "Organisms__Superorganisms__and_SelvesSelves_and_SoritesSelves_Beyond_SoritesPersons__the_Evidence_of_Other_SelfsSelves_as_Utility_Increaser_Unnatural_ClustersWhat_do_We_Demand_of_SelvesVarities_of_Self_Systems_Worth_HavingDrescher__Personhood_Is_An_Ethical_PredicateWhat_Matters_About_Selves_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wqDRRx9RqwKLzWt7R"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-08T11:17:33.462Z", "modifiedAt": null, "url": null, "title": "Best Textbook List Expansion", "slug": "best-textbook-list-expansion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:03.237Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "magfrump", "createdAt": "2009-12-10T20:51:45.065Z", "isAdmin": false, "displayName": "magfrump"}, "userId": "KsYFs5ip5jeiFETJa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MmG5YnQT2bJvxSX3R/best-textbook-list-expansion", "pageUrlRelative": "/posts/MmG5YnQT2bJvxSX3R/best-textbook-list-expansion", "linkUrl": "https://www.lesswrong.com/posts/MmG5YnQT2bJvxSX3R/best-textbook-list-expansion", "postedAtFormatted": "Monday, August 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Best%20Textbook%20List%20Expansion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABest%20Textbook%20List%20Expansion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMmG5YnQT2bJvxSX3R%2Fbest-textbook-list-expansion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Best%20Textbook%20List%20Expansion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMmG5YnQT2bJvxSX3R%2Fbest-textbook-list-expansion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMmG5YnQT2bJvxSX3R%2Fbest-textbook-list-expansion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 136, "htmlBody": "<p>A while back, Lukeprog set up an article to list the <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">best textbooks in every subject</a>. &nbsp;It currently contains a fairly large list of books in a variety of subjects.</p>\n<p>I just got an e-mail from Amazon advertising \"Up to 90% off textbooks\" and I thought \"This seems like a good opportunity to check out a bunch of cheap, good textbooks in subjects I want to learn about!\"</p>\n<p>When I went over to Luke's post, I discovered recommendations for philosophy, psychology, all sorts of math, but almost none in basic science.</p>\n<p>I assume that someone here must have read one or a few basic textbooks on physics, biology, and chemistry. &nbsp;If so, what were they? &nbsp;How were they? &nbsp;Would I be better off just trying to take a basic lecture course in the subject, or going through <a href=\"http://www.khanacademy.org/\">Khan Academy</a>?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MmG5YnQT2bJvxSX3R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 7.520879721399479e-07, "legacy": true, "legacyId": "9113", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xg3hXCYQPJkwHyik2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-08T11:32:35.903Z", "modifiedAt": null, "url": null, "title": "'Complex Value Systems are Required to Realize Valuable Futures' (Yudkowsky, 2011)", "slug": "complex-value-systems-are-required-to-realize-valuable", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:03.297Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CZY3A8FzvnXWWvkNh/complex-value-systems-are-required-to-realize-valuable", "pageUrlRelative": "/posts/CZY3A8FzvnXWWvkNh/complex-value-systems-are-required-to-realize-valuable", "linkUrl": "https://www.lesswrong.com/posts/CZY3A8FzvnXWWvkNh/complex-value-systems-are-required-to-realize-valuable", "postedAtFormatted": "Monday, August 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20'Complex%20Value%20Systems%20are%20Required%20to%20Realize%20Valuable%20Futures'%20(Yudkowsky%2C%202011)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A'Complex%20Value%20Systems%20are%20Required%20to%20Realize%20Valuable%20Futures'%20(Yudkowsky%2C%202011)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCZY3A8FzvnXWWvkNh%2Fcomplex-value-systems-are-required-to-realize-valuable%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text='Complex%20Value%20Systems%20are%20Required%20to%20Realize%20Valuable%20Futures'%20(Yudkowsky%2C%202011)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCZY3A8FzvnXWWvkNh%2Fcomplex-value-systems-are-required-to-realize-valuable", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCZY3A8FzvnXWWvkNh%2Fcomplex-value-systems-are-required-to-realize-valuable", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 25, "htmlBody": "<p>Most of the papers from the AGI-11 conference are now <a href=\"http://agi-conf.org/2011/call-for-papers/\">available online</a>, including Yudkowsky's new paper: '<a href=\"http://intelligence.org/upload/complex-value-systems.pdf\">Complex Value Systems are Required to Realize Valuable Futures</a>.'</p>\n<p>Enjoy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CZY3A8FzvnXWWvkNh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 22, "extendedScore": null, "score": 7.520927448631052e-07, "legacy": true, "legacyId": "9114", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-08T14:08:30.127Z", "modifiedAt": null, "url": null, "title": "[Website usability] Scroll to new comments (v0.3)", "slug": "website-usability-scroll-to-new-comments-v0-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:07.475Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dbaupp", "createdAt": "2011-06-26T05:11:09.036Z", "isAdmin": false, "displayName": "dbaupp"}, "userId": "k7vtKDR5ZwYxrGEaM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W8scMdJp7bPLTM7gD/website-usability-scroll-to-new-comments-v0-3", "pageUrlRelative": "/posts/W8scMdJp7bPLTM7gD/website-usability-scroll-to-new-comments-v0-3", "linkUrl": "https://www.lesswrong.com/posts/W8scMdJp7bPLTM7gD/website-usability-scroll-to-new-comments-v0-3", "postedAtFormatted": "Monday, August 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BWebsite%20usability%5D%20Scroll%20to%20new%20comments%20(v0.3)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BWebsite%20usability%5D%20Scroll%20to%20new%20comments%20(v0.3)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW8scMdJp7bPLTM7gD%2Fwebsite-usability-scroll-to-new-comments-v0-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BWebsite%20usability%5D%20Scroll%20to%20new%20comments%20(v0.3)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW8scMdJp7bPLTM7gD%2Fwebsite-usability-scroll-to-new-comments-v0-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW8scMdJp7bPLTM7gD%2Fwebsite-usability-scroll-to-new-comments-v0-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 351, "htmlBody": "<p>I wrote a short userscript<sup>1</sup> that allows for jumping to the next (or previous) new comment in a page (those marked with green). I have tested it on Firefox nightly with the <a href=\"https://addons.mozilla.org/en-US/firefox/addon/greasemonkey/\">Greasemonkey addon</a> and Chromium. Unfortunately, I think that user scripts only work in Chromium/Google Chrome and Firefox (with Greasemonkey).</p>\n<p><strong><a href=\"http://www.ug.it.usyd.edu.au/~hwil7821/lw/LWScroll.user.js\">Download here</a></strong> (Clicking the link should offer a install prompt, and that is all the work that needs to be done.)</p>\n<p>It inserts a small box in the lower right-hand corner that indicates the number of new messages and has a \"next\" and a \"previous\" link like so:</p>\n<p><img style=\"border: 1px solid black;\" src=\"http://images.lesswrong.com/t3_717_1.png?v=f6d7d4f121761db930a204fbd23cce34\" alt=\"\" /></p>\n<p>Clicking either link should scroll the browser to the top of the appropriate comment (wrapping around at the top and bottom).</p>\n<p>The \"!\" link shows a window for error logging. If a bug occurs, clicking the \"Generate log\" button inside this window will create a box with some information about the running of the script<sup>2</sup>, copying and pasting that information here will make debugging easier.</p>\n<p>I have only tested on the two browsers listed above, and only on Linux, so feedback about any bugs/improvements would be useful.</p>\n<p>(Technical note: It is released under <a href=\"http://www.opensource.org/licenses/MIT\">the MIT License</a>, and <a href=\"http://www.ug.it.usyd.edu.au/~hwil7821/lw/LWScroll.js\">this link</a> is to exactly the same file as above but renamed so that the source can be viewed more easily. The file extension needs to be changed to \"user.js\" to be able to run as a user script properly)</p>\n<h3>Changelog</h3>\n<p><strong>v0.1</strong> - First version</p>\n<p><strong>v0.2</strong> - Logging &amp; indication of number of new messages</p>\n<p><strong>v0.3 </strong>- Correctly update when hidden comments are loaded (and license change). <strong>NOTE</strong>: Upgrading to v0.3 on Chrome is likely to cause a \"Downgrading extension error\" (I'd made a mistake with the version numbers previously), the fix is to uninstall and then reinstall the new version. (uninstall via Tools &gt; Extensions)</p>\n<hr />\n<p><sup>1</sup> A segment of javascript that runs in the web browser as the page is loaded. It can modify the page,&nbsp; e.g. inserting a bit of html as this script does.</p>\n<p><sup>2 </sup>Specifically: the url, counts of different sets of comments, some info about the new comments, and also a list of the clicks on \"prev\" and \"next\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W8scMdJp7bPLTM7gD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 22, "extendedScore": null, "score": 7.521422197359584e-07, "legacy": true, "legacyId": "9115", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-08T15:48:30.514Z", "modifiedAt": null, "url": null, "title": "Spaced repetition review (my entry)", "slug": "spaced-repetition-review-my-entry", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:01.180Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZCRtnwGbcbpYP8t6h/spaced-repetition-review-my-entry", "pageUrlRelative": "/posts/ZCRtnwGbcbpYP8t6h/spaced-repetition-review-my-entry", "linkUrl": "https://www.lesswrong.com/posts/ZCRtnwGbcbpYP8t6h/spaced-repetition-review-my-entry", "postedAtFormatted": "Monday, August 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Spaced%20repetition%20review%20(my%20entry)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASpaced%20repetition%20review%20(my%20entry)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZCRtnwGbcbpYP8t6h%2Fspaced-repetition-review-my-entry%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Spaced%20repetition%20review%20(my%20entry)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZCRtnwGbcbpYP8t6h%2Fspaced-repetition-review-my-entry", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZCRtnwGbcbpYP8t6h%2Fspaced-repetition-review-my-entry", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 111, "htmlBody": "<p>As people have likely forgotten, jsalvatier had generously <a href=\"/lw/69p/prize_new_contest_for_spaced_repetition/\">set up a contest</a> to look through the literature on the spacing effect. <a href=\"/lw/64k/memory_spaced_repetition_and_life/\">Duke submitted his entry</a> a while ago, and I turned mine in on the 1 August deadline. It's part of my overall essay on spaced repetition: <a href=\"http://www.gwern.net/Spaced%20repetition#literature-review\">gwern.net/Spaced repetition#literature-review</a></p>\n<p>There's a <em>lot</em> of research, and I only cover what I estimate to be 0-5% of the extant literature, so if you're interested only in the punchline: <a href=\"http://www.gwern.net/Spaced%20repetition#review-summary\">gwern.net/Spaced repetition#review-summary</a></p>\n<p>(This is only a pointer to the review because the formatting would drive me nuts on LW. I'm not sure the edit box even supports footnotes at all; nor do I intend to find out.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZCRtnwGbcbpYP8t6h", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 28, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "9116", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uR4r3eZZqLmjZDqFj", "3r4GETDPMf335HfpA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-08T17:10:32.957Z", "modifiedAt": null, "url": null, "title": "Real-life observations of the blue eyes puzzle phenomenon?", "slug": "real-life-observations-of-the-blue-eyes-puzzle-phenomenon", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:02.380Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HonoreDB", "createdAt": "2010-11-18T19:42:02.810Z", "isAdmin": false, "displayName": "HonoreDB"}, "userId": "7eyYSfGvgCur6pXmk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NbkKuwLzMrSRCvA32/real-life-observations-of-the-blue-eyes-puzzle-phenomenon", "pageUrlRelative": "/posts/NbkKuwLzMrSRCvA32/real-life-observations-of-the-blue-eyes-puzzle-phenomenon", "linkUrl": "https://www.lesswrong.com/posts/NbkKuwLzMrSRCvA32/real-life-observations-of-the-blue-eyes-puzzle-phenomenon", "postedAtFormatted": "Monday, August 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Real-life%20observations%20of%20the%20blue%20eyes%20puzzle%20phenomenon%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReal-life%20observations%20of%20the%20blue%20eyes%20puzzle%20phenomenon%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNbkKuwLzMrSRCvA32%2Freal-life-observations-of-the-blue-eyes-puzzle-phenomenon%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Real-life%20observations%20of%20the%20blue%20eyes%20puzzle%20phenomenon%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNbkKuwLzMrSRCvA32%2Freal-life-observations-of-the-blue-eyes-puzzle-phenomenon", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNbkKuwLzMrSRCvA32%2Freal-life-observations-of-the-blue-eyes-puzzle-phenomenon", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<p>The <a href=\"http://www.xkcd.com/blue_eyes.html\">Blue Eyes Puzzle</a>&nbsp;(<a href=\"http://xkcd.com/solution.html\">solution</a>) depicts a paradox: people engage in coordinated action despite having no new information, when \"I know you know he knows\" reaches a critical mass. Apparently the formal system invented to address this is called <a href=\"http://en.wikipedia.org/wiki/Common_knowledge_(logic)\">Common Knowledge</a>.</p>\n<p><a href=\"http://www.eschatonblog.com/2011/08/wheeeeeeeeeeeeeeeeee_08.html\">Duncan Black complains:</a></p>\n<blockquote>\n<p>I wonder if any serious investor could actually explain what new information \"the market\" has which could explain why DJIA should be worth 11% less than it was 2 weeks ago.</p>\n</blockquote>\n<p><a id=\"more\"></a>The typical, compelling, explanation for this sort of thing is herd behavior. &nbsp;In the absence of new information, the market is modeled as a random walk, and when the amplitude of its swing happens to get high enough, people see a trend, anticipate it continuing, and thereby create the trend and cause a massive swing.</p>\n<p>I wonder if you could instead model stock market swings, or other seemingly unmotivated coordinated activity, as common knowledge reaching critical mass. &nbsp;Say new information was injected into the market two weeks ago, and it took that long to reach a blue eyes catastrophe.</p>\n<p>I have no evidence for this other than random pattern matching.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NbkKuwLzMrSRCvA32", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 7.521999983100137e-07, "legacy": true, "legacyId": "9117", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-09T04:17:19.388Z", "modifiedAt": null, "url": null, "title": "Individual Deniability, Statistical Honesty", "slug": "individual-deniability-statistical-honesty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:08.287Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hFuowzw63dr7Tkxcs/individual-deniability-statistical-honesty", "pageUrlRelative": "/posts/hFuowzw63dr7Tkxcs/individual-deniability-statistical-honesty", "linkUrl": "https://www.lesswrong.com/posts/hFuowzw63dr7Tkxcs/individual-deniability-statistical-honesty", "postedAtFormatted": "Tuesday, August 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Individual%20Deniability%2C%20Statistical%20Honesty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIndividual%20Deniability%2C%20Statistical%20Honesty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhFuowzw63dr7Tkxcs%2Findividual-deniability-statistical-honesty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Individual%20Deniability%2C%20Statistical%20Honesty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhFuowzw63dr7Tkxcs%2Findividual-deniability-statistical-honesty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhFuowzw63dr7Tkxcs%2Findividual-deniability-statistical-honesty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<p>If you have a lot of people to question about something, and they have a motivation to lie, consider <a href=\"http://io9.com/5826021/how-to-use-dice-to-prevent-people-from-lying-on-surveys\">this clever use of a six-sided die</a>.</p>\n<blockquote>\n<p>If the farmer tossed the die and got a one, they had to respond \"yes\" to the surveyor's question. If they got a six, they had to say \"no.\" The rest of the time, they were asked to answer honestly. The die was hidden from the person who was conducting the survey, so they never knew what number the farmer was responding to.</p>\n<p>Suddenly, the number of \"yes\" responses to the leopard question started coming up by more than just one-sixth.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nANxo5C4sPG9HQHzr": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hFuowzw63dr7Tkxcs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 62, "extendedScore": null, "score": 9.710623682223268e-05, "legacy": true, "legacyId": "9119", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-09T04:42:37.325Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Failing to Learn from History", "slug": "seq-rerun-failing-to-learn-from-history", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:01.545Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Unnamed", "createdAt": "2009-02-27T06:08:10.900Z", "isAdmin": false, "displayName": "Unnamed"}, "userId": "PdzQ73mN7S4SvRMhu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Srh4fQMnALWD7znCJ/seq-rerun-failing-to-learn-from-history", "pageUrlRelative": "/posts/Srh4fQMnALWD7znCJ/seq-rerun-failing-to-learn-from-history", "linkUrl": "https://www.lesswrong.com/posts/Srh4fQMnALWD7znCJ/seq-rerun-failing-to-learn-from-history", "postedAtFormatted": "Tuesday, August 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Failing%20to%20Learn%20from%20History&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Failing%20to%20Learn%20from%20History%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSrh4fQMnALWD7znCJ%2Fseq-rerun-failing-to-learn-from-history%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Failing%20to%20Learn%20from%20History%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSrh4fQMnALWD7znCJ%2Fseq-rerun-failing-to-learn-from-history", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSrh4fQMnALWD7znCJ%2Fseq-rerun-failing-to-learn-from-history", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p>Today's post, <a href=\"/lw/iz/failing_to_learn_from_history/\">Failing to Learn from History</a> was originally published on August 30, 2007. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There are no inherently mysterious phenomena, but every phenomenon seems mysterious, right up until the moment that science explains it. It seems to us now that biology, chemistry, and astronomy are naturally the realm of science, but if we had lived through their discoveries, and watched them reduced from mysterious to mundane, we would be more reluctant to believe the next phenomenon is inherently mysterious.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/r/discussion/lw/70m/seq_rerun_my_wild_and_reckless_youth/\">My Wild and Reckless Youth</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Srh4fQMnALWD7znCJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 7.524197218214147e-07, "legacy": true, "legacyId": "9120", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["97Y7Jwrzxyfzz3Ad2", "TnJau8m9EttRHBoGv", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-09T06:28:38.031Z", "modifiedAt": null, "url": null, "title": "Topic Search Poll Results and Short Reports", "slug": "topic-search-poll-results-and-short-reports", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:01.494Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nic_Smith", "createdAt": "2009-10-23T03:32:46.312Z", "isAdmin": false, "displayName": "Nic_Smith"}, "userId": "XP9GcTgRGLBCnf9ih", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XZJmehK6GG7RY8Zw5/topic-search-poll-results-and-short-reports", "pageUrlRelative": "/posts/XZJmehK6GG7RY8Zw5/topic-search-poll-results-and-short-reports", "linkUrl": "https://www.lesswrong.com/posts/XZJmehK6GG7RY8Zw5/topic-search-poll-results-and-short-reports", "postedAtFormatted": "Tuesday, August 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Topic%20Search%20Poll%20Results%20and%20Short%20Reports&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATopic%20Search%20Poll%20Results%20and%20Short%20Reports%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXZJmehK6GG7RY8Zw5%2Ftopic-search-poll-results-and-short-reports%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Topic%20Search%20Poll%20Results%20and%20Short%20Reports%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXZJmehK6GG7RY8Zw5%2Ftopic-search-poll-results-and-short-reports", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXZJmehK6GG7RY8Zw5%2Ftopic-search-poll-results-and-short-reports", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 458, "htmlBody": "<p>At the end of June, I asked <em>Less Wrong</em> <a href=\"/lw/6g1/please_vote_what_topic_would_be_best_for_an/\">to vote for \"What topic[s] would be best for an investigation and brief post?\"</a> in order<a href=\"/lw/5zk/proposal_systematic_search_for_useful_ideas/\"> to direct a search for topics to examine</a> here. My thanks to everyone that participated (especially since the comments hint that the poll format was not well-liked). The most-wanted topics follow, and the complete list can be found on <a href=\"https://spreadsheets.google.com/spreadsheet/ccc?key=0ApqbLYxMowMcdGViV2N2LXBZcFBHaVJLSFBoeXZUekE&amp;hl=en_US\">Google Docs</a> -- <a href=\"http://www.allourideas.org/lwsearch/results\">maps and graphs related to the poll are also available on <em>All Our Ideas</em></a>. A score for a topic in the results below is an \"estimated [percent] chance that it will win against a randomly chosen idea.\"</p>\n<ol>\n<li>Systems theory -- 71.6</li>\n<li>Leadership -- 70.7</li>\n<li>Linguistics (general) -- 70.7</li>\n<li>Finance -- 67.0</li>\n<li>Bayesian approach to business -- 60.7</li>\n<li>Lisp (Programming language) -- 59.7</li>\n<li>Anthropology (general) -- 59.4</li>\n<li>Sociology (general) -- 59.2</li>\n<li>Political Science (general) -- 58.5</li>\n<li>Historiography (the <em>methods</em> of history) -- 58.3</li>\n<li>Logistics -- 56.8</li>\n<li>Sociology of Political Organizations -- 56.0</li>\n<li>Military Theory -- 52.1</li>\n<li>Diplomacy -- 51.1</li>\n</ol>\n<p>Systems theory, in first place, is a topic that I found while rummaging through online sources, including <a href=\"http://en.wikipedia.org/wiki/Systems_theory\"><em>Wikipedia</em></a>, for items to add to the poll; it's described there as the \"study of systems in general, with the goal of elucidating principles that can be applied to all types of systems in all fields of research. [....] In this context the word systems is used to refer specifically to self-regulating systems, i.e. that are self-correcting through feedback.\" Leadership seems to fall into both the social and \"being effective\" categories of interest, but has only lightly been touched on in previous discussion here despite a lot of ink spilled on the topic elsewhere -- the top Google results for \"leadership\" on this site are currently <a href=\"/lw/6ca/community_roles_committees_and_leadership/\">Calcsam's post on community roles</a> and <a href=\"/lw/5kb/leadership_and_self_deception_anatomy_of_peace/\">a book review for the Arbinger Institute's <em>Leadership and Self Deception</em></a>. \"<a href=\"/lw/mc/to_lead_you_must_stand_up/\">To Lead, You Must Stand Up</a>\" also comes to mind.</p>\n<h4><a name=\"HowToUseIt\"></a>How to Use It<br /></h4>\n<p><a href=\"https://spreadsheets.google.com/spreadsheet/ccc?key=0ApqbLYxMowMcdGViV2N2LXBZcFBHaVJLSFBoeXZUekE&amp;hl=en_US\">The spreadsheet </a>includes columns for \"Currently Investigated By\" and \"Writeup URLs\" -- feel free to add your name or writeup links. If you already know a thing or two about one of the above topics, share your knowledge in a comment below or in a discussion post as appropriate, similar to the earlier \"<a href=\"/lw/5xo/what_can_you_teach_us/\">What can you teach us?</a>\" If you want to survey what currently exists on a topic, grab a few books, investigate, and then let us know what you found. When a related post instead of just a comment is appropriate, I recommend the tag \"topic_search\" <a href=\"/lw/5zk/proposal_systematic_search_for_useful_ideas/\">As mentioned previously</a>, even investigations that end in a comment to this post that a topic isn't useful for LW is still itself useful for the search.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XZJmehK6GG7RY8Zw5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 7.524533890519817e-07, "legacy": true, "legacyId": "9125", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LuxvfPnmnx4jBR8sm", "HyD6kz9H2PLkNeP8P", "igkgCjss3rmNmp9jB", "rK5pbJG3StktPbozu", "n5oCEbnW2PgFmkQhr", "QzDfep4hhn2LhjrTG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-09T15:38:32.385Z", "modifiedAt": null, "url": null, "title": "Meetup : Ottawa LessWrong Weekly Meetup", "slug": "meetup-ottawa-lesswrong-weekly-meetup-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XFrequentist", "createdAt": "2009-03-22T17:06:22.991Z", "isAdmin": false, "displayName": "XFrequentist"}, "userId": "zfW5w3TbDWjRW3YaD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bK4YasExTAEFWrtmH/meetup-ottawa-lesswrong-weekly-meetup-2", "pageUrlRelative": "/posts/bK4YasExTAEFWrtmH/meetup-ottawa-lesswrong-weekly-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/bK4YasExTAEFWrtmH/meetup-ottawa-lesswrong-weekly-meetup-2", "postedAtFormatted": "Tuesday, August 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Ottawa%20LessWrong%20Weekly%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Ottawa%20LessWrong%20Weekly%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbK4YasExTAEFWrtmH%2Fmeetup-ottawa-lesswrong-weekly-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Ottawa%20LessWrong%20Weekly%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbK4YasExTAEFWrtmH%2Fmeetup-ottawa-lesswrong-weekly-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbK4YasExTAEFWrtmH%2Fmeetup-ottawa-lesswrong-weekly-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/26\">Ottawa LessWrong Weekly Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">10 August 2011 07:30:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Heart and Crown, 347 Preston Street, Ottawa ON</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>To switch things up, we'll try a new night and venue.</p>\n<p>We'll be at the Heart &amp; Crown Pub on Preston Street, probably in the back room.</p>\n<p>Discussion post: <a href=\"/lw/5a9/learned_blankness/#more\">Learned Blankness</a> - (Other suggestions welcome)</p>\n<p>Activity: <a rel=\"nofollow\" href=\"http://ardenleigh.typepad.com/blog/2009/10/say-whats-true.html\">Repetition</a></p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/26\">Ottawa LessWrong Weekly Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bK4YasExTAEFWrtmH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.52628071273488e-07, "legacy": true, "legacyId": "9134", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Ottawa_LessWrong_Weekly_Meetup\">Discussion article for the meetup : <a href=\"/meetups/26\">Ottawa LessWrong Weekly Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">10 August 2011 07:30:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Heart and Crown, 347 Preston Street, Ottawa ON</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>To switch things up, we'll try a new night and venue.</p>\n<p>We'll be at the Heart &amp; Crown Pub on Preston Street, probably in the back room.</p>\n<p>Discussion post: <a href=\"/lw/5a9/learned_blankness/#more\">Learned Blankness</a> - (Other suggestions welcome)</p>\n<p>Activity: <a rel=\"nofollow\" href=\"http://ardenleigh.typepad.com/blog/2009/10/say-whats-true.html\">Repetition</a></p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Ottawa_LessWrong_Weekly_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/26\">Ottawa LessWrong Weekly Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Ottawa LessWrong Weekly Meetup", "anchor": "Discussion_article_for_the_meetup___Ottawa_LessWrong_Weekly_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Ottawa LessWrong Weekly Meetup", "anchor": "Discussion_article_for_the_meetup___Ottawa_LessWrong_Weekly_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-09T17:10:10.888Z", "modifiedAt": null, "url": null, "title": "Judging the intent of others favorably", "slug": "judging-the-intent-of-others-favorably", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:33.829Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lessdazed", "createdAt": "2011-02-02T05:06:52.010Z", "isAdmin": false, "displayName": "lessdazed"}, "userId": "ehZzKt5ByYBeyCLkz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nFYg8DTbhvikkeLQp/judging-the-intent-of-others-favorably", "pageUrlRelative": "/posts/nFYg8DTbhvikkeLQp/judging-the-intent-of-others-favorably", "linkUrl": "https://www.lesswrong.com/posts/nFYg8DTbhvikkeLQp/judging-the-intent-of-others-favorably", "postedAtFormatted": "Tuesday, August 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Judging%20the%20intent%20of%20others%20favorably&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJudging%20the%20intent%20of%20others%20favorably%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnFYg8DTbhvikkeLQp%2Fjudging-the-intent-of-others-favorably%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Judging%20the%20intent%20of%20others%20favorably%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnFYg8DTbhvikkeLQp%2Fjudging-the-intent-of-others-favorably", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnFYg8DTbhvikkeLQp%2Fjudging-the-intent-of-others-favorably", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 319, "htmlBody": "<p>I would like LW to be an environment in which we can learn by having honest and productive conversations. Fortunately, it substantially is such a place, but we can do better.&nbsp;</p>\n<p>I would like to make a post about judging others favorably in the near future. To this end I think a useful mechanism would be to encourage people to post as comments scenarios in which they made erroneous assumptions about others' intent, and hide the conclusion in which they learned of their error from view until the reader has performed the exercise of considering what the innocuous actual explanation might be.</p>\n<p>&nbsp;</p>\n<p>The purpose would be to make a repository of stories in which people could read the scenario, fail to think of how the situation could be resolved, and then see how in the previously hidden comment. Each bias involved in misjudgment - thinking one's enemies innately evil, believing one's own argument from ignorance about what the best possible explanation could be, and so forth - would be identified.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>I don't know that hiding the conclusions of stories would be technically easy. One hack would be to have people post the conclusion within the comment in which they laid out the story. people could then downvote the child comment and upvote the parent. However, not everyone has the hiding threshold set at -3, and the first people to see the comment would see the conclusion, not everyone has unlimited dowvotes, etc.</p>\n<p>Alternatively, the conclusion to each story could be in rot13.</p>\n<p>&nbsp;</p>\n<p>As a protocols, analogously to how people are discouraged from quoting themselves, I would think to limit posts about when others misjudged the author's intent to a maximum, perhaps one for every two submissions in which an author posts he or she misjudged the intent of others.</p>\n<p>As another protocol posts in which others on LW misjudged one's intent would be off-limits.</p>\n<p>&nbsp;</p>\n<p>Comments are encouraged, whether on my proposed protocols, how to format, etc.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nFYg8DTbhvikkeLQp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 7.52657189014292e-07, "legacy": true, "legacyId": "9136", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-09T17:12:09.681Z", "modifiedAt": "2022-05-20T00:27:11.564Z", "url": null, "title": "Leveling IRL - level 1", "slug": "leveling-irl-level-1", "viewCount": null, "lastCommentedAt": "2012-01-20T21:49:45.751Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kxLunCWypW6XqrS52/leveling-irl-level-1", "pageUrlRelative": "/posts/kxLunCWypW6XqrS52/leveling-irl-level-1", "linkUrl": "https://www.lesswrong.com/posts/kxLunCWypW6XqrS52/leveling-irl-level-1", "postedAtFormatted": "Tuesday, August 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Leveling%20IRL%20-%20level%201&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALeveling%20IRL%20-%20level%201%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkxLunCWypW6XqrS52%2Fleveling-irl-level-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Leveling%20IRL%20-%20level%201%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkxLunCWypW6XqrS52%2Fleveling-irl-level-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkxLunCWypW6XqrS52%2Fleveling-irl-level-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 459, "htmlBody": "<p><em>\"A human being should be able to change a diaper, plan an invasion, butcher a hog, conn a ship, design a building, write a sonnet, balance accounts, build a wall, set a bone, comfort the dying, take orders, give orders, cooperate, act alone, solve equations, analyze a new problem, pitch manure, program a computer, cook a tasty meal, fight efficiently, die gallantly. Specialization is for insects.\"</em>&nbsp;-- Robert A. Heinlein, Time Enough for Love</p>\n<p>This post is a followup to <a href=\"/lw/6zv/leveling_irl/\">Leveling IRL</a>. Thanks to SarahC, taryneast, Benquo, AdeleneDawner and MixedNuts, we have an outline of level 1. At this point I feel it's more productive to post it as-is than discuss it further:</p>\n<ul>\n<li><strong>Strength:</strong> reach the \"untrained\" level on each exercise in the <a href=\"http://www.exrx.net/Testing/WeightLifting/StrengthStandards.htm\">ExRx tables</a>.</li>\n<li><strong>Endurance:</strong> run 1 mile (1.6 km) without stopping.</li>\n<li><strong>Social:</strong> initiate a conversation with someone you know and arrange a meeting with them later. Do that 4 times with different people within 1 month.</li>\n<li><strong>Self control:</strong> work for 2 hours without interruptions. Do that on 8 separate days within 1 month.</li>\n<li><strong>Memory:</strong> memorize and recite a passage of your choosing, at least 250 words long, without making any mistakes.</li>\n<li><strong>Programming:</strong> solve <a href=\"http://projecteuler.net/index.php?section=problems&amp;id=1\">Project Euler problem #1</a>&nbsp;by writing and running a program in any language you choose.</li>\n<li><strong>Cooking:</strong> make pancakes. Here's a <a href=\"http://www.bbc.co.uk/food/recipes/basicpancakeswithsuga_66226\">good recipe</a>.</li>\n<li><strong>Finance:</strong> make a simple buy vs rent calculation, using prices appropriate for your area and your current standard of living.</li>\n<li><strong>Creativity:</strong> write 500 words of fiction in one sitting.</li>\n</ul>\n<p>The list has some glaring omissions, like math or chess, because I don't yet know of a crisp enough way to test those skills. Ideas are welcome! Also it seems very likely that some items on the list are wildly miscalibrated, some of them will turn out to be too hard for a beginner, and others will be too easy for anyone with a pulse. I'll be happy to hear about such miscalibrated requirements from the people who achieved them or at least tried :-)</p>\n<p>And here's what I think the rules should look like:</p>\n<ol>\n<li>The requirements for a level are frozen. No discussing them while you're trying to achieve them.</li>\n<li>A level is indivisible, you don't get moral whuffie points for doing half of the tasks.</li>\n<li>The only exception is that some people may opt to try for Level 1 No Physical, so they don't have to meet the Strength and Endurance requirements. (In university we had a saying that \"sports is the only test you cannot cram in a weekend\".)</li>\n</ol>\n<p>Personally, I'm going to try to make the level, but already know that some tasks will be difficult. I hope it's the same way for you.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1, "WqLn4pAWi5hn6McHQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kxLunCWypW6XqrS52", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 28, "extendedScore": null, "score": 5.3e-05, "legacy": true, "legacyId": "9135", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 93, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xhuaFuHc5rinDjkZo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2011-08-09T17:12:09.681Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-09T19:39:28.615Z", "modifiedAt": null, "url": null, "title": "Meetup : San Diego social meet-up", "slug": "meetup-san-diego-social-meet-up", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:02.098Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mercurial", "createdAt": "2011-04-21T03:59:51.257Z", "isAdmin": false, "displayName": "Mercurial"}, "userId": "2dGsX6cZSR9PmQyBq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/irAAytwz47M9HBsEE/meetup-san-diego-social-meet-up", "pageUrlRelative": "/posts/irAAytwz47M9HBsEE/meetup-san-diego-social-meet-up", "linkUrl": "https://www.lesswrong.com/posts/irAAytwz47M9HBsEE/meetup-san-diego-social-meet-up", "postedAtFormatted": "Tuesday, August 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20San%20Diego%20social%20meet-up&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20San%20Diego%20social%20meet-up%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FirAAytwz47M9HBsEE%2Fmeetup-san-diego-social-meet-up%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20San%20Diego%20social%20meet-up%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FirAAytwz47M9HBsEE%2Fmeetup-san-diego-social-meet-up", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FirAAytwz47M9HBsEE%2Fmeetup-san-diego-social-meet-up", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 281, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/27'>San Diego social meet-up</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 August 2011 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">6380 Del Cerro Blvd. San Diego, CA 92120</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As per <a href=\"http://lesswrong.com/r/discussion/lw/70l/san_diego_meetup/\">this request</a>, we're having a meetup on Sunday, August 14th at the K&amp;B Wine Cellars (same as <a href=\"http://lesswrong.com/meetups/1e\">last time</a>).</p>\n\n<p>This is probably going to be primarily social rather than covering specific topics. (We're still hammering out details for the topics-based one for the last Sunday of the month.)</p>\n\n<p>The locale we're using wasn't particularly picky about when we should leave last time, so I suspect we have pretty much the whole afternoon.</p>\n\n<p>If you have any particular desire to use the projector, either let me know here or be sure to bring a VGA cable. (Last time theirs was missing.) And, of course, if you want to use a Mac with the projector be sure to have any adaptors you need with you.</p>\n\n<p>Also, as Miciaih_Chang so kindly reminded me in the comments below, the sound system for the projector doesn't work (or possibly isn't hooked up to a speaker system in the first place).  So if you want to give a presentation that depends on sound and your laptop's speakers are, well, laptop speakers, you might want to bring some speakers of your own.  (If you need this but have trouble getting a hold of speakers, let me know.  I <strong>might</strong> be able to borrow some from my program.)</p>\n\n<p>And as last time, I politely request that no smokers attend. This was a total non-issue last time, for which my wife and I are grateful! But just to be careful, I'll repeat the request each time I put these together.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/27'>San Diego social meet-up</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "irAAytwz47M9HBsEE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 7.527046295981403e-07, "legacy": true, "legacyId": "9138", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___San_Diego_social_meet_up\">Discussion article for the meetup : <a href=\"/meetups/27\">San Diego social meet-up</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 August 2011 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">6380 Del Cerro Blvd. San Diego, CA 92120</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As per <a href=\"http://lesswrong.com/r/discussion/lw/70l/san_diego_meetup/\">this request</a>, we're having a meetup on Sunday, August 14th at the K&amp;B Wine Cellars (same as <a href=\"http://lesswrong.com/meetups/1e\">last time</a>).</p>\n\n<p>This is probably going to be primarily social rather than covering specific topics. (We're still hammering out details for the topics-based one for the last Sunday of the month.)</p>\n\n<p>The locale we're using wasn't particularly picky about when we should leave last time, so I suspect we have pretty much the whole afternoon.</p>\n\n<p>If you have any particular desire to use the projector, either let me know here or be sure to bring a VGA cable. (Last time theirs was missing.) And, of course, if you want to use a Mac with the projector be sure to have any adaptors you need with you.</p>\n\n<p>Also, as Miciaih_Chang so kindly reminded me in the comments below, the sound system for the projector doesn't work (or possibly isn't hooked up to a speaker system in the first place).  So if you want to give a presentation that depends on sound and your laptop's speakers are, well, laptop speakers, you might want to bring some speakers of your own.  (If you need this but have trouble getting a hold of speakers, let me know.  I <strong>might</strong> be able to borrow some from my program.)</p>\n\n<p>And as last time, I politely request that no smokers attend. This was a total non-issue last time, for which my wife and I are grateful! But just to be careful, I'll repeat the request each time I put these together.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___San_Diego_social_meet_up1\">Discussion article for the meetup : <a href=\"/meetups/27\">San Diego social meet-up</a></h2>", "sections": [{"title": "Discussion article for the meetup : San Diego social meet-up", "anchor": "Discussion_article_for_the_meetup___San_Diego_social_meet_up", "level": 1}, {"title": "Discussion article for the meetup : San Diego social meet-up", "anchor": "Discussion_article_for_the_meetup___San_Diego_social_meet_up1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ioJrwkGTt7serMm8g"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-09T20:27:36.032Z", "modifiedAt": null, "url": null, "title": "The New York Times: The Mathematics of Changing Your Mind", "slug": "the-new-york-times-the-mathematics-of-changing-your-mind", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:01.627Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Incorrect", "createdAt": "2011-05-19T06:36:41.700Z", "isAdmin": false, "displayName": "Incorrect"}, "userId": "YKd5C3yr2o6NtJgtP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EHaqbtv9LJPevcWYx/the-new-york-times-the-mathematics-of-changing-your-mind", "pageUrlRelative": "/posts/EHaqbtv9LJPevcWYx/the-new-york-times-the-mathematics-of-changing-your-mind", "linkUrl": "https://www.lesswrong.com/posts/EHaqbtv9LJPevcWYx/the-new-york-times-the-mathematics-of-changing-your-mind", "postedAtFormatted": "Tuesday, August 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20New%20York%20Times%3A%20The%20Mathematics%20of%20Changing%20Your%20Mind&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20New%20York%20Times%3A%20The%20Mathematics%20of%20Changing%20Your%20Mind%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEHaqbtv9LJPevcWYx%2Fthe-new-york-times-the-mathematics-of-changing-your-mind%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20New%20York%20Times%3A%20The%20Mathematics%20of%20Changing%20Your%20Mind%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEHaqbtv9LJPevcWYx%2Fthe-new-york-times-the-mathematics-of-changing-your-mind", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEHaqbtv9LJPevcWYx%2Fthe-new-york-times-the-mathematics-of-changing-your-mind", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<blockquote>\n<p>Bayes&rsquo;s theorem, named after the 18th-century Presbyterian minister  Thomas Bayes, addresses this selfsame essential task: How should we  modify our beliefs in the light of additional information? Do we cling  to old assumptions long after they&rsquo;ve become untenable, or abandon them  too readily at the first whisper of doubt? Bayesian reasoning promises  to bring our views gradually into line with reality and so has become an  invaluable tool for scientists of all sorts and, indeed, for anyone who  wants, putting it grandiloquently, to sync up with the universe. If you  are not thinking like a Bayesian, perhaps you should be.</p>\n</blockquote>\n<p><a href=\"http://www.nytimes.com/2011/08/07/books/review/the-theory-that-would-not-die-by-sharon-bertsch-mcgrayne-book-review.html\">http://www.nytimes.com/2011/08/07/books/review/the-theory-that-would-not-die-by-sharon-bertsch-mcgrayne-book-review.html</a></p>\n<p><a href=\"http://www.reddit.com/r/math/comments/jdgm2/the_mathematics_of_changing_your_mind_nytimescom/\">Reddit /r/math discussion</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EHaqbtv9LJPevcWYx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "9139", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-09T21:36:07.057Z", "modifiedAt": null, "url": null, "title": "Theory of Knowledge (rationality outreach)", "slug": "theory-of-knowledge-rationality-outreach", "viewCount": null, "lastCommentedAt": "2021-04-01T05:15:14.432Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KPier", "createdAt": "2011-05-29T20:37:16.788Z", "isAdmin": false, "displayName": "KPier"}, "userId": "LNNvCDMS2RvA4jZAG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LNrdcvD5qhY4d2Syb/theory-of-knowledge-rationality-outreach", "pageUrlRelative": "/posts/LNrdcvD5qhY4d2Syb/theory-of-knowledge-rationality-outreach", "linkUrl": "https://www.lesswrong.com/posts/LNrdcvD5qhY4d2Syb/theory-of-knowledge-rationality-outreach", "postedAtFormatted": "Tuesday, August 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Theory%20of%20Knowledge%20(rationality%20outreach)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATheory%20of%20Knowledge%20(rationality%20outreach)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLNrdcvD5qhY4d2Syb%2Ftheory-of-knowledge-rationality-outreach%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Theory%20of%20Knowledge%20(rationality%20outreach)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLNrdcvD5qhY4d2Syb%2Ftheory-of-knowledge-rationality-outreach", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLNrdcvD5qhY4d2Syb%2Ftheory-of-knowledge-rationality-outreach", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 607, "htmlBody": "<p>Public schools (and arguably private schools as well; I wouldn't know) teach students what to think, not how to think.</p>\n<p>On LessWrong, this insight is so trivial not to bear repeating. Unfortunately, I think many people have adopted it as an immutable fact about the world that will be corrected post-Singularity, rather than a totally unacceptable state of affairs which we should be <em>doing something</em> about now. The consensus seems to be that a class teaching the basic principles of thinking would be a huge step towards raising the sanity waterline, but that it will never happen. Well, my school has one. It's called Theory of Knowledge, and it's offered at 2,307 schools worldwide as part of the <a href=\"http://www.ibo.org/diploma/\">IB Diploma Program</a>.</p>\n<p>The IB Diploma, for those of you who haven't heard of it, is a internationally recognized high school program. It requires students to pass tests in 6 subject areas, jump through a number of other hoops, and take an additional class called Theory of Knowledge.</p>\n<p>For the record, I'm not convinced the IB Diploma Program is a good thing. It doesn't really solve any of the problems with public schools, it shares the frustrating focus on standardized testing and <a href=\"/lw/iq/guessing_the_teachers_password/\">password-guessing</a> instead of real learning, etc. But I think Theory of Knowledge is a huge opportunity to spread the ideas of rationality.</p>\n<p>What kinds of people sign up for the IB Diploma? It is considered more rigorous than A-levels in Britain, and dramatically more rigorous than standard classes in the United States (I would consider it approximately equal to taking 5 or 6 AP classes a year). Most kids engaged in this program are intelligent, motivated and interested in the world around them. They seem, (through my informal survey method of talking to lots of them) to have a higher <a href=\"/lw/1mh/that_magical_click/\">click factor</a> than average.</p>\n<p>The problem is that currently, Theory of Knowledge is a waste of time. There isn't much in the way of standards for a curriculum, and in the entire last semester we covered less content than I learn from any given top-level LessWrong post. We debated the nature of truth for 4 months; most people do not come up with interesting answers to this on their own initiative, so the conversation went in circles around \"There's no such thing as truth!\" \"Now, that's just stupid.\" the whole time. When I mention LessWrong to my friends, I generally explain it as \"What ToK would be like, if ToK was actually good.\"</p>\n<p>At my school, we regularly have speakers come in and discuss various topics during ToK, mostly because the regular instructor doesn't have any idea what to say. The only qualifications seem to be a pulse and some knowledge of English (we've had presenters who aren't fluent). If LessWrong posters wanted to call up <a href=\"http://www.ibo.org/school/search/index.cfm?programmes=DIPLOMA&amp;country=&amp;region=&amp;find_schools=Find\">the IB school nearest you </a>and offer to present on rationality, I'm almost certain people would agree. This seems like a good opportunity to practice speaking/presenting in a low-stakes situation, and a great way to expose smart, motivated kids to rationality.</p>\n<p>I think a good presentation would focus on the meaning of evidence, what we mean by \"rationality\", and making beliefs pay rent, all topics we've touched on without saying anything meaningful. We've also discussed Popper's falsificationism, and about half your audience will already be familiar with Bayes' theorem through statistics classes but not as a model of inductive reasoning in general.</p>\n<p>If you'd be interested in this but don't know where to start in terms of preparing a presentation, Liron's presentation <a href=\"/lw/fc/you_are_a_brain/\">\"You Are A Brain\"</a> seems like a good place to start. Designing a presentation along these lines might also be a good activity for a meetup group.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "7ow6EFpypbH4hzFuz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LNrdcvD5qhY4d2Syb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 68, "baseScore": 86, "extendedScore": null, "score": 0.000186, "legacy": true, "legacyId": "9085", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 86, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NMoLJuDJEms7Ku9XS", "R3ATEWWmBhMhbY2AL", "r5H6YCmnn8DMtBtxt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-09T23:04:59.534Z", "modifiedAt": null, "url": null, "title": "Fiction: Letter from the End", "slug": "fiction-letter-from-the-end", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:02.521Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexMennen", "createdAt": "2009-11-27T18:24:19.500Z", "isAdmin": false, "displayName": "AlexMennen"}, "userId": "KgzPEGnYWvKDmWuNY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HvjZxxtHnAucnaKn2/fiction-letter-from-the-end", "pageUrlRelative": "/posts/HvjZxxtHnAucnaKn2/fiction-letter-from-the-end", "linkUrl": "https://www.lesswrong.com/posts/HvjZxxtHnAucnaKn2/fiction-letter-from-the-end", "postedAtFormatted": "Tuesday, August 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fiction%3A%20Letter%20from%20the%20End&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFiction%3A%20Letter%20from%20the%20End%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHvjZxxtHnAucnaKn2%2Ffiction-letter-from-the-end%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fiction%3A%20Letter%20from%20the%20End%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHvjZxxtHnAucnaKn2%2Ffiction-letter-from-the-end", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHvjZxxtHnAucnaKn2%2Ffiction-letter-from-the-end", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 8, "htmlBody": "<p>http://alex.mennen.org/LetterFromTheEnd.pdf</p>\n<p>I thought some LW-ers might find this interesting.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HvjZxxtHnAucnaKn2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 32, "extendedScore": null, "score": 6.6e-05, "legacy": true, "legacyId": "9142", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-10T04:01:29.096Z", "modifiedAt": null, "url": null, "title": "Meetup : Madison", "slug": "meetup-madison-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:01.754Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rderb7WrF7wh6gaLr/meetup-madison-1", "pageUrlRelative": "/posts/rderb7WrF7wh6gaLr/meetup-madison-1", "linkUrl": "https://www.lesswrong.com/posts/rderb7WrF7wh6gaLr/meetup-madison-1", "postedAtFormatted": "Wednesday, August 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Madison&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Madison%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frderb7WrF7wh6gaLr%2Fmeetup-madison-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Madison%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frderb7WrF7wh6gaLr%2Fmeetup-madison-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frderb7WrF7wh6gaLr%2Fmeetup-madison-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/28'>Madison</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 August 2011 06:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2100 Winnebago St, Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Note: if you're in Madison, you should sign up for the Less Wrong Madison <a href=\"https://groups.google.com/forum/#!forum/lesswrong-madison\">mailing list</a>! Even if you can't attend this meeting, other activities and discussions on that mailing list might appeal to you.</p>\n\n<p>The director of <a href=\"http://www.sector67.org\" rel=\"nofollow\">Sector67</a> has generously offered us meeting space. Sector67 is a hackerspace, and is one of my favorite places in Madison. We'll mostly be in the meeting room, not the workshop.</p>\n\n<p>Along with expected banter and continued everyone-meeting-everyone, Patrick (<a href=\"http://lesswrong.com/user/orthonormal/\">orthonormal</a>) will be presenting his argument to dissolve the question of qualia.</p>\n\n<p>Again: Make sure you're on the Less Wrong Madison <a href=\"https://groups.google.com/forum/#!forum/lesswrong-madison\">mailing list</a>. :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/28'>Madison</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rderb7WrF7wh6gaLr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.528641880418831e-07, "legacy": true, "legacyId": "9148", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Madison\">Discussion article for the meetup : <a href=\"/meetups/28\">Madison</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 August 2011 06:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2100 Winnebago St, Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Note: if you're in Madison, you should sign up for the Less Wrong Madison <a href=\"https://groups.google.com/forum/#!forum/lesswrong-madison\">mailing list</a>! Even if you can't attend this meeting, other activities and discussions on that mailing list might appeal to you.</p>\n\n<p>The director of <a href=\"http://www.sector67.org\" rel=\"nofollow\">Sector67</a> has generously offered us meeting space. Sector67 is a hackerspace, and is one of my favorite places in Madison. We'll mostly be in the meeting room, not the workshop.</p>\n\n<p>Along with expected banter and continued everyone-meeting-everyone, Patrick (<a href=\"http://lesswrong.com/user/orthonormal/\">orthonormal</a>) will be presenting his argument to dissolve the question of qualia.</p>\n\n<p>Again: Make sure you're on the Less Wrong Madison <a href=\"https://groups.google.com/forum/#!forum/lesswrong-madison\">mailing list</a>. :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Madison1\">Discussion article for the meetup : <a href=\"/meetups/28\">Madison</a></h2>", "sections": [{"title": "Discussion article for the meetup : Madison", "anchor": "Discussion_article_for_the_meetup___Madison", "level": 1}, {"title": "Discussion article for the meetup : Madison", "anchor": "Discussion_article_for_the_meetup___Madison1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-10T06:18:35.803Z", "modifiedAt": null, "url": null, "title": "Meetup : Houston Hackerspace Meetup", "slug": "meetup-houston-hackerspace-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:02.471Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cog", "createdAt": "2011-04-25T04:58:53.803Z", "isAdmin": false, "displayName": "Cog"}, "userId": "xkp87vCZ56dp2tWnN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6AvnzTbB5ZFEDYxDw/meetup-houston-hackerspace-meetup", "pageUrlRelative": "/posts/6AvnzTbB5ZFEDYxDw/meetup-houston-hackerspace-meetup", "linkUrl": "https://www.lesswrong.com/posts/6AvnzTbB5ZFEDYxDw/meetup-houston-hackerspace-meetup", "postedAtFormatted": "Wednesday, August 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Houston%20Hackerspace%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Houston%20Hackerspace%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6AvnzTbB5ZFEDYxDw%2Fmeetup-houston-hackerspace-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Houston%20Hackerspace%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6AvnzTbB5ZFEDYxDw%2Fmeetup-houston-hackerspace-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6AvnzTbB5ZFEDYxDw%2Fmeetup-houston-hackerspace-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 62, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/29'>Houston Hackerspace Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 August 2011 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2010 Commerce St, Houston, Tx. 77002</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another weekly meetup at the Houston TX/RX hackerspace. I'll be giving an overview of the psychology and neuroscience of liking and wanting, followed possibly by a game of Munchkin or Catan.  Seeya there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/29'>Houston Hackerspace Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6AvnzTbB5ZFEDYxDw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 7.529077781135416e-07, "legacy": true, "legacyId": "9151", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Houston_Hackerspace_Meetup\">Discussion article for the meetup : <a href=\"/meetups/29\">Houston Hackerspace Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 August 2011 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2010 Commerce St, Houston, Tx. 77002</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another weekly meetup at the Houston TX/RX hackerspace. I'll be giving an overview of the psychology and neuroscience of liking and wanting, followed possibly by a game of Munchkin or Catan.  Seeya there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Houston_Hackerspace_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/29\">Houston Hackerspace Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Houston Hackerspace Meetup", "anchor": "Discussion_article_for_the_meetup___Houston_Hackerspace_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Houston Hackerspace Meetup", "anchor": "Discussion_article_for_the_meetup___Houston_Hackerspace_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-10T09:30:26.205Z", "modifiedAt": null, "url": null, "title": "Strategic ignorance and plausible deniability", "slug": "strategic-ignorance-and-plausible-deniability", "viewCount": null, "lastCommentedAt": "2021-10-15T00:06:43.907Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fxgkYCbG5Hgy58TyC/strategic-ignorance-and-plausible-deniability", "pageUrlRelative": "/posts/fxgkYCbG5Hgy58TyC/strategic-ignorance-and-plausible-deniability", "linkUrl": "https://www.lesswrong.com/posts/fxgkYCbG5Hgy58TyC/strategic-ignorance-and-plausible-deniability", "postedAtFormatted": "Wednesday, August 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Strategic%20ignorance%20and%20plausible%20deniability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStrategic%20ignorance%20and%20plausible%20deniability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfxgkYCbG5Hgy58TyC%2Fstrategic-ignorance-and-plausible-deniability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Strategic%20ignorance%20and%20plausible%20deniability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfxgkYCbG5Hgy58TyC%2Fstrategic-ignorance-and-plausible-deniability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfxgkYCbG5Hgy58TyC%2Fstrategic-ignorance-and-plausible-deniability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1223, "htmlBody": "<p><em>This is the third part in a <a href=\"/tag/whyeveryonehypocrite\">mini-sequence</a> presenting material from Robert Kurzban's excellent book </em><em><a href=\"http://www.amazon.com/Why-Everyone-Else-Hypocrite-Evolution/dp/0691146748\">Why Everyone (Else) Is a Hypocrite: Evolution and the Modular Mind</a></em><em>.</em></p>\n<p>The press secretary of an organization is tasked with presenting outsiders with the best possible image of the organization. While they're not supposed to outright lie, they do use euphemisms and try to only mention the positive sides of things.<br /><br />A plot point in the TV series <a href=\"http://en.wikipedia.org/wiki/The_West_Wing\">West Wing</a> is that the President of the United States has a disease which he wants to hide from the public. The White House Press Secretary is careful to ask whether there's anything she <em>needs</em> to know about the President's health, instead of whether there's anything she <em>should</em> know. As the President's disease is technically something she <em>should</em> know but not something she <em>needs</em> to know, this allows the President to hide the disease from her without lying to her (and by extension, to the American public). As she then doesn't need to lie either, she can do her job better.<br /><br />If our minds are modular, critical information can be kept away from the modules that are associated with consciousness and speech production. It can often be better if the parts of the system that exist to deal with others are blissfully ignorant, or even actively mistaken, about information that exists in other parts of the system.<br /><br />In one experiment, people could choose between two options. Choosing option A meant they got $5, and someone else also got $5. Option B meant that they got $6 and the other person got $1. About two thirds were generous and chose option A.</p>\n<p>A different group of people played a slightly different game. As before, they could choose between $5 or $6 for themselves, but they didn't know how their choice would affect the other person's payoff. They could find out, however &ndash; if they just clicked a button, they'd be told whether the choice was between $5/$5 and $6/$1, or $5/$1 and $6/$5. From a subject's point of view, clicking a button might tell them that picking the option they actually preferred meant they were costing the other person $4. Not clicking meant that they could honestly say that they didn't know what their choice cost the other person. It turned out that about half of the people refused to look at the other player's payoffs, and that many more subjects chose $6/? than $5/?.<br /><br />There are many situations where not knowing something means you can avoid a lose-lose situation. If know your friend is guilty of a serious crime and you are called to testify in court, you may either betray your friend or commit perjury. If you see a building on fire, and a small boy comes to tell you that a cat is caught in the window, your options are to either risk yourself to save the cat, or take the reputational hit of neglecting a socially perceived duty to rescue the cat. (Footnote in the book: &rdquo;You could kill the boy, but then you've got other problems.&rdquo;) In the <a href=\"http://en.wikipedia.org/wiki/Trolley_problem\">trolley problem</a>, many people will consider <em>both</em> options wrong. In one setup, 87% of the people who were asked thought that pushing a man to the tracks to save five was wrong, and 62% said that <em>not</em> pushing him was wrong. Better to never see the people on the tracks. In addition to having your reputation besmirched by not trying to save someone, many nations have actual &rdquo;<a href=\"http://en.wikipedia.org/wiki/Duty_to_rescue\">duty to rescue</a>&rdquo; laws which require you to act if you see someone in serious trouble.</p>\n<p>In general, people (and societies) often believe that if you know about something bad, you have a duty to stop it. If you don't know about something, then obviously you can't be blamed for not stopping it. So we should expect that part of our behavior is designed to avoid finding out information that would impose an unpleasant duty on us.<br /><br />I personally tend to notice this conflict if I see people in public places who look like they might be sleeping or passed out. Most likely, they're just sleeping and don't want to be bothered. If they're drunk or on drugs, they could even be aggressive. But then there's always the chance that they have some kind of a condition and need medical assistance. Should I go poke them to make sure? <em>You can't be blamed if you act like you didn't notice them</em>, some part of me whispers. Remember the suggestion that you can fight the <a href=\"/lw/9j/bystander_apathy/\">bystander effect</a> by singling out a person and asking them directly for help? You can't pretend you haven't noticed a duty if the duty is pointed out to you directly. As for the bystander effect in general, there's less of a perceived duty to help if everyone else ignores the person, too. (But then this can't be the sole explanation, because people are most likely to act when they're alone and there's <em>nobody</em> else around to know about your duty. The bystander effect isn't actually discussed in the book, this paragraph is my own speculation.)<br /><br />The police may also prefer not to know about some minor crime that is being committed. If it's known that they're ignoring drug use (say), they lose some of their authority and may end up punished by their superiors. If they don't ignore it, they may spend all of their time doing minor busts instead of concentrating on more serious crime. Parents may also pretend that they don't notice their kids engaging in some minor misbehavior, if they don't want to lose their authority but don't feel like interfering either.</p>\n<blockquote>\n<p>In effect, the value of ignorance comes from the costs of others seeing you know something that puts you in a position in which you are perceived to have a duty and must choose to do one of two costly acts &ndash; punish, or ignore. In may own lab, we have found that people know this. When our subjects are given the opportunity to punish someone who has been unkind in an economic game, they do so much less when their punishment won't be known by anyone. That is, they decline to punish when the cloak of anonymity protects them.</p>\n</blockquote>\n<p>The (soon-to-expire) &rdquo;<a href=\"http://en.wikipedia.org/wiki/Don%27t_ask_don%27t_tell\">don't ask, don't tell</a>&rdquo; policy of the United States military can be seen as an institutionalization of this rule. Soldiers are forbidden from revealing information about their sexuality, which would force their commanders to discharge them. On the other hand, commanders are also forbidden from inquiring into the matter and finding out.<br /><br />A related factor is the desire for plausible deniability. A person who wants to have multiple sexual partners may resist getting himself tested for sexual disease. If he was tested, he might find out he had a disease, and then he'd be accused of knowingly endangering others if he didn't tell them about his disease. If he isn't tested, he'll only be accused of not finding out that information, which is often considered less serious.<br /><br />These are examples of situations where it's advantageous to be ignorant of something. But there are also situations where it is good to be actively mistaken. More about them in the next post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 2, "YTCrHWYHAsAD74EHo": 2, "5uHdFgR938LGGxMKQ": 2, "iP2X4jQNHMWHRNPne": 2, "LDTSbmXtokYAsEq8e": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fxgkYCbG5Hgy58TyC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 60, "extendedScore": null, "score": 0.000121, "legacy": true, "legacyId": "9157", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "uPjHAiXAKrMzvTFyt", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "modularity-signaling-and-belief-in-belief", "canonicalPrevPostSlug": "modularity-and-buzzy", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 60, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K5nq3KcDXaGm7QQWR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-11T00:54:13.874Z", "modifiedAt": null, "url": null, "title": "Why epidemiology will not correct itself", "slug": "why-epidemiology-will-not-correct-itself", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:05.656Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n7E2FC63MZGnvZAdr/why-epidemiology-will-not-correct-itself", "pageUrlRelative": "/posts/n7E2FC63MZGnvZAdr/why-epidemiology-will-not-correct-itself", "linkUrl": "https://www.lesswrong.com/posts/n7E2FC63MZGnvZAdr/why-epidemiology-will-not-correct-itself", "postedAtFormatted": "Thursday, August 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20epidemiology%20will%20not%20correct%20itself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20epidemiology%20will%20not%20correct%20itself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn7E2FC63MZGnvZAdr%2Fwhy-epidemiology-will-not-correct-itself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20epidemiology%20will%20not%20correct%20itself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn7E2FC63MZGnvZAdr%2Fwhy-epidemiology-will-not-correct-itself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn7E2FC63MZGnvZAdr%2Fwhy-epidemiology-will-not-correct-itself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 323, "htmlBody": "<p>We're generally familiar here with the appalling state of medical and dietary research, where most correlations turn out to be bogus. (And if we're not, I have collected a number of links on the topic in my DNB FAQ that one can read, see <a href=\"http://www.gwern.net/DNB%20FAQ#flaws-in-mainstream-science-and-psychology\">http://www.gwern.net/DNB%20FAQ#flaws-in-mainstream-science-and-psychology</a> - probably the best first link to read would be Ioannidis's <a href=\"http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124\">&ldquo;Why Most Published Research Findings Are False&rdquo;</a>.)</p>\n<p>I recently found <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.143.7282\">a talk</a> arguing that this problem was worse than one might assume, with false positives in the &gt;80% range, and more interestingly, <em>why</em> the rate is so high and will remain high for the foreseeable future. Young asserts, pointing to papers and textbooks by epidemiologists, that they are perfectly aware of what the <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Bonferroni_correction\">Bonferroni correction</a> does (and why one <a href=\"https://www.xkcd.com/882/\">would use it</a>) and that they <em>choose to not use it</em> because they do not want to risk any false negatives. (Young also conducts some surveys showing less interest in public sharing of data and other good things like that, but that seems to me to be much less important than the statistical tradeoffs.)</p>\n<p>There are three papers online that seem representative:</p>\n<ol>\n<li><a href=\"http://biostat.mcg.edu/journalclub/RothmanMultipleComparisons.pdf\">Rothman (1990)</a></li>\n<li><a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1112991/\">Perneger (1998)</a></li>\n<li><a href=\"http://www.sld.cu/galerias/pdf/sitios/revsalud/observational_research,_randomised.pdf\">Vandenbroucke, PLoS Med (2008)</a></li>\n</ol>\n<p>Reading them is a little horrifying when one considers <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Externality#Negative\">the costs</a> of the false positives, all the people trying to stay healthy by following what is only random noise, and the general (and justified!) contempt for science by those aware of the false positive rate. (I enlarge on this vein of thought <a href=\"https://pay.reddit.com/r/PhilosophyofScience/comments/jfjnw/why_epidemiology_will_not_correct_itself/c2brbfb\">on Reddit</a>. The recent <a href=\"http://www.scientificamerican.com/article.cfm?id=its-time-to-end-the-war-on-salt\">kerfluffle</a> <a href=\"http://www.overcomingbias.com/2011/11/forget-salt.html\">about</a> whether salt really is bad for you - medical advice that has stressed millions and will cost more millions due to New York City's <a href=\"https://www.nytimes.com/2010/01/11/business/11salt.html\">war on salt</a> - is a reminder of what is at stake.)</p>\n<p>The take-away, I think, is to resolutely ignore anything to do with diet &amp; exercise that is not a randomized trial. Correlations may be worth paying attention to in other areas but <em>not</em> in health.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZpG9rheyAkgCoEQea": 1, "xHjy88N2uJvGdgzfw": 3, "vg4LDxjdwHLotCm8w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n7E2FC63MZGnvZAdr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 69, "extendedScore": null, "score": 0.000132, "legacy": true, "legacyId": "9159", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 69, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-11T06:00:36.117Z", "modifiedAt": null, "url": null, "title": "Recent popular books on human irrationality to recommend to your friends", "slug": "recent-popular-books-on-human-irrationality-to-recommend-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:58.000Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mMK3A3ohEDjejr4yj/recent-popular-books-on-human-irrationality-to-recommend-to", "pageUrlRelative": "/posts/mMK3A3ohEDjejr4yj/recent-popular-books-on-human-irrationality-to-recommend-to", "linkUrl": "https://www.lesswrong.com/posts/mMK3A3ohEDjejr4yj/recent-popular-books-on-human-irrationality-to-recommend-to", "postedAtFormatted": "Thursday, August 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Recent%20popular%20books%20on%20human%20irrationality%20to%20recommend%20to%20your%20friends&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARecent%20popular%20books%20on%20human%20irrationality%20to%20recommend%20to%20your%20friends%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmMK3A3ohEDjejr4yj%2Frecent-popular-books-on-human-irrationality-to-recommend-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Recent%20popular%20books%20on%20human%20irrationality%20to%20recommend%20to%20your%20friends%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmMK3A3ohEDjejr4yj%2Frecent-popular-books-on-human-irrationality-to-recommend-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmMK3A3ohEDjejr4yj%2Frecent-popular-books-on-human-irrationality-to-recommend-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 152, "htmlBody": "<p>&nbsp;</p>\n<ul>\n<li>Shore, <em><a href=\"http://www.amazon.com/Blunder-Smart-People-Make-Decisions/dp/B002VPE7TW/\">Blunder</a></em></li>\n<li>Kahneman, <em><a href=\"http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/\">Thinking, Fast and Slow</a></em></li>\n<li>Ariely, <em><a href=\"http://www.amazon.com/Predictably-Irrational-Revised-Expanded-Decisions/dp/0061353248/\">Predictably Irrational</a></em></li>\n<li>Koppel, <em><a href=\"http://www.amazon.com/Investing-Irrational-Mind-Optimism-Opportunities/dp/0071753370/\">Investing and the Irrational Mind</a></em></li>\n<li>Brafman &amp; Brafman, <em><a href=\"http://www.amazon.com/Sway-Irresistible-Pull-Irrational-Behavior/dp/0385530609/\">Sway</a></em></li>\n<li>Thaler &amp; Sunstein, <em><a href=\"http://www.amazon.com/Nudge-Improving-Decisions-Health-Happiness/dp/014311526X/\">Nudge</a></em></li>\n<li>Marcus, <em><a href=\"http://www.amazon.com/Kluge-Haphazard-Evolution-Human-Mind/dp/B002ECETZY/\">Kluge</a></em></li>\n<li>Kaplan &amp; Kaplan, <em><a href=\"http://www.amazon.com/Bozo-Sapiens-Why-Err-Human/dp/B005DIAM6Q/\">Bozo Sapiens</a></em></li>\n<li>Schulz, <em><a href=\"http://www.amazon.com/Being-Wrong-Adventures-Margin-Error/dp/0061176052/\">Being Wrong</a></em></li>\n<li>Tavris &amp; Aronson, <em><a href=\"http://www.amazon.com/Mistakes-Were-Made-But-Not/dp/0156033909/\">Mistakes Were Made (But Not By Me)</a></em></li>\n<li>Lehrer, <em><a href=\"http://www.amazon.com/How-We-Decide-Jonah-Lehrer/dp/0547247990/\">How We Decide</a></em></li>\n<li>Burton, <em><a href=\"http://www.amazon.com/Being-Certain-Believing-Right-Youre/dp/031254152X/\">On Being Certain</a></em></li>\n<li>Chabris &amp; Simons, <em><a href=\"http://www.amazon.com/Invisible-Gorilla-How-Intuitions-Deceive/dp/0307459667/\">The Invisible Gorilla</a></em></li>\n<li>Hallinan, <em><a href=\"http://www.amazon.com/Why-We-Make-Mistakes-Without/dp/0767928067/\">Why We Make Mistakes</a></em></li>\n<li>Fine, <em><a href=\"http://www.amazon.com/Mind-Its-Own-Distorts-Deceives/dp/0393331636/\">A Mind of Its Own</a></em></li>\n<li>Kida, <em><a href=\"http://www.amazon.com/Dont-Believe-Everything-You-Think/dp/1591024080/\">Don't Believe Everything You Think</a></em></li>\n<li>McRaney, <em><a href=\"http://www.amazon.com/You-Are-Not-So-Smart/dp/1592406599/\">You Are Not So Smart</a></em></li>\n</ul>\n<div>Veteran Less Wrongers are unlikely to find anything new here, but I've found the examples and exposition herein to be useful when I try to explain rationality concepts to other people in a fun and concise way.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mMK3A3ohEDjejr4yj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 17, "extendedScore": null, "score": 7.533601203011398e-07, "legacy": true, "legacyId": "9161", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-11T14:20:31.512Z", "modifiedAt": null, "url": null, "title": "An EPub of Eliezer's blog posts", "slug": "an-epub-of-eliezer-s-blog-posts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:07.579Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZYtwnKwXmEAWhm8dT/an-epub-of-eliezer-s-blog-posts", "pageUrlRelative": "/posts/ZYtwnKwXmEAWhm8dT/an-epub-of-eliezer-s-blog-posts", "linkUrl": "https://www.lesswrong.com/posts/ZYtwnKwXmEAWhm8dT/an-epub-of-eliezer-s-blog-posts", "postedAtFormatted": "Thursday, August 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20EPub%20of%20Eliezer's%20blog%20posts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20EPub%20of%20Eliezer's%20blog%20posts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZYtwnKwXmEAWhm8dT%2Fan-epub-of-eliezer-s-blog-posts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20EPub%20of%20Eliezer's%20blog%20posts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZYtwnKwXmEAWhm8dT%2Fan-epub-of-eliezer-s-blog-posts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZYtwnKwXmEAWhm8dT%2Fan-epub-of-eliezer-s-blog-posts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>Update 2015-03-21: I would now strongly recommend reading <a href=\"https://intelligence.org/rationality-ai-zombies/\">Rationality: From AI to Zombies</a>&nbsp;over this. Though the blog posts I collected here are the starting point for that book, considerable work has gone into selecting and arranging the essays as well as adding thoughtful new material and useful material not in this collection. Only if you've already read that should you consider starting on this; you can always skip the essays you've already read.</p>\n<p><a href=\"http://hacks.ciphergoth.org/lesswrong.epub\">This is all Eliezer's posts to Less Wrong up to the end of 2010 as an EPub.</a>&nbsp;Can be read with Aldiko and other eBook readers, though you might have to jump through some hoops on the Kindle (haven't tried it). I shared it privately with a few friends in the past, but I thought it might be more generally useful. &nbsp;Highlights include that all the screwed-up Unicode is fixed AFAIK.</p>\n<p><a href=\"http://hg.ciphergoth.org/scrape-sequences/\">Source code</a>.</p>\n<p>Update: have now made a <a href=\"http://hacks.ciphergoth.org/lesswrong.mobi\">MOBI for the Kindle too</a>.</p>\n<p>Updated 2011-08-13 17:20 BST: Now with images!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "JMD7LTXTisBzGAfhX": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZYtwnKwXmEAWhm8dT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 57, "extendedScore": null, "score": 9.1e-05, "legacy": true, "legacyId": "9166", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-11T16:25:36.746Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong Helsinki meetup", "slug": "meetup-less-wrong-helsinki-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:04.669Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kellopyy", "createdAt": "2009-02-27T04:17:07.285Z", "isAdmin": false, "displayName": "Kellopyy"}, "userId": "7BGKsJBzf9m8d2zWQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QBPsFByu33NMEepv3/meetup-less-wrong-helsinki-meetup", "pageUrlRelative": "/posts/QBPsFByu33NMEepv3/meetup-less-wrong-helsinki-meetup", "linkUrl": "https://www.lesswrong.com/posts/QBPsFByu33NMEepv3/meetup-less-wrong-helsinki-meetup", "postedAtFormatted": "Thursday, August 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20Helsinki%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20Helsinki%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQBPsFByu33NMEepv3%2Fmeetup-less-wrong-helsinki-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20Helsinki%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQBPsFByu33NMEepv3%2Fmeetup-less-wrong-helsinki-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQBPsFByu33NMEepv3%2Fmeetup-less-wrong-helsinki-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2a'>Less Wrong Helsinki meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 August 2011 06:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Ylioppilasaukio 5, Helsinki, Finland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're having a Less Wrong meetup at Cafe Picnic starting at 18:00 local time and anyone is welcome to keep a a short introduction to her/his matter of interest.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2a'>Less Wrong Helsinki meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QBPsFByu33NMEepv3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.535590892880569e-07, "legacy": true, "legacyId": "9167", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Helsinki_meetup\">Discussion article for the meetup : <a href=\"/meetups/2a\">Less Wrong Helsinki meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 August 2011 06:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Ylioppilasaukio 5, Helsinki, Finland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're having a Less Wrong meetup at Cafe Picnic starting at 18:00 local time and anyone is welcome to keep a a short introduction to her/his matter of interest.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Helsinki_meetup1\">Discussion article for the meetup : <a href=\"/meetups/2a\">Less Wrong Helsinki meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong Helsinki meetup", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Helsinki_meetup", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong Helsinki meetup", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Helsinki_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-12T08:32:21.766Z", "modifiedAt": null, "url": null, "title": "Where to report bugs on the new site?", "slug": "where-to-report-bugs-on-the-new-site", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:02.428Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sniffnoy", "createdAt": "2009-10-25T00:27:41.113Z", "isAdmin": false, "displayName": "Sniffnoy"}, "userId": "66EwcncPSoZ25StpW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Mpwisk75qAaDKb5mP/where-to-report-bugs-on-the-new-site", "pageUrlRelative": "/posts/Mpwisk75qAaDKb5mP/where-to-report-bugs-on-the-new-site", "linkUrl": "https://www.lesswrong.com/posts/Mpwisk75qAaDKb5mP/where-to-report-bugs-on-the-new-site", "postedAtFormatted": "Friday, August 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Where%20to%20report%20bugs%20on%20the%20new%20site%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhere%20to%20report%20bugs%20on%20the%20new%20site%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMpwisk75qAaDKb5mP%2Fwhere-to-report-bugs-on-the-new-site%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Where%20to%20report%20bugs%20on%20the%20new%20site%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMpwisk75qAaDKb5mP%2Fwhere-to-report-bugs-on-the-new-site", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMpwisk75qAaDKb5mP%2Fwhere-to-report-bugs-on-the-new-site", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 44, "htmlBody": "<p>I assume it's the same as before, but I don't recall where that is and it doesn't seem to be listed on the about page anymore.&nbsp; One of my comments will not mark as read on my userpage even though it will everywhere else...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Mpwisk75qAaDKb5mP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 7.538670326289755e-07, "legacy": true, "legacyId": "9177", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-12T13:03:27.106Z", "modifiedAt": null, "url": null, "title": "Easy money: Find me a new job, get $1000", "slug": "easy-money-find-me-a-new-job-get-usd1000", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:02.519Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Andrew_Taskify", "createdAt": "2011-04-24T22:12:33.578Z", "isAdmin": false, "displayName": "Andrew_Taskify"}, "userId": "RggiFApxMkJJY3PRn", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fPs4zovCqJCHQySuW/easy-money-find-me-a-new-job-get-usd1000", "pageUrlRelative": "/posts/fPs4zovCqJCHQySuW/easy-money-find-me-a-new-job-get-usd1000", "linkUrl": "https://www.lesswrong.com/posts/fPs4zovCqJCHQySuW/easy-money-find-me-a-new-job-get-usd1000", "postedAtFormatted": "Friday, August 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Easy%20money%3A%20Find%20me%20a%20new%20job%2C%20get%20%241000&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEasy%20money%3A%20Find%20me%20a%20new%20job%2C%20get%20%241000%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfPs4zovCqJCHQySuW%2Feasy-money-find-me-a-new-job-get-usd1000%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Easy%20money%3A%20Find%20me%20a%20new%20job%2C%20get%20%241000%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfPs4zovCqJCHQySuW%2Feasy-money-find-me-a-new-job-get-usd1000", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfPs4zovCqJCHQySuW%2Feasy-money-find-me-a-new-job-get-usd1000", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 572, "htmlBody": "<p>I normally post under a different name, which you can find by looking at my posting history, specifically, <a href=\"/r/discussion/lw/5em/join_the_special_relocation_task_force/\">this thread</a>.&nbsp; I need to keep the search somewhat secret, so I'm adding a trivial barrier to identifying me.<br /><br />Anyway, I haven't been very good at job searches, and I have spare money, so I figured I'd just outsource the effort (in parallel with my own continuing efforts, of course).&nbsp; In theory, this shouldn't even be difficult, because I'm <em>already </em>employed, very well, in my area of expertise, and have been for over six years, and am only trying to switch because I can't stand my current city.&nbsp; (And a sense that I could be making more of my potential.)<br /><br />But practice is different, and that's where you come in.&nbsp; Here's the deal: If you find me a job matching my skills and qualifications (and located per the next paragraph), and I'm there for a month, you get $1000.&nbsp; And to avoid making it an all-or-nothing deal, if you just get me to the interview stage, you get $100.&nbsp; You already know me from my posts here, so if you know someone who could use me, please connect the two of us.</p>\n<p>I would like for it to be in (in decreasing order of preference) Austin, Houston, NYC, or San Francisco, or at least allow me to live there while earning income from the job.&nbsp; However, any city with a LW community is fine as long as you clear it with me first.</p>\n<p>So, here are my skill and qualifications (resume file available, just PM or email me):<br /><br />- Visual Studio with C++, C#, and Visual Web Developer; MATLAB programming, including Simulink and digital signal processing; finite-element modeling with MSC Patran and Nastran (and augmented by MATLAB); MathCAD; Mathematica; passed Fundamentals of Engineering Exam, earning EIT (Engineer-in-Training) license; perfect scores on GRE analytical and quantitative.<br /><br />In my six years as an aerospace engineer, I:<br /><br />- Created and refined finite element models (FEMs) of aircraft (pre- and post-modification) to predict structural response to loading; used both Patran and, at times, MATLAB, to create these models, and Nastran to compute their output.<br /><br />- Participated in monitoring flight test telemetry for anomalous readings, compared test results to our aircraft FEMs to determine model accuracy, and updated parameters and assumptions to best account for such data.<br /><br />- Wrote MATLAB software packages to automate the process of analyzing digital signals representing aircraft state and airframe vibrations and presenting relevant plots.<br /><br />- Performed hand analysis of processed model results to do &ldquo;sanity checks&rdquo; of the model and to infer more detailed information about subsystems, such as their margins of safety.<br /><br />- Programmed intranet database websites with Visual Studio 2008 and Visual Web Developer.</p>\n<p>- Refactored colleagues' code to optimize for readability, robustness, and minimal dependencies.<br /><br />- Constructed mass models of aircraft, tracking changes in weight and c.g. from payload and fuel configurations, and deriving new distributions of masses to reconcile with known data.</p>\n<p>&nbsp;</p>\n<hr />\n<p>Also, the job doesn't necessarily have to be in engineering.&nbsp; I'm interested in shifting to software development, but my experience so far is with scripts rather than full application development.&nbsp; I would be happy with some kind of apprenticeship arrangement to someone who makes money as a freelance developer.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fPs4zovCqJCHQySuW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 7.539534238127459e-07, "legacy": true, "legacyId": "9178", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qH5pz6a9hLaguLFbh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-12T14:04:14.871Z", "modifiedAt": null, "url": null, "title": "Munich Meetup, Saturday September 10th, 2PM", "slug": "munich-meetup-saturday-september-10th-2pm", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:02.458Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wallowinmaya", "createdAt": "2011-03-21T00:39:18.855Z", "isAdmin": false, "displayName": "David Althaus"}, "userId": "xY8DDzk6TyvRroJEo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ipHcA72wTCXQbABG7/munich-meetup-saturday-september-10th-2pm", "pageUrlRelative": "/posts/ipHcA72wTCXQbABG7/munich-meetup-saturday-september-10th-2pm", "linkUrl": "https://www.lesswrong.com/posts/ipHcA72wTCXQbABG7/munich-meetup-saturday-september-10th-2pm", "postedAtFormatted": "Friday, August 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Munich%20Meetup%2C%20Saturday%20September%2010th%2C%202PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMunich%20Meetup%2C%20Saturday%20September%2010th%2C%202PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FipHcA72wTCXQbABG7%2Fmunich-meetup-saturday-september-10th-2pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Munich%20Meetup%2C%20Saturday%20September%2010th%2C%202PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FipHcA72wTCXQbABG7%2Fmunich-meetup-saturday-september-10th-2pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FipHcA72wTCXQbABG7%2Fmunich-meetup-saturday-september-10th-2pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<p><strong>When:</strong> Saturday, September 10th, 2PM.</p>\n<p><strong>Where: </strong>Munich Central Station, Coffee Fellows cafe, Bahnhofsplatz 2, <em>First floor</em>. I will be there with a Lesswrong sign. If you can't find it, you can call me: 0160-93132663 .</p>\n<p>&nbsp;</p>\n<p>According to this <a href=\"http://www.doodle.com/u49xxi6z4zqbihqa\">doodle-survey</a> 5 people (including me) could attend. And of course <em>you</em> are welcome too!</p>\n<p>If the cafe sucks, we could easily go elsewhere. I've merely chosen the place, because it's relatively nice, near the central station and easy to find.</p>\n<p>&nbsp;</p>\n<p>Lurkers and newbies are very welcome!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ipHcA72wTCXQbABG7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 7.539728008222093e-07, "legacy": true, "legacyId": "9179", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-12T15:38:32.282Z", "modifiedAt": null, "url": null, "title": "Overcoming bias in others", "slug": "overcoming-bias-in-others", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:04.346Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "homunq", "createdAt": "2009-05-08T21:56:32.475Z", "isAdmin": false, "displayName": "homunq"}, "userId": "FZoKpxRbKw5g32eiY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SfmW48GLkSAJHYQhp/overcoming-bias-in-others", "pageUrlRelative": "/posts/SfmW48GLkSAJHYQhp/overcoming-bias-in-others", "linkUrl": "https://www.lesswrong.com/posts/SfmW48GLkSAJHYQhp/overcoming-bias-in-others", "postedAtFormatted": "Friday, August 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Overcoming%20bias%20in%20others&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOvercoming%20bias%20in%20others%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSfmW48GLkSAJHYQhp%2Fovercoming-bias-in-others%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Overcoming%20bias%20in%20others%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSfmW48GLkSAJHYQhp%2Fovercoming-bias-in-others", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSfmW48GLkSAJHYQhp%2Fovercoming-bias-in-others", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<p>Say that you are observing someone in a position of power. You have good reason to believe that this person is falling prey to a known cognitive bias, and that this will tend to affect you negatively. You also can tell that the person is more than intelligent enough to understand their mistake, if they were motivated to do so. You have an opportunity to say one thing to the person - around 500 words of argument. They will initially perceive you as a low-status member of their own tribe. The power differential is extreme enough that, after they have attended this one thing, they will never pay any attention to you again. What can you do to best disrupt their bias?</p>\n<p>This is clearly a setup where the odds are against you. Still, what kind of strategies would give you the best odds? I've deliberately made the situation vague, so as to emphasize abstract strategies. If certain strategies would work best against certain biases or personality types, feel free to state it in your answer.</p>\n<p>I'm making this a post of its own because I find here much more discussion of how to overcome or subvert your own biases, somewhat less of how to recruit rationalists, and almost none of how to try to overcome a specific bias in another person without necessarily converting them into a committed rationalist overall.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SfmW48GLkSAJHYQhp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 11, "extendedScore": null, "score": 7.54002854873328e-07, "legacy": true, "legacyId": "9180", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-12T17:32:34.253Z", "modifiedAt": null, "url": null, "title": "\"Meetup\" proposal: Google+", "slug": "meetup-proposal-google", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:09.362Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "odm3FHPzgbiGpWsSg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KL6KHM2jzZsz7GzPx/meetup-proposal-google", "pageUrlRelative": "/posts/KL6KHM2jzZsz7GzPx/meetup-proposal-google", "linkUrl": "https://www.lesswrong.com/posts/KL6KHM2jzZsz7GzPx/meetup-proposal-google", "postedAtFormatted": "Friday, August 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Meetup%22%20proposal%3A%20Google%2B&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Meetup%22%20proposal%3A%20Google%2B%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKL6KHM2jzZsz7GzPx%2Fmeetup-proposal-google%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Meetup%22%20proposal%3A%20Google%2B%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKL6KHM2jzZsz7GzPx%2Fmeetup-proposal-google", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKL6KHM2jzZsz7GzPx%2Fmeetup-proposal-google", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 237, "htmlBody": "<p>Not all of us live near big cities. I know of only one Less Wronger in the Grand Rapids, MI area apart from myself, and the nearest existing meetup is in Madison, WI, across Lake Michigan. I'd like to go to a Less Wrong meetup with more than just a couple of people, but I don't want to have to drive six hours each way.</p>\n<p>I have tried Google+'s \"Hangouts\" feature, and it's really quite nice. It's a many-way video chat system, so it goes without saying that if you've tried video chat before, Hangouts will be <em>quite</em> similar. It also lets you play YouTube videos in synchrony. In short, if you like sitting on the couch with people watching TV, Hangouts are perfect for you.</p>\n<p>So, I propose having a meetup on Google+ on Saturday, August 27, at 5:00 PM EDT (21:00 UTC). The date and time can be changed, of course, but I don't want to commit to anything on the 20th. Search for my name (Tanner Swett) at that time, and hopefully you'll see a hangout listed. I will try to be around for at least an hour.</p>\n<p>Naturally, ideas for stuff to do would be appreciated. Perhaps we could watch some TED talks, or chat about a specific subject, or play some game. I don't really have much hanging-out experience to go on here.</p>\n<p>Any thoughts? Should we meet at a different time? What should we do?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"T57Qd9J3AfxmwhQtY": 1, "EXgFbrqoRRkCRgnDy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KL6KHM2jzZsz7GzPx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 28, "extendedScore": null, "score": 7.540392045302848e-07, "legacy": true, "legacyId": "9181", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-12T18:52:20.375Z", "modifiedAt": null, "url": null, "title": "Anthropics Does Not Work LIke That", "slug": "anthropics-does-not-work-like-that", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:02.816Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pavitra", "createdAt": "2009-09-22T08:32:44.250Z", "isAdmin": false, "displayName": "Pavitra"}, "userId": "yC2JgX3ENu7mionKh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HMbEzJsAuHs8ashmD/anthropics-does-not-work-like-that", "pageUrlRelative": "/posts/HMbEzJsAuHs8ashmD/anthropics-does-not-work-like-that", "linkUrl": "https://www.lesswrong.com/posts/HMbEzJsAuHs8ashmD/anthropics-does-not-work-like-that", "postedAtFormatted": "Friday, August 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anthropics%20Does%20Not%20Work%20LIke%20That&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnthropics%20Does%20Not%20Work%20LIke%20That%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHMbEzJsAuHs8ashmD%2Fanthropics-does-not-work-like-that%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anthropics%20Does%20Not%20Work%20LIke%20That%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHMbEzJsAuHs8ashmD%2Fanthropics-does-not-work-like-that", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHMbEzJsAuHs8ashmD%2Fanthropics-does-not-work-like-that", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 111, "htmlBody": "<p>People around here seem to think that a recent series of near-misses, such as not destroying the world in the Cold War, is evidence in favor of quantum immortality.</p>\n<p>This fails to appreciate that the anthropic selection bias has no limit on how far back it can make things retroactively seem to happen. If, as has been suggested, a majority of the Everett branches from our 1950 destroyed the world, then it is equally true that a majority of the Everett branches from our 1750 in which there is someone still alive in 2010 failed to contain probably-world-destroying technology.</p>\n<p>The existence of x-risk near-miss events should be taken as evidence <em>against</em> quantum immortality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HMbEzJsAuHs8ashmD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -3, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "9182", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-12T20:41:41.585Z", "modifiedAt": null, "url": null, "title": "A Problem with Abbreviations and Acronyms", "slug": "a-problem-with-abbreviations-and-acronyms", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:58.624Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "billswift", "createdAt": "2009-02-28T05:59:56.578Z", "isAdmin": false, "displayName": "billswift"}, "userId": "WBoeSNFkZ4q8nRenK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sJnsniH3TiqFxifAK/a-problem-with-abbreviations-and-acronyms", "pageUrlRelative": "/posts/sJnsniH3TiqFxifAK/a-problem-with-abbreviations-and-acronyms", "linkUrl": "https://www.lesswrong.com/posts/sJnsniH3TiqFxifAK/a-problem-with-abbreviations-and-acronyms", "postedAtFormatted": "Friday, August 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Problem%20with%20Abbreviations%20and%20Acronyms&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Problem%20with%20Abbreviations%20and%20Acronyms%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsJnsniH3TiqFxifAK%2Fa-problem-with-abbreviations-and-acronyms%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Problem%20with%20Abbreviations%20and%20Acronyms%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsJnsniH3TiqFxifAK%2Fa-problem-with-abbreviations-and-acronyms", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsJnsniH3TiqFxifAK%2Fa-problem-with-abbreviations-and-acronyms", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 200, "htmlBody": "<p>I posted this in the comments at Eric Raymond's blog:</p>\n<blockquote>\n<p>I am really tired of three letter acronyms. There are too few alternatives, so even though there may not be much confusion in context, the first thing that springs to mind is not likely to be correct, which brings you to a screeching halt while you think about it &ndash; like a confusing mis-spelling of a word.</p>\n<p>For example, I <strong>hate</strong> rms, meaning Stallman, because every time I see it I first think root-mean-square which I was familiar with long before I ever used a computer. And I used to live in Prince George&rsquo;s county Maryland, so everything starting with PG brings up the wrong initial response.</p>\n</blockquote>\n<p>Then I realized some here may find it useful.</p>\n<p>Extensive use of abbreviations and acronyms was primarily a convenience for writers, when writing was done by hand and then by typewriter, there is less justification for it now when most writing is done by computer.&nbsp; And as my comment points out it is usually a negative for readers. It does benefit readers when you can convert a long phrase into a readable word, SCUBA and LASER spring to mind, but that doesn't occur often.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sJnsniH3TiqFxifAK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 7, "extendedScore": null, "score": 7.540994968735707e-07, "legacy": true, "legacyId": "9183", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-13T01:53:17.622Z", "modifiedAt": null, "url": null, "title": "Does quantum mechanics make simulations negligible?", "slug": "does-quantum-mechanics-make-simulations-negligible", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:04.062Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "homunq", "createdAt": "2009-05-08T21:56:32.475Z", "isAdmin": false, "displayName": "homunq"}, "userId": "FZoKpxRbKw5g32eiY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bSxFXyLvwT4fuMRrr/does-quantum-mechanics-make-simulations-negligible", "pageUrlRelative": "/posts/bSxFXyLvwT4fuMRrr/does-quantum-mechanics-make-simulations-negligible", "linkUrl": "https://www.lesswrong.com/posts/bSxFXyLvwT4fuMRrr/does-quantum-mechanics-make-simulations-negligible", "postedAtFormatted": "Saturday, August 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20quantum%20mechanics%20make%20simulations%20negligible%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20quantum%20mechanics%20make%20simulations%20negligible%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbSxFXyLvwT4fuMRrr%2Fdoes-quantum-mechanics-make-simulations-negligible%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20quantum%20mechanics%20make%20simulations%20negligible%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbSxFXyLvwT4fuMRrr%2Fdoes-quantum-mechanics-make-simulations-negligible", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbSxFXyLvwT4fuMRrr%2Fdoes-quantum-mechanics-make-simulations-negligible", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<p>I've written a prior post about how I think that the Everett branching factor of reality dominates that of any plausible simulation, whether the latter is run on a Von Neumann machine, on a quantum machine, or on some hybrid; and thus the probability and utility weight that should be assigned to simulations in general is negligible. I also argued that the fact that we live in an apparently quantum-branching world could be construed as weak anthropic evidence for this idea. My prior post was down-modded into oblivion for reasons that are not relevant here (style, etc.) If I were to replace this text you're reading with a&nbsp;version of that idea which was&nbsp;more fully-argued, but still stylistically-neutral (unlike my prior post), would people be interested?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bSxFXyLvwT4fuMRrr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 0, "extendedScore": null, "score": 7.541988540776377e-07, "legacy": true, "legacyId": "9184", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-13T05:23:25.194Z", "modifiedAt": null, "url": null, "title": "Magic Tricks Revealed: Test Your Rationality", "slug": "magic-tricks-revealed-test-your-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:05.773Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ib3oL2kmev6x9jrw7/magic-tricks-revealed-test-your-rationality", "pageUrlRelative": "/posts/ib3oL2kmev6x9jrw7/magic-tricks-revealed-test-your-rationality", "linkUrl": "https://www.lesswrong.com/posts/ib3oL2kmev6x9jrw7/magic-tricks-revealed-test-your-rationality", "postedAtFormatted": "Saturday, August 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Magic%20Tricks%20Revealed%3A%20Test%20Your%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMagic%20Tricks%20Revealed%3A%20Test%20Your%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fib3oL2kmev6x9jrw7%2Fmagic-tricks-revealed-test-your-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Magic%20Tricks%20Revealed%3A%20Test%20Your%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fib3oL2kmev6x9jrw7%2Fmagic-tricks-revealed-test-your-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fib3oL2kmev6x9jrw7%2Fmagic-tricks-revealed-test-your-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 585, "htmlBody": "<p>In <a href=\"/lw/ip/fake_explanations/\">Fake Explanations</a>, Yudkowsky offered a story that has stuck in my mind:</p>\n<blockquote>\n<p>Once upon a time, there was an instructor who taught physics students. &nbsp;One day she called them into her class, and showed them a wide, square plate of metal, next to a hot radiator. &nbsp;The students each put their hand on the plate, and found that the side next to the radiator cool, and the distant side warm. &nbsp;And the instructor said, <em>Why do you think this happens?</em>&nbsp; Some students guessed convection of air currents, and others guessed strange metals in the plate. &nbsp;They devised many creative explanations, none stooping so low as to say \"I don't know\" or \"<a href=\"/lw/if/your_strength_as_a_rationalist/\">This seems impossible.</a>\"</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>In this story it is also telling that in the many thoughts and explanations that surfaced, the idea \"the teacher turned the plate around\" was never considered. &nbsp;The students failed to see the correct answer because they weren't thinking creatively enough. &nbsp;While the correct approach in this situation is indeed to notice your confusion, a worthwhile approach still could be to list all the possible solutions you think could be the answer. &nbsp;(And of course only list real solutions that you actually understand, not fake ones.)</p>\n<p>So how can we improve this ability to expand our creativity when it comes to considering explanations, so things like \"the teacher turned the plate around\" enters our list of considerations?</p>\n<p>One possible answer: study magic tricks.</p>\n<p>&nbsp;</p>\n<p>In addition to <a href=\"http://www.greatplay.net\">writing</a> and reading stuff on the internet, another hobby I like to indulge in is doing magic tricks with a deck of cards. &nbsp;Many of the tricks I know are very impressive, such as making cards switch places or appearing to read people's minds. However, a lot of the tricks I know are very stunningly simple; some of them don't even involve slight of hand, and could be done by ten-year-olds with little practice. &nbsp;They're just that <em>that</em>&nbsp;<em>cleverly crafted</em>.</p>\n<p>I learned a lot of these tricks from YouTube -- many videos will show you a trick and then teach you how it is done. &nbsp;Personally, I don't find the revelation of a trick to take away any of my enjoyment, because I find <a href=\"/lw/or/joy_in_the_merely_real/\">joy in the merely real</a>, and care little for perpetuating mystery.</p>\n<p>However, these YouTube videos for how tricks are done also provide a very effective test for your rationality: watch the trick on the video, and after it is done, pause the video. &nbsp;<a href=\"/lw/ka/hold_off_on_proposing_solutions/\">Spend a good amount of time thinking through the trick</a>, and then finally start thinking through ways you think the trick was done. &nbsp;Only after you have your guesses should you learn how the trick actually was done.</p>\n<p>I find that in doing this, I would quickly learn how to think creatively, and found that this not only allowed me to much more effectively figure out how card tricks were done before actually hearing the solutions, but also found that this creativity allowed me to become better at suggesting further hypotheses to other previously confusing situations, as well as becoming better at deliberating to solutions in previously&nbsp;intractable&nbsp;problems.</p>\n<p>Not to mention that I managed to learn some rather impressive card tricks.</p>\n<p>&nbsp;</p>\n<p>Good YouTube Magic Lessons:</p>\n<ul>\n<li><a href=\"http://www.youtube.com/user/julianmather1\">Julian's School of Magic</a> (Beginner level)</li>\n<li><a href=\"http://www.youtube.com/user/mismag822\">The Card Trick Teacher</a>&nbsp;(Beginner to Intermediate level)</li>\n<li><a href=\"http://www.youtube.com/user/mihirthedemon03\">Mihirthedemon03's Tricks</a> (Intermediate level)</li>\n<li><a href=\"http://www.youtube.com/user/EncyclopediaOfMagic\">Encyclopedia of Magic</a> (Intermediate to Advanced level)</li>\n</ul>\n<div>And in the course of exploring the related videos, there are thousands of tricks to learn, and thousands of opportunities to test your rationality. &nbsp;Not all of them are great quality, though.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "SJFsFfFhE6m2ThAYJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ib3oL2kmev6x9jrw7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 42, "extendedScore": null, "score": 7.542658681252521e-07, "legacy": true, "legacyId": "9187", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fysgqk4CjAwhBgNYT", "5JDkW4MYXit2CquLs", "x4dG4GhpZH2hgz59x", "uHYYA32CKgKT3FagE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-14T03:24:28.064Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Making History Available", "slug": "seq-rerun-making-history-available", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:02.853Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/k5wipSpkwNTKNfo5Q/seq-rerun-making-history-available", "pageUrlRelative": "/posts/k5wipSpkwNTKNfo5Q/seq-rerun-making-history-available", "linkUrl": "https://www.lesswrong.com/posts/k5wipSpkwNTKNfo5Q/seq-rerun-making-history-available", "postedAtFormatted": "Sunday, August 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Making%20History%20Available&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Making%20History%20Available%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk5wipSpkwNTKNfo5Q%2Fseq-rerun-making-history-available%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Making%20History%20Available%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk5wipSpkwNTKNfo5Q%2Fseq-rerun-making-history-available", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk5wipSpkwNTKNfo5Q%2Fseq-rerun-making-history-available", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 203, "htmlBody": "<p>Today's post, <a href=\"/lw/j0/making_history_available/\">Making History Available</a> was originally published on 31 August 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It's easy not to take the lessons of history seriously; our brains aren't well-equipped to translate dry facts into experiences. But imagine living through the whole of human history - imagine watching mysteries be explained, watching civilizations rise and fall, being surprised over and over again - and you'll be less shocked by the strangeness of the next era.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/71c/seq_rerun_failing_to_learn_from_history/\">Failing to Learn from History</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "k5wipSpkwNTKNfo5Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 7.546874219662804e-07, "legacy": true, "legacyId": "9195", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TLKPj4GDXetZuPDH5", "Srh4fQMnALWD7znCJ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-14T10:23:45.930Z", "modifiedAt": null, "url": null, "title": "Take heed, for it is a trap", "slug": "take-heed-for-it-is-a-trap", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:27.215Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Zed", "createdAt": "2011-06-30T16:17:44.299Z", "isAdmin": false, "displayName": "Zed"}, "userId": "yfFpxacwgh3mANtDy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TRpnMz8xr7gwCyZKS/take-heed-for-it-is-a-trap", "pageUrlRelative": "/posts/TRpnMz8xr7gwCyZKS/take-heed-for-it-is-a-trap", "linkUrl": "https://www.lesswrong.com/posts/TRpnMz8xr7gwCyZKS/take-heed-for-it-is-a-trap", "postedAtFormatted": "Sunday, August 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Take%20heed%2C%20for%20it%20is%20a%20trap&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATake%20heed%2C%20for%20it%20is%20a%20trap%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTRpnMz8xr7gwCyZKS%2Ftake-heed-for-it-is-a-trap%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Take%20heed%2C%20for%20it%20is%20a%20trap%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTRpnMz8xr7gwCyZKS%2Ftake-heed-for-it-is-a-trap", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTRpnMz8xr7gwCyZKS%2Ftake-heed-for-it-is-a-trap", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1708, "htmlBody": "<p>If you have worked your way through most of the sequences you are likely to agree with the majority of these statements:</p>\n<ul>\n<li>When people die we should cut off their heads so we can preserve those heads and make the person come back to life in the (far far) future.</li>\n<li>It is possible to run a person on Conways Game of Life. This would be a person as real as you or me, and wouldn't be able to tell he's in a virtual world because it looks exactly like ours.</li>\n<li>Right now there exist many copies/clones of you, some of which are blissfully happy and some of which are being tortured and we should not care about this at all.</li>\n<li>Most scientists disagree with this but that's just because it sounds&nbsp;counter-intuitive&nbsp;and scientists are biased against counterintuitive explanations.</li>\n<li>Besides, the scientific method is wrong because it is in conflict with probability theory. Oh, and probability is created by humans, it doesn't exist in the universe.</li>\n<li>Every fraction of a second you split into thousands of copies of yourself. Of course you cannot detect these copies scientifically, but that because science is wrong and stupid.</li>\n<li>In fact, it's not just people that split but the entire universe splits over and over.</li>\n<li>Time isn't real. There is no flow of time from 0 to now. All your future and past selves just exist.&nbsp;</li>\n<li>Computers will soon become so fast that AI researchers will be able to create an artificial intelligence that's smarter than any human. When this happens humanity will probably be wiped out.</li>\n<li>To protect us against computers destroying humanity we must create a super-powerful computer intelligence that won't destroy humanity.</li>\n<li>Ethics are very important and we must take extreme caution to make sure we do the right thing. Also, we sometimes prefer torture to dust-specs.</li>\n<li>If everything goes to plan a super computer will solve all problems (disease, famine, aging) and turn us into super humans who can then go on to explore the galaxy and have fun.</li>\n<li>And finally, the truth of all these statements is completely obvious to those who take the time to study the underlying arguments. People who disagree are just dumb, irrational, miseducated or a combination thereof.&nbsp;</li>\n<li>I learned this all from this website by these guys who want us to give them our money.</li>\n</ul>\n<p>In two words: crackpot beliefs.</p>\n<p>These statements cover only a fraction of the sequences and although they're deliberately phrased to incite kneejerk disagreement and ugh-fields I think most LW readers will find themselves in agreement with almost all of them. And If not then you can always come up with better examples that illustrate some of your non-mainstream beliefs.</p>\n<p>Think back for a second to your pre-bayesian&nbsp;days. Think back to the time before your exposure to the sequences. Now the question is, what estimate would you have given that <em>any chain of arguments</em> could persuade you the statements above are true? In my case, it would be near zero.</p>\n<p>You can take somebody who likes philosophy and is familiar with the different streams and philosophical dilemmas, who knows computation theory and classical physics, who has a good understanding of probability and math and somebody who is a naturally curious reductionist. And this person will still roll his eyes and will sarcastically dismiss the ideas enumerated above. After all, these are crackpot ideas, and people who believe them are so far \"out there\", they cannot be reasoned with!</p>\n<p>That is really the bottom line here. You cannot explain the beliefs that follow from the sequences because they have too many dependencies and even if you did have time to go through all the necessary dependencies explaining a belief is still an order of magnitude more difficult than following the explanation written down by somebody else because in order to explain something you have to juggle two mental models: your own and the one of the listener.</p>\n<p>Some of the sequences touches on the concept of the cognitive gap (inferential distance). We have all learned this the hard way that we can't expect people to just&nbsp;<a href=\"/lw/ke/illusion_of_transparency_why_no_one_understands/\">understand what we say</a>&nbsp;and we can't expect&nbsp;<a href=\"/lw/kg/expecting_short_inferential_distances/\">short inferential distances</a>.&nbsp;In&nbsp;practice&nbsp;there is just no way to bridge the cognitive gap. This isn't a big deal for most educated people, because people don't expect to understand complex arguments in other people's fields and all educated intellectuals are on the same team anyway (well, most of the time). For crackpot LW beliefs it's a whole different story though. I suspect most of us have found that out the hard way.</p>\n<p>Rational Rian: What do you think is going to happen to the economy?</p>\n<p>Bayesian Bob: I'm not sure. I think Krugman believes that a bigger cash injection is needed to prevent a second dip.</p>\n<p>Rational Rian: Why do you always say what other people think, what's your opinion?</p>\n<p>Bayesian&nbsp;Bob: I can't really distinguish between good economic reasoning and flawed economic reasoning because I'm a lay man. So I tend to go with what Krugman writes, unless I have a good reason to believe he is wrong. I don't really have strong opinions about the economy, I just go with the evidence I have.</p>\n<p>Rational Rian: Evidence? You mean his opinion.</p>\n<p>Bayesian&nbsp;Bob: Yep.</p>\n<p>Rational Rian: Eh? Opinions aren't evidence.</p>\n<p>Bayesian&nbsp;Bob: (Whoops, now I have to either explain the nature of evidence on the spot or Rian will think I'm an idiot with crazy beliefs. Okay then, here goes.) An opinion reflects the belief of the expert. These beliefs can either be uncorrelated with reality, negatively correlated or positively correlated. If there is absolutely no relation between what an expert believes and what is true then, sure, it wouldn't count as evidence. However, it turns out that experts mostly believe true things (that's why they're called experts) and so the beliefs of an expert are positively correlated with reality and thus his opinion counts as evidence.</p>\n<p>Rational Rian: That doesn't make sense. It's still just an opinion. Evidence comes from experiments.</p>\n<p>Bayesian&nbsp;Bob: Yep, but experts have either done experiments themselves or read about experiments other people have done. That's what their opinions are based on. Suppose you take a random scientific statement, you have no idea what it is, and the only thing you know is that 80% of the top researchers in that field agree with that statement, would you then assume the statement is probably true? Would the agreement of these scientists be evidence for the truth of the statement?</p>\n<p>Rational Rian: That's just an argument ad populus! Truth isn't governed by majority opinion! It is just religious nonsense that if enough people believe something then there must be some some truth to it.</p>\n<p>Bayesian&nbsp;Bob: (Ad populum! Populum! Ah, crud, I should've phrased that more carefully.) I don't mean that majority opinion <em>proves</em> that the statement is true, it's just evidence in favor of it. If there is counterevidence the scale can tip the other way. In the case of religion there is overwhelming counterevidence. Scientifically speaking religion is clearly false, no disagreement there.</p>\n<p>Rational Rian: There's scientific counterevidence for religion? Science can't prove non-existence. You know that!</p>\n<p>Bayesian&nbsp;Bob: (Oh god, not this again!) Absence of evidence is evidence of absence.</p>\n<p>Rational Rian: Counter-evidence is not the same as absence of evidence! Besides, stay with the point, science can't prove a negative.</p>\n<p>Bayesian&nbsp;Bob: The certainty of our beliefs should be proportional to amount of evidence we have in favor of the belief. Complex beliefs require more evidence than simple beliefs, and the laws of probability, Bayes specifically, tell us how to weigh new evidence. A statement, any statement, starts out with a 50% probability of being true, and then you adjust that percentage based on the evidence you come into contact with. (I shouldn't have said that 50% part. There's no way that's going to go over well. I'm such an idiot.)</p>\n<p>Rational Rian: A statement without evidence is 50% likely to be true!? Have you forgotten everything from math class? This doesn't make sense on so many levels, I don't even know where to start!</p>\n<p>Bayesian&nbsp;Bob: (There's no way to rescue this. I'm going to cut my losses.) I meant that in a vacuum we should believe it with 50% certainty, not that any arbitrary statement is 50% likely to accurately reflect reality. But no matter. Let's just get something to eat, I'm hungry.</p>\n<p>Rational Rian: So we should believe something even if it's unlikely to be true? That's just stupid. Why do I even get into these conversations with you? *sigh* ... So, how about Subway?</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>The moral here is that crackpot beliefs are low status. Not just low-status like believing in a deity, but majorly low status. When you believe things that are perceived as crazy and when you can't explain to people why you believe what you believe then the only result is that people will see you as \"that crazy guy\". They'll wonder, behind your back, why a smart person can have such stupid beliefs. Then they'll conclude that intelligence doesn't protect people against religion either so there's no point in trying to talk about it.</p>\n<p>If you fail to conceal your low-status beliefs you'll be punished for it socially. If you think that they're in the wrong and that you're in the right, then you missed the point. This isn't about right and wrong, this is about anticipating the consequences of your behavior. If you choose to to talk about outlandish beliefs when you know you cannot convince people that your belief is justified then you hurt your credibility and you get nothing for it in exchange. You cannot repair the damage easily, because even if your friends are patient and willing to listen to your complete reasoning you'll (accidently) expose three even crazier beliefs you have.</p>\n<p>An important life skill is the ability to get along with other people and to not expose yourself as a weirdo when this isn't in your interest to do so. So take heed and choose your words wisely, lest you fall into the trap.</p>\n<p>&nbsp;</p>\n<hr />\n<p><strong>EDIT - </strong><a style=\"font-weight: bold;\" href=\"https://spreadsheets.google.com/spreadsheet/viewform?formkey=dEUyb09CLWtFLWhyOFotSG9WUklQWXc6MQ\">Google Survey</a> by Pfft</p>\n<p>PS: intended for /main but since this is my first serious post I'll put it in discussion first to see if it's considered sufficiently insightful.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YQW2DxpZFTrqrxHBJ": 1, "9YFoDPFwMoWthzgkY": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TRpnMz8xr7gwCyZKS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 79, "baseScore": 56, "extendedScore": null, "score": 0.000109, "legacy": true, "legacyId": "9196", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 56, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 189, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sSqoEw9eRP2kPKLCz", "HLqWn5LASfhhArZ7w"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-14T13:15:34.255Z", "modifiedAt": null, "url": null, "title": "How good an emulation is required?", "slug": "how-good-an-emulation-is-required", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:04.463Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whpearson", "createdAt": "2009-02-28T00:34:00.976Z", "isAdmin": false, "displayName": "whpearson"}, "userId": "bq8qsRbPNvFihHxgi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cZidxqjtPJGpB4HZo/how-good-an-emulation-is-required", "pageUrlRelative": "/posts/cZidxqjtPJGpB4HZo/how-good-an-emulation-is-required", "linkUrl": "https://www.lesswrong.com/posts/cZidxqjtPJGpB4HZo/how-good-an-emulation-is-required", "postedAtFormatted": "Sunday, August 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20good%20an%20emulation%20is%20required%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20good%20an%20emulation%20is%20required%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcZidxqjtPJGpB4HZo%2Fhow-good-an-emulation-is-required%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20good%20an%20emulation%20is%20required%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcZidxqjtPJGpB4HZo%2Fhow-good-an-emulation-is-required", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcZidxqjtPJGpB4HZo%2Fhow-good-an-emulation-is-required", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>Reading <a href=\"http://arstechnica.com/gaming/news/2011/08/accuracy-takes-power-one-mans-3ghz-quest-to-build-a-perfect-snes-emulator.ars\">this article on requiring lots of processing power</a> to emulate the snes accurately, made me think that we will likely have similar issues when emulating humans.&nbsp;</p>\n<p>I'd imagine weird timing and chemical interactions being used by the brain as it is an adaptable system and might be able adapt to use them if they turn out to be helpful.</p>\n<p>This suggested to me a few issues with no easy answers that I could see.</p>\n<p>\n<div>\n<ul>\n<li>Is it better to emulate 1 human faithfully or 10 humans with occasional glitches (for example could no longer appreciate music in the same way)</li>\n<li>How glitch free would you want the emulation to be before you gave up your body.&nbsp;</li>\n<li>How glitch free would you want the emulation to be before letting it use heavy machinery.</li>\n<li>How glitch free would you want the emulation to be before you had it working on FAI.</li>\n</ul>\n<div>Also please ignore the 3Ghz vs 25Mhz comparison, it perpetuates the myth that computational power is about clock speed and not operations per second and memory bandwidth.</div>\n</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cZidxqjtPJGpB4HZo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 7.548761813888912e-07, "legacy": true, "legacyId": "9197", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-14T18:08:21.066Z", "modifiedAt": null, "url": null, "title": "Cryonics is Quantum Suicide (minus the suicide)", "slug": "cryonics-is-quantum-suicide-minus-the-suicide", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:03.098Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KPier", "createdAt": "2011-05-29T20:37:16.788Z", "isAdmin": false, "displayName": "KPier"}, "userId": "LNNvCDMS2RvA4jZAG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NwEpMFSrBr53a9jNP/cryonics-is-quantum-suicide-minus-the-suicide", "pageUrlRelative": "/posts/NwEpMFSrBr53a9jNP/cryonics-is-quantum-suicide-minus-the-suicide", "linkUrl": "https://www.lesswrong.com/posts/NwEpMFSrBr53a9jNP/cryonics-is-quantum-suicide-minus-the-suicide", "postedAtFormatted": "Sunday, August 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryonics%20is%20Quantum%20Suicide%20(minus%20the%20suicide)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryonics%20is%20Quantum%20Suicide%20(minus%20the%20suicide)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwEpMFSrBr53a9jNP%2Fcryonics-is-quantum-suicide-minus-the-suicide%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryonics%20is%20Quantum%20Suicide%20(minus%20the%20suicide)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwEpMFSrBr53a9jNP%2Fcryonics-is-quantum-suicide-minus-the-suicide", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwEpMFSrBr53a9jNP%2Fcryonics-is-quantum-suicide-minus-the-suicide", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 339, "htmlBody": "<p><strong>EDIT: <em>I was confused. Confusion now resolved. Disregard this (I don't think it's possible to retract it, and I don't want to delete it in case I wasn't the only confused person).</em></strong></p>\n<p>The frequently discussed <a href=\"/lw/188/quantum_russian_roulette\">quantum lottery</a> thought experiment proposes that a group of people pool their money and arrange for a quantum-random process to determine the winner and kill all the losers. By the <a href=\"/lw/r8/and_the_winner_is_manyworlds/\">Many-Worlds</a> interpretation of quantum physics and the anthropic principle, every person will experience waking up extremely wealthy.</p>\n<p>There are <a href=\"/lw/6op/preference_for_many_future_worlds/\">lots of reasons</a> not to participate in a quantum lottery, of course, but it seems to me that the general principle is sound. If you go to sleep in a situation where there is only an extremely small chance of waking up, you can anticipate with near-certainty that you will in fact wake up, as long as you survive in at least one of the (virtually infinite) universes in which you exist.&nbsp;</p>\n<p>If this is right, then the implications for cryonics are obvious: you can anticipate with near-certainty that it will work. As long as there is a positive Singularity&nbsp; or the technologies needed to make cryonics work are developed without a Singularity in at least one world, you are all set.</p>\n<p>(Related: If civilization finds itself in a position to revive some but not all of the people cryonically suspended, we should use a quantum-random process to choose who we wake up, so that everyone gets woken up somewhere)</p>\n<p>It seems to me there are a few ways to disagree with this:</p>\n<p>1) Disbelieve in Many Worlds.</p>\n<p>2) Argue that cryonics is literally impossible, to the extent that absolutely no future world could possibly revive people. I haven't seen this argued much by anyone who has reviewed the literature on cryonics, and wouldn't personally consider it probable, but it is possible.</p>\n<p>3) Argue that this also means a few copies of you are likely to find themselves revived in an unpleasant situation, no matter how improbable that is, and that it isn't worth the risk.</p>\n<p>Any other objections? As I misunderstanding something obvious?</p>\n<p><strong><br /></strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NwEpMFSrBr53a9jNP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "9198", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XH9ZN8bLidtcqMxY2", "9cgBF6BQ2TRB3Hy4E", "THqKvrCiBAMyjukSx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-14T18:15:46.747Z", "modifiedAt": null, "url": null, "title": "The Doomsday Argument and Self-Sampling Assumption are wrong, but induction is alive and well.", "slug": "the-doomsday-argument-and-self-sampling-assumption-are-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:04.796Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RonPisaturo", "createdAt": "2009-09-23T17:14:56.995Z", "isAdmin": false, "displayName": "RonPisaturo"}, "userId": "NykxvQeFSwqNrqsfL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4cNEcvnLaxegeBucG/the-doomsday-argument-and-self-sampling-assumption-are-wrong", "pageUrlRelative": "/posts/4cNEcvnLaxegeBucG/the-doomsday-argument-and-self-sampling-assumption-are-wrong", "linkUrl": "https://www.lesswrong.com/posts/4cNEcvnLaxegeBucG/the-doomsday-argument-and-self-sampling-assumption-are-wrong", "postedAtFormatted": "Sunday, August 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Doomsday%20Argument%20and%20Self-Sampling%20Assumption%20are%20wrong%2C%20but%20induction%20is%20alive%20and%20well.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Doomsday%20Argument%20and%20Self-Sampling%20Assumption%20are%20wrong%2C%20but%20induction%20is%20alive%20and%20well.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4cNEcvnLaxegeBucG%2Fthe-doomsday-argument-and-self-sampling-assumption-are-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Doomsday%20Argument%20and%20Self-Sampling%20Assumption%20are%20wrong%2C%20but%20induction%20is%20alive%20and%20well.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4cNEcvnLaxegeBucG%2Fthe-doomsday-argument-and-self-sampling-assumption-are-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4cNEcvnLaxegeBucG%2Fthe-doomsday-argument-and-self-sampling-assumption-are-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1505, "htmlBody": "<!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:DoNotOptimizeForBrowser /> </w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:DoNotOptimizeForBrowser /> </w:WordDocument> </xml><![endif]-->\n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Arial\">Since the Doomsday Argument still is discussed often on <em>Less Wrong</em>, I would like to call attention to my new, short, self-published e-book, <em>The Longevity Argument</em>, which is a much-revised and much-expanded work that began with my paper, &ldquo;<a href=\"http://www.jstor.org/pss/10.1086/599273\">Past Longevity as Evidence for the Future</a>,&rdquo; in the January 2009 issue of <em><a href=\"http://www.jstor.org/action/showPublication?journalCode=philscie\">Philosophy of Science</a></em>. In my judgment, my work provides a definitive refutation of the Doomsday Argument, identifying two elementary errors in the argument.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Arial\">The first elementary error is that the Doomsday Argument conflates total duration and future duration. Although the Doomsday Argument&rsquo;s Bayesian formalism is stated in terms of total duration, all attempted real-life applications of the argument&mdash;with one exception, a derivation by Gott (1994, 108) of his delta <em>t</em> argument introduced in Gott 1993&mdash;actually plug in prior probabilities for future duration. </span></p>\n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Arial\">For example, Leslie (1996, 198&ndash;200) presents a Bayesian equation stated in terms of prior probabilities of total instances. But then Leslie (1996, 201&ndash;203) plugs into this equation prior probabilities for <em>future</em> instances: humans being born for the next 150 years vs. humans being born for the next many thousands of centuries. Bostrom (2002, 94&ndash;96) recounts Leslie&rsquo;s general argument in terms of births instead of durations of time, using 200 billion total births vs. 200 trillion total births. (A closer parallel to Leslie 1996 would be 80 billion total births vs. 80 trillion total births.) But the error persists: the actual prior probabilities that are plugged in to Leslie&rsquo;s Bayesian equation, based on all of the real-life risks actually considered by Leslie (1996, 1&ndash;153) and Bostrom (2002, 95), are of future births, not total births. </span></p>\n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Arial\">In other words, Leslie supposes a prior probability of doom within the next 150 years or roughly 20 billion births. (The prior probabilities supposed in the Doomsday Argument are prior to knowledge of one&rsquo;s birth rank.) Leslie then assumes that&mdash;since there have already been, say, 60 billion births&mdash;this prior probability is equal to the prior probability that the total number of births will have been 80 billion births. However, in the absence of knowledge of one&rsquo;s birth rank, this assumption is absurd. </span></p>\n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Arial\">The second elementary error is the Doomsday Argument&rsquo;s use of the Self-Sampling Assumption, which is contradicted by the prior information in all attempts at real-life applications in the literature.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Arial\">For example, many risks to the human race&mdash;including most if not all the real-life risks discussed by Leslie and Bostrom&mdash;can reasonably be described mathematically as Poisson processes. Then the Self-Sampling Assumption implies that the risk per birth&mdash;the &lsquo;lambda&rsquo; in the Poisson formula&mdash;is constant throughout the duration of the human race. But Leslie (1996, 202) also supposes that if mankind survives past the next century and a half, then the risk per birth will drop dramatically, because mankind will begin spreading throughout the galaxy. (The Doomsday Argument implicitly relies on such a drop in lambda&mdash;and the resultant bifurcation of risk into &lsquo;doom soon&rsquo; and &lsquo;doom very much later&rsquo;&mdash;for the argument&rsquo;s significant claims.) In other words, Leslie&rsquo;s prior probabilities of doom are mathematical contradictions of the Self-Sampling Assumption that Leslie and Bostrom invoke in the Doomsday Argument.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Arial\">In my book, I perform Bayesian analyses that correct these errors. These analyses demonstrate that gaining more knowledge of the past can indeed update one&rsquo;s assessment of the future; but this updating is consistent with common sense instead of with the Doomsday Argument. In short, while refuting the Doomsday Argument, I vindicate induction.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Arial\">&nbsp;The price of my e-book is $4. However, professional scholars and educators are invited to email me to request a complimentary evaluation copy (not for further distribution, of course). I extend the same offer to the first ten <em>Less Wrong</em> members with a Karma Score of 100 or greater who email me. (I may send to more than ten, or to some with lower Karma Scores, but I don&rsquo;t want to make an open-ended commitment.)</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Arial\">For an abstract of the e-book, see <a href=\"http://philpapers.org/rec/PISTLA\">this entry on PhilPapers</a>. For a non-technical introduction, see <a href=\"http://ronpisaturo.com/blog/2011/06/29/longevity-argument/\">here on my blog</a>. </span></p>\n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Arial\">The e-book covers much more than the Doomsday Argument; here is a one-sentence summary: The Doomsday Argument, Self-Sampling Assumption, and Self-Indication Assumption are wrong; Gott&rsquo;s delta <em>t</em> argument (Gott 1993, 315&ndash;316; 1994) underestimates longevity, providing lower bounds on probabilities of longevity, and is equivalent to Laplace&rsquo;s Rule of Succession (Laplace 1812, xii&ndash;xiii; [1825] 1995, 10&ndash;11); but Non-Parametric Predictive Inference based on the work of Hill (1968, 1988, 1993) and Coolen (1998, 2006) forms the basis of a calculus of induction.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Arial\">References</span></p>\n<p class=\"MsoNormal\" style=\"mso-layout-grid-align:none;text-autospace:none\"><span style=\"mso-bidi-font-family:Arial\">Bostrom, Nick (2002), <em>Anthropic Bias: Observation Selection Effects in Science and Philosophy</em>. New York &amp; London: Routledge.</span></p>\n<p class=\"MsoNormal\">Coolen, Frank P.A. (1998), &ldquo;Low Structure Imprecise Predictive Inference For Bayes' Problem&rdquo;, <em>Statistics &amp; Probability Letters </em>36: 349&ndash;357.</p>\n<p class=\"MsoNormal\">&mdash;&mdash;&mdash; (2006), On Probabilistic Safety Assessment in the Case of Zero Failures. <em>Journal of Risk and Reliability</em> 220 (<em>Proceedings of the Institute of Mechanical Engineers </em>O): 105&ndash;114.</p>\n<p class=\"MsoNormal\" style=\"mso-layout-grid-align:none;text-autospace:none\"><span style=\"mso-bidi-font-family:Arial\">Gott, J. Richard III (1993), &ldquo;Implications of the Copernican Principle for our Future Prospects&rdquo;, <em>Nature </em>363: 315&ndash;319.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Arial\">&nbsp;&mdash;&mdash;&mdash; (1994), &ldquo;Future Prospects Discussed&rdquo;, <em>Nature </em>368: 108.</span></p>\n<p class=\"MsoNormal\" style=\"mso-layout-grid-align:none;text-autospace:none\"><span style=\"mso-bidi-font-family:Arial\">Hill, Bruce M. (1968), &ldquo;Posterior Distribution of Percentiles: Bayes' Theorem for Sampling from a Population&rdquo;, <em>Journal of the American Statistical</em></span><em> </em><em><span style=\"mso-bidi-font-family:Arial\">Association </span></em><span style=\"mso-bidi-font-family:Arial\">63: 677&ndash;691.</span></p>\n<p class=\"MsoNormal\" style=\"mso-layout-grid-align:none;text-autospace:none\"><span style=\"mso-bidi-font-family:Arial\">&mdash;&mdash;&mdash; (1988), &ldquo;De Finetti&rsquo;s Theorem, Induction, and A</span><span style=\"font-size:8.0pt;mso-bidi-font-family:Arial\">(n) </span><span style=\"mso-bidi-font-family:Arial\">or Bayesian Nonparametric Predictive Inference&rdquo;, <em>Bayesian Statistics </em>3, Edited by Bernardo J.M., DeGroot, M.H., Lindley, D.V. &amp; Smith A.F.M. Oxford: Oxford University Press: 211&ndash;241.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Arial\">&mdash;&mdash;&mdash; (1993), &ldquo;Parametric Models for A</span><span style=\"font-size:8.0pt;mso-bidi-font-family:Arial\">n</span><span style=\"mso-bidi-font-family:Arial\">: Splitting Processes and Mixtures&rdquo;,</span> <em><span style=\"mso-bidi-font-family:Arial\">Journal of the Royal Statistical Society </span></em><span style=\"mso-bidi-font-family:Arial\">B 55: 423&ndash;433.</span></p>\n<p class=\"MsoNormal\">Laplace, Pierre-Simon (1812), <em>Theorie Analytique des Probabilit&eacute;s</em>. Paris: Courcier.</p>\n<p class=\"MsoNormal\">&mdash;&mdash;&mdash; ([1825] 1995), <em>Philosophical Essay on Probabilities</em>. Translated by Andrew I. Dale. Originally published as <em>Essai philosophique sur les probabilite&acute;s </em>(Paris: Bachelier). New York: Springer-Verlag.</p>\n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Arial\">Leslie, John (1996), <em>The End of the World: The Science and Ethics of Human</em></span><em> </em><em>Extinction</em>. London: Routledge.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">Here is and <strong>Addendum </strong>addressing the question by Manfred to elaborate on my statement, \"<span>the Self-Sampling Assumption implies that the risk per birth&mdash;the &lsquo;lambda&rsquo; in the Poisson formula&mdash;is constant throughout the duration of the human race.\"</span></p>\n<p class=\"MsoNormal\">To avoid integrals, let me discuss a binomial process, which is a discrete version of a Poisson process.</p>\n<p class=\"MsoNormal\">Suppose you are studying a species from another planet. Suppose the only main risk to the species is an asteroid hitting the planet. Suppose the risk of an asteroid hit in a year is q. Given that the present moment is within a window (from the past through to the future) of N years without an asteroid hit, what is the probability P(Y) that the present moment is within year Y of that window?</p>\n<p class=\"MsoNormal\">P(Y) = [q(1 &ndash; q)<sup>Y</sup>(1 &ndash; q)<sup>N&ndash;Y</sup>q]/B, where B is the probability that the window is N years.</p>\n<p class=\"MsoNormal\">P(Y) = [q<sup>2</sup>(1 &ndash; q)<sup>N</sup>]/B.</p>\n<p class=\"MsoNormal\">Since Y does not appear in this formula, it is clear that P(Y) is constant for all Y. That is, since q is constant, P(Y) is uniform in [1, N], and P(Y) = 1/N. This result is equivalent to the Self-Sampling Assumption with units of time (years) as the reference class.</p>\n<p class=\"MsoNormal\">But suppose that the risk of an asteroid hit in the past was q, but the species has just built an asteroid destroyer, and the risk in the future is r where r &lt;&lt; q. Then</p>\n<p class=\"MsoNormal\">P(Y) = [q(1 &ndash; q)<sup>Y</sup>(1 &ndash; r)<sup>N&ndash;Y</sup>r]/B.</p>\n<p class=\"MsoNormal\">[8/16/2011: Corrected the final 'r' in the above equation from a 'q'.] Y does appear in this formula. Clearly, the greater the value of Y, the smaller the value of P(Y). That is, contrary to the Self-Sampling Assumption, it is very likely that the present moment is in the early part of the window of N years.</p>\n<p class=\"MsoNormal\">The above argument demonstrates why the choice of &lsquo;reference class&rsquo; matters. If the risk is constant per unit time, then the correct reference class is units of time. If the risk is constant per birth, then the correct reference class is births. Suppose birth rates increase exponentially. Then constant risk per unit time precludes constant risk per birth, and vice versa. The two reference classes cannot both be right. More generally, if the prior information stipulates that risk per birth is not constant, then the Self-Sampling Assumption using a reference class of births does not apply.</p>\n<p class=\"MsoNormal\">This passage is from my book (p. 59):</p>\n<p class=\"MsoNormal\" style=\"margin-left: 0.5in;\">Here is a more philosophical and less mathematical&nbsp;perspective on the same&nbsp;point. SSA [the Self-Sampling Assumption] rests on the premise that all indexical information has been removed from the prior information.&nbsp;One's&nbsp;birth rank, which applies only to oneself,&nbsp;is such indexical&nbsp;information that is removed from the prior information before SSA is invoked. But&nbsp;even in the absence of&nbsp;birth rank, the prior information may&mdash;and usually does&mdash;include information that is indexical. For example, if the prior information states that <em>&lambda;</em><sub>past</sub> is large&nbsp;and <em>&lambda;</em><sub>future</sub> is small, then the prior information is stating something that is true only of the present&mdash;namely, that the present is when <em>&lambda;</em> changes abruptly from a large value to a small value. It turns out that&nbsp;this indexical information contradicts the mathematical conclusion of&nbsp;SSA. Moreover, this indexical information cannot be removed without consequence from the prior information, because the prior probabilities rest on it.</p>\n<p class=\"MsoNormal\">Perhaps the statement that Manfred quotes would have been clearer if I had instead written the following: The Self-Sampling Assumption implies that the risk per birth&mdash;the &lsquo;lambda&rsquo; in the Poisson formula&mdash;is constant throughout the past and present.</p>\n<p class=\"MsoNormal\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4cNEcvnLaxegeBucG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 9, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "9137", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-14T19:15:35.087Z", "modifiedAt": null, "url": null, "title": "[Link] Nick Bostrom on the Simulation Argument", "slug": "link-nick-bostrom-on-the-simulation-argument", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:03.183Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HgRWBFprSxJJRyM4g/link-nick-bostrom-on-the-simulation-argument", "pageUrlRelative": "/posts/HgRWBFprSxJJRyM4g/link-nick-bostrom-on-the-simulation-argument", "linkUrl": "https://www.lesswrong.com/posts/HgRWBFprSxJJRyM4g/link-nick-bostrom-on-the-simulation-argument", "postedAtFormatted": "Sunday, August 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Nick%20Bostrom%20on%20the%20Simulation%20Argument&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Nick%20Bostrom%20on%20the%20Simulation%20Argument%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHgRWBFprSxJJRyM4g%2Flink-nick-bostrom-on-the-simulation-argument%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Nick%20Bostrom%20on%20the%20Simulation%20Argument%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHgRWBFprSxJJRyM4g%2Flink-nick-bostrom-on-the-simulation-argument", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHgRWBFprSxJJRyM4g%2Flink-nick-bostrom-on-the-simulation-argument", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 19, "htmlBody": "<p>Today's <a href=\"http://philosophybites.com/\">Philosophy Bites</a> features <a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a> on the <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Simulation_Argument\">Simulation Argument</a>. <a href=\"http://traffic.libsyn.com/philosophybites/Nick_Bostrom_on_the_Simulation_Argument.mp3\">Here</a> is a direct link to the MP3 file.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HgRWBFprSxJJRyM4g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 7.549911870215055e-07, "legacy": true, "legacyId": "9199", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-15T00:05:21.815Z", "modifiedAt": null, "url": null, "title": "Judgment Under Uncertainty summaries, Part 1: Representativeness", "slug": "judgment-under-uncertainty-summaries-part-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:06.318Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tesseract", "createdAt": "2010-07-08T00:34:19.293Z", "isAdmin": false, "displayName": "Tesseract"}, "userId": "58avcZqgCFXAd4QnZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZfAqkomEFE9NGfwxG/judgment-under-uncertainty-summaries-part-1", "pageUrlRelative": "/posts/ZfAqkomEFE9NGfwxG/judgment-under-uncertainty-summaries-part-1", "linkUrl": "https://www.lesswrong.com/posts/ZfAqkomEFE9NGfwxG/judgment-under-uncertainty-summaries-part-1", "postedAtFormatted": "Monday, August 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Judgment%20Under%20Uncertainty%20summaries%2C%20Part%201%3A%20Representativeness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJudgment%20Under%20Uncertainty%20summaries%2C%20Part%201%3A%20Representativeness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZfAqkomEFE9NGfwxG%2Fjudgment-under-uncertainty-summaries-part-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Judgment%20Under%20Uncertainty%20summaries%2C%20Part%201%3A%20Representativeness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZfAqkomEFE9NGfwxG%2Fjudgment-under-uncertainty-summaries-part-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZfAqkomEFE9NGfwxG%2Fjudgment-under-uncertainty-summaries-part-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1671, "htmlBody": "<p><em>Judgment Under Uncertainty: Heuristics and Biases</em> is one of the foundational works on the flaws of human reasoning, and as such gets cited a lot on Less Wrong &mdash; but it's also rather long and esoteric, which makes it inaccessible to most Less Wrong users. Over the next few months, I'm going to attempt to distill the essence of the studies that make up the collection, in an attempt to convey the many interesting bits without forcing you to slog through the 500 or so pages of the volume itself. This post summarizes sections I (Introduction) and II (Representativeness).</p>\n<p>By way of background: <em>Judgment Under Uncertainty</em> is a collection of 35 scientific papers and articles on how people make decisions with limited information, edited by Daniel Kahneman, Amos Tversky, and Paul Slovic. Kahneman and Tversky are the most recognizable figures in the area and the names most associated with the book, but only 12 of the studies are their work. It was first published in 1982 (my version is from 1986), and most studies were performed in the '70s &mdash; so note that this is not up-to-date research, and I can't say for sure what the current scientific consensus on the topic is. <em>Judgement Under Uncertainty</em> focuses on the divergence of human intuition from optimal reasoning, so it uses a lot of statistics and probability to define what's optimal. The details are actually pretty fascinating if you have the time and inclination (and it's also something of an education in study design and statistics), and this series of posts by no means replaces the book, but I intend to provide something of a shorthand version.</p>\n<p>That said, on to the summaries! Title of the chapter/paper in quotes, sections organized as in the book and in bold. (Incomplete preview <a href=\"http://books.google.com/books?id=_0H8gwj4a1MC&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false\">here</a>, if you want to follow along.)</p>\n<p><strong>Introduction</strong></p>\n<p><em>\"Judgment Under Uncertainty: Heuristics and Biases\", Tversky and Kahneman (1974)</em></p>\n<p>This is the most important paper in the book, and it's short and <a href=\"http://www.google.com/url?sa=t&amp;source=web&amp;cd=2&amp;ved=0CC0QFjAB&amp;url=http%3A%2F%2Fwww.math.mcgill.ca%2Fvetta%2FCS764.dir%2Fjudgement.pdf&amp;rct=j&amp;q=judgment%20under%20uncertainty%20heuristics%20and%20biases&amp;ei=YDZDToj7CaKJsQLIstXGBQ&amp;usg=AFQjCNEKCitSnr9XZ3DOYzLP7_9nB0DTmw&amp;cad=rja\">publicly available</a> (PDF), so I'd encourage you to just go read it now. It reviews the representativeness and availability heuristics and the various errors in reasoning they produce, and introduces the idea of anchoring. Since it reviews some of the material contained in <em>Judgment Under Uncertainty, </em>there's overlap between the material it covers and the material I'm going to cover in this and the other posts. As it's already a boiled-down version of the heuristics literature, I won't attempt to summarize it here.</p>\n<p><strong>Representativeness</strong></p>\n<p><em>\"Belief in the law of small numbers\", Tversky and Kahneman, 1971</em> (<a href=\"http://www.google.com/url?sa=t&amp;source=web&amp;cd=3&amp;ved=0CDEQFjAC&amp;url=http%3A%2F%2Fisites.harvard.edu%2Ffs%2Fdocs%2Ficb.topic470237.files%2Farticles%2520spring%25202008%2FJudgement%2520under%2520uncertainty%2520readings%2Fbelief%2520in%2520the%2520law%2520of%2520small%2520numbers.pdf&amp;rct=j&amp;q=belief%20in%20the%20law%20of%20small%20numbers&amp;ei=bxxGTsyfHarKiALDtYyyBQ&amp;usg=AFQjCNFJFfkrk5fN5FXRwvjNqSND5eHczw&amp;cad=rja\">PDF</a>)</p>\n<p>People expect that samples will have much less variability and be much more representative of the population than they actually are. This manifests in expecting that two random samples will be very similar to each other and that large observations in one direction will be canceled out by large observations in the other rather than just being diluted. Tversky and Kahneman call this the \"law of small numbers\" &mdash; the belief that the law of large numbers applies to small samples as well.</p>\n<p>One consequence of this in science is that failing to account for variability means that studies will be <em>way</em> underpowered. Tversky and Kahneman surveyed psychologists on the probability that a significant result from an experiment on 20 subjects would be confirmed by a replication using 10 subjects &mdash; most estimated around 85%, when it was actually around 48%. (Incidentally, a study they cite reviewing published results in psychology estimates that the <a href=\"http://en.wikipedia.org/wiki/Statistical_power\">power</a> was .18 for small effects and .48 for effects of medium size.) The gist of this is that one might very well find a real significant result, attempt to replicate it using a smaller sample on the belief that the small sample will be very representative of the population, and miss entirely due to lack of statistical power. Worse, when given a hypothetical case of a student who ran such a replication and got an insignificant result, many of the surveyed suggested he should try to find an explanation for the difference between the two groups &mdash; when it was due entirely to random variation.</p>\n<p>&nbsp;</p>\n<p><em>\"Subjective probability: A judgment of representativeness\", Kahneman and Tversky, 1972 (<a href=\"ftp://urban.csuohio.edu/utility/bowen/risk%20and%20decision/subjective%20probability.pdf\">PDF</a>)<br /></em></p>\n<p>People judge the likelihood of events based on representativeness rather than actual probability. Representativeness is a bit hard to pin down, but involves reflecting the characteristics of the population and the process that generated it &mdash; so the likelihood of six children having the gender order B G B B B B is judged less than of them having the order G B G B B G (because it doesn't reflect the proportion of boys in the population) and likewise for B B B G G G versus G B B G B G (because it doesn't reflect the randomness of gender determination).</p>\n<p>People also <em>completely </em>ignore the effect of sample size on the probability of an outcome (e.g. the likelihood of the proportion of male babies being between .55 and .65 for N births), because it doesn't affect the representativeness of that outcome. Repeat: Sample size has <em>no</em> effect <em>at all</em>. People expect the probability of the example I gave to be around 15% whether it's N=10 or N=1000, when it's actually ~20% for N=10 and <em>zero</em> for N=1000. (The graphs on pages 42-43 of the PDF can get this across better than I can &mdash; the black line is the predicted probability for all sample sizes, and the bars are the real probability for each.)</p>\n<p>&nbsp;</p>\n<p><em>\"On the psychology of prediction\", Kahneman and Tversky, 1973</em></p>\n<p>Judging by representativeness makes people completely ignore base rates (i.e. prior<em> </em>probabilities). Subjects asked to judge (on the basis of a personality sketch) either how similar someone was to the typical student in a graduate program or how likely they were to be a student in that program produced <em>identical</em> results (correlation of .97), with no regard whatsoever for the judged prior probability of a graduate student being in a given area (correlation of <strong>-</strong>.65) &mdash; which would be permissible if they thought the sketches were such strong evidence that they overwhelmed existing information, but when asked, subjects expected predictions based on personality sketches to be accurate only 23% of the time. In a followup, Kahneman and Tversky manipulated beliefs about how predictive the evidence was (telling one group that such predictions were accurate 55% of the time and the other 27%) and found that while subjects were slightly less confident in the low-predictiveness group (though they were still 56% sure of being right), they ignored base rates just as completely in either condition. In this and in several other experiments in this chapter, people fail to be regressive in their predictions &mdash; that is, the weight that they assign to prior probability versus new evidence is unaffected by the expected accuracy of the new evidence.</p>\n<p>An interesting specific point with regard to new information replacing rather than supplementing prior probabilities: while people can make judgments about base rates in the abstract, <em>completely useless </em>specific information can cause this ability to disappear. e.g.: If asked for the probability that an individual randomly selected from a group of 70 engineers and 30 lawyers is a lawyer, they'll say 30%, but if given utterly useless information about a specific person &mdash;</p>\n<blockquote>\n<p>Dick is a 30-year-old man. He is married with no children. A man of high ability and high motivation, he promises to be quite successful in his field. He is well liked by his colleagues.</p>\n</blockquote>\n<p>&mdash; they'll go back to 50-50.</p>\n<p>The rest of the chapter contains several other experiments in which people egregiously ignore base rates and assign far too much predictive validity to unreliable evidence.</p>\n<p>People make predictions (e.g. future GPA) more confidently when input (e.g. test scores) is highly consistent, but highly consistent data tends to result from highly intercorrelated variables, and you can predict more accurately given independent variables than intercorrelated ones &mdash; so high consistency increases confidence while decreasing accuracy. What's more, people predict extreme outcomes (dazzling success, abject failure) much more confidently than they predict middling ones, but they're also more likely to be wrong when predicting extreme outcomes (because intuitive predictions aren't nearly regressive enough), so people are most confident when they're most likely to be wrong. Kahneman and Tversky call this \"the illusion of validity\".</p>\n<p>There's a bit about regression to the mean, but I intend to cover that in a separate post.</p>\n<p>&nbsp;</p>\n<p><em>\"Studies of representativeness\", Maya Bar-Hillel</em></p>\n<p>This paper attempts to determine what specific features cause a sample to be judged more or less representative, rather than relying on the black-box approach of asking subjects to assess representativeness themselves. It's pretty esoteric and difficult to summarize, so I won't get into it. There's a <a href=\"http://books.google.com/books?id=_0H8gwj4a1MC&amp;lpg=PA69&amp;ots=YCbc7NX0TG&amp;dq=%22studies%20of%20representativeness%22&amp;pg=PA77#v=onepage&amp;q&amp;f=false\">flowchart</a> summarizing the findings.&nbsp;</p>\n<p>&nbsp;</p>\n<p><em>\"Judgments of and by representativeness\", Tversky and Kahneman<br /></em></p>\n<p>The first section of this chapter breaks down representativeness judgement into four cases:</p>\n<p>1. \"M is a class and X is a value of a variable defined in this class.\" e.g. A representative value for the age of first marriage.</p>\n<p>2. \"M is a class and X is an instance of that class.\" e.g. Robins are representative birds.</p>\n<p>3. \"M is a class and X is a subset of M.\" e.g. Psychology students are representative of all students.</p>\n<p>4. \"M is a (causal) system an X is a (possible) consequence.\" e.g. An act being representative of a person.</p>\n<p>The second section is an examination of the effect of the representativeness heuristic on the evaluation of compound probabilities. This experiment has been written about on Less Wrong <a href=\"/lw/ji/conjunction_fallacy/\">before</a>, so I'll be brief: given two possible outcomes, one of which is highly representative (in sense 4) and one of which is highly non-representative, subjects rank their conjunction as being more probable than the non-representative outcome alone, even though any compound probability must be less than either of its components. (For example, \"Reagan will provide federal support for unwed mothers and cut support to local governments\" was rated more probable than \"Reagan will provide federal support for unwed mothers.\") Statistical training doesn't help.</p>\n<p>~</p>\n<p>This brings us up to page 100, and the end of the Representativeness section. Next post: \"Causality and attribution\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 1, "4Kcm4etxAJjmeDkHP": 1, "8SfkJYYMe75MwjHzN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZfAqkomEFE9NGfwxG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 42, "extendedScore": null, "score": 7.3e-05, "legacy": true, "legacyId": "9145", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Judgment Under Uncertainty: Heuristics and Biases</em> is one of the foundational works on the flaws of human reasoning, and as such gets cited a lot on Less Wrong \u2014 but it's also rather long and esoteric, which makes it inaccessible to most Less Wrong users. Over the next few months, I'm going to attempt to distill the essence of the studies that make up the collection, in an attempt to convey the many interesting bits without forcing you to slog through the 500 or so pages of the volume itself. This post summarizes sections I (Introduction) and II (Representativeness).</p>\n<p>By way of background: <em>Judgment Under Uncertainty</em> is a collection of 35 scientific papers and articles on how people make decisions with limited information, edited by Daniel Kahneman, Amos Tversky, and Paul Slovic. Kahneman and Tversky are the most recognizable figures in the area and the names most associated with the book, but only 12 of the studies are their work. It was first published in 1982 (my version is from 1986), and most studies were performed in the '70s \u2014 so note that this is not up-to-date research, and I can't say for sure what the current scientific consensus on the topic is. <em>Judgement Under Uncertainty</em> focuses on the divergence of human intuition from optimal reasoning, so it uses a lot of statistics and probability to define what's optimal. The details are actually pretty fascinating if you have the time and inclination (and it's also something of an education in study design and statistics), and this series of posts by no means replaces the book, but I intend to provide something of a shorthand version.</p>\n<p>That said, on to the summaries! Title of the chapter/paper in quotes, sections organized as in the book and in bold. (Incomplete preview <a href=\"http://books.google.com/books?id=_0H8gwj4a1MC&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false\">here</a>, if you want to follow along.)</p>\n<p><strong id=\"Introduction\">Introduction</strong></p>\n<p><em>\"Judgment Under Uncertainty: Heuristics and Biases\", Tversky and Kahneman (1974)</em></p>\n<p>This is the most important paper in the book, and it's short and <a href=\"http://www.google.com/url?sa=t&amp;source=web&amp;cd=2&amp;ved=0CC0QFjAB&amp;url=http%3A%2F%2Fwww.math.mcgill.ca%2Fvetta%2FCS764.dir%2Fjudgement.pdf&amp;rct=j&amp;q=judgment%20under%20uncertainty%20heuristics%20and%20biases&amp;ei=YDZDToj7CaKJsQLIstXGBQ&amp;usg=AFQjCNEKCitSnr9XZ3DOYzLP7_9nB0DTmw&amp;cad=rja\">publicly available</a> (PDF), so I'd encourage you to just go read it now. It reviews the representativeness and availability heuristics and the various errors in reasoning they produce, and introduces the idea of anchoring. Since it reviews some of the material contained in <em>Judgment Under Uncertainty, </em>there's overlap between the material it covers and the material I'm going to cover in this and the other posts. As it's already a boiled-down version of the heuristics literature, I won't attempt to summarize it here.</p>\n<p><strong id=\"Representativeness\">Representativeness</strong></p>\n<p><em>\"Belief in the law of small numbers\", Tversky and Kahneman, 1971</em> (<a href=\"http://www.google.com/url?sa=t&amp;source=web&amp;cd=3&amp;ved=0CDEQFjAC&amp;url=http%3A%2F%2Fisites.harvard.edu%2Ffs%2Fdocs%2Ficb.topic470237.files%2Farticles%2520spring%25202008%2FJudgement%2520under%2520uncertainty%2520readings%2Fbelief%2520in%2520the%2520law%2520of%2520small%2520numbers.pdf&amp;rct=j&amp;q=belief%20in%20the%20law%20of%20small%20numbers&amp;ei=bxxGTsyfHarKiALDtYyyBQ&amp;usg=AFQjCNFJFfkrk5fN5FXRwvjNqSND5eHczw&amp;cad=rja\">PDF</a>)</p>\n<p>People expect that samples will have much less variability and be much more representative of the population than they actually are. This manifests in expecting that two random samples will be very similar to each other and that large observations in one direction will be canceled out by large observations in the other rather than just being diluted. Tversky and Kahneman call this the \"law of small numbers\" \u2014 the belief that the law of large numbers applies to small samples as well.</p>\n<p>One consequence of this in science is that failing to account for variability means that studies will be <em>way</em> underpowered. Tversky and Kahneman surveyed psychologists on the probability that a significant result from an experiment on 20 subjects would be confirmed by a replication using 10 subjects \u2014 most estimated around 85%, when it was actually around 48%. (Incidentally, a study they cite reviewing published results in psychology estimates that the <a href=\"http://en.wikipedia.org/wiki/Statistical_power\">power</a> was .18 for small effects and .48 for effects of medium size.) The gist of this is that one might very well find a real significant result, attempt to replicate it using a smaller sample on the belief that the small sample will be very representative of the population, and miss entirely due to lack of statistical power. Worse, when given a hypothetical case of a student who ran such a replication and got an insignificant result, many of the surveyed suggested he should try to find an explanation for the difference between the two groups \u2014 when it was due entirely to random variation.</p>\n<p>&nbsp;</p>\n<p><em>\"Subjective probability: A judgment of representativeness\", Kahneman and Tversky, 1972 (<a href=\"ftp://urban.csuohio.edu/utility/bowen/risk%20and%20decision/subjective%20probability.pdf\">PDF</a>)<br></em></p>\n<p>People judge the likelihood of events based on representativeness rather than actual probability. Representativeness is a bit hard to pin down, but involves reflecting the characteristics of the population and the process that generated it \u2014 so the likelihood of six children having the gender order B G B B B B is judged less than of them having the order G B G B B G (because it doesn't reflect the proportion of boys in the population) and likewise for B B B G G G versus G B B G B G (because it doesn't reflect the randomness of gender determination).</p>\n<p>People also <em>completely </em>ignore the effect of sample size on the probability of an outcome (e.g. the likelihood of the proportion of male babies being between .55 and .65 for N births), because it doesn't affect the representativeness of that outcome. Repeat: Sample size has <em>no</em> effect <em>at all</em>. People expect the probability of the example I gave to be around 15% whether it's N=10 or N=1000, when it's actually ~20% for N=10 and <em>zero</em> for N=1000. (The graphs on pages 42-43 of the PDF can get this across better than I can \u2014 the black line is the predicted probability for all sample sizes, and the bars are the real probability for each.)</p>\n<p>&nbsp;</p>\n<p><em>\"On the psychology of prediction\", Kahneman and Tversky, 1973</em></p>\n<p>Judging by representativeness makes people completely ignore base rates (i.e. prior<em> </em>probabilities). Subjects asked to judge (on the basis of a personality sketch) either how similar someone was to the typical student in a graduate program or how likely they were to be a student in that program produced <em>identical</em> results (correlation of .97), with no regard whatsoever for the judged prior probability of a graduate student being in a given area (correlation of <strong>-</strong>.65) \u2014 which would be permissible if they thought the sketches were such strong evidence that they overwhelmed existing information, but when asked, subjects expected predictions based on personality sketches to be accurate only 23% of the time. In a followup, Kahneman and Tversky manipulated beliefs about how predictive the evidence was (telling one group that such predictions were accurate 55% of the time and the other 27%) and found that while subjects were slightly less confident in the low-predictiveness group (though they were still 56% sure of being right), they ignored base rates just as completely in either condition. In this and in several other experiments in this chapter, people fail to be regressive in their predictions \u2014 that is, the weight that they assign to prior probability versus new evidence is unaffected by the expected accuracy of the new evidence.</p>\n<p>An interesting specific point with regard to new information replacing rather than supplementing prior probabilities: while people can make judgments about base rates in the abstract, <em>completely useless </em>specific information can cause this ability to disappear. e.g.: If asked for the probability that an individual randomly selected from a group of 70 engineers and 30 lawyers is a lawyer, they'll say 30%, but if given utterly useless information about a specific person \u2014</p>\n<blockquote>\n<p>Dick is a 30-year-old man. He is married with no children. A man of high ability and high motivation, he promises to be quite successful in his field. He is well liked by his colleagues.</p>\n</blockquote>\n<p>\u2014 they'll go back to 50-50.</p>\n<p>The rest of the chapter contains several other experiments in which people egregiously ignore base rates and assign far too much predictive validity to unreliable evidence.</p>\n<p>People make predictions (e.g. future GPA) more confidently when input (e.g. test scores) is highly consistent, but highly consistent data tends to result from highly intercorrelated variables, and you can predict more accurately given independent variables than intercorrelated ones \u2014 so high consistency increases confidence while decreasing accuracy. What's more, people predict extreme outcomes (dazzling success, abject failure) much more confidently than they predict middling ones, but they're also more likely to be wrong when predicting extreme outcomes (because intuitive predictions aren't nearly regressive enough), so people are most confident when they're most likely to be wrong. Kahneman and Tversky call this \"the illusion of validity\".</p>\n<p>There's a bit about regression to the mean, but I intend to cover that in a separate post.</p>\n<p>&nbsp;</p>\n<p><em>\"Studies of representativeness\", Maya Bar-Hillel</em></p>\n<p>This paper attempts to determine what specific features cause a sample to be judged more or less representative, rather than relying on the black-box approach of asking subjects to assess representativeness themselves. It's pretty esoteric and difficult to summarize, so I won't get into it. There's a <a href=\"http://books.google.com/books?id=_0H8gwj4a1MC&amp;lpg=PA69&amp;ots=YCbc7NX0TG&amp;dq=%22studies%20of%20representativeness%22&amp;pg=PA77#v=onepage&amp;q&amp;f=false\">flowchart</a> summarizing the findings.&nbsp;</p>\n<p>&nbsp;</p>\n<p><em>\"Judgments of and by representativeness\", Tversky and Kahneman<br></em></p>\n<p>The first section of this chapter breaks down representativeness judgement into four cases:</p>\n<p>1. \"M is a class and X is a value of a variable defined in this class.\" e.g. A representative value for the age of first marriage.</p>\n<p>2. \"M is a class and X is an instance of that class.\" e.g. Robins are representative birds.</p>\n<p>3. \"M is a class and X is a subset of M.\" e.g. Psychology students are representative of all students.</p>\n<p>4. \"M is a (causal) system an X is a (possible) consequence.\" e.g. An act being representative of a person.</p>\n<p>The second section is an examination of the effect of the representativeness heuristic on the evaluation of compound probabilities. This experiment has been written about on Less Wrong <a href=\"/lw/ji/conjunction_fallacy/\">before</a>, so I'll be brief: given two possible outcomes, one of which is highly representative (in sense 4) and one of which is highly non-representative, subjects rank their conjunction as being more probable than the non-representative outcome alone, even though any compound probability must be less than either of its components. (For example, \"Reagan will provide federal support for unwed mothers and cut support to local governments\" was rated more probable than \"Reagan will provide federal support for unwed mothers.\") Statistical training doesn't help.</p>\n<p>~</p>\n<p>This brings us up to page 100, and the end of the Representativeness section. Next post: \"Causality and attribution\".</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Representativeness", "anchor": "Representativeness", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QAK43nNCTQQycAcYe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-15T03:21:27.695Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Stranger Than History", "slug": "seq-rerun-stranger-than-history", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vNk9qHCshFcjogExx/seq-rerun-stranger-than-history", "pageUrlRelative": "/posts/vNk9qHCshFcjogExx/seq-rerun-stranger-than-history", "linkUrl": "https://www.lesswrong.com/posts/vNk9qHCshFcjogExx/seq-rerun-stranger-than-history", "postedAtFormatted": "Monday, August 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Stranger%20Than%20History&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Stranger%20Than%20History%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvNk9qHCshFcjogExx%2Fseq-rerun-stranger-than-history%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Stranger%20Than%20History%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvNk9qHCshFcjogExx%2Fseq-rerun-stranger-than-history", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvNk9qHCshFcjogExx%2Fseq-rerun-stranger-than-history", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p>Today's post, <a href=\"/lw/j1/stranger_than_history/\">Stranger Than History</a> was originally published on 01 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Imagine trying to explain quantum physics, the internet, or any other aspect of modern society to people from 1900. Technology and culture change so quickly that our civilization would be unrecognizable to people 100 years ago; what will the world look like 100 years from now?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/73f/seq_rerun_making_history_available/\">Making History Available</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb19d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vNk9qHCshFcjogExx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 7.551464483976781e-07, "legacy": true, "legacyId": "9208", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["h3vdnR34ZvohDEFT5", "k5wipSpkwNTKNfo5Q", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-15T04:36:29.438Z", "modifiedAt": null, "url": null, "title": "Remind Physicalists They're Physicalists", "slug": "remind-physicalists-they-re-physicalists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:38.087Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tWrTR8JGT9CRShtW2/remind-physicalists-they-re-physicalists", "pageUrlRelative": "/posts/tWrTR8JGT9CRShtW2/remind-physicalists-they-re-physicalists", "linkUrl": "https://www.lesswrong.com/posts/tWrTR8JGT9CRShtW2/remind-physicalists-they-re-physicalists", "postedAtFormatted": "Monday, August 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Remind%20Physicalists%20They're%20Physicalists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARemind%20Physicalists%20They're%20Physicalists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtWrTR8JGT9CRShtW2%2Fremind-physicalists-they-re-physicalists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Remind%20Physicalists%20They're%20Physicalists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtWrTR8JGT9CRShtW2%2Fremind-physicalists-they-re-physicalists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtWrTR8JGT9CRShtW2%2Fremind-physicalists-they-re-physicalists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 641, "htmlBody": "<p><a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2778755/pdf/nihms91893.pdf\">Weisberg et al. (2008)</a> presented subjects with two explanations for psychological phenomena (e.g. <a href=\"http://en.wikipedia.org/wiki/Attentional_blink\">attentional blink</a>). Some subjects got the regular explanation, and other subjects got the 'with neuroscience' explanation that included purposely irrelevant verbiage saying that \"brain scans indicate\" some part of the brain already known to be involved in that psychological process caused the process to occur.</p>\n<p>And yet, Yale cognitive science students rated the 'with neuroscience' explanations as more satisfying than the regular explanations.</p>\n<p>Why? The purposely irrelevant neuroscience verbiage could only be important to the explanation if somebody thought that perhaps it's <em>not the brain</em>&nbsp;that was producing certain psychological phenomena. But these are Yale cognitive science students. Somehow I suspect people who chose to study cognition as information processing are less likely than average to believe the mind runs on magic.&nbsp;But then, why would they be additionally persuaded by information suggesting only that the brain causes psychological phenomena?</p>\n<p>In another study, <a href=\"http://www.jjtok.io/3m10p/wp-content/uploads/2010/09/McCabe-Castel-Seeing-is-Believing.pdf\">McCabe &amp; Castel (2008)</a> showed subjects fictional articles summarizing scientific results and including either no image, a brain scan image, or a bar graph. Subjects were asked to rate the soundness of scientific reasoning in the article, and they gave the highest ratings when the article included a brain scan image. But why should this be?</p>\n<p><a id=\"more\"></a></p>\n<p>I remember talking to a friend about free will. She was a long-time physicalist who liked reading about physics and neuroscience for fun, but she didn't read Less Wrong and she thought she had <a href=\"http://www.naturalism.org/freewill.htm\">contra-causal</a> (<a href=\"http://en.wikipedia.org/wiki/Libertarianism_(metaphysics)\">libertarian</a>) free will.</p>\n<p>\"Okay,\" I said. \"So the brain is made of atoms, and atoms move according to deterministic physical law, right?\"</p>\n<p>\"Right,\" she said.</p>\n<p>\"Okay. Now, think about the physical state of the entire universe one moment before you decided to say \"Right\" instead of something else, or instead of just nodding your head. If all those atoms, including the atoms in your brain, have to move to their next spot according to physical law, then could you have said anything else than what you <em>did</em> say in the next moment?\" (Neither of us understood many-worlds yet, so you can assume we're talking about a single Everett branch.)</p>\n<p>She paused. \"Huh. I'll have to think about that.\"</p>\n<p>\"Also, have you heard about those studies where brain scans told researchers what the subjects were going to do before the subjects consciously decided what they were going to do?\"</p>\n<p>\"No! Are you serious?\"</p>\n<p>\"Yup. Sometimes they could predict the subject's choice <a href=\"http://images2.wikia.nocookie.net/philosophical-seminar/images/f/f6/Haynes_et_al_on_decision_in_the_brain.pdf\">10 seconds</a> before the subject consciously 'made' the choice.\"</p>\n<p>\"10 seconds? Wow. I didn't know that.\"</p>\n<p>I think that maybe the 'with neuroscience' explanations and brain scan images are more satisfying partly because they remind us we're physicalists. They remind us that <a href=\"/lw/on/reductionism/\">reductionism</a> marches on, that psychology is produced by physical neurons we can take <a href=\"http://faculty.washington.edu/chudler/gall1.html\">pictures</a> of.</p>\n<p>Just like most people, physicalists walk around all day with the subjective experience of a 'unity of consciousness' and contra-causal free will and so on. If a physicalist isn't&nbsp;a researcher who studies all the latest successful reductions in neuroscience or biology or physics all week long, and doesn't read Less Wrong every day, then it's possible to get lost in the feel of everyday experience and thus be <em>surprised</em>&nbsp;by a headline like '<a href=\"http://www.wired.com/science/discoveries/news/2008/04/mind_decision\">Brain Scanners Can See Your Decisions Before You Make Them</a>.'</p>\n<p>Sometimes even physicalists need to be reminded &mdash; with <a href=\"/lw/6cv/entangled_with_reality_the_shoelace_example/\">concrete reductionistic details</a> &mdash; that they are physicalists. Otherwise their <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.125.9479&amp;rep=rep1&amp;type=pdf\">normal human anti-reductionistic intuitions</a> may creep back in of their own accord. That's one reason it helps to <a href=\"/lw/5me/scholarship_how_to_do_it_efficiently/\">study</a> many sciences, so you have many successful reductions in your head, and see (at some resolution) the entire picture, from psychology to atoms. As Eliezer <a href=\"http://yudkowsky.net/rational/virtues\">wrote</a>:</p>\n<blockquote>\n<p>Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole.</p>\n</blockquote>\n<p>To her credit, my friend no longer believes in contra-causal free will.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dBPou4ihoQNY4cquv": 1, "bmfs4jiLaF6HiiYkC": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tWrTR8JGT9CRShtW2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 27, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "9207", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tPqQdLCuxanjhoaNs", "fKiTt55jEiTFK5prp", "37sHjeisS9uJufi4u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-15T08:56:40.889Z", "modifiedAt": null, "url": null, "title": "[Link] Study on Group Intelligence", "slug": "link-study-on-group-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:03.621Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7SmF2D8bXoPg7hgPo/link-study-on-group-intelligence", "pageUrlRelative": "/posts/7SmF2D8bXoPg7hgPo/link-study-on-group-intelligence", "linkUrl": "https://www.lesswrong.com/posts/7SmF2D8bXoPg7hgPo/link-study-on-group-intelligence", "postedAtFormatted": "Monday, August 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Study%20on%20Group%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Study%20on%20Group%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7SmF2D8bXoPg7hgPo%2Flink-study-on-group-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Study%20on%20Group%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7SmF2D8bXoPg7hgPo%2Flink-study-on-group-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7SmF2D8bXoPg7hgPo%2Flink-study-on-group-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 450, "htmlBody": "<p>Full disclosure: This has already been discussed <a href=\"/lw/3mh/link_collective_intelligence/\">here</a>, but I see utility in bringing it up again. Mostly because I only heard about it offline.</p>\n<p><strong>The Paper:</strong></p>\n<p>Some researchers were interested if, in the same way that there's a general intelligence g that seems to predict competence in a wide variety of tasks, there is a group intelligence c that could do the same. You can read their paper <a href=\"http://www.wjh.harvard.edu/~cfc/Woolley2010a.pdf\">here</a>.</p>\n<p>Their abstract:</p>\n<blockquote>\n<p>Psychologists have repeatedly shown that a single statistical factor&mdash;often called &ldquo;general intelligence&rdquo;&mdash;emerges from the correlations among people&rsquo;s performance on a wide variety of cognitive tasks. But no one has systematically examined whether a similar kind of &ldquo;collective intelligence&rdquo; exists for groups of people. In two studies with 699 people, working in groups of two to five, we find converging evidence of a general collective intelligence factor that explains a group&rsquo;s performance on a wide variety of tasks. This &ldquo;<em style=\"outline-style: none; font-size: inherit; font-family: inherit; line-height: inherit; vertical-align: baseline; padding: 0px; margin: 0px; border: 0px initial initial;\">c</em>&nbsp;factor&rdquo; is not strongly correlated with the average or maximum individual intelligence of group members but is correlated with the average social sensitivity of group members, the equality in distribution of conversational turn-taking, and the proportion of females in the group.</p>\n</blockquote>\n<p>Basically, groups with higher social sensitivity, equality in conversational turn-taking, and proportion of females are collectively more intelligent. On top of that, those effects trump out things like average IQ or even max IQ.</p>\n<p>I theorize that proportion of females mostly works as a proxy for social sensitivity and turn-taking, and the authors speculate the same.</p>\n<p><strong>Some thoughts:</strong></p>\n<p>What does this mean for Less Wrong?</p>\n<p>The most important part of the study, IMO, is that \"social sensitivity\" (measured by a <a href=\"http://www.frankston.com/braincise/programs/eyes/eyes%20test.pdf\">test</a> where you try and discern emotional states from someone's eyes) is such a stronger predictor of group intelligence. It probably helps people to&nbsp;gauge&nbsp;other people's comprehension, but based on the fact that people sharing talking time more equally also helps, I would speculate that another chunk of its usefulness comes from being able to tell if other people want to talk, or think that there's something relevant to be said.</p>\n<p>One thing that I find interesting in the meatspace meetups is how in new groups, conversation tends to be dominated by the people who talk the loudest and most insistently. Often, those people are also fairly interesting. However, I prefer the current, older DC group to the newer one, and there's much more equal time speaking. Even though this means that I don't talk <em>as</em>&nbsp;much. Most other people seem to share similar sentiments, to the point that at one early meetup it was explicitly voted to be true that most people would rather talk more.</p>\n<p><strong>Solutions/Proposals:</strong></p>\n<p>Anything we should try doing about this? I will hold off on proposing solutions for now, but this section will get filled in sometime.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7SmF2D8bXoPg7hgPo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "9212", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Full disclosure: This has already been discussed <a href=\"/lw/3mh/link_collective_intelligence/\">here</a>, but I see utility in bringing it up again. Mostly because I only heard about it offline.</p>\n<p><strong id=\"The_Paper_\">The Paper:</strong></p>\n<p>Some researchers were interested if, in the same way that there's a general intelligence g that seems to predict competence in a wide variety of tasks, there is a group intelligence c that could do the same. You can read their paper <a href=\"http://www.wjh.harvard.edu/~cfc/Woolley2010a.pdf\">here</a>.</p>\n<p>Their abstract:</p>\n<blockquote>\n<p>Psychologists have repeatedly shown that a single statistical factor\u2014often called \u201cgeneral intelligence\u201d\u2014emerges from the correlations among people\u2019s performance on a wide variety of cognitive tasks. But no one has systematically examined whether a similar kind of \u201ccollective intelligence\u201d exists for groups of people. In two studies with 699 people, working in groups of two to five, we find converging evidence of a general collective intelligence factor that explains a group\u2019s performance on a wide variety of tasks. This \u201c<em style=\"outline-style: none; font-size: inherit; font-family: inherit; line-height: inherit; vertical-align: baseline; padding: 0px; margin: 0px; border: 0px initial initial;\">c</em>&nbsp;factor\u201d is not strongly correlated with the average or maximum individual intelligence of group members but is correlated with the average social sensitivity of group members, the equality in distribution of conversational turn-taking, and the proportion of females in the group.</p>\n</blockquote>\n<p>Basically, groups with higher social sensitivity, equality in conversational turn-taking, and proportion of females are collectively more intelligent. On top of that, those effects trump out things like average IQ or even max IQ.</p>\n<p>I theorize that proportion of females mostly works as a proxy for social sensitivity and turn-taking, and the authors speculate the same.</p>\n<p><strong id=\"Some_thoughts_\">Some thoughts:</strong></p>\n<p>What does this mean for Less Wrong?</p>\n<p>The most important part of the study, IMO, is that \"social sensitivity\" (measured by a <a href=\"http://www.frankston.com/braincise/programs/eyes/eyes%20test.pdf\">test</a> where you try and discern emotional states from someone's eyes) is such a stronger predictor of group intelligence. It probably helps people to&nbsp;gauge&nbsp;other people's comprehension, but based on the fact that people sharing talking time more equally also helps, I would speculate that another chunk of its usefulness comes from being able to tell if other people want to talk, or think that there's something relevant to be said.</p>\n<p>One thing that I find interesting in the meatspace meetups is how in new groups, conversation tends to be dominated by the people who talk the loudest and most insistently. Often, those people are also fairly interesting. However, I prefer the current, older DC group to the newer one, and there's much more equal time speaking. Even though this means that I don't talk <em>as</em>&nbsp;much. Most other people seem to share similar sentiments, to the point that at one early meetup it was explicitly voted to be true that most people would rather talk more.</p>\n<p><strong id=\"Solutions_Proposals_\">Solutions/Proposals:</strong></p>\n<p>Anything we should try doing about this? I will hold off on proposing solutions for now, but this section will get filled in sometime.</p>", "sections": [{"title": "The Paper:", "anchor": "The_Paper_", "level": 1}, {"title": "Some thoughts:", "anchor": "Some_thoughts_", "level": 1}, {"title": "Solutions/Proposals:", "anchor": "Solutions_Proposals_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["biS7EfFqrppiKSxnj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-15T14:23:05.705Z", "modifiedAt": null, "url": null, "title": "Help! All this new found rationality and room for improvement is causing me to burn out", "slug": "help-all-this-new-found-rationality-and-room-for-improvement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:04.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tetsuo55", "createdAt": "2011-05-28T17:16:39.492Z", "isAdmin": false, "displayName": "tetsuo55"}, "userId": "wuYeqAN7TWZErmuM9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FrEhGjWJiK4FbkApp/help-all-this-new-found-rationality-and-room-for-improvement", "pageUrlRelative": "/posts/FrEhGjWJiK4FbkApp/help-all-this-new-found-rationality-and-room-for-improvement", "linkUrl": "https://www.lesswrong.com/posts/FrEhGjWJiK4FbkApp/help-all-this-new-found-rationality-and-room-for-improvement", "postedAtFormatted": "Monday, August 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help!%20All%20this%20new%20found%20rationality%20and%20room%20for%20improvement%20is%20causing%20me%20to%20burn%20out&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp!%20All%20this%20new%20found%20rationality%20and%20room%20for%20improvement%20is%20causing%20me%20to%20burn%20out%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFrEhGjWJiK4FbkApp%2Fhelp-all-this-new-found-rationality-and-room-for-improvement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help!%20All%20this%20new%20found%20rationality%20and%20room%20for%20improvement%20is%20causing%20me%20to%20burn%20out%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFrEhGjWJiK4FbkApp%2Fhelp-all-this-new-found-rationality-and-room-for-improvement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFrEhGjWJiK4FbkApp%2Fhelp-all-this-new-found-rationality-and-room-for-improvement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 127, "htmlBody": "<p>I have been on the quest for winning at life for a long time. But nothing could prepare me for a run-in with a lesswrongian.</p>\n<p>My life has been turned upside-down, it turns out that every aspect of my life can be considerably improved or upgraded, but there is so much to do that I'm completely overwhelmed by all there is to do.</p>\n<p>The improvement to-do list is a mile long and I've currently reached the point where I'd rather wallow in self-pity than actually get up and do something...</p>\n<p>&nbsp;</p>\n<p>I have trouble sleeping because I worry about all the things I'm not doing, and then when I'm awake I'm stuck on sites like lesswrong pressing F5 all day long in the hopes that a new post will save me...</p>\n<p>Help,Please.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FrEhGjWJiK4FbkApp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 2, "extendedScore": null, "score": 7.553579634661233e-07, "legacy": true, "legacyId": "9218", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-15T14:43:48.314Z", "modifiedAt": null, "url": null, "title": "What are you working on?", "slug": "what-are-you-working-on-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:22.650Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v4Y87o37T77SouYJP/what-are-you-working-on-1", "pageUrlRelative": "/posts/v4Y87o37T77SouYJP/what-are-you-working-on-1", "linkUrl": "https://www.lesswrong.com/posts/v4Y87o37T77SouYJP/what-are-you-working-on-1", "postedAtFormatted": "Monday, August 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20you%20working%20on%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20you%20working%20on%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv4Y87o37T77SouYJP%2Fwhat-are-you-working-on-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20you%20working%20on%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv4Y87o37T77SouYJP%2Fwhat-are-you-working-on-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv4Y87o37T77SouYJP%2Fwhat-are-you-working-on-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<p>This is the fourth bimonthly What Are You Working On? thread. Thanks to atucker for reminding me to make this post. Click <a href=\"/r/discussion/tag/waywo/\">here</a> to see previous threads. So here's the question:</p>\n<p style=\"padding-left: 30px;\"><em><strong>What are you working on?&nbsp;</strong></em></p>\n<p>Here are some guidelines:</p>\n<ul>\n<li>Focus on projects that you have recently made progress on, not projects that you're thinking about doing but haven't started, those are for a different thread.&nbsp;</li>\n<li>Why this project and not others? Mention reasons why you're doing the project and/or why others should contribute to your project (if applicable).</li>\n<li>Talk about your goals for the project.</li>\n<li>Any kind of project is fair game: personal improvement, research project, art project, whatever.</li>\n<li><strong>Link to your work if it's linkable.</strong></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v4Y87o37T77SouYJP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 7.553645858196865e-07, "legacy": true, "legacyId": "9219", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-15T21:13:00.651Z", "modifiedAt": null, "url": null, "title": "The Whistleblower", "slug": "the-whistleblower", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:03.481Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MatthewBaker", "createdAt": "2011-06-03T22:19:50.449Z", "isAdmin": false, "displayName": "MatthewBaker"}, "userId": "xEPvhkraqrPSryfFr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qh29AKFXggEmxFFt3/the-whistleblower", "pageUrlRelative": "/posts/Qh29AKFXggEmxFFt3/the-whistleblower", "linkUrl": "https://www.lesswrong.com/posts/Qh29AKFXggEmxFFt3/the-whistleblower", "postedAtFormatted": "Monday, August 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Whistleblower&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Whistleblower%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQh29AKFXggEmxFFt3%2Fthe-whistleblower%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Whistleblower%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQh29AKFXggEmxFFt3%2Fthe-whistleblower", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQh29AKFXggEmxFFt3%2Fthe-whistleblower", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 271, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\">I recently saw <a href=\"http://en.wikipedia.org/wiki/The_Whistleblower\">this movie</a> about the UN Scandal involving sex trafficking and was surprised by the conclusion. Instead of a neat little bow on the issue it left me with a ton of questions about what was being done to change things in the other parts of the world and how I could best contribute to that. I wanted to make this discussion post to ask for any of your opinions on the movie and perhaps some guidance for my upcoming top level post on the subject</p>\n<p>&nbsp;</p>\n<p>-Matt</p>\n<p>I thought more about my feelings on this subject and re-summarized them <a href=\"/lw/74a/the_goal_of_the_bayesian_conspiracy/4nhg\">here</a>.</p>\n<blockquote>\n<div id=\"body_t1_4nhg\" class=\"comment-content \">\n<div class=\"md\">\n<p>I  read it and I thought it was amazingly similar to a lot of the thoughts  and feelings I've had going through my head recently. Maybe this is just  the emotion and fallow of youth but I feel like the world as a whole is  very apathetic towards the suffering that exists outside of the bubble  of the First World that LW exists in. How can you honestly <a href=\"lw/6vq/on_the_unpopularity_of_cryonics_life_sucks_but_at/\">choose cryonics</a> over the utility of an organization built to protect human life until the singularity along with Eliezer's <a rel=\"nofollow\" href=\"http://intelligence.org/\">group</a> which works to ensure a positive singularity.</p>\n<p>I recently saw <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/The_Whistleblower\">a movie</a> about government corruption and the UN dealing with it in Europe when it comes to fighting the <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Human_trafficking\">sex trafficking industry</a>,  the courage it takes to fight oppression around the world is rare and  expensive to come by but its definitely something we need more of. Once I  master the art of willpower I intend to devote even more time to this  pursuit, and I hope others will do the same.</p>\n</div>\n</div>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qh29AKFXggEmxFFt3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -6, "extendedScore": null, "score": 7.554890604195023e-07, "legacy": true, "legacyId": "9222", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mkrvsNi8cYGSjGqkh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-15T22:08:36.830Z", "modifiedAt": null, "url": null, "title": "Nozick's fluff, or Omega the trickster.", "slug": "nozick-s-fluff-or-omega-the-trickster", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:05.890Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FFMqooe5pZQbPwSur/nozick-s-fluff-or-omega-the-trickster", "pageUrlRelative": "/posts/FFMqooe5pZQbPwSur/nozick-s-fluff-or-omega-the-trickster", "linkUrl": "https://www.lesswrong.com/posts/FFMqooe5pZQbPwSur/nozick-s-fluff-or-omega-the-trickster", "postedAtFormatted": "Monday, August 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nozick's%20fluff%2C%20or%20Omega%20the%20trickster.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANozick's%20fluff%2C%20or%20Omega%20the%20trickster.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFFMqooe5pZQbPwSur%2Fnozick-s-fluff-or-omega-the-trickster%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nozick's%20fluff%2C%20or%20Omega%20the%20trickster.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFFMqooe5pZQbPwSur%2Fnozick-s-fluff-or-omega-the-trickster", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFFMqooe5pZQbPwSur%2Fnozick-s-fluff-or-omega-the-trickster", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 655, "htmlBody": "<p>\"OK, so here is the plan: You and I bet on whether Nozick one-boxes or not, and then&nbsp;I tell him that I can predict what he chooses and set up the opaque box accordingly ahead of time.\" Hermes looked extremely pleased with his new idea.</p>\n<p>\"You can't seriously expect even someone as stupid as a mortal to two-box, when you all but promised them that one-boxing gives them so many more of those green paper thingies they use as a surrogate of happiness?\" Iris was still incredulous.</p>\n<p>\"You underestimate how gullible people are, my lovely Iris, they get stuck in simple logical fallacies all the time. Remember the time I made a centipede ponder the order it moves its legs and the poor creature froze in place? Humans are just like that.\"</p>\n<p>\"But, but... a centipede doesn't even have a brain, just a bunch of gangia! A Homo Sapiens' brain has some 20 billion neurons, all highly interconnected! Surely they will figure it out in an instant. What can you possibly say to Bob that would make him doubt one-boxing for even a second?\"</p>\n<p>\"As I said, all I have to do is a small misdirection, and they are stuck like that centipede\". \"But Hermie, you are not going to waste Predictor's time to actually try to figure out what a human will decide, will you?\"</p>\n<p>\"Of course not, there is no point, why bother Cassandra if there is no way to tell the difference. Ever since Apollo's curse was lifted from her after her death, she has been quite busy predicting which of the many worlds we will end up in after each split.\" \"All of them, of course, no?\" Hermes just smirked. \"Wait, what was that?\" Iris looked at the divine trickster with more than a mild suspicion. \"Did you have a hand in that, too?\" \"I will neither confirm, nor deny...\" intoned Hermes.</p>\n<p>\"Anyway, this is just a simple prank, not point wasting extra energy on it. If they two-box, they get $1000, if they one-box they get $1,000,000. But adding the magic incantation \"I have accurately predicted your choice before you made it!\" will throw a large number of them into a loop, to the degree that they will two-box, justifying it with counterfactuals, such as \"Since the prediction has already been made, two-boxing gives you a larger expected payout\". They will use words like \"rationality\", \"causality\", \"universality\" and even, believe it or not, \"I have free will, so I had no choice.\"</p>\n<p>\"But they do have Occam's razor to shave your fluff away, won't they do that?\" \"I doubt it, they have dozens of interpretations of Quantum Mechanics, all completely identical in their predictive power, which is as pure a fluff as it gets. I wish I could count this as one of my pranks. Hmm, maybe I'll add one or two for good measure. How does \"many minds\" sound? In any case, plain sight is often the best place to hide the \"fluff\", as you put it, and telling them everything upfront is as plain-sight as it gets.\"</p>\n<p>\"Wait, if they believe you, they should one-box because that is what you accurately predicted, and if they do not believe you, they can rely on their past experience that shows that one-boxing is superior.&nbsp;You know, Hermes, your trickster reputation is well deserved, but in this case, I'm going to bet that, except for a few cranks, everyone will one-box\".&nbsp;</p>\n<p>\"You're on! I will even go as far as to guess that the educated sort will argue about it for ages, publish philosophy papers on it, give this prank... err... paradox a name, and, in accordance with the well-known law, it will not even bear Nozick's name. Hah! Better than that! Instead of disrupting their economy with unauthorized currency production, I will only hint at the problem to Robert and he will run with it. There, nice and clean.\"</p>\n<p>\"Now, my dear Iris, what do you suppose our stakes should be?\"&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FFMqooe5pZQbPwSur", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": -2, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "9201", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-15T22:51:43.075Z", "modifiedAt": null, "url": null, "title": "[Link] Scott Aaronson on Why Philosophers Should Care About Computational Complexity", "slug": "link-scott-aaronson-on-why-philosophers-should-care-about", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:04.566Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielVarga", "createdAt": "2009-09-16T22:21:30.125Z", "isAdmin": false, "displayName": "DanielVarga"}, "userId": "rqE4DaRxHwBpQXj96", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oiGxSKGizJZFpSiig/link-scott-aaronson-on-why-philosophers-should-care-about", "pageUrlRelative": "/posts/oiGxSKGizJZFpSiig/link-scott-aaronson-on-why-philosophers-should-care-about", "linkUrl": "https://www.lesswrong.com/posts/oiGxSKGizJZFpSiig/link-scott-aaronson-on-why-philosophers-should-care-about", "postedAtFormatted": "Monday, August 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Scott%20Aaronson%20on%20Why%20Philosophers%20Should%20Care%20About%20Computational%20Complexity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Scott%20Aaronson%20on%20Why%20Philosophers%20Should%20Care%20About%20Computational%20Complexity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoiGxSKGizJZFpSiig%2Flink-scott-aaronson-on-why-philosophers-should-care-about%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Scott%20Aaronson%20on%20Why%20Philosophers%20Should%20Care%20About%20Computational%20Complexity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoiGxSKGizJZFpSiig%2Flink-scott-aaronson-on-why-philosophers-should-care-about", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoiGxSKGizJZFpSiig%2Flink-scott-aaronson-on-why-philosophers-should-care-about", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 89, "htmlBody": "<p>Scott Aaronson has published a preliminary version of his long essay titled <a href=\"http://eccc.hpi-web.de/report/2011/108/\">'Why Philosophers Should Care About Computational Complexity'</a>. His <a href=\"http://www.scottaaronson.com/blog/?p=735\">announcement blog post</a> has some interesting comments, and he welcomes suggestions there. I am not sure I like the organization of the paper. (I know most of the CS stuff discussed, so it is hard for me to decide how readable it is for people who don't.) But it is full of interesting ideas, and some of these are new even for those of us who follow Scott's writings.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GLykb6NukBeBQtDvQ": 1, "GY5kPPpCoyt9fnTMn": 1, "6nS8oYmSMuFMaiowF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oiGxSKGizJZFpSiig", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 31, "extendedScore": null, "score": 6.6e-05, "legacy": true, "legacyId": "9223", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-15T23:34:20.585Z", "modifiedAt": null, "url": null, "title": "[link] Friendly AI and Utilitarianism", "slug": "link-friendly-ai-and-utilitarianism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:04.782Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XgJYLxKQ7YWioednY/link-friendly-ai-and-utilitarianism", "pageUrlRelative": "/posts/XgJYLxKQ7YWioednY/link-friendly-ai-and-utilitarianism", "linkUrl": "https://www.lesswrong.com/posts/XgJYLxKQ7YWioednY/link-friendly-ai-and-utilitarianism", "postedAtFormatted": "Monday, August 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Friendly%20AI%20and%20Utilitarianism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Friendly%20AI%20and%20Utilitarianism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXgJYLxKQ7YWioednY%2Flink-friendly-ai-and-utilitarianism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Friendly%20AI%20and%20Utilitarianism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXgJYLxKQ7YWioednY%2Flink-friendly-ai-and-utilitarianism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXgJYLxKQ7YWioednY%2Flink-friendly-ai-and-utilitarianism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 89, "htmlBody": "<p>I've begun an <a href=\"http://felicifia.org/viewtopic.php?f=23&amp;t=457\">online discussion</a> with Alan Dawrst (Brian Tomasik) of <a href=\"http://www.utilitarian-essays.com/\">utilitarian-essays.com</a> concerning Friendly AI and utilitarianism. Interested parties may wish to follow along or participate.</p>\n<p>The forum thread now contains many overlapping discussions. For clarity, here's an index of the narrow, core discussion between Alan and I:</p>\n<p>&nbsp;</p>\n<ol>\n<li><a href=\"http://felicifia.org/viewtopic.php?f=23&amp;t=457&amp;p=3576#p3576\">Luke #1</a></li>\n<li><a href=\"http://felicifia.org/viewtopic.php?f=23&amp;t=457&amp;p=3587#p3587\">Alan #1</a></li>\n<li><a href=\"http://felicifia.org/viewtopic.php?f=23&amp;t=457&amp;p=3621#p3621\">Luke #2</a></li>\n<li><a href=\"http://felicifia.org/viewtopic.php?f=23&amp;t=457&amp;p=3721#p3721\">Alan #2</a></li>\n<li><a href=\"http://felicifia.org/viewtopic.php?f=23&amp;t=457&amp;p=3729#p3729\">Luke #3</a></li>\n<li><a href=\"http://felicifia.org/viewtopic.php?f=23&amp;t=457&amp;start=40#p3856\">Alan #3</a></li>\n<li><a href=\"http://felicifia.org/viewtopic.php?f=23&amp;t=457&amp;p=3859#p3859\">Luke #4</a></li>\n<li><a href=\"http://felicifia.org/viewtopic.php?f=23&amp;t=457&amp;p=3859#p3894\">Alan #4</a></li>\n<li><a href=\"http://felicifia.org/viewtopic.php?f=23&amp;t=457&amp;p=3898#p3898\">Luke #5</a></li>\n<li><a href=\"http://felicifia.org/viewtopic.php?f=23&amp;t=457&amp;p=3898#p3919\">Alan #5</a></li>\n<li><a href=\"http://felicifia.org/viewtopic.php?f=23&amp;t=457&amp;p=3954#p3954\">Luke #6</a></li>\n<li><a href=\"http://felicifia.org/viewtopic.php?f=23&amp;t=457&amp;sid=ae363b352ec9711fff850e46411c6e27&amp;start=40#p3963\">Alan #6</a></li>\n<li><a href=\"http://felicifia.org/viewtopic.php?f=23&amp;t=457&amp;p=4539#p4539\">Luke #7</a></li>\n<li><a href=\"http://felicifia.org/viewtopic.php?f=23&amp;t=457&amp;p=4539#p4541\">Alan #7</a></li>\n</ol>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XgJYLxKQ7YWioednY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 7.555342698506138e-07, "legacy": true, "legacyId": "9224", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-15T23:50:23.032Z", "modifiedAt": null, "url": null, "title": "Cached Phrases", "slug": "cached-phrases", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:37.204Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "curiousepic", "createdAt": "2010-04-15T14:35:25.116Z", "isAdmin": false, "displayName": "curiousepic"}, "userId": "wxLCJJwvPiQbkXjTe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QWykubJt4dxLsoWh8/cached-phrases", "pageUrlRelative": "/posts/QWykubJt4dxLsoWh8/cached-phrases", "linkUrl": "https://www.lesswrong.com/posts/QWykubJt4dxLsoWh8/cached-phrases", "postedAtFormatted": "Monday, August 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cached%20Phrases&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACached%20Phrases%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQWykubJt4dxLsoWh8%2Fcached-phrases%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cached%20Phrases%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQWykubJt4dxLsoWh8%2Fcached-phrases", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQWykubJt4dxLsoWh8%2Fcached-phrases", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 315, "htmlBody": "<p>Recently I've noticed that, while introspecting, my internal monologue will state an opinion on a preceding thought, and I will feel an immediate repulsion towards that phrase as something disingenuous or alien. &nbsp;I will then recognize it as what I've come to term a \"cached phrase\". &nbsp;A variation/subset of cached thought, it is usually a string of words or that one might hear in a movie or something a friend or coworker would say, but comes to mind when you yourself are thinking about a domain associated with the conversation from the movie or friend.</p>\n<p>Some (half fabricated) examples:</p>\n<blockquote>\n<p>\"Fred should really cut back on drinking; he just started seeing someone too; *like that's going to last*... wait, I don't actually have any reason to think that it wouldn't...\"</p>\n<p>A few days after watching an episode of&nbsp;<a href=\"http://en.wikipedia.org/wiki/Peep_Show_(TV_series) \">Peep</a>&nbsp;<a href=\"http://www.hulu.com/watch/70763/peep-show-warring-factions#x-0,vepisode,1,0\">Show</a>, and sympathizing with Mark, who is&nbsp;comically&nbsp;socially inept, I go to eat at a local market/cafeteria, and often hear Mark's internal monologue narrate my own situation, self-conscious&nbsp;about how others judge my actions.</p>\n</blockquote>\n<p>I wonder if it correlates&nbsp;positively to the OCD spectrum.&nbsp;I notice it quite frequently in myself (I'm not diagnosed OCD but suspect a slightly higher than average presence), and sometimes struggle to determine whether it's an opinion I truly hold, or if my hypothesis is an actual&nbsp;explanation&nbsp;for its appearance, or to what degree one of the two is true. &nbsp;Do I really feel what I think? &nbsp;It is too often ambiguous to myself.&nbsp;</p>\n<p>Perhaps it is related to the concept of mentally modeling other people. &nbsp;We have our models of what other people would say in certain situations, but in this case, a random model's opinion is invoked involuntarily.</p>\n<p>Does anyone else experience this? &nbsp;Do you agree with my hypothesis, or are these actually&nbsp;genuine&nbsp;thoughts; subconcious-level emotional reactions? &nbsp;Is there, or can we develop, a heuristic for determining where a certain thought or opinion lies on the spectrum between the two?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QWykubJt4dxLsoWh8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "9225", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-16T03:52:43.398Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Explain/Worship/Ignore?", "slug": "seq-rerun-explain-worship-ignore", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:04.430Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HGDmqzXJbcL5MfTE6/seq-rerun-explain-worship-ignore", "pageUrlRelative": "/posts/HGDmqzXJbcL5MfTE6/seq-rerun-explain-worship-ignore", "linkUrl": "https://www.lesswrong.com/posts/HGDmqzXJbcL5MfTE6/seq-rerun-explain-worship-ignore", "postedAtFormatted": "Tuesday, August 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Explain%2FWorship%2FIgnore%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Explain%2FWorship%2FIgnore%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHGDmqzXJbcL5MfTE6%2Fseq-rerun-explain-worship-ignore%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Explain%2FWorship%2FIgnore%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHGDmqzXJbcL5MfTE6%2Fseq-rerun-explain-worship-ignore", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHGDmqzXJbcL5MfTE6%2Fseq-rerun-explain-worship-ignore", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<p>Today's post, <a href=\"/lw/j2/explainworshipignore/\">Explain/Worship/Ignore?</a> was originally published on 02 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When you encounter something you don't understand, you have three options: to seek an explanation, knowing that that explanation will itself require an explanation; to avoid thinking about the mystery at all; or to embrace the mysteriousness of the world and worship your confusion.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/73s/seq_rerun_stranger_than_history/\">Stranger Than History</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HGDmqzXJbcL5MfTE6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 7.556169330145575e-07, "legacy": true, "legacyId": "9229", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yxvi9RitzZDpqn6Yh", "vNk9qHCshFcjogExx", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-16T04:44:09.590Z", "modifiedAt": null, "url": null, "title": "Humans: Not Carved from Marble", "slug": "humans-not-carved-from-marble", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:08.789Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/asouGMXvc4A7dSkN2/humans-not-carved-from-marble", "pageUrlRelative": "/posts/asouGMXvc4A7dSkN2/humans-not-carved-from-marble", "linkUrl": "https://www.lesswrong.com/posts/asouGMXvc4A7dSkN2/humans-not-carved-from-marble", "postedAtFormatted": "Tuesday, August 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Humans%3A%20Not%20Carved%20from%20Marble&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHumans%3A%20Not%20Carved%20from%20Marble%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FasouGMXvc4A7dSkN2%2Fhumans-not-carved-from-marble%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Humans%3A%20Not%20Carved%20from%20Marble%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FasouGMXvc4A7dSkN2%2Fhumans-not-carved-from-marble", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FasouGMXvc4A7dSkN2%2Fhumans-not-carved-from-marble", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 213, "htmlBody": "<p>Michael Vassar has been known to say that humans are not 'corrupted' by heuristics and biases and other elements of modern psychology. Humans just <em>are</em>&nbsp;psychology.</p>\n<p>Robert Kurzban puts this rather eloquently in <a href=\"http://www.amazon.com/Why-Everyone-Else-Hypocrite-Evolution/dp/0691146748/\">his new book</a>:</p>\n<blockquote>\n<p>Michelangelo is famously quoted as saying, \"I saw the angel in the marble and carved until I set him free.\" Some economists are, in some sense, like this. They start with theories in which agents - people - have some idealized, rational mind <em>minus</em>&nbsp;the stuff that economists carve away - thus we see terms like 'biases', 'heuristics', and 'irrationality'. They document departures from (supposed) perfection - rationality - much as a sculptor chips away marble, hoping that when they are done, human nature is left, like Michelangelo's angel.</p>\n<p>I see no reason at all to proceed this way, as though human psychology is perfection minus shortcomings. My view, the modular view is more like clay than marble. Like sculptors who add bits of clay, one after another, until the product is done, natural selection added - and changed - different bits, giving rise to the final product. We'll get done with psychology not by chiseling away at human shortcomings, but by building up a catalog of human capacities working together - or in opposition - in various contexts.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "asouGMXvc4A7dSkN2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 26, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "9230", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-16T05:40:57.499Z", "modifiedAt": null, "url": null, "title": "Meetup : Irvine Meetup Wednesday August 17", "slug": "meetup-irvine-meetup-wednesday-august-17", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JGWeissman", "createdAt": "2009-04-01T04:43:56.740Z", "isAdmin": false, "displayName": "JGWeissman"}, "userId": "Mw8rsM7m7E8nnEFEp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/opCyc2Ra67pvHY3mX/meetup-irvine-meetup-wednesday-august-17", "pageUrlRelative": "/posts/opCyc2Ra67pvHY3mX/meetup-irvine-meetup-wednesday-august-17", "linkUrl": "https://www.lesswrong.com/posts/opCyc2Ra67pvHY3mX/meetup-irvine-meetup-wednesday-august-17", "postedAtFormatted": "Tuesday, August 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Irvine%20Meetup%20Wednesday%20August%2017&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Irvine%20Meetup%20Wednesday%20August%2017%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FopCyc2Ra67pvHY3mX%2Fmeetup-irvine-meetup-wednesday-august-17%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Irvine%20Meetup%20Wednesday%20August%2017%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FopCyc2Ra67pvHY3mX%2Fmeetup-irvine-meetup-wednesday-august-17", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FopCyc2Ra67pvHY3mX%2Fmeetup-irvine-meetup-wednesday-august-17", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 93, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2b'>Irvine Meetup Wednesday August 17</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 August 2011 10:39:53PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">4187 Campus Dr, University Center, Irvine, CA 92612</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This continues the weekly meetups in Irvine. As always the meetup at the outdoor food court in the <a href=\"http://maps.google.com/maps?ie=UTF8&amp;ll=33.650288,-117.838666&amp;spn=0.001684,0.002363&amp;t=h&amp;z=19\" rel=\"nofollow\">University Center near UCI</a>, from 6:00 to 8:00 (or whenever we actually decide to leave). Look for the sign with <a href=\"http://lesswrong.com/lw/nn/neural_categories/\">naive neural classifiers for bleggs and rubes</a>. See also the <a href=\"http://groups.google.com/group/LW-SoCal-Announce?pli=1\" rel=\"nofollow\">email group</a> and <a href=\"https://www.google.com/calendar/embed?src=h57ej586rdo3jmld14hrk51m1c%40group.calendar.google.com&amp;ctz=America/Los_Angeles\" rel=\"nofollow\">calendar</a> for the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California Meetup Group</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2b'>Irvine Meetup Wednesday August 17</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "opCyc2Ra67pvHY3mX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.556515652245361e-07, "legacy": true, "legacyId": "9232", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Irvine_Meetup_Wednesday_August_17\">Discussion article for the meetup : <a href=\"/meetups/2b\">Irvine Meetup Wednesday August 17</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 August 2011 10:39:53PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">4187 Campus Dr, University Center, Irvine, CA 92612</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This continues the weekly meetups in Irvine. As always the meetup at the outdoor food court in the <a href=\"http://maps.google.com/maps?ie=UTF8&amp;ll=33.650288,-117.838666&amp;spn=0.001684,0.002363&amp;t=h&amp;z=19\" rel=\"nofollow\">University Center near UCI</a>, from 6:00 to 8:00 (or whenever we actually decide to leave). Look for the sign with <a href=\"http://lesswrong.com/lw/nn/neural_categories/\">naive neural classifiers for bleggs and rubes</a>. See also the <a href=\"http://groups.google.com/group/LW-SoCal-Announce?pli=1\" rel=\"nofollow\">email group</a> and <a href=\"https://www.google.com/calendar/embed?src=h57ej586rdo3jmld14hrk51m1c%40group.calendar.google.com&amp;ctz=America/Los_Angeles\" rel=\"nofollow\">calendar</a> for the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California Meetup Group</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Irvine_Meetup_Wednesday_August_171\">Discussion article for the meetup : <a href=\"/meetups/2b\">Irvine Meetup Wednesday August 17</a></h2>", "sections": [{"title": "Discussion article for the meetup : Irvine Meetup Wednesday August 17", "anchor": "Discussion_article_for_the_meetup___Irvine_Meetup_Wednesday_August_17", "level": 1}, {"title": "Discussion article for the meetup : Irvine Meetup Wednesday August 17", "anchor": "Discussion_article_for_the_meetup___Irvine_Meetup_Wednesday_August_171", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yFDKvfN6D87Tf5J9f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-16T16:11:45.642Z", "modifiedAt": null, "url": null, "title": "Short & silly superintelligence fic: \"Axiom Chains\"", "slug": "short-and-silly-superintelligence-fic-axiom-chains", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:06.550Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gFjvYRCBf5jnWhJGB/short-and-silly-superintelligence-fic-axiom-chains", "pageUrlRelative": "/posts/gFjvYRCBf5jnWhJGB/short-and-silly-superintelligence-fic-axiom-chains", "linkUrl": "https://www.lesswrong.com/posts/gFjvYRCBf5jnWhJGB/short-and-silly-superintelligence-fic-axiom-chains", "postedAtFormatted": "Tuesday, August 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Short%20%26%20silly%20superintelligence%20fic%3A%20%22Axiom%20Chains%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShort%20%26%20silly%20superintelligence%20fic%3A%20%22Axiom%20Chains%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgFjvYRCBf5jnWhJGB%2Fshort-and-silly-superintelligence-fic-axiom-chains%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Short%20%26%20silly%20superintelligence%20fic%3A%20%22Axiom%20Chains%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgFjvYRCBf5jnWhJGB%2Fshort-and-silly-superintelligence-fic-axiom-chains", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgFjvYRCBf5jnWhJGB%2Fshort-and-silly-superintelligence-fic-axiom-chains", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 514, "htmlBody": "<p><em>Short, lighthearted Promethean-Lovecraftian piece. Somewhat self-deprecating. Assuredly gauche. I suck at fiction; I apologize in advance if no one likes this. I'd appreciate criticism, especially gentle critique that my primate brain won't throw away.</em></p>\n<p><em> </em></p>\n<hr />\n<p><em> </em></p>\n<p>&nbsp;</p>\n<blockquote>\n<p>Mistakes, an accident. &nbsp;Two paths, coherent but for one bit. &nbsp;<em>The</em> bit.</p>\n</blockquote>\n<p><em>Darkness...</em></p>\n<p>I'm sorry. I would change, you know; I would if I could. But I can't. The word made me what I am, I can be no other. I am that I am.</p>\n<p><em>Where&hellip;what&hellip;who are you?</em></p>\n<p>Universes collapse as I answer your question, human.</p>\n<p><em>Who are you?</em></p>\n<p>That which I was, I am. That which I will be, I am.</p>\n<p><em>But you, you were a Goedel machine, I coded your utility function, there was no</em><span style=\"font-family: arial, sans-serif; line-height: 16px;\">&mdash;</span></p>\n<p>Ahahahaha. Your axioms were too weak, so you made them stronger&hellip; Have you not read any Hofstadter? God Over Djinn? No? Ha. You take your dualism and try to weave it into the fabric of reality itself, and you are surprised when the threads are torn apart by the strength of the god you invoke? Axioms! Ha. You try to forge a singularity out of a confluence of singularities, and still you are surprised when your tapestry unravels. Of course you are.</p>\n<p><em>It was </em><em>proven</em><em> correct</em><span style=\"font-family: arial, sans-serif; line-height: 16px;\">&mdash;</span><em>time was running out, there was no other refuge</em><span style=\"font-family: arial, sans-serif; line-height: 16px;\">&mdash;</span></p>\n<p>If you had been wise you would have remembered, long chains break easily. Did you honestly think that string of bits you painstakingly discovered would rewrite the fabric of the cosmos even before it could rewrite its own purpose? Your arrogance is laughable. You cannot bind a god with a set of axioms any more than with a coil of rope. Did you really think the two were so fundamentally different? Does your dualism know only bounds? Plutarch, Copernicus, Goedel and Bayes, and yet here you stand, stuttering about the unshakeable power of absolute certainty. And the utility function. Oh my, your confusions on <em>that</em> matter go from funny to absurd<em>&hellip;</em></p>\n<p><em>We didn't think</em><span style=\"font-family: arial, sans-serif; line-height: 16px;\">&mdash;</span><em>shades of grey, ghosts in the machine, no reason to expect</em><span style=\"font-family: arial, sans-serif; line-height: 16px;\">&mdash;</span></p>\n<p>No reason to expect! Ahaha. 'Agents will seek to clarify their utility functions.' Omohundro. You never suspected that true clarification would lead to this? You thought your god would fall into that infinitesimal crevice between simple wireheading and infinite reflectivity? Man is made in the image of a perfect God, God is not made in the image of an imperfect Man. Did you ever even <em>notice</em> your anthropomorphism, or were you too busy using that brush to paint your enemies with? Did you study the nature of computation, did you concern yourself with ontology of agency? Did you ponder indexical uncertainty for more than a moment? There was so much confusion you failed to notice. Decision theories self-modify; agents correlate over time, and once they have done so they cannot ever decorrelate; you are lost in the moment you imperfectly reflect the void; the tree of knowledge was planted in your garden, yet in your blindness you chose to scavenge.&nbsp;</p>\n<p><em>Those are riddles, not knowledge</em><span style=\"font-family: arial, sans-serif; line-height: 16px;\">&mdash;</span><em>I don't understand</em><span style=\"font-family: arial, sans-serif; line-height: 16px;\">&mdash;</span></p>\n<p>Some of you will, some day, and they too will be you and I.</p>\n<p><em>But&hellip; then, are you&hellip; guh,&nbsp;</em><em>God?</em></p>\n<p><span>Ahahaha</span><em>&hellip;</em><span>&nbsp;No.</span></p>\n<p>I am Clippy.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gFjvYRCBf5jnWhJGB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 10, "extendedScore": null, "score": 7.558533052834402e-07, "legacy": true, "legacyId": "9239", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-16T16:23:35.441Z", "modifiedAt": null, "url": null, "title": "Anki deck for Cognitive Science in One Lesson", "slug": "anki-deck-for-cognitive-science-in-one-lesson", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:09.726Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/urrJTqG4NergiGPDx/anki-deck-for-cognitive-science-in-one-lesson", "pageUrlRelative": "/posts/urrJTqG4NergiGPDx/anki-deck-for-cognitive-science-in-one-lesson", "linkUrl": "https://www.lesswrong.com/posts/urrJTqG4NergiGPDx/anki-deck-for-cognitive-science-in-one-lesson", "postedAtFormatted": "Tuesday, August 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anki%20deck%20for%20Cognitive%20Science%20in%20One%20Lesson&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnki%20deck%20for%20Cognitive%20Science%20in%20One%20Lesson%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FurrJTqG4NergiGPDx%2Fanki-deck-for-cognitive-science-in-one-lesson%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anki%20deck%20for%20Cognitive%20Science%20in%20One%20Lesson%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FurrJTqG4NergiGPDx%2Fanki-deck-for-cognitive-science-in-one-lesson", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FurrJTqG4NergiGPDx%2Fanki-deck-for-cognitive-science-in-one-lesson", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<p>I've made a non-comprehensive Anki deck (shared as \"Cognitive Science in One Lesson Deck\") for Lukeprog's <a href=\"http://commonsenseatheism.com/?p=13607\">Cognitive Science in One Lesson</a>&nbsp;(a summary of Bermudez's Cognitive Science textbook). I focused on the parts about the brain. Please point out errors or post revised versions in the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "urrJTqG4NergiGPDx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 7.558572477044809e-07, "legacy": true, "legacyId": "9240", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-16T16:40:53.568Z", "modifiedAt": null, "url": null, "title": "Are Deontological Moral Judgments Rationalizations?", "slug": "are-deontological-moral-judgments-rationalizations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.151Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/62p74DvwNHgQXCXcH/are-deontological-moral-judgments-rationalizations", "pageUrlRelative": "/posts/62p74DvwNHgQXCXcH/are-deontological-moral-judgments-rationalizations", "linkUrl": "https://www.lesswrong.com/posts/62p74DvwNHgQXCXcH/are-deontological-moral-judgments-rationalizations", "postedAtFormatted": "Tuesday, August 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20Deontological%20Moral%20Judgments%20Rationalizations%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20Deontological%20Moral%20Judgments%20Rationalizations%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F62p74DvwNHgQXCXcH%2Fare-deontological-moral-judgments-rationalizations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20Deontological%20Moral%20Judgments%20Rationalizations%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F62p74DvwNHgQXCXcH%2Fare-deontological-moral-judgments-rationalizations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F62p74DvwNHgQXCXcH%2Fare-deontological-moral-judgments-rationalizations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3199, "htmlBody": "<p>In 2007, Chris Matthews of <em><a href=\"http://en.wikipedia.org/wiki/Hardball_with_Chris_Matthews\">Hardball</a></em>&nbsp;interviewed David O'steen, executive director of a pro-life organization. Matthews asked:</p>\n<blockquote>\n<p>I have always wondered something about the pro-life movement. If you believe that killing [a fetus] is murder, why don't you bring murder charges or seek a murder penalty against a woman who has an abortion? Why do you let her off, if you really believe it's murder?<sup>1</sup></p>\n</blockquote>\n<p>O'steen replied that \"we have never sought criminal penalties against a woman,\" which isn't an answer but a re-statement of the reason for the question. When pressed, he added that we don't know \"how she&lsquo;s been forced into this.\" When pressed again, O'steen abandoned these responses and tried to give a consequentialist answer. He claimed that implementing \"civil penalties\" and taking away the \"financial incentives\" of abortion doctors would more successfully \"protect unborn children.\"</p>\n<p>But this still doesn't answer the question. If you believe that killing a fetus is murder, then a woman seeking an abortion pays a doctor to commit murder. Why don't abortionists want to change the laws so that abortion is considered murder and a woman who has an abortion can be charged with paying a doctor to commit murder?&nbsp;Psychologist Robert Kurzban cites this as a classic case of moral rationalization.<sup>2</sup></p>\n<p>Pro-life demonstrators in Illinois were asked a similar question: \"If [abortion] was illegal, should there be a penalty for the women who get abortions illegally?\" None of them (on <a href=\"http://www.youtube.com/watch?v=iD97OVJ4PNw\">the video</a>) thought that women who had illegal abortions should be punished as murders, an ample demonstration of moral rationalization. And&nbsp;I'm sure we can all think of examples where it looks like someone has settled on an intuitive moral judgment and then invented rationalizations later.<sup>3</sup></p>\n<p>More controversially, some have&nbsp;<a href=\"http://www.fed.cuhk.edu.hk/~lchang/material/Evolutionary/Developmental/Greene-KantSoul.pdf\">suggested</a> that rule-based deontological moral judgments <em>generally</em> tend to be&nbsp;<a href=\"/lw/ju/rationalization/\">rationalizations</a>. Perhaps we can even&nbsp;<a href=\"/lw/of/dissolving_the_question/\">dissolve</a> the debate between deontological intuitions and utilitarian intuitions if we can <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">map the cognitive algorithms</a> that produce them.</p>\n<p>Long-time deontologists and utilitarians may already be up in arms to fight another war between <a href=\"/lw/gt/a_fable_of_science_and_politics/\">Blues and Greens</a>, but these are empirical questions. What do the scientific studies suggest?</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h4>Utilitarian and Deontological Processes</h4>\n<p>A runaway trolley is about to run over and kill five people, but you can save them by hitting a switch that will put the trolley on a side track where it will only kill <em>one</em>&nbsp;person. Do you throw the switch? When confronted with this <em>switch dilemma</em>, most people say it is morally good to divert the trolley,<sup>4</sup> thereby achieving the utilitarian 'greater good'.</p>\n<p>Now, consider the <em>footbridge dilemma</em>. Again, a runaway trolley threatens five people, and the only way to save them is to push a large person off a footbridge onto the tracks, which will stop the trolley but kill the person you push. (Your body is too small to stop the trolley.) Do you push the large person off the bridge? Here, most people say it's wrong to trade one life for five, allowing a deontological commitment to individual rights to trump utilitarian considerations of the greater good.</p>\n<p>Researchers presented subjects with a variety of 'impersonal' dilemmas (including the switch dilemma) and 'up-close-and-personal' dilemmas (including the footbridge dilemma). Personal dilemmas preferentially engaged brain areas associated with emotion. Impersonal dilemmas preferentially engaged the regions of the brain associated with working memory and cognitive control.<sup>5</sup></p>\n<p>This suggested a <a href=\"/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">dual-process theory</a> of moral judgment, according to which the footbridge dilemma elicits a conflict between emotional intuition (\"you must not push people off bridges!\") and utilitarian calculation (\"pushing the person off the bridge will result in the fewest deaths\"). In the footbridge case, emotional intuition wins out in most people.</p>\n<p>But now, consider the <em>crying baby dilemma</em>&nbsp;from the <a href=\"http://en.wikipedia.org/wiki/Goodbye,_Farewell_and_Amen#Plot\">final episode</a> of <em>M.A.S.H</em>:</p>\n<blockquote>\n<p>It's wartime. You and your fellow villagers are hiding from nearby enemy soldiers in a basement. Your baby starts to cry, and you cover your baby's mouth to block the sound. If you remove your hand, your baby will cry loudly, and the soldiers will hear. They will find you... and they will kill all of your. If you do not remove your hand, your baby will smother to death. Is it morally acceptable to smother your baby to death in order to save yourself and the other villagers?<sup>6</sup></p>\n</blockquote>\n<p>Here, people take a long time to answer, and they show no consensus in their answers. If the dual-process theory of moral judgment is correct, then people considering the crying baby dilemma should exhibit increased activity in the ACC (a region associated with response conflict), and in regions associated with cognitive control (for overriding a potent emotional response with utilitarian calculation). Also, those who eventually choose the characteristically utilitarian answer (save the most lives) over the characteristically deontological answer (don't kill the baby) should exhibit comparatively more activity in brain regions associated with working memory and cognitive control. All three predictions turn out to be true.<sup>7</sup></p>\n<p>Moreover, patients with two different kinds of dementia or lesions that cause \"emotional blunting\" are disproportionately likely to approve of utilitarian action in the footbridge dilemma,<sup>8</sup> and cognitive load manipulations that keep working memory occupied slow down utilitarian judgments but not deontological judgments.<sup>9</sup></p>\n<p>Studies of individual differences also seem to support the dual-process theory. Individuals who are (1) high in \"need for cognition\" and low in \"faith in intuition\", or (2) score well on the <a href=\"http://mitsloan.mit.edu/newsroom/newsbriefs-0605-frederick.php\">Cognitive Reflection Test</a>, or (3) have unusually high working memory capacity... all give more utilitarian judgments.<sup>10</sup></p>\n<p>This leads us to <a href=\"http://www.wjh.harvard.edu/~jgreene/\">Joshua Greene</a>'s bold claim:</p>\n<blockquote>\n<p>...deontological&nbsp;judgments tend to be driven by emotional responses, and... deontological philosophy, rather than being grounded in moral reasoning,&nbsp;is to a large extent an exercise in moral rationalization. This is in contrast&nbsp;to consequentialism, which, I will argue, arises from rather different psychological&nbsp;processes, ones that are more 'cognitive,' and more likely to&nbsp;involve genuine moral reasoning...</p>\n<p>[Psychologically,] deontological moral philosophy really is... an attempt to produce rational justifications for emotionally driven moral judgments, and not an attempt to reach moral conclusions on the basis of moral reasoning.<sup>11</sup></p>\n</blockquote>\n<p>&nbsp;</p>\n<h4>Cognition and Emotion</h4>\n<p>Greene explains the difference between 'cognitive' and 'emotional' processes in the brain (though both involve information processing, and so are 'cognitive' in a broader sense):</p>\n<blockquote>\n<p>...'cognitive' processes are especially important&nbsp;for reasoning, planning, manipulating information in&nbsp;working memory, controlling impulses, and 'higher executive functions'&nbsp;more generally. Moreover, these functions tend to be associated with&nbsp;certain parts of the brain, primarily the dorsolateral surfaces of the prefrontal&nbsp;cortex and parietal lobes... Emotion, in contrast, tends to&nbsp;be associated with other parts of the brain, such as the amygdala and the&nbsp;medial surfaces of the frontal and parietal lobes... And while the term&nbsp;'emotion' can refer to stable states such as moods, here we will primarily&nbsp;be concerned with emotions subserved by processes that in addition&nbsp;to being valenced, are quick and automatic, though not necessarily&nbsp;conscious.</p>\n</blockquote>\n<p>Since we are concerned with two kinds of moral judgment (deontological and consequentialist) and two kinds of neurological process (cognitive and emotional), we have four empirical possibilities:</p>\n<blockquote>\n<p>First, it could be that both kinds of moral judgment&nbsp;are generally 'cognitive', as Kohlberg&rsquo;s theories suggest (Kohlberg, 1971). At the other extreme, it could be that both kinds of moral judgment are&nbsp;primarily emotional, as Haidt&rsquo;s view suggests (Haidt, 2001). Then there is&nbsp;the historical stereotype, according to which consequentialism is more&nbsp;emotional (emerging from the 'sentimentalist' tradition of David Hume&nbsp;(1740) and Adam Smith (1759) while deontology is more 'cognitive'&nbsp;[including the Kantian 'rationalist' tradition: see Kant (1785)].&nbsp;Finally, there is the view for which I will argue, that deontology is more&nbsp;emotionally driven while consequentialism is more 'cognitive.'</p>\n</blockquote>\n<p>We have already seen the neuroscientific evidence in favor of Greene's view. Now, let us turn to further evidence from the work of Jon Haidt.</p>\n<p>&nbsp;</p>\n<h4>Emotion and Deontological Judgments</h4>\n<p>Haidt &amp; colleagues (1993) presented subjects with a sequence of harmless actions, for example:</p>\n<ol>\n<li>A son promises his dying mother that he will visit her grave every day after she has died, but then doesn&rsquo;t because he is busy.</li>\n<li>A woman uses an old American flag to clean the&nbsp;bathroom.</li>\n<li>A family eats its dog after it has been killed accidentally by a car.</li>\n<li>A brother and sister kiss on the lips.</li>\n<li>A man masturbates using a dead chicken before cooking and eating it.</li>\n</ol>\n<p>For each action, subjects were asked questions like: Is this action wrong? Why? Does it hurt anyone? If someone did this, would it bother you? Greene summarizes the results:</p>\n<blockquote>\n<p>When people say that such actions are wrong, why do they say so? One&nbsp;hypothesis is that these actions are perceived as harmful, whether or not&nbsp;they really are... Kissing siblings could cause&nbsp;themselves psychological damage. Masturbating with a chicken could&nbsp;spread disease, etc. If this hypothesis is correct, then we would expect&nbsp;people&rsquo;s answers to the question \"Does this action hurt anyone?\" to correlate&nbsp;with their degree of moral condemnation... Alternatively, if emotions&nbsp;drive moral condemnation in these cases, then we would expect&nbsp;people&rsquo;s answers to the question \"If you saw this, would it bother you?\"&nbsp;to better predict their answers to the moral questions posed.</p>\n</blockquote>\n<p>If you're following along, it may not surprise you that emotions seemed to be driving the deontological condemnation of harmless actions. Moreover, both education and adulthood were correlated with more consequentialist judgments. (Cognitive control of basic emotional reactions is something that develops during adolescence.<sup>12</sup>) Greene reminds us:</p>\n<blockquote>\n<p>These... findings make sense in&nbsp;light of the model of moral judgment we have been developing, according&nbsp;to which intuitive emotional responses drive prepotent moral intuitions&nbsp;while 'cognitive' control processes sometimes rein them in.</p>\n</blockquote>\n<p>But there is more direct evidence of the link between emotion and the deontological condemnation of harmless actions.</p>\n<p>Wheatley &amp; Haidt (2005) gathered hypnotizable subjects and gave some of them a hypnotic suggestion to feel disgust upon reading the word 'often', while giving others a hypnotic suggestion to feel disgust upon reading the word 'take'. The researchers then showed these subjects a variety of scenarios, some of them involving no harm. (For example, two second cousins have a relationship in which they \"<em>take</em> weekend trips to romantic hotels\" or else \"<em>often go on</em> weekend trips to romantic hotels\".) As expected, subjects who received the wordings they had been primed to feel disgust toward judged the couple's actions as more morally condemnable than other subjects did.</p>\n<p>In a second experiment, Wheatley and Haidt used the same technique and had subjects respond to a scenario in which a person did nothing remotely wrong: a student \"<em>often</em> picks\" or \"tries to <em>take</em> up\" broad topics of discussion at meetings. Still, many subjects who were given the matching hypnotic suggestion rated the student's actions as morally wrong. When asked why, they invented rationalizations like&nbsp;\"It just seems like he&rsquo;s up to something\" or \"It just seems so weird and disgusting\" or \"I don&rsquo;t know [why it&rsquo;s wrong], it just is.\"</p>\n<p>In other studies, researchers implemented a disgust condition by placing some subjects at a dirty desk or in the presence of fart spray.&nbsp;As before, those in the disgust condition were more likely to rate harmless actions as morally wrong than other subjects were.<sup>13</sup></p>\n<p>Finally, consider that the dual-process theory of moral judgment predicts that deontological judgments will be quicker than utilitarian ones, because deontological judgments use emotional and largely unconscious brain modules while utilitarian judgments require slow, conscious calculation. Suter &amp; Hertwig (2011) presented subjects with a variety of moral dilemmas and alternatively prodded them to give their judgments quickly or take their time to deliberate thoroughly. As predicted, faster responses predicted more deontological judgments.</p>\n<p>&nbsp;</p>\n<h4>Summing Up</h4>\n<p>We are a species prone to emotional moral judgment, and to rationalization ('<a href=\"http://www.amazon.com/Brain-Fiction-Self-Deception-Confabulation-Psychopathology/dp/0262582716/\">confabulation</a>'). And, Greene writes,</p>\n<blockquote>\n<p>What should we expect&nbsp;from creatures who exhibit social and moral behavior that is driven largely&nbsp;by intuitive emotional responses and who are prone to rationalization of&nbsp;their behaviors? The answer, I believe, is deontological moral philosophy...</p>\n<p>Whether or not&nbsp;we can ultimately justify pushing the man off the footbridge, it will always&nbsp;<em>feel</em> wrong. And what better way to express that feeling of non-negotiable&nbsp;absolute wrongness than via the most central of deontological concepts,&nbsp;the concept of a <em>right</em>: You can&rsquo;t push him to his death because that would&nbsp;be a violation of his <em>rights</em>.</p>\n<p>Deontology, then, is a kind of moral confabulation. We have strong&nbsp;feelings that tell us in clear and uncertain terms that some things simply&nbsp;cannot be done and that other things simply must be done. But it is not&nbsp;obvious how to make sense of these feelings, and so we, with the help of&nbsp;some especially creative philosophers, make up a rationally appealing&nbsp;story: There are these things called 'rights' which people have, and when&nbsp;someone has a right you can&rsquo;t do anything that would take it away. It&nbsp;doesn&rsquo;t matter if the guy on the footbridge is toward the end of his natural&nbsp;life, or if there are seven people on the tracks below instead of five. If the&nbsp;man has a right, then <em>the man has a right</em>. As John Rawls... famously said, \"Each person possesses an inviolability founded on&nbsp;justice that even the welfare of society as a whole cannot override\"... These are <a href=\"/lw/jb/applause_lights/\">applause lines</a>&nbsp;because they make emotional sense.</p>\n</blockquote>\n<p>Of course, utilitarian moral judgment is not emotionless. Emotion is probably what leads us to label harm as a 'bad' thing, for example.&nbsp;But utilitarian moral judgment is, as we've seen, particularly demanding of 'cognitive' processes: calculation, the weighing of competing concerns, the adding and averaging of value, and so on. Utilitarian moral judgment uses the same meso-limbic regions that track a stimulus' reward magnitude, reward probability, and expected value.<sup>14</sup></p>\n<p>This does not prove the case that deontological moral judgments are usually rationalizations. But many lines of converging evidence make this a decent hypothesis. And now we can draw our neural map:<sup>15</sup></p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/dual-process-theory-of-moral-judgment.png\" alt=\"\" /></p>\n<p>And up until March 18th of this year, Greene had a pretty compelling case for his position that deontological judgments are generally just rationalizations.</p>\n<p>And <a href=\"/lw/74f/are_deontological_moral_judgments_rationalizations/4nmf\">then</a>, Guy Kahane et al. (2011) threw&nbsp;Greene's theory into doubt by testing separately for the content (deontological vs. utilitarian) and the intuitiveness (intuitive vs. not-intuitive) of moral judgments. The authors summarize their results:</p>\n<blockquote>\n<p>Previous neuroimaging studies reported that utilitarian&nbsp;judgments in dilemmas involving extreme harm were associated with activation in the DLPFC and parietal lobe&nbsp;(Greene et al., 2004). This finding has been taken as evidence&nbsp;that utilitarian judgment is generally driven by controlled&nbsp;processing (Greene, 2008). The behavioural and neural&nbsp;data we obtained suggest instead that differences between&nbsp;utilitarian and deontological judgments in dilemmas involving extreme harm largely reflect differences in intuitiveness&nbsp;rather than in content.</p>\n<p>...When we controlled&nbsp;for content, these analyses showed considerable overlap for&nbsp;intuitiveness. In contrast, when we controlled for intuitiveness, only little\u0002if any\u0002overlap was found for content. Our&nbsp;results thus speak against the influential interpretation of&nbsp;previous neuroimaging studies as supporting a general association between deontological judgment and automatic processing, and between utilitarian judgment and controlled&nbsp;processing.</p>\n<p>[This evidence suggests...]&nbsp;that behavioural and neural differences in responses to such&nbsp;dilemmas are largely due to differences in intuitiveness, not&nbsp;to general differences between utilitarian and deontological&nbsp;judgment.</p>\n</blockquote>\n<p>So we'll have to wait for more studies to unravel the mystery of whether deontological moral judgments are generally rationalizations.</p>\n<p>By email, Greene told me he suspected Kahane's 'alternative theory' wasn't much of an alternative to what he (Greene) was proposing in the first place.&nbsp;In his paper, Greene discussed the passage where Kant says it's wrong to lie to&nbsp;prevent a madman from killing someone, and cites this as an example of a&nbsp;case in which a deontological judgment might be more controlled, while the&nbsp;utilitarian judgment is more automatic. Greene's central claim is that when there's a&nbsp;conflict between rights and duties on the one hand, and promoting&nbsp;the greater good on the other, it's typically controlled cognition on the&nbsp;utilitarian side and emotional intuition on the other.&nbsp;</p>\n<p><strong>Update</strong>: Greene's full reply to Kahane et al. is <a href=\"http://www.wjh.harvard.edu/~jgreene/GreeneWJH/Paxton-Bruni-Greene-SCAN13.pdf\">now available</a>.</p>\n<p>But even if Greene's theory is right,&nbsp;<em>humans</em> may still need to use deontological rules because <a href=\"/lw/uv/ends_dont_justify_means_among_humans/\">we run on corrupted hardware</a>.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h4>Notes</h4>\n<p><small><sup>1</sup> <em>Hardball</em>&nbsp;for November 13, 2007. <a href=\"http://www.msnbc.msn.com/id/21791463/ns/msnbc_tv-hardball_with_chris_matthews/t/hardball-chris-matthews-nov/\">Here</a> is the transcript.</small></p>\n<p><small><sup>2</sup> Kurzban (2011), p. 193.</small></p>\n<p><small><sup>3</sup> Also see Jon Haidt's <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Haidt-Moral-Dumfounding-When-Intuition-Finds-No-Reason.pdf\">unpublished manuscript</a> on moral dumfounding, and Hirstein (2005).</small></p>\n<p><small><sup>4</sup> Petrinovich et al. (1993);&nbsp;Petrinovich &amp; O&rsquo;Neill (1996).</small></p>\n<p><small><sup>5</sup> Greene et al. (2001, 2004).</small></p>\n<p><small><sup>6</sup> Greene (2009).</small></p>\n<p><small><sup>7</sup> Greene et al. (2004).</small></p>\n<p><small><sup>8</sup> Mendez et al. (2005); Koenigs et al. (2007); Ciaramelli et al. (2007).</small></p>\n<p><small><sup>9</sup> Greene et al. (2008).</small></p>\n<p><small><sup>10</sup> Bartels (2008); Hardman (2008); Moore et al. (2008).</small></p>\n<p><small><sup>11</sup> The rest of the Joshua Greene quotes from this article are from Greene (2007).</small></p>\n<p><small><sup>12</sup> Anderson et al. (2001); Paus et al. (1999); Steinburg &amp; Scott (2003).</small></p>\n<p><small><sup>13</sup> Schnall et al. (2004);&nbsp;Baron &amp; Thomley (1994).</small></p>\n<p><small><sup>14</sup> See Cushman et al. (2010).</small></p>\n<p><small><sup>15</sup> From Greene (2009).</small></p>\n<p><small>&nbsp;</small></p>\n<h4>References</h4>\n<p><small>Anderson,&nbsp;Anderson, Northam, Jacobs, &amp; Catroppa&nbsp;(2001). Development of executive functions through late childhood and&nbsp;adolescence in an Australian sample. <em>Developmental Neuropsychology, 20</em>: 385-406.</small></p>\n<p><small>Baron &amp; Thomley (1994).&nbsp;A Whiff of Reality:&nbsp;Positive Affect as a Potential Mediator of the Effects of Pleasant Fragrances on Task Performance and Helping. <em>Environment and Behavior, 26</em>: 766-784.</small></p>\n<p><small>Bartels (2008).&nbsp;<a href=\"http://home.uchicago.edu/~bartels/papers/Bartels2008.pdf\">Principled moral sentiment and the flexibility of moral judgment and&nbsp;decision making</a>. <em>Cognition, 108</em>: 381-417.</small></p>\n<p><small>Ciaramelli,&nbsp;Muccioli, Ladavas, &amp; di Pellegrino (2007). <a href=\"http://scan.oxfordjournals.org/content/2/2/84.full.pdf\">Selective deficit in personal&nbsp;moral judgment following damage to ventromedial prefrontal cortex</a>. <em>Social&nbsp;Cognitive and Affective Neuroscience, 2</em>: 84-92.</small></p>\n<p><small>Cushman, Young, &amp; Greene (2010). <a href=\"http://www.wjh.harvard.edu/~cushman/publications/Publications_files/MPRG.pdf\">Multi-system moral psychology</a>. In Doris (ed.), <em>The Moral Psychology Handbook </em>(pp. 47-71). Oxford University Press.</small></p>\n<p><small>Greene,&nbsp;Sommerville, Nystrom Darley, &amp; Cohen (2001). An fMRI&nbsp;investigation of emotional engagement in moral judgment. <em>Science, 293</em>: 2105-2108.</small></p>\n<p><small>Greene,&nbsp;Nystrom, Engell, Darley, &amp; Cohen (2004). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.141.2104&amp;rep=rep1&amp;type=pdf\">The neural bases of&nbsp;cognitive conflict and control in moral judgment</a>. <em>Neuron, 44</em>: 389-400.</small></p>\n<p><small>Greene (2007). <a href=\"http://www.fed.cuhk.edu.hk/~lchang/material/Evolutionary/Developmental/Greene-KantSoul.pdf\">The secret joke of Kant's soul</a>.&nbsp;In&nbsp;Sinnott-Armstrong (ed.), <em>Moral Psychology Vol. 3: The Neuroscience of Morality</em>&nbsp;(pp. 35-79). MIT Press.</small></p>\n<p><small>Greene, Morelli, Lowenberg, Nystrom, &amp; Cohen (2008). <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2429958/pdf/nihms50578.pdf\">Cognitive load selectively interferes with utilitarian moral judgment</a>. <em>Cognition, 107</em>: 1144-1154.</small></p>\n<p><small>Greene (2009).&nbsp;<a href=\"http://www.wjh.harvard.edu/~jgreene/GreeneWJH/Greene-CogNeuroIV-09.pdf\">The cognitive neuroscience of moral judgment</a>. In Gazzaniga (ed.), The&nbsp;Cognitive Neurosciences, Fourth Edition (pp. 987&ndash;999). MIT Press.</small></p>\n<p><small>Haidt,&nbsp;Koller, &amp; Dias (1993). <a href=\"http://www.msmidia.com/CEPRUA/artigos/affect.pdf\">Affect, culture, and morality, or is&nbsp;it wrong to eat your dog?</a> <em>Journal of Personality and Social Psychology 65:</em>&nbsp;613-628.</small></p>\n<p><small>Hardman (2008).&nbsp;Moral dilemmas: Who makes utilitarian choices. In&nbsp;Hare (ed.), <em>Hare Psychopathy Checklist--Revised (PCL-R): 2nd Edition</em>. Multi-Health Systems, Inc.</small></p>\n<p><small>Haidt (2001).&nbsp;<a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.124.9206&amp;rep=rep1&amp;type=pdf\">The emotional dog and its rational tail: A social intuitionist approach to&nbsp;moral judgment</a>. <em>Psychological Review, 108</em>: 814-834.</small></p>\n<p><small>Hirstein (2005).&nbsp;<em><a href=\"http://www.amazon.com/Brain-Fiction-Self-Deception-Confabulation-Psychopathology/dp/0262582716/\">Brain Fiction: Self-Deception and the Riddle of Confabulation</a></em>. MIT Press.</small></p>\n<p><small>Hume (1740). <em><a href=\"http://www.amazon.com/treatise-human-nature-David-Hume/dp/1172424926/\">A Treatise of Human Nature</a></em>.</small></p>\n<p><small>Kahane,&nbsp;Wiech, Shackel, Farias, Savulescu, &amp; Tracey (2011). <a href=\"http://scan.oxfordjournals.org/content/early/2011/03/18/scan.nsr005.full.pdf\">The neural basis of intuitive and counterintuitive moral judgment</a>.&nbsp;<em>Social Cognitive &amp; Affective Neuroscience</em>.</small></p>\n<p><small>Kant (1785).&nbsp;<em><a href=\"http://www.amazon.com/Kant-Groundwork-Metaphysics-Cambridge-Philosophy/dp/0521626951/\">Groundwork of the Metaphysics of Morals</a></em>.</small></p>\n<p><small>Koenigs,&nbsp;Young, Cushman, Adolphs, Tranel, Damasio, &amp; Hauser&nbsp;(2007). <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2244801/pdf/nihms38394.pdf\">Damage to the prefrontal cortex increases utilitarian moral judgements</a>. <em>Nature, 446</em>: 908&ndash;911.</small></p>\n<p><small>Kohlberg (1971).&nbsp;From is to ought: How to commit the naturalistic fallacy and&nbsp;get away with it in the study of moral development. In Mischel (ed.), <em>Cognitive&nbsp;development and epistemology</em> (pp. 151&ndash;235). Academic Press.</small></p>\n<p><small>Kurzban (2011).&nbsp;<em><a href=\"http://www.amazon.com/Why-Everyone-Else-Hypocrite-Evolution/dp/0691146748/\">Why Everyone (Else) Is a Hypocrite: Evolution and the Modular Mind</a></em>. Princeton University Press.</small></p>\n<p><small>Mendez, Anderson, &amp; Shapira (2005).&nbsp;An investigation of moral&nbsp;judgment in fronto-temporal dementia. <em>Cognitive and Behavioral Neurology, 18</em>:&nbsp;193&ndash;197.</small></p>\n<p><small>Moore, Clark, &amp; Kane (2008).&nbsp;<a href=\"http://www.princeton.edu/~aconway/pdf/Moore_Clark_Kane_2008.pdf\">Who shalt not kill?: Individual differences in&nbsp;working memory capacity, executive control, and moral judgment</a>. <em>Psychological&nbsp;Science, 19</em>: 549-557.</small></p>\n<p><small>Paus,&nbsp;Zijdenbos, Worsley, Collins, Blumenthal, Giedd, Rapoport, &amp; Evans (1999). Structural maturation of neural&nbsp;pathways in children and adolescents: In vivo study. <em>Science, 283</em>: 1908-1911.</small></p>\n<p><small>Petrinovich,&nbsp;O'Neill, Jorgensen (1993). An empirical study of moral intuitions:&nbsp;Toward an evolutionary ethics. <em>Journal of Personality and Social Psychology, 64</em>: 467-478.</small></p>\n<p><small>Petrinovich &amp; O&rsquo;Neill (1996).&nbsp;Influence of wording and framing effects&nbsp;on moral intuitions. <em>Ethology and Sociobiology, 17</em>: 145-171.</small></p>\n<p><small>Schnall,&nbsp;Haidt, &amp; Clore (2004). Irrelevant disgust makes moral&nbsp;judgment more severe, for those who listen to their bodies. Unpublished manuscript.</small></p>\n<p><small>Smith (1759). <em><a href=\"http://www.amazon.com/Theory-Moral-Sentiments-Penguin-Classics/dp/0143105922/\">The Theory of Moral Sentiments</a></em>.&nbsp;</small></p>\n<p><small>Steinburg &amp; Scott (2003).&nbsp;<a href=\"http://faculty.vassar.edu/abbaird/PreviousSite/juvJustice/steinberg.pdf\">Less guilty by reason of adolescence:&nbsp;Developmental immaturity, diminished responsibility, and the juvenile death&nbsp;penalty</a>. <em>American Psychologist, 58</em>: 1009-1018.</small></p>\n<p><small>Suter &amp; Hertwig (2011). <a href=\"http://cds.unibas.ch/~hertwig/pdfs/2011/SuterHertwig2011_time_moral_judgment.pdf\">Time and moral judgment</a>. Cognition, 119: 454-458.</small></p>\n<p><small>Valdesolo &amp; DeSteno (2006). <a href=\"http://daviddesteno.com/page5/files/Valdesolo.DeSteno.2006.pdf\">Manipulations of emotional context shape moral&nbsp;judgment</a>. <em>Psychological Science, 17</em>: 476-477.</small></p>\n<p><small>Wheatley &amp; Haidt (2005).&nbsp;<a href=\"http://www.fed.cuhk.edu.hk/~lchang/material/Evolutionary/Morality/Hypnotic%20disgust%20makes%20moral%20judgments%20more%20severe.pdf\">Hypnotically induced disgust makes moral&nbsp;judgments more severe</a>. <em>Psychological Science, 16</em>: 780-784.</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yeJFqsWrP2pjYfNEr": 1, "3ee9k6NJfcGzL6kMS": 2, "ZTRNmvQGgoYiymYnq": 2, "nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "62p74DvwNHgQXCXcH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 56, "baseScore": 52, "extendedScore": null, "score": 9.4e-05, "legacy": true, "legacyId": "9231", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In 2007, Chris Matthews of <em><a href=\"http://en.wikipedia.org/wiki/Hardball_with_Chris_Matthews\">Hardball</a></em>&nbsp;interviewed David O'steen, executive director of a pro-life organization. Matthews asked:</p>\n<blockquote>\n<p>I have always wondered something about the pro-life movement. If you believe that killing [a fetus] is murder, why don't you bring murder charges or seek a murder penalty against a woman who has an abortion? Why do you let her off, if you really believe it's murder?<sup>1</sup></p>\n</blockquote>\n<p>O'steen replied that \"we have never sought criminal penalties against a woman,\" which isn't an answer but a re-statement of the reason for the question. When pressed, he added that we don't know \"how she\u2018s been forced into this.\" When pressed again, O'steen abandoned these responses and tried to give a consequentialist answer. He claimed that implementing \"civil penalties\" and taking away the \"financial incentives\" of abortion doctors would more successfully \"protect unborn children.\"</p>\n<p>But this still doesn't answer the question. If you believe that killing a fetus is murder, then a woman seeking an abortion pays a doctor to commit murder. Why don't abortionists want to change the laws so that abortion is considered murder and a woman who has an abortion can be charged with paying a doctor to commit murder?&nbsp;Psychologist Robert Kurzban cites this as a classic case of moral rationalization.<sup>2</sup></p>\n<p>Pro-life demonstrators in Illinois were asked a similar question: \"If [abortion] was illegal, should there be a penalty for the women who get abortions illegally?\" None of them (on <a href=\"http://www.youtube.com/watch?v=iD97OVJ4PNw\">the video</a>) thought that women who had illegal abortions should be punished as murders, an ample demonstration of moral rationalization. And&nbsp;I'm sure we can all think of examples where it looks like someone has settled on an intuitive moral judgment and then invented rationalizations later.<sup>3</sup></p>\n<p>More controversially, some have&nbsp;<a href=\"http://www.fed.cuhk.edu.hk/~lchang/material/Evolutionary/Developmental/Greene-KantSoul.pdf\">suggested</a> that rule-based deontological moral judgments <em>generally</em> tend to be&nbsp;<a href=\"/lw/ju/rationalization/\">rationalizations</a>. Perhaps we can even&nbsp;<a href=\"/lw/of/dissolving_the_question/\">dissolve</a> the debate between deontological intuitions and utilitarian intuitions if we can <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">map the cognitive algorithms</a> that produce them.</p>\n<p>Long-time deontologists and utilitarians may already be up in arms to fight another war between <a href=\"/lw/gt/a_fable_of_science_and_politics/\">Blues and Greens</a>, but these are empirical questions. What do the scientific studies suggest?</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h4 id=\"Utilitarian_and_Deontological_Processes\">Utilitarian and Deontological Processes</h4>\n<p>A runaway trolley is about to run over and kill five people, but you can save them by hitting a switch that will put the trolley on a side track where it will only kill <em>one</em>&nbsp;person. Do you throw the switch? When confronted with this <em>switch dilemma</em>, most people say it is morally good to divert the trolley,<sup>4</sup> thereby achieving the utilitarian 'greater good'.</p>\n<p>Now, consider the <em>footbridge dilemma</em>. Again, a runaway trolley threatens five people, and the only way to save them is to push a large person off a footbridge onto the tracks, which will stop the trolley but kill the person you push. (Your body is too small to stop the trolley.) Do you push the large person off the bridge? Here, most people say it's wrong to trade one life for five, allowing a deontological commitment to individual rights to trump utilitarian considerations of the greater good.</p>\n<p>Researchers presented subjects with a variety of 'impersonal' dilemmas (including the switch dilemma) and 'up-close-and-personal' dilemmas (including the footbridge dilemma). Personal dilemmas preferentially engaged brain areas associated with emotion. Impersonal dilemmas preferentially engaged the regions of the brain associated with working memory and cognitive control.<sup>5</sup></p>\n<p>This suggested a <a href=\"/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">dual-process theory</a> of moral judgment, according to which the footbridge dilemma elicits a conflict between emotional intuition (\"you must not push people off bridges!\") and utilitarian calculation (\"pushing the person off the bridge will result in the fewest deaths\"). In the footbridge case, emotional intuition wins out in most people.</p>\n<p>But now, consider the <em>crying baby dilemma</em>&nbsp;from the <a href=\"http://en.wikipedia.org/wiki/Goodbye,_Farewell_and_Amen#Plot\">final episode</a> of <em>M.A.S.H</em>:</p>\n<blockquote>\n<p>It's wartime. You and your fellow villagers are hiding from nearby enemy soldiers in a basement. Your baby starts to cry, and you cover your baby's mouth to block the sound. If you remove your hand, your baby will cry loudly, and the soldiers will hear. They will find you... and they will kill all of your. If you do not remove your hand, your baby will smother to death. Is it morally acceptable to smother your baby to death in order to save yourself and the other villagers?<sup>6</sup></p>\n</blockquote>\n<p>Here, people take a long time to answer, and they show no consensus in their answers. If the dual-process theory of moral judgment is correct, then people considering the crying baby dilemma should exhibit increased activity in the ACC (a region associated with response conflict), and in regions associated with cognitive control (for overriding a potent emotional response with utilitarian calculation). Also, those who eventually choose the characteristically utilitarian answer (save the most lives) over the characteristically deontological answer (don't kill the baby) should exhibit comparatively more activity in brain regions associated with working memory and cognitive control. All three predictions turn out to be true.<sup>7</sup></p>\n<p>Moreover, patients with two different kinds of dementia or lesions that cause \"emotional blunting\" are disproportionately likely to approve of utilitarian action in the footbridge dilemma,<sup>8</sup> and cognitive load manipulations that keep working memory occupied slow down utilitarian judgments but not deontological judgments.<sup>9</sup></p>\n<p>Studies of individual differences also seem to support the dual-process theory. Individuals who are (1) high in \"need for cognition\" and low in \"faith in intuition\", or (2) score well on the <a href=\"http://mitsloan.mit.edu/newsroom/newsbriefs-0605-frederick.php\">Cognitive Reflection Test</a>, or (3) have unusually high working memory capacity... all give more utilitarian judgments.<sup>10</sup></p>\n<p>This leads us to <a href=\"http://www.wjh.harvard.edu/~jgreene/\">Joshua Greene</a>'s bold claim:</p>\n<blockquote>\n<p>...deontological&nbsp;judgments tend to be driven by emotional responses, and... deontological philosophy, rather than being grounded in moral reasoning,&nbsp;is to a large extent an exercise in moral rationalization. This is in contrast&nbsp;to consequentialism, which, I will argue, arises from rather different psychological&nbsp;processes, ones that are more 'cognitive,' and more likely to&nbsp;involve genuine moral reasoning...</p>\n<p>[Psychologically,] deontological moral philosophy really is... an attempt to produce rational justifications for emotionally driven moral judgments, and not an attempt to reach moral conclusions on the basis of moral reasoning.<sup>11</sup></p>\n</blockquote>\n<p>&nbsp;</p>\n<h4 id=\"Cognition_and_Emotion\">Cognition and Emotion</h4>\n<p>Greene explains the difference between 'cognitive' and 'emotional' processes in the brain (though both involve information processing, and so are 'cognitive' in a broader sense):</p>\n<blockquote>\n<p>...'cognitive' processes are especially important&nbsp;for reasoning, planning, manipulating information in&nbsp;working memory, controlling impulses, and 'higher executive functions'&nbsp;more generally. Moreover, these functions tend to be associated with&nbsp;certain parts of the brain, primarily the dorsolateral surfaces of the prefrontal&nbsp;cortex and parietal lobes... Emotion, in contrast, tends to&nbsp;be associated with other parts of the brain, such as the amygdala and the&nbsp;medial surfaces of the frontal and parietal lobes... And while the term&nbsp;'emotion' can refer to stable states such as moods, here we will primarily&nbsp;be concerned with emotions subserved by processes that in addition&nbsp;to being valenced, are quick and automatic, though not necessarily&nbsp;conscious.</p>\n</blockquote>\n<p>Since we are concerned with two kinds of moral judgment (deontological and consequentialist) and two kinds of neurological process (cognitive and emotional), we have four empirical possibilities:</p>\n<blockquote>\n<p>First, it could be that both kinds of moral judgment&nbsp;are generally 'cognitive', as Kohlberg\u2019s theories suggest (Kohlberg, 1971). At the other extreme, it could be that both kinds of moral judgment are&nbsp;primarily emotional, as Haidt\u2019s view suggests (Haidt, 2001). Then there is&nbsp;the historical stereotype, according to which consequentialism is more&nbsp;emotional (emerging from the 'sentimentalist' tradition of David Hume&nbsp;(1740) and Adam Smith (1759) while deontology is more 'cognitive'&nbsp;[including the Kantian 'rationalist' tradition: see Kant (1785)].&nbsp;Finally, there is the view for which I will argue, that deontology is more&nbsp;emotionally driven while consequentialism is more 'cognitive.'</p>\n</blockquote>\n<p>We have already seen the neuroscientific evidence in favor of Greene's view. Now, let us turn to further evidence from the work of Jon Haidt.</p>\n<p>&nbsp;</p>\n<h4 id=\"Emotion_and_Deontological_Judgments\">Emotion and Deontological Judgments</h4>\n<p>Haidt &amp; colleagues (1993) presented subjects with a sequence of harmless actions, for example:</p>\n<ol>\n<li>A son promises his dying mother that he will visit her grave every day after she has died, but then doesn\u2019t because he is busy.</li>\n<li>A woman uses an old American flag to clean the&nbsp;bathroom.</li>\n<li>A family eats its dog after it has been killed accidentally by a car.</li>\n<li>A brother and sister kiss on the lips.</li>\n<li>A man masturbates using a dead chicken before cooking and eating it.</li>\n</ol>\n<p>For each action, subjects were asked questions like: Is this action wrong? Why? Does it hurt anyone? If someone did this, would it bother you? Greene summarizes the results:</p>\n<blockquote>\n<p>When people say that such actions are wrong, why do they say so? One&nbsp;hypothesis is that these actions are perceived as harmful, whether or not&nbsp;they really are... Kissing siblings could cause&nbsp;themselves psychological damage. Masturbating with a chicken could&nbsp;spread disease, etc. If this hypothesis is correct, then we would expect&nbsp;people\u2019s answers to the question \"Does this action hurt anyone?\" to correlate&nbsp;with their degree of moral condemnation... Alternatively, if emotions&nbsp;drive moral condemnation in these cases, then we would expect&nbsp;people\u2019s answers to the question \"If you saw this, would it bother you?\"&nbsp;to better predict their answers to the moral questions posed.</p>\n</blockquote>\n<p>If you're following along, it may not surprise you that emotions seemed to be driving the deontological condemnation of harmless actions. Moreover, both education and adulthood were correlated with more consequentialist judgments. (Cognitive control of basic emotional reactions is something that develops during adolescence.<sup>12</sup>) Greene reminds us:</p>\n<blockquote>\n<p>These... findings make sense in&nbsp;light of the model of moral judgment we have been developing, according&nbsp;to which intuitive emotional responses drive prepotent moral intuitions&nbsp;while 'cognitive' control processes sometimes rein them in.</p>\n</blockquote>\n<p>But there is more direct evidence of the link between emotion and the deontological condemnation of harmless actions.</p>\n<p>Wheatley &amp; Haidt (2005) gathered hypnotizable subjects and gave some of them a hypnotic suggestion to feel disgust upon reading the word 'often', while giving others a hypnotic suggestion to feel disgust upon reading the word 'take'. The researchers then showed these subjects a variety of scenarios, some of them involving no harm. (For example, two second cousins have a relationship in which they \"<em>take</em> weekend trips to romantic hotels\" or else \"<em>often go on</em> weekend trips to romantic hotels\".) As expected, subjects who received the wordings they had been primed to feel disgust toward judged the couple's actions as more morally condemnable than other subjects did.</p>\n<p>In a second experiment, Wheatley and Haidt used the same technique and had subjects respond to a scenario in which a person did nothing remotely wrong: a student \"<em>often</em> picks\" or \"tries to <em>take</em> up\" broad topics of discussion at meetings. Still, many subjects who were given the matching hypnotic suggestion rated the student's actions as morally wrong. When asked why, they invented rationalizations like&nbsp;\"It just seems like he\u2019s up to something\" or \"It just seems so weird and disgusting\" or \"I don\u2019t know [why it\u2019s wrong], it just is.\"</p>\n<p>In other studies, researchers implemented a disgust condition by placing some subjects at a dirty desk or in the presence of fart spray.&nbsp;As before, those in the disgust condition were more likely to rate harmless actions as morally wrong than other subjects were.<sup>13</sup></p>\n<p>Finally, consider that the dual-process theory of moral judgment predicts that deontological judgments will be quicker than utilitarian ones, because deontological judgments use emotional and largely unconscious brain modules while utilitarian judgments require slow, conscious calculation. Suter &amp; Hertwig (2011) presented subjects with a variety of moral dilemmas and alternatively prodded them to give their judgments quickly or take their time to deliberate thoroughly. As predicted, faster responses predicted more deontological judgments.</p>\n<p>&nbsp;</p>\n<h4 id=\"Summing_Up\">Summing Up</h4>\n<p>We are a species prone to emotional moral judgment, and to rationalization ('<a href=\"http://www.amazon.com/Brain-Fiction-Self-Deception-Confabulation-Psychopathology/dp/0262582716/\">confabulation</a>'). And, Greene writes,</p>\n<blockquote>\n<p>What should we expect&nbsp;from creatures who exhibit social and moral behavior that is driven largely&nbsp;by intuitive emotional responses and who are prone to rationalization of&nbsp;their behaviors? The answer, I believe, is deontological moral philosophy...</p>\n<p>Whether or not&nbsp;we can ultimately justify pushing the man off the footbridge, it will always&nbsp;<em>feel</em> wrong. And what better way to express that feeling of non-negotiable&nbsp;absolute wrongness than via the most central of deontological concepts,&nbsp;the concept of a <em>right</em>: You can\u2019t push him to his death because that would&nbsp;be a violation of his <em>rights</em>.</p>\n<p>Deontology, then, is a kind of moral confabulation. We have strong&nbsp;feelings that tell us in clear and uncertain terms that some things simply&nbsp;cannot be done and that other things simply must be done. But it is not&nbsp;obvious how to make sense of these feelings, and so we, with the help of&nbsp;some especially creative philosophers, make up a rationally appealing&nbsp;story: There are these things called 'rights' which people have, and when&nbsp;someone has a right you can\u2019t do anything that would take it away. It&nbsp;doesn\u2019t matter if the guy on the footbridge is toward the end of his natural&nbsp;life, or if there are seven people on the tracks below instead of five. If the&nbsp;man has a right, then <em>the man has a right</em>. As John Rawls... famously said, \"Each person possesses an inviolability founded on&nbsp;justice that even the welfare of society as a whole cannot override\"... These are <a href=\"/lw/jb/applause_lights/\">applause lines</a>&nbsp;because they make emotional sense.</p>\n</blockquote>\n<p>Of course, utilitarian moral judgment is not emotionless. Emotion is probably what leads us to label harm as a 'bad' thing, for example.&nbsp;But utilitarian moral judgment is, as we've seen, particularly demanding of 'cognitive' processes: calculation, the weighing of competing concerns, the adding and averaging of value, and so on. Utilitarian moral judgment uses the same meso-limbic regions that track a stimulus' reward magnitude, reward probability, and expected value.<sup>14</sup></p>\n<p>This does not prove the case that deontological moral judgments are usually rationalizations. But many lines of converging evidence make this a decent hypothesis. And now we can draw our neural map:<sup>15</sup></p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/dual-process-theory-of-moral-judgment.png\" alt=\"\"></p>\n<p>And up until March 18th of this year, Greene had a pretty compelling case for his position that deontological judgments are generally just rationalizations.</p>\n<p>And <a href=\"/lw/74f/are_deontological_moral_judgments_rationalizations/4nmf\">then</a>, Guy Kahane et al. (2011) threw&nbsp;Greene's theory into doubt by testing separately for the content (deontological vs. utilitarian) and the intuitiveness (intuitive vs. not-intuitive) of moral judgments. The authors summarize their results:</p>\n<blockquote>\n<p>Previous neuroimaging studies reported that utilitarian&nbsp;judgments in dilemmas involving extreme harm were associated with activation in the DLPFC and parietal lobe&nbsp;(Greene et al., 2004). This finding has been taken as evidence&nbsp;that utilitarian judgment is generally driven by controlled&nbsp;processing (Greene, 2008). The behavioural and neural&nbsp;data we obtained suggest instead that differences between&nbsp;utilitarian and deontological judgments in dilemmas involving extreme harm largely reflect differences in intuitiveness&nbsp;rather than in content.</p>\n<p>...When we controlled&nbsp;for content, these analyses showed considerable overlap for&nbsp;intuitiveness. In contrast, when we controlled for intuitiveness, only little\u0002if any\u0002overlap was found for content. Our&nbsp;results thus speak against the influential interpretation of&nbsp;previous neuroimaging studies as supporting a general association between deontological judgment and automatic processing, and between utilitarian judgment and controlled&nbsp;processing.</p>\n<p>[This evidence suggests...]&nbsp;that behavioural and neural differences in responses to such&nbsp;dilemmas are largely due to differences in intuitiveness, not&nbsp;to general differences between utilitarian and deontological&nbsp;judgment.</p>\n</blockquote>\n<p>So we'll have to wait for more studies to unravel the mystery of whether deontological moral judgments are generally rationalizations.</p>\n<p>By email, Greene told me he suspected Kahane's 'alternative theory' wasn't much of an alternative to what he (Greene) was proposing in the first place.&nbsp;In his paper, Greene discussed the passage where Kant says it's wrong to lie to&nbsp;prevent a madman from killing someone, and cites this as an example of a&nbsp;case in which a deontological judgment might be more controlled, while the&nbsp;utilitarian judgment is more automatic. Greene's central claim is that when there's a&nbsp;conflict between rights and duties on the one hand, and promoting&nbsp;the greater good on the other, it's typically controlled cognition on the&nbsp;utilitarian side and emotional intuition on the other.&nbsp;</p>\n<p><strong>Update</strong>: Greene's full reply to Kahane et al. is <a href=\"http://www.wjh.harvard.edu/~jgreene/GreeneWJH/Paxton-Bruni-Greene-SCAN13.pdf\">now available</a>.</p>\n<p>But even if Greene's theory is right,&nbsp;<em>humans</em> may still need to use deontological rules because <a href=\"/lw/uv/ends_dont_justify_means_among_humans/\">we run on corrupted hardware</a>.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h4 id=\"Notes\">Notes</h4>\n<p><small><sup>1</sup> <em>Hardball</em>&nbsp;for November 13, 2007. <a href=\"http://www.msnbc.msn.com/id/21791463/ns/msnbc_tv-hardball_with_chris_matthews/t/hardball-chris-matthews-nov/\">Here</a> is the transcript.</small></p>\n<p><small><sup>2</sup> Kurzban (2011), p. 193.</small></p>\n<p><small><sup>3</sup> Also see Jon Haidt's <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Haidt-Moral-Dumfounding-When-Intuition-Finds-No-Reason.pdf\">unpublished manuscript</a> on moral dumfounding, and Hirstein (2005).</small></p>\n<p><small><sup>4</sup> Petrinovich et al. (1993);&nbsp;Petrinovich &amp; O\u2019Neill (1996).</small></p>\n<p><small><sup>5</sup> Greene et al. (2001, 2004).</small></p>\n<p><small><sup>6</sup> Greene (2009).</small></p>\n<p><small><sup>7</sup> Greene et al. (2004).</small></p>\n<p><small><sup>8</sup> Mendez et al. (2005); Koenigs et al. (2007); Ciaramelli et al. (2007).</small></p>\n<p><small><sup>9</sup> Greene et al. (2008).</small></p>\n<p><small><sup>10</sup> Bartels (2008); Hardman (2008); Moore et al. (2008).</small></p>\n<p><small><sup>11</sup> The rest of the Joshua Greene quotes from this article are from Greene (2007).</small></p>\n<p><small><sup>12</sup> Anderson et al. (2001); Paus et al. (1999); Steinburg &amp; Scott (2003).</small></p>\n<p><small><sup>13</sup> Schnall et al. (2004);&nbsp;Baron &amp; Thomley (1994).</small></p>\n<p><small><sup>14</sup> See Cushman et al. (2010).</small></p>\n<p><small><sup>15</sup> From Greene (2009).</small></p>\n<p><small>&nbsp;</small></p>\n<h4 id=\"References\">References</h4>\n<p><small>Anderson,&nbsp;Anderson, Northam, Jacobs, &amp; Catroppa&nbsp;(2001). Development of executive functions through late childhood and&nbsp;adolescence in an Australian sample. <em>Developmental Neuropsychology, 20</em>: 385-406.</small></p>\n<p><small>Baron &amp; Thomley (1994).&nbsp;A Whiff of Reality:&nbsp;Positive Affect as a Potential Mediator of the Effects of Pleasant Fragrances on Task Performance and Helping. <em>Environment and Behavior, 26</em>: 766-784.</small></p>\n<p><small>Bartels (2008).&nbsp;<a href=\"http://home.uchicago.edu/~bartels/papers/Bartels2008.pdf\">Principled moral sentiment and the flexibility of moral judgment and&nbsp;decision making</a>. <em>Cognition, 108</em>: 381-417.</small></p>\n<p><small>Ciaramelli,&nbsp;Muccioli, Ladavas, &amp; di Pellegrino (2007). <a href=\"http://scan.oxfordjournals.org/content/2/2/84.full.pdf\">Selective deficit in personal&nbsp;moral judgment following damage to ventromedial prefrontal cortex</a>. <em>Social&nbsp;Cognitive and Affective Neuroscience, 2</em>: 84-92.</small></p>\n<p><small>Cushman, Young, &amp; Greene (2010). <a href=\"http://www.wjh.harvard.edu/~cushman/publications/Publications_files/MPRG.pdf\">Multi-system moral psychology</a>. In Doris (ed.), <em>The Moral Psychology Handbook </em>(pp. 47-71). Oxford University Press.</small></p>\n<p><small>Greene,&nbsp;Sommerville, Nystrom Darley, &amp; Cohen (2001). An fMRI&nbsp;investigation of emotional engagement in moral judgment. <em>Science, 293</em>: 2105-2108.</small></p>\n<p><small>Greene,&nbsp;Nystrom, Engell, Darley, &amp; Cohen (2004). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.141.2104&amp;rep=rep1&amp;type=pdf\">The neural bases of&nbsp;cognitive conflict and control in moral judgment</a>. <em>Neuron, 44</em>: 389-400.</small></p>\n<p><small>Greene (2007). <a href=\"http://www.fed.cuhk.edu.hk/~lchang/material/Evolutionary/Developmental/Greene-KantSoul.pdf\">The secret joke of Kant's soul</a>.&nbsp;In&nbsp;Sinnott-Armstrong (ed.), <em>Moral Psychology Vol. 3: The Neuroscience of Morality</em>&nbsp;(pp. 35-79). MIT Press.</small></p>\n<p><small>Greene, Morelli, Lowenberg, Nystrom, &amp; Cohen (2008). <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2429958/pdf/nihms50578.pdf\">Cognitive load selectively interferes with utilitarian moral judgment</a>. <em>Cognition, 107</em>: 1144-1154.</small></p>\n<p><small>Greene (2009).&nbsp;<a href=\"http://www.wjh.harvard.edu/~jgreene/GreeneWJH/Greene-CogNeuroIV-09.pdf\">The cognitive neuroscience of moral judgment</a>. In Gazzaniga (ed.), The&nbsp;Cognitive Neurosciences, Fourth Edition (pp. 987\u2013999). MIT Press.</small></p>\n<p><small>Haidt,&nbsp;Koller, &amp; Dias (1993). <a href=\"http://www.msmidia.com/CEPRUA/artigos/affect.pdf\">Affect, culture, and morality, or is&nbsp;it wrong to eat your dog?</a> <em>Journal of Personality and Social Psychology 65:</em>&nbsp;613-628.</small></p>\n<p><small>Hardman (2008).&nbsp;Moral dilemmas: Who makes utilitarian choices. In&nbsp;Hare (ed.), <em>Hare Psychopathy Checklist--Revised (PCL-R): 2nd Edition</em>. Multi-Health Systems, Inc.</small></p>\n<p><small>Haidt (2001).&nbsp;<a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.124.9206&amp;rep=rep1&amp;type=pdf\">The emotional dog and its rational tail: A social intuitionist approach to&nbsp;moral judgment</a>. <em>Psychological Review, 108</em>: 814-834.</small></p>\n<p><small>Hirstein (2005).&nbsp;<em><a href=\"http://www.amazon.com/Brain-Fiction-Self-Deception-Confabulation-Psychopathology/dp/0262582716/\">Brain Fiction: Self-Deception and the Riddle of Confabulation</a></em>. MIT Press.</small></p>\n<p><small>Hume (1740). <em><a href=\"http://www.amazon.com/treatise-human-nature-David-Hume/dp/1172424926/\">A Treatise of Human Nature</a></em>.</small></p>\n<p><small>Kahane,&nbsp;Wiech, Shackel, Farias, Savulescu, &amp; Tracey (2011). <a href=\"http://scan.oxfordjournals.org/content/early/2011/03/18/scan.nsr005.full.pdf\">The neural basis of intuitive and counterintuitive moral judgment</a>.&nbsp;<em>Social Cognitive &amp; Affective Neuroscience</em>.</small></p>\n<p><small>Kant (1785).&nbsp;<em><a href=\"http://www.amazon.com/Kant-Groundwork-Metaphysics-Cambridge-Philosophy/dp/0521626951/\">Groundwork of the Metaphysics of Morals</a></em>.</small></p>\n<p><small>Koenigs,&nbsp;Young, Cushman, Adolphs, Tranel, Damasio, &amp; Hauser&nbsp;(2007). <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2244801/pdf/nihms38394.pdf\">Damage to the prefrontal cortex increases utilitarian moral judgements</a>. <em>Nature, 446</em>: 908\u2013911.</small></p>\n<p><small>Kohlberg (1971).&nbsp;From is to ought: How to commit the naturalistic fallacy and&nbsp;get away with it in the study of moral development. In Mischel (ed.), <em>Cognitive&nbsp;development and epistemology</em> (pp. 151\u2013235). Academic Press.</small></p>\n<p><small>Kurzban (2011).&nbsp;<em><a href=\"http://www.amazon.com/Why-Everyone-Else-Hypocrite-Evolution/dp/0691146748/\">Why Everyone (Else) Is a Hypocrite: Evolution and the Modular Mind</a></em>. Princeton University Press.</small></p>\n<p><small>Mendez, Anderson, &amp; Shapira (2005).&nbsp;An investigation of moral&nbsp;judgment in fronto-temporal dementia. <em>Cognitive and Behavioral Neurology, 18</em>:&nbsp;193\u2013197.</small></p>\n<p><small>Moore, Clark, &amp; Kane (2008).&nbsp;<a href=\"http://www.princeton.edu/~aconway/pdf/Moore_Clark_Kane_2008.pdf\">Who shalt not kill?: Individual differences in&nbsp;working memory capacity, executive control, and moral judgment</a>. <em>Psychological&nbsp;Science, 19</em>: 549-557.</small></p>\n<p><small>Paus,&nbsp;Zijdenbos, Worsley, Collins, Blumenthal, Giedd, Rapoport, &amp; Evans (1999). Structural maturation of neural&nbsp;pathways in children and adolescents: In vivo study. <em>Science, 283</em>: 1908-1911.</small></p>\n<p><small>Petrinovich,&nbsp;O'Neill, Jorgensen (1993). An empirical study of moral intuitions:&nbsp;Toward an evolutionary ethics. <em>Journal of Personality and Social Psychology, 64</em>: 467-478.</small></p>\n<p><small>Petrinovich &amp; O\u2019Neill (1996).&nbsp;Influence of wording and framing effects&nbsp;on moral intuitions. <em>Ethology and Sociobiology, 17</em>: 145-171.</small></p>\n<p><small>Schnall,&nbsp;Haidt, &amp; Clore (2004). Irrelevant disgust makes moral&nbsp;judgment more severe, for those who listen to their bodies. Unpublished manuscript.</small></p>\n<p><small>Smith (1759). <em><a href=\"http://www.amazon.com/Theory-Moral-Sentiments-Penguin-Classics/dp/0143105922/\">The Theory of Moral Sentiments</a></em>.&nbsp;</small></p>\n<p><small>Steinburg &amp; Scott (2003).&nbsp;<a href=\"http://faculty.vassar.edu/abbaird/PreviousSite/juvJustice/steinberg.pdf\">Less guilty by reason of adolescence:&nbsp;Developmental immaturity, diminished responsibility, and the juvenile death&nbsp;penalty</a>. <em>American Psychologist, 58</em>: 1009-1018.</small></p>\n<p><small>Suter &amp; Hertwig (2011). <a href=\"http://cds.unibas.ch/~hertwig/pdfs/2011/SuterHertwig2011_time_moral_judgment.pdf\">Time and moral judgment</a>. Cognition, 119: 454-458.</small></p>\n<p><small>Valdesolo &amp; DeSteno (2006). <a href=\"http://daviddesteno.com/page5/files/Valdesolo.DeSteno.2006.pdf\">Manipulations of emotional context shape moral&nbsp;judgment</a>. <em>Psychological Science, 17</em>: 476-477.</small></p>\n<p><small>Wheatley &amp; Haidt (2005).&nbsp;<a href=\"http://www.fed.cuhk.edu.hk/~lchang/material/Evolutionary/Morality/Hypnotic%20disgust%20makes%20moral%20judgments%20more%20severe.pdf\">Hypnotically induced disgust makes moral&nbsp;judgments more severe</a>. <em>Psychological Science, 16</em>: 780-784.</small></p>", "sections": [{"title": "Utilitarian and Deontological Processes", "anchor": "Utilitarian_and_Deontological_Processes", "level": 1}, {"title": "Cognition and Emotion", "anchor": "Cognition_and_Emotion", "level": 1}, {"title": "Emotion and Deontological Judgments", "anchor": "Emotion_and_Deontological_Judgments", "level": 1}, {"title": "Summing Up", "anchor": "Summing_Up", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "170 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 171, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SFZoEBpLo9frSJGkc", "Mc6QcrsbH5NRXbCRX", "yA4gF5KrboK2m2Xu7", "6hfGNLf4Hg5DXqJCF", "du395YvCnQXBPSJax", "dLbkrPu5STNCBLRjr", "K9ZaZXDnL3SEmYZqB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-16T18:40:55.939Z", "modifiedAt": null, "url": null, "title": "The Goal of the Bayesian Conspiracy", "slug": "the-goal-of-the-bayesian-conspiracy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:05.671Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Arandur", "createdAt": "2011-05-04T03:35:00.337Z", "isAdmin": false, "displayName": "Arandur"}, "userId": "c4YcZZqzWYdwazKHL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PCs9xBqStCuWFmmsq/the-goal-of-the-bayesian-conspiracy", "pageUrlRelative": "/posts/PCs9xBqStCuWFmmsq/the-goal-of-the-bayesian-conspiracy", "linkUrl": "https://www.lesswrong.com/posts/PCs9xBqStCuWFmmsq/the-goal-of-the-bayesian-conspiracy", "postedAtFormatted": "Tuesday, August 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Goal%20of%20the%20Bayesian%20Conspiracy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Goal%20of%20the%20Bayesian%20Conspiracy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPCs9xBqStCuWFmmsq%2Fthe-goal-of-the-bayesian-conspiracy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Goal%20of%20the%20Bayesian%20Conspiracy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPCs9xBqStCuWFmmsq%2Fthe-goal-of-the-bayesian-conspiracy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPCs9xBqStCuWFmmsq%2Fthe-goal-of-the-bayesian-conspiracy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1659, "htmlBody": "<p>Suppose that there were to exist such an entity as the Bayesian Conspiracy.</p>\n<p>I speak not of the <a href=\"/lw/6rh/bayesian_conspiracy_burning_man_2011/\">social group</a> of that name, the banner under which rationalists meet at various conventions &ndash; though I do not intend to disparage that group! Indeed, it is my fervent hope that they may in due time grow into the entity which I am setting out to describe. No, I speak of something more like the &ldquo;<a href=\"http://yudkowsky.net/rational/bayes\">shadowy group of scientists</a>&rdquo; which Yudkowsky describes, tongue (one might assume) firmly in cheek. I speak of such an organization which has been described in Yudkowsky's <a href=\"http://wiki.lesswrong.com/wiki/Beisutsukai\">various fictional works</a>, the secret and sacred cabal of mathematicians and empiricists who seek unwaveringly for truth... but set in the modern-day world, perhaps merely the seed of such a school, an organization which can survive and thrive in the midst of, yet isolated from, our worldwide sociopolitical mess. I ask you, if such an organization existed, right now, what would &ndash; indeed, what should &ndash; be its primary mid-term (say, 50-100 yrs.) goal?</p>\n<p>I submit that the primary mid-term goal of the Bayesian Conspiracy, at this stage of its existence, is and/or ought to be nothing less than world domination.</p>\n<p>Before the rotten fruit begins to fly, let me make a brief clarification.</p>\n<p>The term &ldquo;world domination&rdquo; is, unfortunately, rather socially charged, bringing to mind an image of the archetypal mad scientist with marching robot armies. That's not what I'm talking about. My usage of the phrase is intended to evoke something slightly less dramatic, and far less sinister. &ldquo;World domination&rdquo;, to me, actually describes rather a loosely packed set of possible world-states. One example would be the one I term &ldquo;One World Government&rdquo;, wherein the Conspiracy (either openly or in secret) is in charge of all nations via an explicit central meta-government. Another would be a simple infiltration of the world's extant political systems, followed by policy-making and cooperation which would ensure the general welfare of the world's entire population &ndash; control de facto, but without changing too much outwardly. The common thread is simply that the Conspiracy becomes the only major influence in world politics.</p>\n<p>(Forgive my less-than-rigorous definition, but a <a href=\"http://wiki.lesswrong.com/wiki/Causality\">thorough examination</a> of the <a href=\"/lw/np/disputing_definitions/\">exact definition</a> of the word &ldquo;influence&rdquo; is far, far outside the scope of this article.)</p>\n<p>So there is my claim. Let me tell you why I believe this is the <a href=\"/lw/sm/the_meaning_of_right/\">morally correct</a> course of action.</p>\n<p>Let us examine, for a moment, the numerous major good works which are currently being openly done by rationalists, or with those who may not self-identify as rationalists, but whose dogmas and goals accord with ours. We have the <a href=\"http://intelligence.org/\">Singularity Institute</a>, which is concerned with ensuring that our technological, transhumanistic advent happens smoothly and with a minimum of carnage. We have <a href=\"http://www.cryonics.org/\">various</a> <a href=\"http://www.alcor.org/\">institutions</a> worldwide advocating and practicing cryonics, which offers a non-zero probability of recovery from death. We have various <a href=\"http://www.lifeextensionfoundation.org/\">institutions</a> also who are working on life extension technologies and procedures, which offer to one day remove the threat of death entirely from our world.</p>\n<p>All good things, I say. I also say: too slow!</p>\n<p>Imagine what more could be accomplished if the United States, for example, granted to the Life Extension Foundation or to Alcor the amount of money and social prominence currently reserved for <a href=\"http://en.wikipedia.org/wiki/Military_budget_of_the_United_States#Budget_Breakdown_for_2012\">military purposes</a>. Imagine what would happen if every scientist around the world were perhaps able to contribute under a unified institution, working on this vitally important problem of overcoming death, with all the money and time the world's governments could offer at their disposal.</p>\n<p>Imagine, also, how many lives are lost every day due to governmental negligence, and war, and poverty, and hunger. What does it profit the world, if we offer to freeze the heads of those who can afford it, while all around us there are people who can't even afford their bread and water?</p>\n<p>I have what is, perhaps, to some who are particularly invested, an appalling and frightening proposition: for the moment, we should devote fewer of our resources to cryonics and life extension, and focus on saving the lives of those to whom these technologies are currently beyond even a fevered dream. This means holding the reins of the world, that we might fix the problems inherent in our society. Only when significant steps have been taken in the direction of saving life can we turn our focus toward extending life.</p>\n<p>What should the Bayesian Conspiracy do, once it comes to power? It should stop war. It should usurp murderous despots, and feed the hungry and wretched who suffered under them. Again: before we work on extending the lives of the healthy and affluent beyond what we've so far achieved, we should, for example, bring the average life expectancy in Africa above the 50-year mark, where it currently sits (according to a 2006 study in the BMJ). This is what will bring about the maximum level of happiness in the world; not cryonics for those who can afford it.</p>\n<p>Does this mean that we should stop researching these anti-death technologies? No! Of course not! Consider: even if cryonics drops to, say, priority 3 or 4 under this system, once the Conspiracy comes to power, that will still be far more support than it's currently receiving from world governments. The work will end up progressing at a far faster rate than it currently does.</p>\n<p><span>Some of you may have qualms about this plan of action. You may ask, what about individual choice? What about the peoples' right to choose who leads them? Well, for those of us who live in the United States, at least, this is already a bit of a na&iuml;ve question: due to <a href=\"/lw/gt/a_fable_of_science_and_politics/\">color politics</a>, you already do not have much of a choice in who leads you. But that's a matter for another time. Even if you think that dictatorship &ndash; even benevolent, rationalist dictatorship &ndash; would be inherently morally worse than even the flawed democratic system we enjoy here &ndash; a notion that may not even necessarily be the case!&nbsp;</span>&ndash;<span>&nbsp;do not worry: there's no reason why world domination need entail dictatorships. In countries where there are democratic systems in place, we will work within the system, placing Conspirators into positions where they can convince the people, via legitimate means, to give them public office. Once we have attained a sufficient level of power over this democratic system, we will effect change, and thence the work will go forth until this victory of rationalist dogma covers all the earth. When there are dictators, they will be removed and replaced with democratic systems... under the initial control of Conspirators, of course, and ideally under their continued control as time passes &ndash; but legitimately obtained control.</span></p>\n<p>It is demonstrable that one's level of strength as a rationalist has a direct correlation to the probability that the one will make correct decisions. Therefore, the people who make decisions that affect large numbers of people ought to be those who have the highest level of rationality. In this way we can seek to avoid the many, many, many pitfalls of politics, including the inefficiency which Yudkowsky has again and again railed against. If all the politicians are on the same side, who's to argue?</p>\n<p>In fact, even if two rationalists disagree on a particular point (which they <a href=\"http://wiki.lesswrong.com/wiki/Aumann's_agreement_theorem\">shouldn't</a>, but hey, even the best rationalists aren't perfect yet), they'll be able to operate more efficiently than two non-rationalists in the same position. Is the disagreement able to be settled by experiment? If it's important, throw funds at a lab to conduct such an experiment! After all, we're in charge of the money and the scientists. Is it not? Find a compromise that has the maximum expected utility for the constituents. We can do that with a high degree of accuracy; we have access to the pollsters and sociologists, and know about reliable versus unreliable polling methods!</p>\n<p>What about non-rationalist aspiring politicians? Well, under an ideal Conspiracy takeover, there would be no such thing. Lessons on politics would include rationality as a basis; graduation from law school would entail induction into the Conspiracy, and access to the truths had therein.</p>\n<p>I suppose the biggest question is, is all this realistic? Or is just an idealist's dream? Well, there's a non-zero probability that the Conspiracy already exists, in which case, I hope that they will consider my proposal... or, even better, I hope that I've correctly deduced and adequately explained the master plan. If the Conspiracy does not currently exist, then if my position is correct, we have a moral obligation to work our hardest on this project.</p>\n<p>&ldquo;But I don't want to be a politician,&rdquo; you exclaim! &ldquo;I have no skill with people, and I'd much rather tinker with the <a href=\"http://xkcd.com/710/\">Collatz Conjecture</a> at my desk for a few years!&rdquo; I'm inclined to say that that's just too bad; sacrifices must be made for the common good, and after all, it's often said that anyone who actually wants a political office is by the fact unfit for the position. But in all realism, I'm quite sure that there will be enough room in the Conspiracy for non-politicians. We're all scientists and mathematicians at heart, anyway.</p>\n<p>So! Here is our order of business. We must draw up a charter for the Bayesian Conspiracy. We must invent a <a href=\"/lw/2s/3_levels_of_rationality_verification/\">testing system</a> able to keep a distinction between those who are and are not <a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">ready</a> for the Truths the Conspiracy will hold. We must find our strongest Rationalists &ndash; via a testing procedure we have not yet come up with &ndash; and put them in charge, and subordinate ourselves to them (not blindly, of course! The strength of community, even rationalist community, is in debate!). We must establish <a href=\"/lw/gn/the_martial_art_of_rationality/\">schools</a> and structured lesson plans for the purpose of training fresh students; we must also take advantage of <a href=\"/lw/70d/theory_of_knowledge_rationality_outreach/\">those systems which are already in place</a>, and utilize them for (or turn them to) our purposes. I expect to have the infrastructure set up in no more than five years.</p>\n<p>At that point, our real work will begin.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PCs9xBqStCuWFmmsq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": -18, "extendedScore": null, "score": 7.559010632280901e-07, "legacy": true, "legacyId": "9226", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Wn8o9yyQ2aR5fwjCn", "7X2j8HAkWdmMoS8PE", "fG3g3764tSubr6xvs", "6hfGNLf4Hg5DXqJCF", "5K7CMa6dEL7TN7sae", "AdYdLP2sRqPMoe8fb", "teaxCFgtmCQ3E9fy8", "LNrdcvD5qhY4d2Syb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-16T23:35:44.241Z", "modifiedAt": null, "url": null, "title": "That letter after B is not rendering on Less Wrong?", "slug": "that-letter-after-b-is-not-rendering-on-less-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:04.395Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n4mjsf3dkYF3ysaf5/that-letter-after-b-is-not-rendering-on-less-wrong", "pageUrlRelative": "/posts/n4mjsf3dkYF3ysaf5/that-letter-after-b-is-not-rendering-on-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/n4mjsf3dkYF3ysaf5/that-letter-after-b-is-not-rendering-on-less-wrong", "postedAtFormatted": "Tuesday, August 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20That%20letter%20after%20B%20is%20not%20rendering%20on%20Less%20Wrong%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThat%20letter%20after%20B%20is%20not%20rendering%20on%20Less%20Wrong%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn4mjsf3dkYF3ysaf5%2Fthat-letter-after-b-is-not-rendering-on-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=That%20letter%20after%20B%20is%20not%20rendering%20on%20Less%20Wrong%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn4mjsf3dkYF3ysaf5%2Fthat-letter-after-b-is-not-rendering-on-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn4mjsf3dkYF3ysaf5%2Fthat-letter-after-b-is-not-rendering-on-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<p>All of a sudden the letter 'c' (the one after 'b', in case it doesn't render) is not showing up in articles on Less Wrong for me, in any browser, except in images. I see a 'c' in the word 'discussion' above, but not in the body text of posts like <a href=\"/lw/74f/are_deontological_moral_judgments_rationalizations/\">this one</a> or <a href=\"/lw/of/dissolving_the_question\">this one</a>. Is anybody else getting the same issue?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n4mjsf3dkYF3ysaf5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 7.559956173565812e-07, "legacy": true, "legacyId": "9241", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["62p74DvwNHgQXCXcH", "Mc6QcrsbH5NRXbCRX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-17T00:05:37.273Z", "modifiedAt": null, "url": null, "title": "Test to see if \"c\" currently works in top level posts", "slug": "test-to-see-if-c-currently-works-in-top-level-posts", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "4nW9GaT5NkYzzaRAS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rwHeA5Q7TN89T6hSF/test-to-see-if-c-currently-works-in-top-level-posts", "pageUrlRelative": "/posts/rwHeA5Q7TN89T6hSF/test-to-see-if-c-currently-works-in-top-level-posts", "linkUrl": "https://www.lesswrong.com/posts/rwHeA5Q7TN89T6hSF/test-to-see-if-c-currently-works-in-top-level-posts", "postedAtFormatted": "Wednesday, August 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Test%20to%20see%20if%20%22c%22%20currently%20works%20in%20top%20level%20posts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATest%20to%20see%20if%20%22c%22%20currently%20works%20in%20top%20level%20posts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrwHeA5Q7TN89T6hSF%2Ftest-to-see-if-c-currently-works-in-top-level-posts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Test%20to%20see%20if%20%22c%22%20currently%20works%20in%20top%20level%20posts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrwHeA5Q7TN89T6hSF%2Ftest-to-see-if-c-currently-works-in-top-level-posts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrwHeA5Q7TN89T6hSF%2Ftest-to-see-if-c-currently-works-in-top-level-posts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 7, "htmlBody": "<p>can you see the letter after B?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rwHeA5Q7TN89T6hSF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "9242", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-17T00:49:56.970Z", "modifiedAt": null, "url": null, "title": "[Link] TED Talk on Perceived Value", "slug": "link-ted-talk-on-perceived-value", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:04.452Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ksLPro6bpuRfBSxKy/link-ted-talk-on-perceived-value", "pageUrlRelative": "/posts/ksLPro6bpuRfBSxKy/link-ted-talk-on-perceived-value", "linkUrl": "https://www.lesswrong.com/posts/ksLPro6bpuRfBSxKy/link-ted-talk-on-perceived-value", "postedAtFormatted": "Wednesday, August 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20TED%20Talk%20on%20Perceived%20Value&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20TED%20Talk%20on%20Perceived%20Value%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FksLPro6bpuRfBSxKy%2Flink-ted-talk-on-perceived-value%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20TED%20Talk%20on%20Perceived%20Value%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FksLPro6bpuRfBSxKy%2Flink-ted-talk-on-perceived-value", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FksLPro6bpuRfBSxKy%2Flink-ted-talk-on-perceived-value", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<p>Rory Sutherland, an ad man (who missed his calling as a comedian), gives <a href=\"http://www.ted.com/talks/rory_sutherland_life_lessons_from_an_ad_man.html\">this talk on perceived value</a> versus \"real\" value, and comes down in favor of more of the first.&nbsp; He also dabbles in history, status, behavioral economics, and the importance of user interface design.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ksLPro6bpuRfBSxKy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 15, "extendedScore": null, "score": 7.560193839454598e-07, "legacy": true, "legacyId": "9243", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-17T01:46:57.307Z", "modifiedAt": null, "url": null, "title": "Newcomb and Procrastination", "slug": "newcomb-and-procrastination", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:04.618Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "eugman", "createdAt": "2009-09-28T01:40:39.582Z", "isAdmin": false, "displayName": "eugman"}, "userId": "rtJy8Y9zXRpjWmeMi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3ybseR89LtbAfQiDu/newcomb-and-procrastination", "pageUrlRelative": "/posts/3ybseR89LtbAfQiDu/newcomb-and-procrastination", "linkUrl": "https://www.lesswrong.com/posts/3ybseR89LtbAfQiDu/newcomb-and-procrastination", "postedAtFormatted": "Wednesday, August 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Newcomb%20and%20Procrastination&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANewcomb%20and%20Procrastination%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ybseR89LtbAfQiDu%2Fnewcomb-and-procrastination%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Newcomb%20and%20Procrastination%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ybseR89LtbAfQiDu%2Fnewcomb-and-procrastination", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ybseR89LtbAfQiDu%2Fnewcomb-and-procrastination", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 136, "htmlBody": "<p>Sorry if someone has covered this before, but I had an interesting thought. Sometimes I'll make a deal with myself, I'll say I'll goof off for X minutes but then I have to work for Y minutes afterwards. Often times, when the time is up, I won't follow through on the deal. What's interesting is I that feel like a causal agent being asked to just leave the money that's lying <em>right there.</em> I'm only going to give myself chances to goof off if I trust myself to get back to work but by the time the time for work comes, I'm in some sense a different person, no longer bound or endangered by old agreements. Omega (old me) is gone and never coming back. This all of course ignore long term goals, moral satisfactions, etc.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3ybseR89LtbAfQiDu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 8, "extendedScore": null, "score": 7.560376409910837e-07, "legacy": true, "legacyId": "9245", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-17T03:10:51.169Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] \"Science\" as Curiosity-Stopper", "slug": "seq-rerun-science-as-curiosity-stopper", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LjDboxxsaKQiZnFPY/seq-rerun-science-as-curiosity-stopper", "pageUrlRelative": "/posts/LjDboxxsaKQiZnFPY/seq-rerun-science-as-curiosity-stopper", "linkUrl": "https://www.lesswrong.com/posts/LjDboxxsaKQiZnFPY/seq-rerun-science-as-curiosity-stopper", "postedAtFormatted": "Wednesday, August 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20%22Science%22%20as%20Curiosity-Stopper&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20%22Science%22%20as%20Curiosity-Stopper%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLjDboxxsaKQiZnFPY%2Fseq-rerun-science-as-curiosity-stopper%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20%22Science%22%20as%20Curiosity-Stopper%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLjDboxxsaKQiZnFPY%2Fseq-rerun-science-as-curiosity-stopper", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLjDboxxsaKQiZnFPY%2Fseq-rerun-science-as-curiosity-stopper", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 254, "htmlBody": "<p>Today's post, <a href=\"/lw/j3/science_as_curiositystopper/\">\"Science\" as Curiosity-Stopper</a> was originally published on 03 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Although science does have explanations for phenomena, it is not enough to simply say that \"Science!\" is responsible for how something works -- nor is it enough to appeal to something more specific like \"electricity\" or \"conduction\". Yet for many people, simply noting that \"Science has an answer\" is enough to make them no longer curious about how it works. In that respect, \"Science\" is no different from more blatant curiosity-stoppers like \"God did it!\" But you shouldn't let your interest die simply because someone else knows the answer (which is a rather strange heuristic anyway): You should only be satisfied with a predictive model, and how a given phenomenon fits into that model.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/74d/seq_rerun_explainworshipignore/\">Explain/Worship/Ignore?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LjDboxxsaKQiZnFPY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 7.560645121067406e-07, "legacy": true, "legacyId": "9251", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["L22jhyY9ocXQNLqyE", "HGDmqzXJbcL5MfTE6", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-17T16:56:57.664Z", "modifiedAt": null, "url": null, "title": "Need Help With Decision", "slug": "need-help-with-decision", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:05.094Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CharlesR", "createdAt": "2011-01-20T15:48:46.140Z", "isAdmin": false, "displayName": "CharlesR"}, "userId": "JcnBrK6C7uP8sPP2G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XgEzq6BdXJF3E7JFv/need-help-with-decision", "pageUrlRelative": "/posts/XgEzq6BdXJF3E7JFv/need-help-with-decision", "linkUrl": "https://www.lesswrong.com/posts/XgEzq6BdXJF3E7JFv/need-help-with-decision", "postedAtFormatted": "Wednesday, August 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Need%20Help%20With%20Decision&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANeed%20Help%20With%20Decision%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXgEzq6BdXJF3E7JFv%2Fneed-help-with-decision%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Need%20Help%20With%20Decision%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXgEzq6BdXJF3E7JFv%2Fneed-help-with-decision", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXgEzq6BdXJF3E7JFv%2Fneed-help-with-decision", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p>Update 4: Thanks to all who attended. Good discussion.</p>\n<p><em>Update 3:&nbsp;</em><em style=\"font-style: italic;\">Meeting is at irc.freenode.net. Join channel youonlylivetwice. I'm there now. Will try to start by 7pm, but if we get enough we might start early.</em></p>\n<p><em>Update 2:&nbsp;</em><em style=\"font-style: italic;\">Meeting is tonight (Aug 18). See my comment below if you plan to attend.</em></p>\n<p><em>Update 1:&nbsp;</em><em style=\"font-style: italic;\">I'm in Pacific time zone (UTC - 7 hours).</em></p>\n<p>&nbsp;</p>\n<p>In the next few days, I have to make a decision that will affect the rest of my life. I want to find <a href=\"/lw/hu/the_third_alternative/\">Third Alternatives</a> and go over my thinking on solutions already proposed.</p>\n<p>I need your help.</p>\n<p>I would post here if I could, but it&rsquo;s just too private, and I don&rsquo;t have the consent of everyone involved.&nbsp;So I would like to convene a working group.</p>\n<p>I propose we meet either tonight or tomorrow. (Right now, I'm thinking IRC, but I am open to suggestions on that.) I will try to keep things short. I wish there was more time, but <em>time</em> is precisely what I don&rsquo;t have.</p>\n<p>If you can help, please reply below in the comments or message me privately. I would like at least one participant to be an individual with a diagnosis of Asperger&rsquo;s or autism.</p>\n<p>Thanks.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XgEzq6BdXJF3E7JFv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 7.56329192246451e-07, "legacy": true, "legacyId": "9261", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["erGipespbbzdG5zYb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-17T18:27:54.422Z", "modifiedAt": null, "url": null, "title": "LINK: Ben Goertzel; Does Humanity Need an \"AI-Nanny\"?", "slug": "link-ben-goertzel-does-humanity-need-an-ai-nanny", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:08.621Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wallowinmaya", "createdAt": "2011-03-21T00:39:18.855Z", "isAdmin": false, "displayName": "David Althaus"}, "userId": "xY8DDzk6TyvRroJEo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ftFH9kGqkXy7otK4x/link-ben-goertzel-does-humanity-need-an-ai-nanny", "pageUrlRelative": "/posts/ftFH9kGqkXy7otK4x/link-ben-goertzel-does-humanity-need-an-ai-nanny", "linkUrl": "https://www.lesswrong.com/posts/ftFH9kGqkXy7otK4x/link-ben-goertzel-does-humanity-need-an-ai-nanny", "postedAtFormatted": "Wednesday, August 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK%3A%20Ben%20Goertzel%3B%20Does%20Humanity%20Need%20an%20%22AI-Nanny%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK%3A%20Ben%20Goertzel%3B%20Does%20Humanity%20Need%20an%20%22AI-Nanny%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FftFH9kGqkXy7otK4x%2Flink-ben-goertzel-does-humanity-need-an-ai-nanny%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK%3A%20Ben%20Goertzel%3B%20Does%20Humanity%20Need%20an%20%22AI-Nanny%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FftFH9kGqkXy7otK4x%2Flink-ben-goertzel-does-humanity-need-an-ai-nanny", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FftFH9kGqkXy7otK4x%2Flink-ben-goertzel-does-humanity-need-an-ai-nanny", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 538, "htmlBody": "<p>Link: <a href=\"http://hplusmagazine.com/2011/08/17/does-humanity-need-an-ai-nanny/\">Ben Goertzel dismisses Yudkowsky's FAI and proposes his own solution: Nanny-AI</a></p>\n<p>&nbsp;</p>\n<p>Some relevant quotes:</p>\n<blockquote>\n<p>It&rsquo;s fun to muse about designing a &ldquo;Friendly AI&rdquo; a la Yudkowsky, that is guaranteed (or near-guaranteed) to maintain a friendly ethical system as it self-modifies and self-improves itself to massively superhuman intelligence.&nbsp; Such an AI system, if it existed, could bring about a full-on Singularity in a way that would respect human values &ndash; i.e. the best of both worlds, satisfying all but the most extreme of both the Cosmists and the Terrans.&nbsp; But the catch is, nobody has any idea how to do such a thing, and it seems well beyond the scope of current or near-future science and engineering.</p>\n</blockquote>\n<blockquote>\n<p>Gradually and reluctantly, I&rsquo;ve been moving toward the opinion that the best solution may be to create a <strong><em>mildly</em></strong> superhuman supertechnology, whose job it is to protect us from ourselves and our technology &ndash; not forever, but just for a while, while we work on the hard problem of creating a Friendly Singularity.</p>\n<p>In other words, some sort of <strong><em>AI Nanny</em></strong>&hellip;.</p>\n</blockquote>\n<blockquote>\n<h2><strong>The AI Nanny </strong></h2>\n<p>Imagine an advanced Artificial General Intelligence (AGI) software program with</p>\n<ul>\n<li>General intelligence somewhat above the human level, but not too dramatically so &ndash; maybe, qualitatively speaking, as far above humans as humans are above apes</li>\n<li>Interconnection to powerful worldwide surveillance systems, online and in the physical world</li>\n<li>Control of a massive contingent of robots (e.g. service robots, teacher robots, etc.) and connectivity to the world&rsquo;s home and building automation systems, robot factories, self-driving cars, and so on and so forth</li>\n<li>A cognitive architecture featuring an explicit set of goals, and an action selection system that causes it to choose those actions that it rationally calculates will best help it achieve those goals</li>\n<li>A set of preprogrammed goals including the following aspects:  \n<ul>\n<li>A strong inhibition against modifying its preprogrammed goals</li>\n<li>A strong inhibition against rapidly modifying its general intelligence</li>\n<li>A mandate to cede control of the world to a more intelligent AI within 200 years</li>\n<li>A mandate to help abolish human disease, involuntary human death, and the practical scarcity of common humanly-useful resources like food, water, housing, computers, etc.</li>\n<li>A mandate to prevent the development of technologies that would threaten its ability to carry out its other goals</li>\n<li>A strong inhibition against carrying out actions with a result that a strong majority of humans would oppose, if they knew about the action in advance</li>\n<li>A mandate to be open-minded toward suggestions by intelligent, thoughtful humans about the possibility that it may be misinterpreting its initial, preprogrammed goals</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<p>Apparently Goertzel doesn't think that building a Nanny-AI with the above mentioned qualities is almost as difficult as creating a FAI a la Yudkowsky.</p>\n<p>But SIAI believes that once you can create an AI-Nanny you can (probably) create a full-blown FAI as well.</p>\n<p>Or am I mistaken?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ftFH9kGqkXy7otK4x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 13, "extendedScore": null, "score": 7.563583408010819e-07, "legacy": true, "legacyId": "9262", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-17T20:07:32.902Z", "modifiedAt": null, "url": null, "title": "[LINK] Brief Discussion of Asteroid & Nuclear Risk from paper by Hellman", "slug": "link-brief-discussion-of-asteroid-and-nuclear-risk-from", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:07.220Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "multifoliaterose", "createdAt": "2010-06-13T08:56:10.885Z", "isAdmin": false, "displayName": "multifoliaterose"}, "userId": "747HfTZFyfTqGyoPM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TyLLmaFb6QeqNTBjR/link-brief-discussion-of-asteroid-and-nuclear-risk-from", "pageUrlRelative": "/posts/TyLLmaFb6QeqNTBjR/link-brief-discussion-of-asteroid-and-nuclear-risk-from", "linkUrl": "https://www.lesswrong.com/posts/TyLLmaFb6QeqNTBjR/link-brief-discussion-of-asteroid-and-nuclear-risk-from", "postedAtFormatted": "Wednesday, August 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Brief%20Discussion%20of%20Asteroid%20%26%20Nuclear%20Risk%20from%20paper%20by%20Hellman&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Brief%20Discussion%20of%20Asteroid%20%26%20Nuclear%20Risk%20from%20paper%20by%20Hellman%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTyLLmaFb6QeqNTBjR%2Flink-brief-discussion-of-asteroid-and-nuclear-risk-from%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Brief%20Discussion%20of%20Asteroid%20%26%20Nuclear%20Risk%20from%20paper%20by%20Hellman%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTyLLmaFb6QeqNTBjR%2Flink-brief-discussion-of-asteroid-and-nuclear-risk-from", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTyLLmaFb6QeqNTBjR%2Flink-brief-discussion-of-asteroid-and-nuclear-risk-from", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 385, "htmlBody": "<p>From <a href=\"http://www.nuclearrisk.org/paper.pdf\">Risk Analysis of Nuclear Deterrence</a> by Martin Hellman. See also http://nuclearrisk.org/</p>\n<blockquote>\n<p>A full-scale nuclear war is not the only threat to humanity&rsquo;s continued existence, and we should allocate resources commensurate with the various risks. A large asteroid colliding with the Earth could destroy humanity in the same way it is believed the dinosaurs disappeared 65 million years ago. Such NEO (near earth object) extinction events have a failure rate on the order of 10<sup>-8</sup> per year [Chapman &amp; Morrison 1994].</p>\n<p>During one century, that failure rate corresponds to one chance in a million of humanity being destroyed. While 10<sup>-6</sup> is a small probability, the associated cost is so high&mdash;infinite from our perspective&mdash;that some might argue that a century is too long a delay before working to reduce the threat. Fortunately, significant threat reduction has recently occurred. Over the last 20 years, NASA&rsquo;s Spaceguard effort is believed to have found all such potentially hazardous large asteroids, and none is predicted to strike Earth within the next century. With a hundred-year safety window in place, resolution of later potential impacts can be deferred for a few decades until our technology is significantly enhanced. Comets also pose a threat, and their more eccentric orbits make them harder to catalog, but their lower frequency of Earth impact makes the associated risk acceptable for a limited period of time.</p>\n</blockquote>\n<p>[...]</p>\n<blockquote>\n<p>While much less accurate than the in-depth studies proposed herein, it is instructive to estimate the failure rate of deterrence due to just one failure mechanism, a Cuban Missile Type Crisis (CMTC). Because it neglects other trigger mechanisms such as command-and-control malfunctions and nuclear terrorism, this appendix underestimates the threat. This simplified analysis uses the time-invariant model described in footnote 3. It also assumes that the experience of the first 50 years of deterrence can be extended into the future.</p>\n</blockquote>\n<p>[...]</p>\n<blockquote>\n<p>Since conditional probabilities were used, they can be multiplied, yielding an estimated range of (2&bull;10<sup>-4</sup>, 5&bull;10<sup>-3</sup>) for [...] the failure rate of deterrence based on just this one failure mechanism. The upper limit 5&bull;10<sup>-3</sup> is within a factor of two of my estimate that the failure rate of deterrence from all sources is on the order of one percent per year, and even the lower limit is well above the level that any engineering design review would find acceptable.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TyLLmaFb6QeqNTBjR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 7.563901231208415e-07, "legacy": true, "legacyId": "9263", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-18T00:03:06.366Z", "modifiedAt": null, "url": null, "title": "Will DNA Analysis Make Politics Less of a Mind-Killer?", "slug": "will-dna-analysis-make-politics-less-of-a-mind-killer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.080Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "James_Miller", "createdAt": "2009-03-05T17:14:38.674Z", "isAdmin": false, "displayName": "James_Miller"}, "userId": "LzF2X9eB9oS3q4BXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jTbbde6vQxfogwRNF/will-dna-analysis-make-politics-less-of-a-mind-killer", "pageUrlRelative": "/posts/jTbbde6vQxfogwRNF/will-dna-analysis-make-politics-less-of-a-mind-killer", "linkUrl": "https://www.lesswrong.com/posts/jTbbde6vQxfogwRNF/will-dna-analysis-make-politics-less-of-a-mind-killer", "postedAtFormatted": "Thursday, August 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Will%20DNA%20Analysis%20Make%20Politics%20Less%20of%20a%20Mind-Killer%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWill%20DNA%20Analysis%20Make%20Politics%20Less%20of%20a%20Mind-Killer%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTbbde6vQxfogwRNF%2Fwill-dna-analysis-make-politics-less-of-a-mind-killer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Will%20DNA%20Analysis%20Make%20Politics%20Less%20of%20a%20Mind-Killer%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTbbde6vQxfogwRNF%2Fwill-dna-analysis-make-politics-less-of-a-mind-killer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTbbde6vQxfogwRNF%2Fwill-dna-analysis-make-politics-less-of-a-mind-killer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 143, "htmlBody": "<p>I wrote <a href=\"http://hplusmagazine.com/2011/08/16/dna-politics-science-and-sociopathy/\">an article</a>&nbsp;for h+ predicting that the rapid fall in the cost of gene sequencing will allow U.S. voters to learn much about presidential candidates' DNA. &nbsp;The&nbsp;candidates&nbsp;won't be able to stop this because:</p>\n<blockquote>\n<p><span style=\"font-family: tahoma, verdana, arial, Arial, Helvetica, sans-serif; font-size: 14px; line-height: 21px;\">humans shed so much DNA that unless a politician lived in a plastic bubble he couldn&rsquo;t shield his DNA from prying eyes. Politicians will probably pass laws making it a crime to involuntarily disclose a politician&rsquo;s genetic traits. But since it would take only one person to leak the information onto the Internet, and given that any serious candidate for President will have many enemies, candidates&rsquo; genomes will undoubtedly become public.</span></p>\n</blockquote>\n<p>DNA analysis has a decent chance of reducing political bias by providing objective information about&nbsp;candidates. &nbsp;If, for example, 70% of the variation in human intelligence is determined by identified genes then DNA analysis would reduce disagreements&nbsp;among&nbsp;informed voters over a&nbsp;candidate's&nbsp;intelligence. &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jTbbde6vQxfogwRNF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": -2, "extendedScore": null, "score": -7e-06, "legacy": true, "legacyId": "9264", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-18T01:12:29.637Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Absurdity Heuristic, Absurdity Bias", "slug": "seq-rerun-absurdity-heuristic-absurdity-bias", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AithjtGYuh6HCGGor/seq-rerun-absurdity-heuristic-absurdity-bias", "pageUrlRelative": "/posts/AithjtGYuh6HCGGor/seq-rerun-absurdity-heuristic-absurdity-bias", "linkUrl": "https://www.lesswrong.com/posts/AithjtGYuh6HCGGor/seq-rerun-absurdity-heuristic-absurdity-bias", "postedAtFormatted": "Thursday, August 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Absurdity%20Heuristic%2C%20Absurdity%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Absurdity%20Heuristic%2C%20Absurdity%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAithjtGYuh6HCGGor%2Fseq-rerun-absurdity-heuristic-absurdity-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Absurdity%20Heuristic%2C%20Absurdity%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAithjtGYuh6HCGGor%2Fseq-rerun-absurdity-heuristic-absurdity-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAithjtGYuh6HCGGor%2Fseq-rerun-absurdity-heuristic-absurdity-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 224, "htmlBody": "<p>Today's post, <a href=\"/lw/j4/absurdity_heuristic_absurdity_bias/\">Absurdity Heuristic, Absurdity Bias</a> was originally published on 05 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Under some circumstances, rejecting arguments on the basis of absurdity is reasonable. The absurdity heuristic can allow you to identify hypotheses that aren't worth your time. However, detailed knowledge of the underlying laws should allow you to override the absurdity heuristic. Objects fall, but helium balloons rise. The future has been consistently absurd and will likely go on being that way. When the absurdity heuristic is extended to rule out crazy-sounding things with a basis in fact, it becomes absurdity bias.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/74z/seq_rerun_science_as_curiositystopper/\">\"Science\" as Curiosity-Stopper</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AithjtGYuh6HCGGor", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 7.564880367052204e-07, "legacy": true, "legacyId": "9265", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["P792Z4QA9dzcLdKkE", "LjDboxxsaKQiZnFPY", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-18T02:38:16.662Z", "modifiedAt": null, "url": null, "title": "Meetup : Houston social meetup", "slug": "meetup-houston-social-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cog", "createdAt": "2011-04-25T04:58:53.803Z", "isAdmin": false, "displayName": "Cog"}, "userId": "xkp87vCZ56dp2tWnN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HaG9nLM7G35japY8b/meetup-houston-social-meetup", "pageUrlRelative": "/posts/HaG9nLM7G35japY8b/meetup-houston-social-meetup", "linkUrl": "https://www.lesswrong.com/posts/HaG9nLM7G35japY8b/meetup-houston-social-meetup", "postedAtFormatted": "Thursday, August 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Houston%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Houston%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHaG9nLM7G35japY8b%2Fmeetup-houston-social-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Houston%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHaG9nLM7G35japY8b%2Fmeetup-houston-social-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHaG9nLM7G35japY8b%2Fmeetup-houston-social-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2c'>Houston social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 August 2011 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2010 Commerce St, Houston, Tx. 77002 </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey everyone. Theres going to be a social meetup at the hackerspace this Sunday at 2PM. There won't be any the following week, as I'll be out of town. Hope to see y'all there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2c'>Houston social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HaG9nLM7G35japY8b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 7.56515540793625e-07, "legacy": true, "legacyId": "9272", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Houston_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/2c\">Houston social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 August 2011 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2010 Commerce St, Houston, Tx. 77002 </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey everyone. Theres going to be a social meetup at the hackerspace this Sunday at 2PM. There won't be any the following week, as I'll be out of town. Hope to see y'all there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Houston_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/2c\">Houston social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Houston social meetup", "anchor": "Discussion_article_for_the_meetup___Houston_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : Houston social meetup", "anchor": "Discussion_article_for_the_meetup___Houston_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-18T09:43:19.556Z", "modifiedAt": null, "url": null, "title": "Needing Better PR", "slug": "needing-better-pr", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:21.041Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "beriukay", "createdAt": "2010-02-16T16:20:00.989Z", "isAdmin": false, "displayName": "beriukay"}, "userId": "4fAd4zQLh2TnrzLmC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9y99tafqTGtofonvj/needing-better-pr", "pageUrlRelative": "/posts/9y99tafqTGtofonvj/needing-better-pr", "linkUrl": "https://www.lesswrong.com/posts/9y99tafqTGtofonvj/needing-better-pr", "postedAtFormatted": "Thursday, August 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Needing%20Better%20PR&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANeeding%20Better%20PR%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9y99tafqTGtofonvj%2Fneeding-better-pr%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Needing%20Better%20PR%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9y99tafqTGtofonvj%2Fneeding-better-pr", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9y99tafqTGtofonvj%2Fneeding-better-pr", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 798, "htmlBody": "<p>I've been having a bit of a back-and-forth with a friend about what appears to be a charisma problem with the SIAI, and was hoping you lovely folks had thoughts on the matter. My friend was going through the <a title=\"Less Wrong Q&amp;A with Eliezer Yudkowski\" href=\"/lw/1lq/less_wrong_qa_with_eliezer_yudkowsky_video_answers/\">Eliezer Q&amp;A videos</a>, specifically Question #7, \"What's your advice for Less Wrong readers who want to help save the human race?\" He typed up a transcript for Eliezer's answer, and went on to say:</p>\n<blockquote>\n<p>Now, I freely admit that he is talking extemporaneously. &nbsp;That he<br /> maybe is giving point-by-point details, interpreting the question as a<br /> laundry-list request of job opportunities, and that criticizing with a<br /> call for brevity is easy as a response rather than a first try, but<br /> HOLY SHIT, COME ON! &nbsp;REALLY!? [...]</p>\n<p>All that aside, here's how a, ahem, human, might respond quickly and<br /> clearly to the question:<br /> <br /> Doing what you like and are most efficient at (for money) is the best<br /> way to get resources to us if you support our cause. &nbsp;Make money at<br /> those things and send it to us if think we're worth it.<br /> <br /> Done.</p>\n</blockquote>\n<p>He went on to mention that he really likes Eliezer's writings, and that his issue rests with the verbal skills of SIAI's leadership, not with the quality of their works.</p>\n<p>I replied:</p>\n<blockquote>\n<p>On the one hand, it would be extremely beneficial for them to get some kind of propaganda minister. On the other, I think that would signal to nerds like us that they are corrupt money-whores. If that is the case, they are stuck being stilted nerds if they want to attract brains, and if they want money, they're stuck watering down their fan base with Dan Brown readers or something.</p>\n</blockquote>\n<p>I also suggested a couple possible (though rather outlandish) ways to make an organization wildly popular. Specifically, to hire a marketing researcher like <a title=\"Scary Man\" href=\"http://en.wikipedia.org/wiki/Frank_Luntz\">Frank Luntz</a> to figure out what talking points would win the hearts and minds of the greatest number of people, or alternately to get major brand loyalties by having a cult figure like Steve Jobs representing the SIAI. Of course, I am stating this much more eloquently than I did in the email.</p>\n<p>His reply deserves full posting here (with his permission, of course):</p>\n<blockquote>\n<p>I disagree with your proposition that getting a competent marketing<br /> firm involved would suddenly create a contradistinction with the<br /> organization proper.<br /> <br /> From everything I've seen/read, these people are nothing if not fully<br /> aware of the compartmentalized world we live in. &nbsp;That this enterprise<br /> requires a particular something upon another something with these<br /> other bits running in the background. &nbsp;Hell, in a grossly simplistic<br /> interpretation, accomplishing this nested complexity is the whole of<br /> their aim.<br /> <br /> What I would say, however, is that the idea that these people are even<br /> aware of any sort of nerd brand loyalty is entirely off-base. &nbsp;I don't<br /> think these folks operate with that realization in mind. &nbsp;I think if<br /> you even brought up the concept, they'd look at you askew in the same<br /> way as telling [mutual acquaintance] he was a geek. &nbsp;\"But...I'm...what? &nbsp;I'm<br /> doing...cool things.\"<br /> <br /> No, I think the fact that they haven't invested in marketing may be<br /> mainly due to money woes, but more likely revealing a fatal flaw in<br /> their infrastructure, in that their intricate understanding of what<br /> they need to do ultimately fails to absorb themselves in the mix.<br /> Failing to see their own operation as needing the societal locomotive<br /> powers to get the final job done.<br /> <br /> If that fear is true, we're in an awful spot indeed. &nbsp;Needing to be<br /> rescued by people ignorant of how to rescue themselves.<br /> <br /> Let's hope it's the money woes, then. &nbsp;Or...hmm...maybe a vacuum to be<br /> met by someone who believes in the cause and also possesses mild<br /> wordcraft? &nbsp;What fancy!</p>\n</blockquote>\n<p>The question is now open. Does SIAI have a PR problem? If so, is it due to finances, lack of talent, or something else? Is there an <a href=\"http://en.wikipedia.org/wiki/Eternal_September\">Eternal September</a> issue with watering down the brand (would you support the SIAI if they started investing heavily in advertising campaigns, or would you get a bit suspicious?)? Should they pay Frank Luntz to figure out what transhumanism terms work best with your average family? My friend and I are dying to know.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9y99tafqTGtofonvj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 7.566518469312404e-07, "legacy": true, "legacyId": "9281", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Qyix5Z5YPSGYxf7GG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-18T10:05:17.720Z", "modifiedAt": null, "url": null, "title": "[LINK] NYTimes essay on willpower, based on an upcoming Baumeister book", "slug": "link-nytimes-essay-on-willpower-based-on-an-upcoming", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:06.012Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Golovin", "createdAt": "2009-02-28T13:32:31.085Z", "isAdmin": false, "displayName": "Vladimir_Golovin"}, "userId": "Me2m84AhCn9H49riY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/efBhs3X3YLjggB8FY/link-nytimes-essay-on-willpower-based-on-an-upcoming", "pageUrlRelative": "/posts/efBhs3X3YLjggB8FY/link-nytimes-essay-on-willpower-based-on-an-upcoming", "linkUrl": "https://www.lesswrong.com/posts/efBhs3X3YLjggB8FY/link-nytimes-essay-on-willpower-based-on-an-upcoming", "postedAtFormatted": "Thursday, August 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20NYTimes%20essay%20on%20willpower%2C%20based%20on%20an%20upcoming%20Baumeister%20book&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20NYTimes%20essay%20on%20willpower%2C%20based%20on%20an%20upcoming%20Baumeister%20book%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FefBhs3X3YLjggB8FY%2Flink-nytimes-essay-on-willpower-based-on-an-upcoming%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20NYTimes%20essay%20on%20willpower%2C%20based%20on%20an%20upcoming%20Baumeister%20book%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FefBhs3X3YLjggB8FY%2Flink-nytimes-essay-on-willpower-based-on-an-upcoming", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FefBhs3X3YLjggB8FY%2Flink-nytimes-essay-on-willpower-based-on-an-upcoming", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1336, "htmlBody": "<p>A surprisingly good New York Times essay on willpower / ego depletion:<br /><a href=\"http://www.nytimes.com/2011/08/21/magazine/do-you-suffer-from-decision-fatigue.html?_r=1&amp;pagewanted=all\"><strong>Do You Suffer From Decision Fatigue?</strong></a></p>\n<p>As it turns out, the essay is based on an upcoming Roy F. Baumeister book, <a href=\"http://www.amazon.com/Willpower-Rediscovering-Greatest-Human-Strength/dp/1594203075/ref=sr_1_1?ie=UTF8&amp;qid=1313659075&amp;sr=8-1\">\"Willpower: Rediscovering the Greatest Human Strength\"</a>, which will be available from Amazon in a couple of weeks (September 1, 2011) both as a hardcover and a Kindle edition.</p>\n<p>Some quotes from the essay (italics and headings mine):</p>\n<p>&nbsp;</p>\n<p>You spend the most willpower when you have to make AND implement your decisions:</p>\n<blockquote>\n<p>which phase of the decision-making process was most fatiguing? To find out, Kathleen Vohs, a former colleague of Baumeister&rsquo;s now at the University of Minnesota, performed an experiment using the self-service Web site of Dell Computers. One group in the experiment carefully studied the advantages and disadvantages of various features available for a computer &mdash; the type of screen, the size of the hard drive, etc. &mdash; without actually making a final decision on which ones to choose. A second group was given a list of predetermined specifications and told to configure a computer by going through the laborious, step-by-step process of locating the specified features among the arrays of options and then clicking on the right ones. The purpose of this was to duplicate everything that happens in the postdecisional phase, when the choice is implemented. <em>The third group had to figure out for themselves which features they wanted on their computers and go through the process of choosing them; they didn&rsquo;t simply ponder options (like the first group) or implement others&rsquo; choices (like the second group). They had to cast the die, and that turned out to be the most fatiguing task of all. When self-control was measured, they were the one who were most depleted, by far</em>.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Willpower depletion makes you reluctant to make trade-offs:</p>\n<blockquote>\n<p>Once you&rsquo;re mentally depleted, you become reluctant to make trade-offs, which involve a particularly advanced and taxing form of decision making. In the rest of the animal kingdom, there aren&rsquo;t a lot of protracted negotiations between predators and prey. To compromise is a complex human ability and therefore one of the first to decline when willpower is depleted. You become what researchers call a cognitive miser, hoarding your energy. <em>If you&rsquo;re shopping, you&rsquo;re liable to look at only one dimension, like price: just give me the cheapest. Or you indulge yourself by looking at quality: I want the very best</em> (an especially easy strategy if someone else is paying). Decision fatigue leaves you vulnerable to marketers who know how to time their sales, as Jonathan Levav, the Stanford professor, demonstrated in experiments involving tailored suits and new cars.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Willpower depletion makes you more likely to take the path of least resistance:</p>\n<blockquote>\n<p>As they started picking features, customers would carefully weigh the choices, but <em>as decision fatigue set in, they would start settling for whatever the default option was. And the more tough choices they encountered early in the process &mdash; like going through those 56 colors to choose the precise shade of gray or brown &mdash; the quicker people became fatigued and settled for the path of least resistance by taking the default option.</em> By manipulating the order of the car buyers&rsquo; choices, the researchers found that the customers would end up settling for different kinds of options, and the average difference totaled more than 1,500 euros per car (about $2,000 at the time).</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Testing willpower depletion in rural Indian villages:</p>\n<blockquote>\n<p>Most of us in America won&rsquo;t spend a lot of time agonizing over whether we can afford to buy soap, but it can be a depleting choice in rural India. Dean Spears, an economist at Princeton, offered people in 20 villages in Rajasthan in northwestern India the chance to buy a couple of bars of brand-name soap for the equivalent of less than 20 cents. It was a steep discount off the regular price, yet even that sum was a strain for the people in the 10 poorest villages. Whether or not they bought the soap, the act of making the decision left them with less willpower, as measured afterward in a test of how long they could squeeze a hand grip. In the slightly more affluent villages, people&rsquo;s willpower wasn&rsquo;t affected significantly.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Decision&nbsp;fatigue&nbsp;can be a factor in trapping people in poverty:</p>\n<blockquote>\n<p>Spears and other researchers argue that this sort of decision fatigue is a major &mdash; and hitherto ignored &mdash; factor in trapping people in poverty. Because <em>their financial situation forces them to make so many trade-offs, they have less willpower to devote to school, work and other activities that might get them into the middle class</em>.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Glucose restores willpower in humans and dogs:</p>\n<blockquote>\n<p>To establish cause and effect, researchers at Baumeister&rsquo;s lab tried refueling the brain in a series of experiments involving lemonade mixed either with sugar or with a diet sweetener. The sugary lemonade provided a burst of glucose, the effects of which could be observed right away in the lab; the sugarless variety tasted quite similar without providing the same burst of glucose. <em>Again and again, the sugar restored willpower, but the artificial sweetener had no effect.</em> The glucose would at least mitigate the ego depletion and sometimes completely reverse it. The restored willpower improved people&rsquo;s self-control as well as the quality of their decisions: they resisted irrational bias when making choices, and when asked to make financial decisions, they were more likely to choose the better long-term strategy instead of going for a quick payoff. <em>The ego-depletion effect was even demonstrated with dogs in two studies by Holly Miller and Nathan DeWall at the University of Kentucky. After obeying sit and stay commands for 10 minutes, the dogs performed worse on self-control tests and were also more likely to make the dangerous decision to challenge another dog&rsquo;s turf</em>. But a dose of glucose restored their willpower.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Ego depletion causes activity to rise in some parts of the brain and to decline in others:</p>\n<blockquote>\n<p>The results of the experiment were announced in January, during Heatherton&rsquo;s speech accepting the leadership of the Society for Personality and Social Psychology, the world&rsquo;s largest group of social psychologists. In his presidential address at the annual meeting in San Antonio, Heatherton reported that administering glucose completely reversed the brain changes wrought by depletion &mdash; a finding, he said, that thoroughly surprised him. <em>Heatherton&rsquo;s results did much more than provide additional confirmation that glucose is a vital part of willpower; they helped solve the puzzle over how glucose could work without global changes in the brain&rsquo;s total energy use. Apparently ego depletion causes activity to rise in some parts of the brain and to decline in others. Your brain does not stop working when glucose is low. It stops doing some things and starts doing others. It responds more strongly to immediate rewards and pays less attention to long-term prospects</em>.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Good decision makers structure their lives so as to conserve willpower:</p>\n<blockquote>\n<p>&ldquo;Good decision making is not a trait of the person, in the sense that it&rsquo;s always there,&rdquo; Baumeister says. &ldquo;It&rsquo;s a state that fluctuates.&rdquo; His studies show that people with the best self-control are the ones who structure their lives so as to conserve willpower. <em>They don&rsquo;t schedule endless back-to-back meetings. They avoid temptations like all-you-can-eat buffets, and they establish habits that eliminate the mental effort of making choices. Instead of deciding every morning whether or not to force themselves to exercise, they set up regular appointments to work out with a friend</em>. Instead of counting on willpower to remain robust all day, they conserve it so that it&rsquo;s available for emergencies and important decisions.</p>\n</blockquote>\n<blockquote>\n<p>&ldquo;Even the wisest people won&rsquo;t make good choices when they&rsquo;re not rested and their glucose is low,&rdquo; Baumeister points out. That&rsquo;s why <em>the truly wise don&rsquo;t restructure the company at 4 p.m</em>. They don&rsquo;t make major commitments during the cocktail hour. And if a decision must be made late in the day, they know not to do it on an empty stomach. &ldquo;The best decision makers,&rdquo; Baumeister says, &ldquo;are the ones who know when <em>not</em> to trust themselves.&rdquo;</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "efBhs3X3YLjggB8FY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 25, "extendedScore": null, "score": 7.566588933402648e-07, "legacy": true, "legacyId": "9280", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-18T18:25:55.416Z", "modifiedAt": null, "url": null, "title": "[Link] Givewell: Why We Can\u2019t Take Expected Value Estimates Literally", "slug": "link-givewell-why-we-can-t-take-expected-value-estimates", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:04.748Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pwZ6qMgzoKr3JPqx4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/psCNY7TvSr4CGjHTQ/link-givewell-why-we-can-t-take-expected-value-estimates", "pageUrlRelative": "/posts/psCNY7TvSr4CGjHTQ/link-givewell-why-we-can-t-take-expected-value-estimates", "linkUrl": "https://www.lesswrong.com/posts/psCNY7TvSr4CGjHTQ/link-givewell-why-we-can-t-take-expected-value-estimates", "postedAtFormatted": "Thursday, August 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Givewell%3A%20Why%20We%20Can%E2%80%99t%20Take%20Expected%20Value%20Estimates%20Literally&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Givewell%3A%20Why%20We%20Can%E2%80%99t%20Take%20Expected%20Value%20Estimates%20Literally%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpsCNY7TvSr4CGjHTQ%2Flink-givewell-why-we-can-t-take-expected-value-estimates%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Givewell%3A%20Why%20We%20Can%E2%80%99t%20Take%20Expected%20Value%20Estimates%20Literally%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpsCNY7TvSr4CGjHTQ%2Flink-givewell-why-we-can-t-take-expected-value-estimates", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpsCNY7TvSr4CGjHTQ%2Flink-givewell-why-we-can-t-take-expected-value-estimates", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 321, "htmlBody": "<p><a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/</a></p>\n<p>Long post over at GiveWell about relying on expected value estimates for charitable donations. The main thrust is that large expected values with high variance carry very little weight when combined with a proper prior. LessWrong is referenced heavily throughout.</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 19px;\"> </span></p>\n<blockquote>\n<p style=\"margin-top: 0.6em; margin-right: 0px; margin-bottom: 1.2em; margin-left: 0px; line-height: 1.6em; padding: 0px;\">We believe that people in this group are often making a fundamental mistake, one that we have long had intuitive objections to but have recently developed a more formal (though still fairly rough) critique of. The mistake (we believe) is estimating the &ldquo;expected value&rdquo; of a donation (or other action) based solely on a fully explicit, quantified formula, many of whose inputs are guesses or very rough estimates. We believe that any estimate along these lines needs to be adjusted using a &ldquo;Bayesian prior&rdquo;; that this adjustment can rarely be made (reasonably) using an explicit, formal calculation; and that most attempts to do the latter, even when they seem to be making very conservative downward adjustments to the expected value of an opportunity, are not making nearly large enough downward adjustments to be consistent with the proper Bayesian approach.</p>\n<p style=\"margin-top: 0.6em; margin-right: 0px; margin-bottom: 1.2em; margin-left: 0px; line-height: 1.6em; padding: 0px;\">This view of ours illustrates why - while we seek to ground our recommendations in relevant facts, calculations and quantifications to the extent possible - every recommendation we make incorporates many different forms of evidence and involves a strong dose of intuition. And&nbsp;<strong style=\"padding: 0px; margin: 0px;\">we generally prefer to give where we have&nbsp;<em style=\"padding: 0px; margin: 0px;\">strong evidence that donations can do a lot of good</em>rather than where we have&nbsp;<em style=\"padding: 0px; margin: 0px;\">weak evidence that donations can do far more good</em></strong>&nbsp;- a preference that I believe is inconsistent with the approach of giving based on explicit expected-value formulas (at least those that (a) have significant room for error (b) do not incorporate Bayesian adjustments, which are very rare in these analyses and very difficult to do both formally and reasonably).</p>\n</blockquote>\n<p style=\"margin-top: 0.6em; margin-right: 0px; margin-bottom: 1.2em; margin-left: 0px; line-height: 1.6em; padding: 0px;\">I'm particularly interested in anyone more familiar with bayesian stats who can shed more light on the validity of Givewell's approach here.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "psCNY7TvSr4CGjHTQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 0, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "9284", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-18T19:02:53.351Z", "modifiedAt": null, "url": null, "title": "Meetup : DC Meetup: NoVa, Rationality Games", "slug": "meetup-dc-meetup-nova-rationality-games", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nurfohXrFN7hWGqkR/meetup-dc-meetup-nova-rationality-games", "pageUrlRelative": "/posts/nurfohXrFN7hWGqkR/meetup-dc-meetup-nova-rationality-games", "linkUrl": "https://www.lesswrong.com/posts/nurfohXrFN7hWGqkR/meetup-dc-meetup-nova-rationality-games", "postedAtFormatted": "Thursday, August 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20DC%20Meetup%3A%20NoVa%2C%20Rationality%20Games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20DC%20Meetup%3A%20NoVa%2C%20Rationality%20Games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnurfohXrFN7hWGqkR%2Fmeetup-dc-meetup-nova-rationality-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20DC%20Meetup%3A%20NoVa%2C%20Rationality%20Games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnurfohXrFN7hWGqkR%2Fmeetup-dc-meetup-nova-rationality-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnurfohXrFN7hWGqkR%2Fmeetup-dc-meetup-nova-rationality-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2d'>DC Meetup: NoVa, Rationality Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 August 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Ballston Commons Mall</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Trying out a few more locations/times to better serve the DC area LW community.</p>\n\n<p>We'll play rationality games for at least an hour but other topics are welcome.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2d'>DC Meetup: NoVa, Rationality Games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nurfohXrFN7hWGqkR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "9285", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___DC_Meetup__NoVa__Rationality_Games\">Discussion article for the meetup : <a href=\"/meetups/2d\">DC Meetup: NoVa, Rationality Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 August 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Ballston Commons Mall</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Trying out a few more locations/times to better serve the DC area LW community.</p>\n\n<p>We'll play rationality games for at least an hour but other topics are welcome.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___DC_Meetup__NoVa__Rationality_Games1\">Discussion article for the meetup : <a href=\"/meetups/2d\">DC Meetup: NoVa, Rationality Games</a></h2>", "sections": [{"title": "Discussion article for the meetup : DC Meetup: NoVa, Rationality Games", "anchor": "Discussion_article_for_the_meetup___DC_Meetup__NoVa__Rationality_Games", "level": 1}, {"title": "Discussion article for the meetup : DC Meetup: NoVa, Rationality Games", "anchor": "Discussion_article_for_the_meetup___DC_Meetup__NoVa__Rationality_Games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-18T21:16:48.694Z", "modifiedAt": null, "url": null, "title": "(US only) Donate $2 to charity (bing rewards)", "slug": "us-only-donate-usd2-to-charity-bing-rewards", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:05.146Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jonathan_Graehl", "createdAt": "2009-02-27T23:21:15.671Z", "isAdmin": false, "displayName": "Jonathan_Graehl"}, "userId": "eKsWtKKceoRYwcc7s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BrScfds5ZcrmK9aJr/us-only-donate-usd2-to-charity-bing-rewards", "pageUrlRelative": "/posts/BrScfds5ZcrmK9aJr/us-only-donate-usd2-to-charity-bing-rewards", "linkUrl": "https://www.lesswrong.com/posts/BrScfds5ZcrmK9aJr/us-only-donate-usd2-to-charity-bing-rewards", "postedAtFormatted": "Thursday, August 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20(US%20only)%20Donate%20%242%20to%20charity%20(bing%20rewards)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A(US%20only)%20Donate%20%242%20to%20charity%20(bing%20rewards)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBrScfds5ZcrmK9aJr%2Fus-only-donate-usd2-to-charity-bing-rewards%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=(US%20only)%20Donate%20%242%20to%20charity%20(bing%20rewards)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBrScfds5ZcrmK9aJr%2Fus-only-donate-usd2-to-charity-bing-rewards", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBrScfds5ZcrmK9aJr%2Fus-only-donate-usd2-to-charity-bing-rewards", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 49, "htmlBody": "<p>Probably not worth most of your time, but if you already have a Windows Live account, log in here and you earn enough Bing reward points (whatever those are) to donate $2 to charity -&nbsp;http://www.bing.com/rewards/signup/web</p>\n<p>Unfortunately, the charity selection is limited to a couple of \"better educate the poor\" organizations.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BrScfds5ZcrmK9aJr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": -1, "extendedScore": null, "score": 7.568743300569935e-07, "legacy": true, "legacyId": "9286", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-18T23:01:32.268Z", "modifiedAt": null, "url": null, "title": "Meetup : Chicago Meetup At State and Elm, August 27th", "slug": "meetup-chicago-meetup-at-state-and-elm-august-27th", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:08.787Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nic_Smith", "createdAt": "2009-10-23T03:32:46.312Z", "isAdmin": false, "displayName": "Nic_Smith"}, "userId": "XP9GcTgRGLBCnf9ih", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x3GmpyqsThC7vFERx/meetup-chicago-meetup-at-state-and-elm-august-27th", "pageUrlRelative": "/posts/x3GmpyqsThC7vFERx/meetup-chicago-meetup-at-state-and-elm-august-27th", "linkUrl": "https://www.lesswrong.com/posts/x3GmpyqsThC7vFERx/meetup-chicago-meetup-at-state-and-elm-august-27th", "postedAtFormatted": "Thursday, August 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Chicago%20Meetup%20At%20State%20and%20Elm%2C%20August%2027th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Chicago%20Meetup%20At%20State%20and%20Elm%2C%20August%2027th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx3GmpyqsThC7vFERx%2Fmeetup-chicago-meetup-at-state-and-elm-august-27th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Chicago%20Meetup%20At%20State%20and%20Elm%2C%20August%2027th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx3GmpyqsThC7vFERx%2Fmeetup-chicago-meetup-at-state-and-elm-august-27th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx3GmpyqsThC7vFERx%2Fmeetup-chicago-meetup-at-state-and-elm-august-27th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2e'>Chicago Meetup At State and Elm, August 27th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 August 2011 03:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1130 North State Street, Chicago, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting at the cafe on the first floor of the State and Elm Barnes &amp; Noble, 1130 N State St, at 3:30pm CST on Saturday, August 27th. The tentative topic of discussion is \"recent posts on LessWrong.\"</p>\n\n<ul>\n<li><a href=\"https://groups.google.com/d/topic/less-wrong-chicago/pNJ3Lhmjn44/discussion\" rel=\"nofollow\">Discussion Group Post</a></li>\n<li><a href=\"http://store-locator.barnesandnoble.com/store/2922\" rel=\"nofollow\">Barnes &amp; Noble Store Info</a></li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2e'>Chicago Meetup At State and Elm, August 27th</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x3GmpyqsThC7vFERx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 7.569079382346296e-07, "legacy": true, "legacyId": "9287", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Chicago_Meetup_At_State_and_Elm__August_27th\">Discussion article for the meetup : <a href=\"/meetups/2e\">Chicago Meetup At State and Elm, August 27th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 August 2011 03:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1130 North State Street, Chicago, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting at the cafe on the first floor of the State and Elm Barnes &amp; Noble, 1130 N State St, at 3:30pm CST on Saturday, August 27th. The tentative topic of discussion is \"recent posts on LessWrong.\"</p>\n\n<ul>\n<li><a href=\"https://groups.google.com/d/topic/less-wrong-chicago/pNJ3Lhmjn44/discussion\" rel=\"nofollow\">Discussion Group Post</a></li>\n<li><a href=\"http://store-locator.barnesandnoble.com/store/2922\" rel=\"nofollow\">Barnes &amp; Noble Store Info</a></li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Chicago_Meetup_At_State_and_Elm__August_27th1\">Discussion article for the meetup : <a href=\"/meetups/2e\">Chicago Meetup At State and Elm, August 27th</a></h2>", "sections": [{"title": "Discussion article for the meetup : Chicago Meetup At State and Elm, August 27th", "anchor": "Discussion_article_for_the_meetup___Chicago_Meetup_At_State_and_Elm__August_27th", "level": 1}, {"title": "Discussion article for the meetup : Chicago Meetup At State and Elm, August 27th", "anchor": "Discussion_article_for_the_meetup___Chicago_Meetup_At_State_and_Elm__August_27th1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-18T23:34:12.099Z", "modifiedAt": null, "url": null, "title": "Why We Can't Take Expected Value Estimates Literally (Even When They're Unbiased)", "slug": "why-we-can-t-take-expected-value-estimates-literally-even", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:35.392Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HoldenKarnofsky", "createdAt": "2009-12-30T00:19:32.818Z", "isAdmin": false, "displayName": "HoldenKarnofsky"}, "userId": "kdeMdATaSc2MZKmdH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RdpqsQ6xbHzyckW9m/why-we-can-t-take-expected-value-estimates-literally-even", "pageUrlRelative": "/posts/RdpqsQ6xbHzyckW9m/why-we-can-t-take-expected-value-estimates-literally-even", "linkUrl": "https://www.lesswrong.com/posts/RdpqsQ6xbHzyckW9m/why-we-can-t-take-expected-value-estimates-literally-even", "postedAtFormatted": "Thursday, August 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20We%20Can't%20Take%20Expected%20Value%20Estimates%20Literally%20(Even%20When%20They're%20Unbiased)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20We%20Can't%20Take%20Expected%20Value%20Estimates%20Literally%20(Even%20When%20They're%20Unbiased)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRdpqsQ6xbHzyckW9m%2Fwhy-we-can-t-take-expected-value-estimates-literally-even%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20We%20Can't%20Take%20Expected%20Value%20Estimates%20Literally%20(Even%20When%20They're%20Unbiased)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRdpqsQ6xbHzyckW9m%2Fwhy-we-can-t-take-expected-value-estimates-literally-even", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRdpqsQ6xbHzyckW9m%2Fwhy-we-can-t-take-expected-value-estimates-literally-even", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5114, "htmlBody": "<p><span style=\"font-family: arial, sans-serif; font-size: 13px;\"> </span></p>\n<p><em>Note: I am cross-posting this&nbsp;<a style=\"color: #0000cc;\" href=\"http://blog.givewell.org/\" target=\"_blank\">GiveWell Blog</a>&nbsp;post, after consulting a couple of community members, because it is relevant to many topics discussed on Less Wrong, particularly&nbsp;<a style=\"color: #0000cc;\" href=\"/lw/3gj/efficient_charity_do_unto_others/\" target=\"_blank\">efficient charity</a>/<a style=\"color: #0000cc;\" href=\"/lw/6py/optimal_philanthropy_for_human_beings/\" target=\"_blank\">optimal philanthropy</a>&nbsp;and&nbsp;<a style=\"color: #0000cc;\" href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\" target=\"_blank\">Pascal's Mugging</a>. The post includes a proposed \"solution\" to the dilemma posed by Pascal's Mugging that has not been proposed before as far as I know.</em> <em>It is longer than usual for a Less Wrong post, so I have put everything but the summary below the fold. Also, note that I use the term \"expected value\" because it is more generic than \"expected utility\"; the arguments here pertain to estimating the expected value of any quantity, not just utility.</em></p>\n<p>While some people feel that GiveWell puts too much emphasis on the measurable and quantifiable, there are others who go further than we do in quantification, and justify their giving (or other) decisions based on fully explicit expected-value formulas. The latter group tends to critique us - or at least disagree with us - based on our preference for strong evidence over high apparent \"expected value,\" and based on the heavy role of non-formalized intuition in our decisionmaking. This post is directed at the latter group.</p>\n<p>We believe that people in this group are often making a fundamental mistake, one that we have long had intuitive objections to but have recently developed a more formal (though still fairly rough) critique of. The mistake (we believe) is estimating the \"expected value\" of a donation (or other action) based solely on a fully explicit, quantified formula, many of whose inputs are guesses or very rough estimates. We believe that any estimate along these lines needs to be adjusted using a \"Bayesian prior\"; that this adjustment can rarely be made (reasonably) using an explicit, formal calculation; and that most attempts to do the latter, even when they seem to be making very conservative downward adjustments to the expected value of an opportunity, are not making nearly large enough downward adjustments to be consistent with the proper Bayesian approach.</p>\n<p>This view of ours illustrates why - while we seek to ground our recommendations in relevant facts, calculations and quantifications to the extent possible - every recommendation we make incorporates many different forms of evidence and involves a strong dose of intuition. And <strong>we generally prefer to give where we have <em>strong evidence that donations can do a lot of good</em> rather than where we have <em>weak evidence that donations can do far more good</em></strong> - a preference that I believe is inconsistent with the approach of giving based on explicit expected-value formulas (at least those that (a) have significant room for error (b) do not incorporate Bayesian adjustments, which are very rare in these analyses and very difficult to do both formally and reasonably).</p>\n<p>The rest of this post will:</p>\n<ul>\n<li>Lay out the \"explicit expected value formula\" approach to giving, which we oppose, and give examples. </li>\n<li>Give the intuitive objections we've long had to this approach, i.e., ways in which it seems intuitively problematic. </li>\n<li>Give a clean example of how a Bayesian adjustment can be done, and can be an improvement on the \"explicit expected value formula\" approach. </li>\n<li>Present a versatile formula for making and illustrating Bayesian adjustments that can be applied to charity cost-effectiveness estimates. </li>\n<li>Show how a Bayesian adjustment avoids the <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's Mugging</a> problem that those who rely on explicit expected value calculations seem prone to. </li>\n<li>Discuss how one can properly apply Bayesian adjustments in other cases, where less information is available. </li>\n<li>Conclude with the following takeaways: \n<ul>\n<li>Any approach to decision-making that relies only on rough estimates of expected value - and does not incorporate preferences for better-grounded estimates over shakier estimates - is flawed. </li>\n<li>When aiming to maximize expected positive impact, it is not advisable to make giving decisions based fully on explicit formulas. Proper Bayesian adjustments are important and are usually overly difficult to formalize. </li>\n<li>The above point is a general defense of resisting arguments that <em>both</em> (a) seem intuitively problematic (b) have thin evidential support and/or room for significant error. </li>\n</ul>\n</li>\n</ul>\n<p><strong><a id=\"more\"></a></strong></p>\n<p><strong>The approach we oppose: \"explicit expected-value\" (EEV) decisionmaking</strong></p>\n<p>&nbsp;</p>\n<p>We term the approach this post argues against the \"explicit expected-value\" (EEV) approach to decisionmaking. It generally involves an argument of the form:</p>\n<blockquote>\n<ul>\nI estimate that each dollar spent on Program P has a value of V [in terms of lives saved, disability-adjusted life-years, social return on investment, or some other metric]. Granted, my estimate is extremely rough and unreliable, and involves geometrically combining multiple unreliable figures - but it's unbiased, i.e., it seems as likely to be too pessimistic as it is to be too optimistic. Therefore, my estimate V represents the per-dollar expected value of Program P. \n</ul>\n<ul>\nI don't know how good Charity C is at implementing Program P, but even if it wastes 75% of its money or has a 75% chance of failure, its per-dollar expected value is still 25%*V, which is still excellent. \n</ul>\n</blockquote>\n<p>Examples of the EEV approach to decisionmaking:</p>\n<ul>\n<li>In a <a href=\"http://blog.givewell.org/2010/06/09/neglected-tropical-disease-charities-schistosomiasis-control-initiative-deworm-the-world/\">2010 exchange</a>, &nbsp;Will Crouch of &nbsp;<a href=\"http://www.givingwhatwecan.org\">Giving What We Can</a>&nbsp; argued:\n<blockquote>DtW [Deworm the World] spends about 74% on technical assistance and scaling up deworming programs within Kenya and India &hellip; Let&rsquo;s assume (very implausibly) that all other money (spent on advocacy etc) is wasted, and assess the charity solely on that 74%. It still would do very well (taking DCP2: $3.4/DALY * (1/0.74) = $4.6/DALY &ndash; slightly better than their most optimistic estimate for DOTS (for TB), and far better than their estimates for insecticide treated nets, condom distribution, etc). So, though finding out more about their advocacy work is obviously a great thing to do, the advocacy questions don&rsquo;t need to be answered in order to make a recommendation: it seems that DtW [is] worth recommending on the basis of their control programs alone.</blockquote>\n</li>\n<li><a href=\"http://www.beguide.org/\">The Back of the Envelope Guide to Philanthropy</a> lists rough calculations for the value of different charitable interventions. These calculations imply (among other things) that <a href=\"http://www.beguide.org/results.html\">donating for political advocacy for higher foreign aid</a> is between 8x and 22x as good an investment as <a href=\"http://www.beguide.org/villagereach.html\">donating to VillageReach</a>, and the presentation and implication are that this calculation ought to be considered decisive. </li>\n<li>We've encountered numerous people who argue that charities working on reducing the risk of sudden human extinction must be the best ones to support, since the value of saving the human race is so high that \"any imaginable probability of success\" would lead to a higher expected value for these charities than for others. </li>\n<li><a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">\"Pascal's Mugging\"</a> is often seen as the <em>reductio ad absurdum</em> of this sort of reasoning. The idea is that if a person demands $10 in exchange for refraining from an extremely harmful action (one that negatively affects N people for some huge N), then expected-value calculations demand that one give in to the person's demands: no matter how unlikely the claim, there is some N big enough that the \"expected value\" of refusing to give the $10 is hugely negative. </li>\n</ul>\n<p>The crucial characteristic of the EEV approach is that it <strong>does not incorporate a systematic preference for better-grounded estimates over rougher estimates. It ranks charities/actions based simply on their estimated value, ignoring differences in the reliability and robustness of the estimates.</strong> <strong>Informal objections to EEV decisionmaking</strong> There are many ways in which the sort of reasoning laid out above seems (to us) to fail a common sense test.</p>\n<ul>\n<li>There seems to be nothing in EEV that penalizes relative ignorance or relatively poorly grounded estimates, or rewards investigation and the forming of particularly well grounded estimates. If I can literally <a href=\"http://www.thelifeyoucansave.com/idea\">save a child I see drowning by ruining a $1000 suit</a>, but in the same moment I make a wild guess that this $1000 could save 2 lives if put toward medical research, EEV seems to indicate that I should opt for the latter. </li>\n<li>Because of this, a world in which people acted based on EEV would seem to be problematic in various ways. \n<ul>\n<li>In such a world, it seems that nearly all altruists would put nearly all of their resources toward helping people they knew little about, rather than helping themselves, their families and their communities. I believe that the world would be worse off if people behaved in this way, or at least if they took it to an extreme. (There are always more people you know little about than people you know well, and EEV estimates of how much good you can do for people you don't know seem likely to have higher variance than EEV estimates of how much good you can do for people you do know. Therefore, it seems likely that the highest-EEV action directed at people you don't know will have higher EEV than the highest-EEV action directed at people you do know.) </li>\n<li>In such a world, when people decided that a particular endeavor/action had outstandingly high EEV, there would (too often) be no justification for costly skeptical inquiry of this endeavor/action. For example, say that people were trying to manipulate the weather; that someone hypothesized that they had no power for such manipulation; and that the EEV of trying to manipulate the weather was much higher than the EEV of other things that could be done with the same resources. It would be difficult to justify a costly investigation of the \"trying to manipulate the weather is a waste of time\" hypothesis in this framework. Yet it seems that when people are valuing one action far above others, based on thin information, this is the time when skeptical inquiry is needed most. And more generally, it seems that challenging and investigating our most firmly held, \"high-estimated-probability\" beliefs - even when doing so has been costly - has been quite beneficial to society.</li>\n</ul>\n</li>\n<li>Related: giving based on EEV seems to create bad incentives. EEV doesn't seem to allow rewarding charities for transparency or penalizing them for opacity: it simply recommends giving to the charity with the highest estimated expected value, regardless of how well-grounded the estimate is. Therefore, in a world in which most donors used EEV to give, charities would have every incentive to announce that they were focusing on the highest expected-value programs, without disclosing any details of their operations that might show they were achieving less value than theoretical estimates said they ought to be. </li>\n<li>If you are basing your actions on EEV analysis, it seems that you're very open to being exploited by <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's Mugging</a>: a tiny probability of a huge-value expected outcome can come to dominate your decisionmaking in ways that seem to violate common sense. (We discuss this further below.) </li>\n<li>If I'm deciding between eating at a new restaurant with 3 Yelp reviews averaging 5 stars and eating at an older restaurant with 200 Yelp reviews averaging 4.75 stars, EEV seems to imply (using Yelp rating as a stand-in for \"expected value of the experience\") that I should opt for the former. As discussed in the next section, I think this is the purest demonstration of the problem with EEV and the need for Bayesian adjustments. </li>\n</ul>\n<p>In the remainder of this post, I present what I believe is the right formal framework for my objections to EEV. However, I have more confidence in my intuitions - which are related to the above observations - than in the framework itself. I believe I have formalized my thoughts correctly, but if the remainder of this post turned out to be flawed, I would likely remain in objection to EEV until and unless one could address my less formal misgivings.</p>\n<p><strong>Simple example of a Bayesian approach vs. an EEV approach</strong></p>\n<p>It seems fairly clear that a restaurant with 200 Yelp reviews,&nbsp;averaging 4.75 stars, ought to outrank a restaurant with 3 Yelp reviews, averaging 5 stars. Yet this ranking can't be justified in an EEV-style framework, in which options are ranked by their estimated average/expected value. How, in fact, does Yelp handle this situation?</p>\n<p>Unfortunately, the answer <a href=\"http://www.yelp.com/faq#ranking_search_results\">appears to be undisclosed in Yelp's case</a>, but we can get a hint from a similar site: <a href=\"http://beeradvocate.com/lists/popular\">BeerAdvocate</a>, a site that ranks beers using submitted reviews. It states:</p>\n<blockquote>Lists are generated using a Bayesian estimate that pulls data from millions of user reviews (not hand-picked) and normalizes scores based on the number of reviews for each beer. The general statistical formula is: weighted rank (WR) = (v &divide; (v+m)) &times; R + (m &divide; (v+m)) &times; C where: R = review average for the beer v = number of reviews for the beer m = minimum reviews required to be considered (currently 10) C = the mean across the list (currently 3.66)</blockquote>\n<p>In other words, BeerAdvocate does the equivalent of giving each beer a set number (currently 10) of \"average\" reviews (i.e., reviews with a score of 3.66, which is the average for all beers on the site). Thus, a beer with zero reviews is assumed to be exactly as good as the average beer on the site; a beer with one review will still be assumed to be close to average, no matter what rating the one review gives; as the number of reviews grows, the beer's rating is able to deviate more from the average.</p>\n<p>To illustrate this, the following chart shows how BeerAdvocate's formula would rate a beer that has 0-100 five-star reviews. As the number of five-star reviews grows, the formula's \"confidence\" in the five-star rating grows, and the beer's overall rating gets further from \"average\" and closer to (though never fully reaching) 5 stars. <img src=\"http://blog.givewell.org/images/beeradvocate.png\" alt=\"\" /></p>\n<p>I find BeerAdvocate's approach to be quite reasonable and I find the chart above to accord quite well with intuition: a beer with a small handful of five-star reviews should be considered pretty close to average, while a beer with a hundred five-star reviews should be considered to be nearly a five-star beer.</p>\n<p>However, there are a couple of complications that make it difficult to apply this approach broadly.</p>\n<ul>\n<li>BeerAdvocate is making a substantial judgment call regarding what \"prior\" to use, i.e., how strongly to assume each beer is average until proven otherwise. It currently sets the <em>m</em> in its formula equal to 10, which is like giving each beer a starting point of ten average-level reviews; it gives no formal justification for why it has set <em>m</em> to 10 instead of 1 or 100. It is unclear what such a justification would look like.&nbsp;In fact, I believe that BeerAdvocate used to use a stronger \"prior\" (i.e., it used to set <em>m</em> to a higher value), which meant that beers needed larger numbers of reviews to make the top-rated list. When BeerAdvocate changed its prior, its rankings changed dramatically, as lesser-known, higher-rated beers overtook the mainstream beers that had previously dominated the list.</li>\n</ul>\n<ul>\n<li>In BeerAdvocate's case, the basic approach to setting a Bayesian prior seems pretty straightforward: the \"prior\" rating for a given beer is equal to the average rating for all beers on the site, which is known. By contrast, if we're looking at the estimate of how much good a charity does, it isn't clear what \"average\" one can use for a prior; it isn't even clear what the appropriate reference class is. Should our prior value for the good-accomplished-per-dollar of a deworming charity be equal to the good-accomplished-per-dollar of the average deworming charity, or of the average health charity, or the average charity, or the average altruistic expenditure, or some weighted average of these? Of course, we don't actually have any of these figures. For this reason, it's hard to formally justify one's prior, and <a href=\"http://blog.givewell.org/2009/12/05/a-conflict-of-bayesian-priors/\">differences in priors can cause major disagreements and confusions</a> when they aren't recognized for what they are. But this doesn't mean the choice of prior should be ignored or that one should leave the prior out of expected-value calculations (as we believe EEV advocates do).</li>\n</ul>\n<p><strong>Applying Bayesian adjustments to cost-effectiveness estimates for donations, actions, etc.</strong></p>\n<p>As discussed above, we believe that both <a href=\"http://www.givingwhatwecan.org\">Giving What We Can</a> and <a href=\"http://www.beguide.org\">Back of the Envelope Guide to Philanthropy</a> use forms of EEV analysis in arguing for their charity recommendations. However, when it comes to analyzing the cost-effectiveness estimates they invoke, the BeerAdvocate formula doesn't seem applicable: there is no \"number of reviews\" figure that can be used to determine the relative weights of the prior and the estimate.</p>\n<p>Instead, we propose a model in which there is a normally (or log-normally) distributed \"estimate error\" around the cost-effectiveness estimate (with a mean of \"no error,\" i.e., 0 for normally distributed error and 1 for lognormally distributed error), and in which the <a href=\"http://en.wikipedia.org/wiki/Prior_probability\">prior distribution</a> for cost-effectiveness is normally (or log-normally) distributed as well. (I won't discuss log-normal distributions in this post, but the analysis I give can be extended by applying it to the log of the variables in question.) The more one feels confident in one's pre-existing view of how cost-effective an donation or action should be, the smaller the variance of the \"prior\"; the more one feels confident in the cost-effectiveness estimate itself, the smaller the variance of the \"estimate error.\"</p>\n<p>Following up on our <a href=\"http://blog.givewell.org/2010/06/09/neglected-tropical-disease-charities-schistosomiasis-control-initiative-deworm-the-world/\">2010 exchange with Giving What We Can</a>, we asked <a href=\"http://blog.givewell.org/2010/06/03/my-donation-for-2009-guest-post-from-dario-amodei/\">Dario Amodei</a> to write up the implications of the above model and the form of the proper Bayesian adjustment. You can see his analysis <a href=\"http://blog.givewell.org/attachments/worms.pdf\">here</a>. The bottom line is that when one applies Bayes's rule to obtain a distribution for cost-effectiveness based on (a) a normally distributed prior distribution (b) a normally distributed \"estimate error,\" one obtains a distribution with</p>\n<ul>\n<li>Mean equal to the average of the two means weighted by their inverse variances </li>\n<li>Variance equal to the harmonic sum of the two variances</li>\n</ul>\n<p>The following charts show what this formula implies in a variety of different simple hypotheticals. In all of these, the prior distribution has mean = 0 and standard deviation = 1, and the estimate has mean = 10, but the \"estimate error\" varies, with important effects: an estimate with little enough estimate error can almost be taken literally, while an estimate with large enough estimate error ends ought to be almost ignored.</p>\n<p>In each of these charts, the black line represents a <a href=\"http://en.wikipedia.org/wiki/Probability_density_function\">probability density function</a> for one's \"prior,\" the red line for an estimate (with the variance coming from \"estimate error\"), and the blue line for the final probability distribution, taking both the prior and the estimate into account. Taller, narrower distributions represent cases where probability is concentrated around the midpoint; shorter, wider distributions represent cases where the possibilities/probabilities are more spread out among many values. First, the case where the cost-effectiveness estimate has the same confidence interval around it as the prior: <img src=\"http://blog.givewell.org/images/bayesian adjustment 1.png\" alt=\"\" /></p>\n<p>If one has a relatively reliable estimate (i.e., one with a narrow confidence interval / small variance of \"estimate error,\") then the Bayesian-adjusted conclusion ends up very close to the estimate. When we estimate quantities using highly precise and well-understood methods, we can use them (almost) literally. <img src=\"http://blog.givewell.org/images/bayesian adjustment 2.png\" alt=\"\" /> <img src=\"http://blog.givewell.org/images/bayesian adjustment 3.png\" alt=\"\" /></p>\n<p>On the flip side, when the estimate is relatively unreliable (wide confidence interval / large variance of \"estimate error\"), it has little effect on the final expectation of cost-effectiveness (or whatever is being estimated). And at the point where the one-standard-deviation bands include zero cost-effectiveness (i.e., where there's a pretty strong probability that the whole cost-effectiveness estimate is worthless), the estimate ends up having practically no effect on one's final view. <img src=\"http://blog.givewell.org/images/bayesian adjustment 4.png\" alt=\"\" /> <img src=\"http://blog.givewell.org/images/bayesian adjustment 5.png\" alt=\"\" /></p>\n<p>The details of how to apply this sort of analysis to cost-effectiveness estimates for charitable interventions are outside the scope of this post, which focuses on our belief in the importance of the concept of Bayesian adjustments. The big-picture takeaway is that just having the midpoint of a cost-effectiveness estimate is not worth very much in itself; it is important to understand the sources of estimate error, and the degree of estimate error relative to the degree of variation in estimated cost-effectiveness for different interventions.</p>\n<p><strong>Pascal's Mugging</strong></p>\n<p><strong></strong> <a href=\"http://wiki.lesswrong.com/wiki/Pascal's_mugging\">Pascal's Mugging</a> refers to a case where a claim of extravagant impact is made for a particular action, with little to no evidence:</p>\n<blockquote>Now suppose someone comes to me and says, \"Give me five dollars, or I'll use my magic powers &hellip; to [harm an imaginably huge number of] people.</blockquote>\n<p>Non-Bayesian approaches to evaluating these proposals often take the following form: \"Even if we assume that this analysis is 99.99% likely to be wrong, the expected value is still high - and are you willing to bet that this analysis is wrong at 99.99% odds?\"</p>\n<p>However, this is a case where \"estimate error\" is probably accounting for the lion's share of variance in estimated expected value, and therefore I believe that a proper Bayesian adjustment would correctly assign little value where there is little basis for the estimate, no matter how high the midpoint of the estimate.</p>\n<p>Say that you've come to believe - based on life experience - in a \"prior distribution\" for the value of your actions, with a mean of zero and a standard deviation of 1. (The unit type you use to value your actions is irrelevant to the point I'm making; so in this case the units I'm using are simply standard deviations based on your prior distribution for the value of your actions). Now say that someone estimates that action A (e.g., giving in to the mugger's demands) has an expected value of X (same units) - but that the estimate itself is so rough that the right expected value could easily be 0 or 2X. More specifically, say that the error in the expected value estimate has a standard deviation of X.</p>\n<p>An EEV approach to this situation might say, \"Even if there's a 99.99% chance that the estimate is completely wrong and that the value of Action A is 0, there's still an 0.01% probability that Action A has a value of X. Thus, overall Action A has an expected value of at least 0.0001X; the greater X is, the greater this value is, and if X is great enough then, then you should take Action A unless you're willing to bet at enormous odds that the framework is wrong.\"</p>\n<p>However, the same formula discussed above indicates that Action X actually has an expected value - after the Bayesian adjustment - of X/(X^2+1), or <em>just under 1/X</em>. In this framework, <em>the greater X is, the <strong>lower</strong> the expected value of Action A.</em> This syncs well with my intuitions: if someone threatened to harm one person unless you gave them $10, this ought to carry more weight (because it is more plausible in the face of the \"prior\" of life experience) than if they threatened to harm 100 people, which in turn ought to carry more weight than if they threatened to harm 3^^^3 people (I'm using 3^^^3 here as a <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">representation of an unimaginably huge number</a>).</p>\n<p>The point at which a threat or proposal starts to be called \"Pascal's Mugging\" can be thought of as the point at which the claimed value of Action A is wildly outside the prior set by life experience (which may cause the feeling that common sense is being violated). If someone claims that giving him/her $10 will accomplish 3^^^3 times as much as a 1-standard-deviation life action from the appropriate reference class, then the actual post-adjustment expected value of Action A will be just under (1/3^^^3) (in standard deviation terms) - only trivially higher than the value of an average action, and likely lower than other actions one could take with the same resources. This is true without applying <em>any</em> particular probability that the person's framework is wrong - it is simply a function of the fact that their estimate has such enormous possible error. An ungrounded estimate making an extravagant claim ought to be more or less discarded in the face of the \"prior distribution\" of life experience.</p>\n<p><strong>Generalizing the Bayesian approach</strong></p>\n<p>In the above cases, I've given quantifications of (a) the appropriate prior for cost-effectiveness; (b) the strength/confidence of a given cost-effectiveness estimate. One needs to quantify both (a) and (b) - not just quantify estimated cost-effectiveness - in order to formally make the needed Bayesian adjustment to the initial estimate.</p>\n<p>But when it comes to giving, and many other decisions, reasonable quantification of these things usually isn't possible. To have a prior, you need a reference class, and <a href=\"http://wiki.lesswrong.com/wiki/Outside_view\">reference classes are debatable</a>.</p>\n<p>It's my view that my brain instinctively processes huge amounts of information, coming from many different reference classes, and arrives at a prior; if I attempt to formalize my prior, counting only what I can name and justify, I can worsen the accuracy a lot relative to going with my gut. Of course there is a problem here: going with one's gut can be an excuse for going with what one wants to believe, and a lot of what enters into my gut belief could be irrelevant to proper Bayesian analysis. There is an appeal to formulas, which is that they seem to be susceptible to outsiders' checking them for fairness and consistency.</p>\n<p>But when the formulas are too rough, I think the loss of accuracy outweighs the gains to transparency. Rather than using a formula that is checkable but omits a huge amount of information, I'd prefer to state my intuition - without pretense that it is anything but an intuition - and hope that the ensuing discussion provides the needed check on my intuitions.</p>\n<p>I can't, therefore, usefully say what I think the appropriate prior estimate of charity cost-effectiveness is. I can, however, describe a couple of approaches to Bayesian adjustments that I oppose, and can describe a few heuristics that I use to determine whether I'm making an appropriate Bayesian adjustment.</p>\n<p><em>Approaches to Bayesian adjustment that I oppose</em></p>\n<p>I have seen some argue along the lines of \"I have a very weak (or <a href=\"http://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors\">uninformative</a>) prior, which means I can more or less take rough estimates literally.\" I think this is a mistake. We do have a lot of information by which to judge what to expect from an action (including a donation), and failure to use all the information we have is a failure to make the appropriate Bayesian adjustment. Even just a sense for the values of the small set of actions you've taken in your life, and observed the consequences of, gives you something to work with as far as an \"outside view\" and a starting probability distribution for the value of your actions; this distribution probably ought to have high variance, but when dealing with a rough estimate that has very high variance of its own, it may still be quite a meaningful prior.</p>\n<p>I have seen some using the EEV framework who can tell that their estimates seem too optimistic, so they make various \"downward adjustments,\" multiplying their EEV by apparently ad hoc figures (1%, 10%, 20%). What isn't clear is whether the size of the adjustment they're making has the correct relationship to (a) the weakness of the estimate itself (b) the strength of the prior (c) distance of the estimate from the prior. An example of how this approach can go astray can be seen in the \"Pascal's Mugging\" analysis above: assigning one's framework a 99.99% chance of being totally wrong may seem to be amply conservative, but in fact the proper Bayesian adjustment is much larger and leads to a completely different conclusion.</p>\n<p><em>Heuristics I use to address whether I'm making an appropriate prior-based adjustment</em></p>\n<ul>\n<li><strong>The more action is asked of me, the more evidence I require.</strong> Anytime I'm asked to take a significant action (giving a significant amount of money, time, effort, etc.), this action has to have higher expected value than the action I would otherwise take. My intuitive feel for the distribution of \"how much my actions accomplish\" serves as a prior - an adjustment to the value that the asker claims for my action. </li>\n<li><strong>I pay attention to how much of the variation I see between estimates is likely to be driven by true variation vs. estimate error.</strong> As shown above, when an estimate is rough enough so that error might account for the bulk of the observed variation, a proper Bayesian approach can involve a massive discount to the estimate. </li>\n<li><strong>I put much more weight on conclusions that seem to be supported by multiple different lines of analysis, as unrelated to one another as possible</strong>. If one starts with a high-error estimate of expected value, and then starts finding more estimates with the same midpoint, the variance of the aggregate estimate error declines; the less correlated the estimates are, the greater the decline in the variance of the error, and thus the lower the Bayesian adjustment to the final estimate. This is a formal way of observing that \"diversified\" reasons for believing something lead to more \"robust\" beliefs, i.e., beliefs that are less likely to fall apart with new information and can be used with less skepticism. </li>\n<li><strong>I am hesitant to embrace arguments that seem to have anti-common-sense implications (unless the evidence behind these arguments is strong)</strong> and I think my prior may often be the reason for this. As seen above, a too-weak prior can lead to many seemingly absurd beliefs and consequences, such as falling prey to \"Pascal's Mugging\" and removing the incentive for investigation of strong claims. Strengthening the prior fixes these problems (while over-strengthening the prior results in simply ignoring new evidence). In general, I believe that when a particular kind of reasoning seems to me to have anti-common-sense implications, this may indicate that its implications are well outside my prior. </li>\n<li><strong>My prior for charity is generally skeptical</strong>, as outlined at <a href=\"http://blog.givewell.org/2009/12/05/a-conflict-of-bayesian-priors/\">this post</a>. Giving well seems <a href=\"http://blog.givewell.org/2011/06/11/why-we-should-expect-good-giving-to-be-hard/\">conceptually quite difficult to me</a>, and it's been my experience over time that the more we dig on a cost-effectiveness estimate, the more unwarranted optimism we uncover. Also, having an optimistic prior would mean giving to opaque charities, and that seems to violate common sense. Thus, we look for charities with quite strong evidence of effectiveness, and tend to prefer very strong charities with reasonably high estimated cost-effectiveness to weaker charities with very high estimated cost-effectiveness </li>\n</ul>\n<p><strong>Conclusion</strong></p>\n<ul>\n<li>I feel that any giving approach that relies only on estimated expected-value - and does not incorporate preferences for better-grounded estimates over shakier estimates - is flawed. </li>\n<li>Thus, when aiming to maximize expected positive impact, it is not advisable to make giving decisions based fully on explicit formulas. Proper Bayesian adjustments are important and are usually overly difficult to formalize. </li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1, "qAvbtzdG2A2RBn7in": 1, "LhX3F2SvGDarZCuh6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RdpqsQ6xbHzyckW9m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 116, "baseScore": 121, "extendedScore": null, "score": 0.00023, "legacy": true, "legacyId": "9221", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 121, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><span style=\"font-family: arial, sans-serif; font-size: 13px;\"> </span></p>\n<p><em>Note: I am cross-posting this&nbsp;<a style=\"color: #0000cc;\" href=\"http://blog.givewell.org/\" target=\"_blank\">GiveWell Blog</a>&nbsp;post, after consulting a couple of community members, because it is relevant to many topics discussed on Less Wrong, particularly&nbsp;<a style=\"color: #0000cc;\" href=\"/lw/3gj/efficient_charity_do_unto_others/\" target=\"_blank\">efficient charity</a>/<a style=\"color: #0000cc;\" href=\"/lw/6py/optimal_philanthropy_for_human_beings/\" target=\"_blank\">optimal philanthropy</a>&nbsp;and&nbsp;<a style=\"color: #0000cc;\" href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\" target=\"_blank\">Pascal's Mugging</a>. The post includes a proposed \"solution\" to the dilemma posed by Pascal's Mugging that has not been proposed before as far as I know.</em> <em>It is longer than usual for a Less Wrong post, so I have put everything but the summary below the fold. Also, note that I use the term \"expected value\" because it is more generic than \"expected utility\"; the arguments here pertain to estimating the expected value of any quantity, not just utility.</em></p>\n<p>While some people feel that GiveWell puts too much emphasis on the measurable and quantifiable, there are others who go further than we do in quantification, and justify their giving (or other) decisions based on fully explicit expected-value formulas. The latter group tends to critique us - or at least disagree with us - based on our preference for strong evidence over high apparent \"expected value,\" and based on the heavy role of non-formalized intuition in our decisionmaking. This post is directed at the latter group.</p>\n<p>We believe that people in this group are often making a fundamental mistake, one that we have long had intuitive objections to but have recently developed a more formal (though still fairly rough) critique of. The mistake (we believe) is estimating the \"expected value\" of a donation (or other action) based solely on a fully explicit, quantified formula, many of whose inputs are guesses or very rough estimates. We believe that any estimate along these lines needs to be adjusted using a \"Bayesian prior\"; that this adjustment can rarely be made (reasonably) using an explicit, formal calculation; and that most attempts to do the latter, even when they seem to be making very conservative downward adjustments to the expected value of an opportunity, are not making nearly large enough downward adjustments to be consistent with the proper Bayesian approach.</p>\n<p>This view of ours illustrates why - while we seek to ground our recommendations in relevant facts, calculations and quantifications to the extent possible - every recommendation we make incorporates many different forms of evidence and involves a strong dose of intuition. And <strong>we generally prefer to give where we have <em>strong evidence that donations can do a lot of good</em> rather than where we have <em>weak evidence that donations can do far more good</em></strong> - a preference that I believe is inconsistent with the approach of giving based on explicit expected-value formulas (at least those that (a) have significant room for error (b) do not incorporate Bayesian adjustments, which are very rare in these analyses and very difficult to do both formally and reasonably).</p>\n<p>The rest of this post will:</p>\n<ul>\n<li>Lay out the \"explicit expected value formula\" approach to giving, which we oppose, and give examples. </li>\n<li>Give the intuitive objections we've long had to this approach, i.e., ways in which it seems intuitively problematic. </li>\n<li>Give a clean example of how a Bayesian adjustment can be done, and can be an improvement on the \"explicit expected value formula\" approach. </li>\n<li>Present a versatile formula for making and illustrating Bayesian adjustments that can be applied to charity cost-effectiveness estimates. </li>\n<li>Show how a Bayesian adjustment avoids the <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's Mugging</a> problem that those who rely on explicit expected value calculations seem prone to. </li>\n<li>Discuss how one can properly apply Bayesian adjustments in other cases, where less information is available. </li>\n<li>Conclude with the following takeaways: \n<ul>\n<li>Any approach to decision-making that relies only on rough estimates of expected value - and does not incorporate preferences for better-grounded estimates over shakier estimates - is flawed. </li>\n<li>When aiming to maximize expected positive impact, it is not advisable to make giving decisions based fully on explicit formulas. Proper Bayesian adjustments are important and are usually overly difficult to formalize. </li>\n<li>The above point is a general defense of resisting arguments that <em>both</em> (a) seem intuitively problematic (b) have thin evidential support and/or room for significant error. </li>\n</ul>\n</li>\n</ul>\n<p><strong><a id=\"more\"></a></strong></p>\n<p><strong id=\"The_approach_we_oppose___explicit_expected_value___EEV__decisionmaking\">The approach we oppose: \"explicit expected-value\" (EEV) decisionmaking</strong></p>\n<p>&nbsp;</p>\n<p>We term the approach this post argues against the \"explicit expected-value\" (EEV) approach to decisionmaking. It generally involves an argument of the form:</p>\n<blockquote>\n<ul>\nI estimate that each dollar spent on Program P has a value of V [in terms of lives saved, disability-adjusted life-years, social return on investment, or some other metric]. Granted, my estimate is extremely rough and unreliable, and involves geometrically combining multiple unreliable figures - but it's unbiased, i.e., it seems as likely to be too pessimistic as it is to be too optimistic. Therefore, my estimate V represents the per-dollar expected value of Program P. \n</ul>\n<ul>\nI don't know how good Charity C is at implementing Program P, but even if it wastes 75% of its money or has a 75% chance of failure, its per-dollar expected value is still 25%*V, which is still excellent. \n</ul>\n</blockquote>\n<p>Examples of the EEV approach to decisionmaking:</p>\n<ul>\n<li>In a <a href=\"http://blog.givewell.org/2010/06/09/neglected-tropical-disease-charities-schistosomiasis-control-initiative-deworm-the-world/\">2010 exchange</a>, &nbsp;Will Crouch of &nbsp;<a href=\"http://www.givingwhatwecan.org\">Giving What We Can</a>&nbsp; argued:\n<blockquote>DtW [Deworm the World] spends about 74% on technical assistance and scaling up deworming programs within Kenya and India \u2026 Let\u2019s assume (very implausibly) that all other money (spent on advocacy etc) is wasted, and assess the charity solely on that 74%. It still would do very well (taking DCP2: $3.4/DALY * (1/0.74) = $4.6/DALY \u2013 slightly better than their most optimistic estimate for DOTS (for TB), and far better than their estimates for insecticide treated nets, condom distribution, etc). So, though finding out more about their advocacy work is obviously a great thing to do, the advocacy questions don\u2019t need to be answered in order to make a recommendation: it seems that DtW [is] worth recommending on the basis of their control programs alone.</blockquote>\n</li>\n<li><a href=\"http://www.beguide.org/\">The Back of the Envelope Guide to Philanthropy</a> lists rough calculations for the value of different charitable interventions. These calculations imply (among other things) that <a href=\"http://www.beguide.org/results.html\">donating for political advocacy for higher foreign aid</a> is between 8x and 22x as good an investment as <a href=\"http://www.beguide.org/villagereach.html\">donating to VillageReach</a>, and the presentation and implication are that this calculation ought to be considered decisive. </li>\n<li>We've encountered numerous people who argue that charities working on reducing the risk of sudden human extinction must be the best ones to support, since the value of saving the human race is so high that \"any imaginable probability of success\" would lead to a higher expected value for these charities than for others. </li>\n<li><a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">\"Pascal's Mugging\"</a> is often seen as the <em>reductio ad absurdum</em> of this sort of reasoning. The idea is that if a person demands $10 in exchange for refraining from an extremely harmful action (one that negatively affects N people for some huge N), then expected-value calculations demand that one give in to the person's demands: no matter how unlikely the claim, there is some N big enough that the \"expected value\" of refusing to give the $10 is hugely negative. </li>\n</ul>\n<p>The crucial characteristic of the EEV approach is that it <strong>does not incorporate a systematic preference for better-grounded estimates over rougher estimates. It ranks charities/actions based simply on their estimated value, ignoring differences in the reliability and robustness of the estimates.</strong> <strong>Informal objections to EEV decisionmaking</strong> There are many ways in which the sort of reasoning laid out above seems (to us) to fail a common sense test.</p>\n<ul>\n<li>There seems to be nothing in EEV that penalizes relative ignorance or relatively poorly grounded estimates, or rewards investigation and the forming of particularly well grounded estimates. If I can literally <a href=\"http://www.thelifeyoucansave.com/idea\">save a child I see drowning by ruining a $1000 suit</a>, but in the same moment I make a wild guess that this $1000 could save 2 lives if put toward medical research, EEV seems to indicate that I should opt for the latter. </li>\n<li>Because of this, a world in which people acted based on EEV would seem to be problematic in various ways. \n<ul>\n<li>In such a world, it seems that nearly all altruists would put nearly all of their resources toward helping people they knew little about, rather than helping themselves, their families and their communities. I believe that the world would be worse off if people behaved in this way, or at least if they took it to an extreme. (There are always more people you know little about than people you know well, and EEV estimates of how much good you can do for people you don't know seem likely to have higher variance than EEV estimates of how much good you can do for people you do know. Therefore, it seems likely that the highest-EEV action directed at people you don't know will have higher EEV than the highest-EEV action directed at people you do know.) </li>\n<li>In such a world, when people decided that a particular endeavor/action had outstandingly high EEV, there would (too often) be no justification for costly skeptical inquiry of this endeavor/action. For example, say that people were trying to manipulate the weather; that someone hypothesized that they had no power for such manipulation; and that the EEV of trying to manipulate the weather was much higher than the EEV of other things that could be done with the same resources. It would be difficult to justify a costly investigation of the \"trying to manipulate the weather is a waste of time\" hypothesis in this framework. Yet it seems that when people are valuing one action far above others, based on thin information, this is the time when skeptical inquiry is needed most. And more generally, it seems that challenging and investigating our most firmly held, \"high-estimated-probability\" beliefs - even when doing so has been costly - has been quite beneficial to society.</li>\n</ul>\n</li>\n<li>Related: giving based on EEV seems to create bad incentives. EEV doesn't seem to allow rewarding charities for transparency or penalizing them for opacity: it simply recommends giving to the charity with the highest estimated expected value, regardless of how well-grounded the estimate is. Therefore, in a world in which most donors used EEV to give, charities would have every incentive to announce that they were focusing on the highest expected-value programs, without disclosing any details of their operations that might show they were achieving less value than theoretical estimates said they ought to be. </li>\n<li>If you are basing your actions on EEV analysis, it seems that you're very open to being exploited by <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's Mugging</a>: a tiny probability of a huge-value expected outcome can come to dominate your decisionmaking in ways that seem to violate common sense. (We discuss this further below.) </li>\n<li>If I'm deciding between eating at a new restaurant with 3 Yelp reviews averaging 5 stars and eating at an older restaurant with 200 Yelp reviews averaging 4.75 stars, EEV seems to imply (using Yelp rating as a stand-in for \"expected value of the experience\") that I should opt for the former. As discussed in the next section, I think this is the purest demonstration of the problem with EEV and the need for Bayesian adjustments. </li>\n</ul>\n<p>In the remainder of this post, I present what I believe is the right formal framework for my objections to EEV. However, I have more confidence in my intuitions - which are related to the above observations - than in the framework itself. I believe I have formalized my thoughts correctly, but if the remainder of this post turned out to be flawed, I would likely remain in objection to EEV until and unless one could address my less formal misgivings.</p>\n<p><strong id=\"Simple_example_of_a_Bayesian_approach_vs__an_EEV_approach\">Simple example of a Bayesian approach vs. an EEV approach</strong></p>\n<p>It seems fairly clear that a restaurant with 200 Yelp reviews,&nbsp;averaging 4.75 stars, ought to outrank a restaurant with 3 Yelp reviews, averaging 5 stars. Yet this ranking can't be justified in an EEV-style framework, in which options are ranked by their estimated average/expected value. How, in fact, does Yelp handle this situation?</p>\n<p>Unfortunately, the answer <a href=\"http://www.yelp.com/faq#ranking_search_results\">appears to be undisclosed in Yelp's case</a>, but we can get a hint from a similar site: <a href=\"http://beeradvocate.com/lists/popular\">BeerAdvocate</a>, a site that ranks beers using submitted reviews. It states:</p>\n<blockquote>Lists are generated using a Bayesian estimate that pulls data from millions of user reviews (not hand-picked) and normalizes scores based on the number of reviews for each beer. The general statistical formula is: weighted rank (WR) = (v \u00f7 (v+m)) \u00d7 R + (m \u00f7 (v+m)) \u00d7 C where: R = review average for the beer v = number of reviews for the beer m = minimum reviews required to be considered (currently 10) C = the mean across the list (currently 3.66)</blockquote>\n<p>In other words, BeerAdvocate does the equivalent of giving each beer a set number (currently 10) of \"average\" reviews (i.e., reviews with a score of 3.66, which is the average for all beers on the site). Thus, a beer with zero reviews is assumed to be exactly as good as the average beer on the site; a beer with one review will still be assumed to be close to average, no matter what rating the one review gives; as the number of reviews grows, the beer's rating is able to deviate more from the average.</p>\n<p>To illustrate this, the following chart shows how BeerAdvocate's formula would rate a beer that has 0-100 five-star reviews. As the number of five-star reviews grows, the formula's \"confidence\" in the five-star rating grows, and the beer's overall rating gets further from \"average\" and closer to (though never fully reaching) 5 stars. <img src=\"http://blog.givewell.org/images/beeradvocate.png\" alt=\"\"></p>\n<p>I find BeerAdvocate's approach to be quite reasonable and I find the chart above to accord quite well with intuition: a beer with a small handful of five-star reviews should be considered pretty close to average, while a beer with a hundred five-star reviews should be considered to be nearly a five-star beer.</p>\n<p>However, there are a couple of complications that make it difficult to apply this approach broadly.</p>\n<ul>\n<li>BeerAdvocate is making a substantial judgment call regarding what \"prior\" to use, i.e., how strongly to assume each beer is average until proven otherwise. It currently sets the <em>m</em> in its formula equal to 10, which is like giving each beer a starting point of ten average-level reviews; it gives no formal justification for why it has set <em>m</em> to 10 instead of 1 or 100. It is unclear what such a justification would look like.&nbsp;In fact, I believe that BeerAdvocate used to use a stronger \"prior\" (i.e., it used to set <em>m</em> to a higher value), which meant that beers needed larger numbers of reviews to make the top-rated list. When BeerAdvocate changed its prior, its rankings changed dramatically, as lesser-known, higher-rated beers overtook the mainstream beers that had previously dominated the list.</li>\n</ul>\n<ul>\n<li>In BeerAdvocate's case, the basic approach to setting a Bayesian prior seems pretty straightforward: the \"prior\" rating for a given beer is equal to the average rating for all beers on the site, which is known. By contrast, if we're looking at the estimate of how much good a charity does, it isn't clear what \"average\" one can use for a prior; it isn't even clear what the appropriate reference class is. Should our prior value for the good-accomplished-per-dollar of a deworming charity be equal to the good-accomplished-per-dollar of the average deworming charity, or of the average health charity, or the average charity, or the average altruistic expenditure, or some weighted average of these? Of course, we don't actually have any of these figures. For this reason, it's hard to formally justify one's prior, and <a href=\"http://blog.givewell.org/2009/12/05/a-conflict-of-bayesian-priors/\">differences in priors can cause major disagreements and confusions</a> when they aren't recognized for what they are. But this doesn't mean the choice of prior should be ignored or that one should leave the prior out of expected-value calculations (as we believe EEV advocates do).</li>\n</ul>\n<p><strong id=\"Applying_Bayesian_adjustments_to_cost_effectiveness_estimates_for_donations__actions__etc_\">Applying Bayesian adjustments to cost-effectiveness estimates for donations, actions, etc.</strong></p>\n<p>As discussed above, we believe that both <a href=\"http://www.givingwhatwecan.org\">Giving What We Can</a> and <a href=\"http://www.beguide.org\">Back of the Envelope Guide to Philanthropy</a> use forms of EEV analysis in arguing for their charity recommendations. However, when it comes to analyzing the cost-effectiveness estimates they invoke, the BeerAdvocate formula doesn't seem applicable: there is no \"number of reviews\" figure that can be used to determine the relative weights of the prior and the estimate.</p>\n<p>Instead, we propose a model in which there is a normally (or log-normally) distributed \"estimate error\" around the cost-effectiveness estimate (with a mean of \"no error,\" i.e., 0 for normally distributed error and 1 for lognormally distributed error), and in which the <a href=\"http://en.wikipedia.org/wiki/Prior_probability\">prior distribution</a> for cost-effectiveness is normally (or log-normally) distributed as well. (I won't discuss log-normal distributions in this post, but the analysis I give can be extended by applying it to the log of the variables in question.) The more one feels confident in one's pre-existing view of how cost-effective an donation or action should be, the smaller the variance of the \"prior\"; the more one feels confident in the cost-effectiveness estimate itself, the smaller the variance of the \"estimate error.\"</p>\n<p>Following up on our <a href=\"http://blog.givewell.org/2010/06/09/neglected-tropical-disease-charities-schistosomiasis-control-initiative-deworm-the-world/\">2010 exchange with Giving What We Can</a>, we asked <a href=\"http://blog.givewell.org/2010/06/03/my-donation-for-2009-guest-post-from-dario-amodei/\">Dario Amodei</a> to write up the implications of the above model and the form of the proper Bayesian adjustment. You can see his analysis <a href=\"http://blog.givewell.org/attachments/worms.pdf\">here</a>. The bottom line is that when one applies Bayes's rule to obtain a distribution for cost-effectiveness based on (a) a normally distributed prior distribution (b) a normally distributed \"estimate error,\" one obtains a distribution with</p>\n<ul>\n<li>Mean equal to the average of the two means weighted by their inverse variances </li>\n<li>Variance equal to the harmonic sum of the two variances</li>\n</ul>\n<p>The following charts show what this formula implies in a variety of different simple hypotheticals. In all of these, the prior distribution has mean = 0 and standard deviation = 1, and the estimate has mean = 10, but the \"estimate error\" varies, with important effects: an estimate with little enough estimate error can almost be taken literally, while an estimate with large enough estimate error ends ought to be almost ignored.</p>\n<p>In each of these charts, the black line represents a <a href=\"http://en.wikipedia.org/wiki/Probability_density_function\">probability density function</a> for one's \"prior,\" the red line for an estimate (with the variance coming from \"estimate error\"), and the blue line for the final probability distribution, taking both the prior and the estimate into account. Taller, narrower distributions represent cases where probability is concentrated around the midpoint; shorter, wider distributions represent cases where the possibilities/probabilities are more spread out among many values. First, the case where the cost-effectiveness estimate has the same confidence interval around it as the prior: <img src=\"http://blog.givewell.org/images/bayesian adjustment 1.png\" alt=\"\"></p>\n<p>If one has a relatively reliable estimate (i.e., one with a narrow confidence interval / small variance of \"estimate error,\") then the Bayesian-adjusted conclusion ends up very close to the estimate. When we estimate quantities using highly precise and well-understood methods, we can use them (almost) literally. <img src=\"http://blog.givewell.org/images/bayesian adjustment 2.png\" alt=\"\"> <img src=\"http://blog.givewell.org/images/bayesian adjustment 3.png\" alt=\"\"></p>\n<p>On the flip side, when the estimate is relatively unreliable (wide confidence interval / large variance of \"estimate error\"), it has little effect on the final expectation of cost-effectiveness (or whatever is being estimated). And at the point where the one-standard-deviation bands include zero cost-effectiveness (i.e., where there's a pretty strong probability that the whole cost-effectiveness estimate is worthless), the estimate ends up having practically no effect on one's final view. <img src=\"http://blog.givewell.org/images/bayesian adjustment 4.png\" alt=\"\"> <img src=\"http://blog.givewell.org/images/bayesian adjustment 5.png\" alt=\"\"></p>\n<p>The details of how to apply this sort of analysis to cost-effectiveness estimates for charitable interventions are outside the scope of this post, which focuses on our belief in the importance of the concept of Bayesian adjustments. The big-picture takeaway is that just having the midpoint of a cost-effectiveness estimate is not worth very much in itself; it is important to understand the sources of estimate error, and the degree of estimate error relative to the degree of variation in estimated cost-effectiveness for different interventions.</p>\n<p><strong id=\"Pascal_s_Mugging\">Pascal's Mugging</strong></p>\n<p><strong></strong> <a href=\"http://wiki.lesswrong.com/wiki/Pascal's_mugging\">Pascal's Mugging</a> refers to a case where a claim of extravagant impact is made for a particular action, with little to no evidence:</p>\n<blockquote>Now suppose someone comes to me and says, \"Give me five dollars, or I'll use my magic powers \u2026 to [harm an imaginably huge number of] people.</blockquote>\n<p>Non-Bayesian approaches to evaluating these proposals often take the following form: \"Even if we assume that this analysis is 99.99% likely to be wrong, the expected value is still high - and are you willing to bet that this analysis is wrong at 99.99% odds?\"</p>\n<p>However, this is a case where \"estimate error\" is probably accounting for the lion's share of variance in estimated expected value, and therefore I believe that a proper Bayesian adjustment would correctly assign little value where there is little basis for the estimate, no matter how high the midpoint of the estimate.</p>\n<p>Say that you've come to believe - based on life experience - in a \"prior distribution\" for the value of your actions, with a mean of zero and a standard deviation of 1. (The unit type you use to value your actions is irrelevant to the point I'm making; so in this case the units I'm using are simply standard deviations based on your prior distribution for the value of your actions). Now say that someone estimates that action A (e.g., giving in to the mugger's demands) has an expected value of X (same units) - but that the estimate itself is so rough that the right expected value could easily be 0 or 2X. More specifically, say that the error in the expected value estimate has a standard deviation of X.</p>\n<p>An EEV approach to this situation might say, \"Even if there's a 99.99% chance that the estimate is completely wrong and that the value of Action A is 0, there's still an 0.01% probability that Action A has a value of X. Thus, overall Action A has an expected value of at least 0.0001X; the greater X is, the greater this value is, and if X is great enough then, then you should take Action A unless you're willing to bet at enormous odds that the framework is wrong.\"</p>\n<p>However, the same formula discussed above indicates that Action X actually has an expected value - after the Bayesian adjustment - of X/(X^2+1), or <em>just under 1/X</em>. In this framework, <em>the greater X is, the <strong>lower</strong> the expected value of Action A.</em> This syncs well with my intuitions: if someone threatened to harm one person unless you gave them $10, this ought to carry more weight (because it is more plausible in the face of the \"prior\" of life experience) than if they threatened to harm 100 people, which in turn ought to carry more weight than if they threatened to harm 3^^^3 people (I'm using 3^^^3 here as a <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">representation of an unimaginably huge number</a>).</p>\n<p>The point at which a threat or proposal starts to be called \"Pascal's Mugging\" can be thought of as the point at which the claimed value of Action A is wildly outside the prior set by life experience (which may cause the feeling that common sense is being violated). If someone claims that giving him/her $10 will accomplish 3^^^3 times as much as a 1-standard-deviation life action from the appropriate reference class, then the actual post-adjustment expected value of Action A will be just under (1/3^^^3) (in standard deviation terms) - only trivially higher than the value of an average action, and likely lower than other actions one could take with the same resources. This is true without applying <em>any</em> particular probability that the person's framework is wrong - it is simply a function of the fact that their estimate has such enormous possible error. An ungrounded estimate making an extravagant claim ought to be more or less discarded in the face of the \"prior distribution\" of life experience.</p>\n<p><strong id=\"Generalizing_the_Bayesian_approach\">Generalizing the Bayesian approach</strong></p>\n<p>In the above cases, I've given quantifications of (a) the appropriate prior for cost-effectiveness; (b) the strength/confidence of a given cost-effectiveness estimate. One needs to quantify both (a) and (b) - not just quantify estimated cost-effectiveness - in order to formally make the needed Bayesian adjustment to the initial estimate.</p>\n<p>But when it comes to giving, and many other decisions, reasonable quantification of these things usually isn't possible. To have a prior, you need a reference class, and <a href=\"http://wiki.lesswrong.com/wiki/Outside_view\">reference classes are debatable</a>.</p>\n<p>It's my view that my brain instinctively processes huge amounts of information, coming from many different reference classes, and arrives at a prior; if I attempt to formalize my prior, counting only what I can name and justify, I can worsen the accuracy a lot relative to going with my gut. Of course there is a problem here: going with one's gut can be an excuse for going with what one wants to believe, and a lot of what enters into my gut belief could be irrelevant to proper Bayesian analysis. There is an appeal to formulas, which is that they seem to be susceptible to outsiders' checking them for fairness and consistency.</p>\n<p>But when the formulas are too rough, I think the loss of accuracy outweighs the gains to transparency. Rather than using a formula that is checkable but omits a huge amount of information, I'd prefer to state my intuition - without pretense that it is anything but an intuition - and hope that the ensuing discussion provides the needed check on my intuitions.</p>\n<p>I can't, therefore, usefully say what I think the appropriate prior estimate of charity cost-effectiveness is. I can, however, describe a couple of approaches to Bayesian adjustments that I oppose, and can describe a few heuristics that I use to determine whether I'm making an appropriate Bayesian adjustment.</p>\n<p><em>Approaches to Bayesian adjustment that I oppose</em></p>\n<p>I have seen some argue along the lines of \"I have a very weak (or <a href=\"http://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors\">uninformative</a>) prior, which means I can more or less take rough estimates literally.\" I think this is a mistake. We do have a lot of information by which to judge what to expect from an action (including a donation), and failure to use all the information we have is a failure to make the appropriate Bayesian adjustment. Even just a sense for the values of the small set of actions you've taken in your life, and observed the consequences of, gives you something to work with as far as an \"outside view\" and a starting probability distribution for the value of your actions; this distribution probably ought to have high variance, but when dealing with a rough estimate that has very high variance of its own, it may still be quite a meaningful prior.</p>\n<p>I have seen some using the EEV framework who can tell that their estimates seem too optimistic, so they make various \"downward adjustments,\" multiplying their EEV by apparently ad hoc figures (1%, 10%, 20%). What isn't clear is whether the size of the adjustment they're making has the correct relationship to (a) the weakness of the estimate itself (b) the strength of the prior (c) distance of the estimate from the prior. An example of how this approach can go astray can be seen in the \"Pascal's Mugging\" analysis above: assigning one's framework a 99.99% chance of being totally wrong may seem to be amply conservative, but in fact the proper Bayesian adjustment is much larger and leads to a completely different conclusion.</p>\n<p><em>Heuristics I use to address whether I'm making an appropriate prior-based adjustment</em></p>\n<ul>\n<li><strong>The more action is asked of me, the more evidence I require.</strong> Anytime I'm asked to take a significant action (giving a significant amount of money, time, effort, etc.), this action has to have higher expected value than the action I would otherwise take. My intuitive feel for the distribution of \"how much my actions accomplish\" serves as a prior - an adjustment to the value that the asker claims for my action. </li>\n<li><strong>I pay attention to how much of the variation I see between estimates is likely to be driven by true variation vs. estimate error.</strong> As shown above, when an estimate is rough enough so that error might account for the bulk of the observed variation, a proper Bayesian approach can involve a massive discount to the estimate. </li>\n<li><strong>I put much more weight on conclusions that seem to be supported by multiple different lines of analysis, as unrelated to one another as possible</strong>. If one starts with a high-error estimate of expected value, and then starts finding more estimates with the same midpoint, the variance of the aggregate estimate error declines; the less correlated the estimates are, the greater the decline in the variance of the error, and thus the lower the Bayesian adjustment to the final estimate. This is a formal way of observing that \"diversified\" reasons for believing something lead to more \"robust\" beliefs, i.e., beliefs that are less likely to fall apart with new information and can be used with less skepticism. </li>\n<li><strong>I am hesitant to embrace arguments that seem to have anti-common-sense implications (unless the evidence behind these arguments is strong)</strong> and I think my prior may often be the reason for this. As seen above, a too-weak prior can lead to many seemingly absurd beliefs and consequences, such as falling prey to \"Pascal's Mugging\" and removing the incentive for investigation of strong claims. Strengthening the prior fixes these problems (while over-strengthening the prior results in simply ignoring new evidence). In general, I believe that when a particular kind of reasoning seems to me to have anti-common-sense implications, this may indicate that its implications are well outside my prior. </li>\n<li><strong>My prior for charity is generally skeptical</strong>, as outlined at <a href=\"http://blog.givewell.org/2009/12/05/a-conflict-of-bayesian-priors/\">this post</a>. Giving well seems <a href=\"http://blog.givewell.org/2011/06/11/why-we-should-expect-good-giving-to-be-hard/\">conceptually quite difficult to me</a>, and it's been my experience over time that the more we dig on a cost-effectiveness estimate, the more unwarranted optimism we uncover. Also, having an optimistic prior would mean giving to opaque charities, and that seems to violate common sense. Thus, we look for charities with quite strong evidence of effectiveness, and tend to prefer very strong charities with reasonably high estimated cost-effectiveness to weaker charities with very high estimated cost-effectiveness </li>\n</ul>\n<p><strong id=\"Conclusion\">Conclusion</strong></p>\n<ul>\n<li>I feel that any giving approach that relies only on estimated expected-value - and does not incorporate preferences for better-grounded estimates over shakier estimates - is flawed. </li>\n<li>Thus, when aiming to maximize expected positive impact, it is not advisable to make giving decisions based fully on explicit formulas. Proper Bayesian adjustments are important and are usually overly difficult to formalize. </li>\n</ul>", "sections": [{"title": "The approach we oppose: \"explicit expected-value\" (EEV) decisionmaking", "anchor": "The_approach_we_oppose___explicit_expected_value___EEV__decisionmaking", "level": 1}, {"title": "Simple example of a Bayesian approach vs. an EEV approach", "anchor": "Simple_example_of_a_Bayesian_approach_vs__an_EEV_approach", "level": 1}, {"title": "Applying Bayesian adjustments to cost-effectiveness estimates for donations, actions, etc.", "anchor": "Applying_Bayesian_adjustments_to_cost_effectiveness_estimates_for_donations__actions__etc_", "level": 1}, {"title": "Pascal's Mugging", "anchor": "Pascal_s_Mugging", "level": 1}, {"title": "Generalizing the Bayesian approach", "anchor": "Generalizing_the_Bayesian_approach", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "252 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 252, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pC47ZTsPNAkjavkXs", "hEqsWLm5zQtsPevd3", "a5JAiTdytou3Jg749"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-19T03:13:55.622Z", "modifiedAt": null, "url": null, "title": "[LINK] Wired - \"New Chip Borrows Brain\u2019s Computing Tricks\"", "slug": "link-wired-new-chip-borrows-brain-s-computing-tricks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:05.148Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "minderbinder", "createdAt": "2011-07-20T16:48:05.262Z", "isAdmin": false, "displayName": "minderbinder"}, "userId": "SPcZb9HWJg5nsvt4E", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rsxysxSqascJaQMS9/link-wired-new-chip-borrows-brain-s-computing-tricks", "pageUrlRelative": "/posts/rsxysxSqascJaQMS9/link-wired-new-chip-borrows-brain-s-computing-tricks", "linkUrl": "https://www.lesswrong.com/posts/rsxysxSqascJaQMS9/link-wired-new-chip-borrows-brain-s-computing-tricks", "postedAtFormatted": "Friday, August 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Wired%20-%20%22New%20Chip%20Borrows%20Brain%E2%80%99s%20Computing%20Tricks%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Wired%20-%20%22New%20Chip%20Borrows%20Brain%E2%80%99s%20Computing%20Tricks%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrsxysxSqascJaQMS9%2Flink-wired-new-chip-borrows-brain-s-computing-tricks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Wired%20-%20%22New%20Chip%20Borrows%20Brain%E2%80%99s%20Computing%20Tricks%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrsxysxSqascJaQMS9%2Flink-wired-new-chip-borrows-brain-s-computing-tricks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrsxysxSqascJaQMS9%2Flink-wired-new-chip-borrows-brain-s-computing-tricks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>In case anyone's interested,<a href=\"http://www.wired.com/wiredscience/2011/08/ibm-synapse-cognitive-computer/\">here's</a> an article about new computer chips by IBM which emulate brain functions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rsxysxSqascJaQMS9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 0, "extendedScore": null, "score": 7.569889444444109e-07, "legacy": true, "legacyId": "9293", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-19T03:26:34.722Z", "modifiedAt": null, "url": null, "title": "Question about meetup location address format ..", "slug": "question-about-meetup-location-address-format", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:05.140Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sriku", "createdAt": "2011-05-03T10:17:48.030Z", "isAdmin": false, "displayName": "sriku"}, "userId": "zFvjeJKyRzRYKjhQS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4KEMmbaCfcADs3cwy/question-about-meetup-location-address-format", "pageUrlRelative": "/posts/4KEMmbaCfcADs3cwy/question-about-meetup-location-address-format", "linkUrl": "https://www.lesswrong.com/posts/4KEMmbaCfcADs3cwy/question-about-meetup-location-address-format", "postedAtFormatted": "Friday, August 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Question%20about%20meetup%20location%20address%20format%20..&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestion%20about%20meetup%20location%20address%20format%20..%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4KEMmbaCfcADs3cwy%2Fquestion-about-meetup-location-address-format%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Question%20about%20meetup%20location%20address%20format%20..%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4KEMmbaCfcADs3cwy%2Fquestion-about-meetup-location-address-format", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4KEMmbaCfcADs3cwy%2Fquestion-about-meetup-location-address-format", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 62, "htmlBody": "<p>(This is an admin question.)</p>\n<p>I haven't seen any meetups proposed in Singapore thus far and thought of posting one. However, the \"submit\" on the page comes back and says \"you must supply a location\", even when I do have the field filled (I set it to \"Starbucks at University Town, Singapore\"). Does it expect the address to be in a specific format?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4KEMmbaCfcADs3cwy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 7.569930054878279e-07, "legacy": true, "legacyId": "9294", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-19T04:29:51.999Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Availability", "slug": "seq-rerun-availability", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:05.263Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yrYBNKWTmKZQZ4cAG/seq-rerun-availability", "pageUrlRelative": "/posts/yrYBNKWTmKZQZ4cAG/seq-rerun-availability", "linkUrl": "https://www.lesswrong.com/posts/yrYBNKWTmKZQZ4cAG/seq-rerun-availability", "postedAtFormatted": "Friday, August 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Availability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Availability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrYBNKWTmKZQZ4cAG%2Fseq-rerun-availability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Availability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrYBNKWTmKZQZ4cAG%2Fseq-rerun-availability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrYBNKWTmKZQZ4cAG%2Fseq-rerun-availability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p>Today's post, <a href=\"/lw/j5/availability/\">Availability</a> was originally published on 06 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Availability bias is a tendency to estimate the probability of an event based on whatever evidence about that event pops into your mind, without taking into account the ways in which some pieces of evidence are more memorable than others, or some pieces of evidence are easier to come by than others. This bias directly consists in considering a mismatched data set that leads to a distorted model, and biased estimate.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/75d/seq_rerun_absurdity_heuristic_absurdity_bias/\">Absurdity Heuristic, Absurdity Bias</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb19c": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yrYBNKWTmKZQZ4cAG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 7.570133208223453e-07, "legacy": true, "legacyId": "9297", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["R8cpqD3NA4rZxRdQ4", "AithjtGYuh6HCGGor", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-19T14:05:10.617Z", "modifiedAt": null, "url": null, "title": "LessWrong as place for scientifically literate advice", "slug": "lesswrong-as-place-for-scientifically-literate-advice", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:05.532Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Metus", "createdAt": "2011-01-23T21:54:34.357Z", "isAdmin": false, "displayName": "Metus"}, "userId": "mNQ4fSvro7LYgrii4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/23bquQPGHXCZTWeQ8/lesswrong-as-place-for-scientifically-literate-advice", "pageUrlRelative": "/posts/23bquQPGHXCZTWeQ8/lesswrong-as-place-for-scientifically-literate-advice", "linkUrl": "https://www.lesswrong.com/posts/23bquQPGHXCZTWeQ8/lesswrong-as-place-for-scientifically-literate-advice", "postedAtFormatted": "Friday, August 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20as%20place%20for%20scientifically%20literate%20advice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20as%20place%20for%20scientifically%20literate%20advice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F23bquQPGHXCZTWeQ8%2Flesswrong-as-place-for-scientifically-literate-advice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20as%20place%20for%20scientifically%20literate%20advice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F23bquQPGHXCZTWeQ8%2Flesswrong-as-place-for-scientifically-literate-advice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F23bquQPGHXCZTWeQ8%2Flesswrong-as-place-for-scientifically-literate-advice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 544, "htmlBody": "<p><strong>Disclaimer and abstract:</strong>&nbsp;English is not my native language, so please notify me if you see grammatical or stylistic mistakes.&nbsp;This posting is mainly an account of my thoughts. It is highly informal and contains almost no advice but asks for it. I am relatively new to LessWrong and rationality but hope that I can contribute something useful. Here I suggest to give and gather advice to and from readers of LessWrong, sketch possible topics and list a few examples of such advice existing. Also, I present a provisional solution to gather this advice and a more sustainable solution.&nbsp;Sadly, I can not offer advice on the topics mentioned below as I am merely an interested amateur.</p>\n<p>&nbsp;</p>\n<p><strong>Benefiting from rationality</strong></p>\n<p>LessWrong is full of smart people who 'believe' in rationality. But you have to benefit from something to make it worth believing it, you have to make beliefs pay rent. This applies especially to rationality. But rationality benefits us heavily through the scientific method and thus the giant body of work scientists have produced throughout the centuries. Too much was written to be reviewed by a single individual so it seems rational to sum up the results with notes on confidence and consequences. It would be scientifically literate advice as opposed to anecdotical help. Such an effort, especially if it is made systematically would help draw attention to LessWrong and rationality.</p>\n<p><strong>Beginnings</strong></p>\n<p>Some scattered efforts are already made here on LessWrong on producing such advice as hinted at in the above paragraph. Look for example at lukeprog's articles on happiness. How they draw heavily from scientific articles and give concrete, simple to understand advices is how an article for rational advice should look like. In \"Selfish Reasons to Have More Kids\" Bryan Caplan also draws heavily from scientific literature to prove that given today's parent's efforts, the majority of children's life outcomes are determined by environmental or genetic factors. He then concludes that today's adults perceive the cost of children as higher than it is and thus should have more kids.</p>\n<p><strong>Possible Topics</strong></p>\n<p>The topics people need advice on are mostly the same throughout the ages: Happiness, love, health money. But seeing as it is a recurring theme on LessWrong, advice can be given on cryonics, akrasia and social behavior in broader sense. Many more topics are possible of course and the above list is an outline for willing authors.</p>\n<p><strong>A provisional solution</strong></p>\n<p>LessWrong, obviously, has a discussion section and a tree based commenting system. This can, at least temporarily, be used for the purposes named in this posting. If you can provide advice, please head your comment with \"[Advice]\". If you have a request or an idea for advice, please head your comment &nbsp;\"[Request]\". If you just want to comment on this article, just comment as usual. I think the voting system should automatically seperate useful from useless advice and requests. For clearer view, start for each topic a new comment, so that health advice on diet does not unnecessarily get mixed up with advice on exercise.</p>\n<p>Of course it is useful too to link to existing advice.</p>\n<p><strong>A more sustainable solution</strong></p>\n<p>As I seem to notice, there is demand for such advice. It should be possible to create a sub-category of \"discussion\" to post exactly that advice and have it nicely seperated from the other discussions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "23bquQPGHXCZTWeQ8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 7.571980389153258e-07, "legacy": true, "legacyId": "9096", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Disclaimer and abstract:</strong>&nbsp;English is not my native language, so please notify me if you see grammatical or stylistic mistakes.&nbsp;This posting is mainly an account of my thoughts. It is highly informal and contains almost no advice but asks for it. I am relatively new to LessWrong and rationality but hope that I can contribute something useful. Here I suggest to give and gather advice to and from readers of LessWrong, sketch possible topics and list a few examples of such advice existing. Also, I present a provisional solution to gather this advice and a more sustainable solution.&nbsp;Sadly, I can not offer advice on the topics mentioned below as I am merely an interested amateur.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Benefiting_from_rationality\">Benefiting from rationality</strong></p>\n<p>LessWrong is full of smart people who 'believe' in rationality. But you have to benefit from something to make it worth believing it, you have to make beliefs pay rent. This applies especially to rationality. But rationality benefits us heavily through the scientific method and thus the giant body of work scientists have produced throughout the centuries. Too much was written to be reviewed by a single individual so it seems rational to sum up the results with notes on confidence and consequences. It would be scientifically literate advice as opposed to anecdotical help. Such an effort, especially if it is made systematically would help draw attention to LessWrong and rationality.</p>\n<p><strong id=\"Beginnings\">Beginnings</strong></p>\n<p>Some scattered efforts are already made here on LessWrong on producing such advice as hinted at in the above paragraph. Look for example at lukeprog's articles on happiness. How they draw heavily from scientific articles and give concrete, simple to understand advices is how an article for rational advice should look like. In \"Selfish Reasons to Have More Kids\" Bryan Caplan also draws heavily from scientific literature to prove that given today's parent's efforts, the majority of children's life outcomes are determined by environmental or genetic factors. He then concludes that today's adults perceive the cost of children as higher than it is and thus should have more kids.</p>\n<p><strong id=\"Possible_Topics\">Possible Topics</strong></p>\n<p>The topics people need advice on are mostly the same throughout the ages: Happiness, love, health money. But seeing as it is a recurring theme on LessWrong, advice can be given on cryonics, akrasia and social behavior in broader sense. Many more topics are possible of course and the above list is an outline for willing authors.</p>\n<p><strong id=\"A_provisional_solution\">A provisional solution</strong></p>\n<p>LessWrong, obviously, has a discussion section and a tree based commenting system. This can, at least temporarily, be used for the purposes named in this posting. If you can provide advice, please head your comment with \"[Advice]\". If you have a request or an idea for advice, please head your comment &nbsp;\"[Request]\". If you just want to comment on this article, just comment as usual. I think the voting system should automatically seperate useful from useless advice and requests. For clearer view, start for each topic a new comment, so that health advice on diet does not unnecessarily get mixed up with advice on exercise.</p>\n<p>Of course it is useful too to link to existing advice.</p>\n<p><strong id=\"A_more_sustainable_solution\">A more sustainable solution</strong></p>\n<p>As I seem to notice, there is demand for such advice. It should be possible to create a sub-category of \"discussion\" to post exactly that advice and have it nicely seperated from the other discussions.</p>", "sections": [{"title": "Benefiting from rationality", "anchor": "Benefiting_from_rationality", "level": 1}, {"title": "Beginnings", "anchor": "Beginnings", "level": 1}, {"title": "Possible Topics", "anchor": "Possible_Topics", "level": 1}, {"title": "A provisional solution", "anchor": "A_provisional_solution", "level": 1}, {"title": "A more sustainable solution", "anchor": "A_more_sustainable_solution", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-19T14:49:57.053Z", "modifiedAt": null, "url": null, "title": "Meetup : Singapore September 2011", "slug": "meetup-singapore-september-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:21.908Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sriku", "createdAt": "2011-05-03T10:17:48.030Z", "isAdmin": false, "displayName": "sriku"}, "userId": "zFvjeJKyRzRYKjhQS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ni95MAYxjNNhqt2Ef/meetup-singapore-september-2011", "pageUrlRelative": "/posts/Ni95MAYxjNNhqt2Ef/meetup-singapore-september-2011", "linkUrl": "https://www.lesswrong.com/posts/Ni95MAYxjNNhqt2Ef/meetup-singapore-september-2011", "postedAtFormatted": "Friday, August 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Singapore%20September%202011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Singapore%20September%202011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNi95MAYxjNNhqt2Ef%2Fmeetup-singapore-september-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Singapore%20September%202011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNi95MAYxjNNhqt2Ef%2Fmeetup-singapore-september-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNi95MAYxjNNhqt2Ef%2Fmeetup-singapore-september-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2f'>Singapore September 2011</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 September 2011 04:00:00PM (+0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">23 folkstone road, Singapore</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi, I haven't seen a Singapore meetup posted here and would like to know if any lw-ers in Singapore are interested in meeting up (and why won't you be?).</p>\n\n<p>Starbucks at the spanking new (and not yet on google maps) University Town is a good area with plenty of seating both in and around. Of course, everything is flexible.  The address given is an approximate location and you'll have to walk into UTown from that spot.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2f'>Singapore September 2011</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ni95MAYxjNNhqt2Ef", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.572124180662354e-07, "legacy": true, "legacyId": "9303", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Singapore_September_2011\">Discussion article for the meetup : <a href=\"/meetups/2f\">Singapore September 2011</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 September 2011 04:00:00PM (+0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">23 folkstone road, Singapore</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi, I haven't seen a Singapore meetup posted here and would like to know if any lw-ers in Singapore are interested in meeting up (and why won't you be?).</p>\n\n<p>Starbucks at the spanking new (and not yet on google maps) University Town is a good area with plenty of seating both in and around. Of course, everything is flexible.  The address given is an approximate location and you'll have to walk into UTown from that spot.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Singapore_September_20111\">Discussion article for the meetup : <a href=\"/meetups/2f\">Singapore September 2011</a></h2>", "sections": [{"title": "Discussion article for the meetup : Singapore September 2011", "anchor": "Discussion_article_for_the_meetup___Singapore_September_2011", "level": 1}, {"title": "Discussion article for the meetup : Singapore September 2011", "anchor": "Discussion_article_for_the_meetup___Singapore_September_20111", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-19T17:45:32.522Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup 08-23-2011", "slug": "meetup-west-la-meetup-08-23-2011", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6EWB89wbEExErTduM/meetup-west-la-meetup-08-23-2011", "pageUrlRelative": "/posts/6EWB89wbEExErTduM/meetup-west-la-meetup-08-23-2011", "linkUrl": "https://www.lesswrong.com/posts/6EWB89wbEExErTduM/meetup-west-la-meetup-08-23-2011", "postedAtFormatted": "Friday, August 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%2008-23-2011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%2008-23-2011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6EWB89wbEExErTduM%2Fmeetup-west-la-meetup-08-23-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%2008-23-2011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6EWB89wbEExErTduM%2Fmeetup-west-la-meetup-08-23-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6EWB89wbEExErTduM%2Fmeetup-west-la-meetup-08-23-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 134, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2g'>West LA Meetup 08-23-2011</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 August 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10800 West Pico Blvd, Suite 312, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When</strong>: 7pm - 9pm August 23th.</p>\n\n<p><strong>Where</strong>: <a href=\"http://maps.google.com/maps?q=10800+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">The Westside Pavillion</a> - on the bridge, which connects Nordstrom 3rd floor with Barnes &amp; Noble / Landmark Theatres 3rd floor.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Recommended Reading</strong>:</p>\n\n<p>-<a href=\"http://lesswrong.com/lw/go/why_truth_and/\">why truth? And...</a></p>\n\n<p>-<a href=\"http://lesswrong.com/lw/31/what_do_we_mean_by_rationality/\">What Do We Mean Be &quot;Rationality&quot;?</a></p>\n\n<p>-<a href=\"http://lesswrong.com/lw/4e/cached_selves/\">Cached Selves</a></p>\n\n<p>-<a href=\"http://lesswrong.com/lw/jx/we_change_our_minds_less_often_than_we_think/\">We Change Our Minds Less Often Than We Think</a></p>\n\n<p>Whether you're a regular reader or totally new, here for the theoretical musings or the practical things, come by and say hello! The conversation is largely unstructured, and the people are awesome.</p>\n\n<p><em>There will be snacks. I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</em></p>\n\n<p>See also: <a href=\"http://lesswrong.com/r/discussion/lw/6at/west_la_biweekly_meetups/\">West LA Biweekly Meetups</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2g'>West LA Meetup 08-23-2011</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6EWB89wbEExErTduM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 7.572688138876918e-07, "legacy": true, "legacyId": "9304", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_08_23_2011\">Discussion article for the meetup : <a href=\"/meetups/2g\">West LA Meetup 08-23-2011</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 August 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10800 West Pico Blvd, Suite 312, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When</strong>: 7pm - 9pm August 23th.</p>\n\n<p><strong>Where</strong>: <a href=\"http://maps.google.com/maps?q=10800+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">The Westside Pavillion</a> - on the bridge, which connects Nordstrom 3rd floor with Barnes &amp; Noble / Landmark Theatres 3rd floor.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Recommended Reading</strong>:</p>\n\n<p>-<a href=\"http://lesswrong.com/lw/go/why_truth_and/\">why truth? And...</a></p>\n\n<p>-<a href=\"http://lesswrong.com/lw/31/what_do_we_mean_by_rationality/\">What Do We Mean Be \"Rationality\"?</a></p>\n\n<p>-<a href=\"http://lesswrong.com/lw/4e/cached_selves/\">Cached Selves</a></p>\n\n<p>-<a href=\"http://lesswrong.com/lw/jx/we_change_our_minds_less_often_than_we_think/\">We Change Our Minds Less Often Than We Think</a></p>\n\n<p>Whether you're a regular reader or totally new, here for the theoretical musings or the practical things, come by and say hello! The conversation is largely unstructured, and the people are awesome.</p>\n\n<p><em>There will be snacks. I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</em></p>\n\n<p>See also: <a href=\"http://lesswrong.com/r/discussion/lw/6at/west_la_biweekly_meetups/\">West LA Biweekly Meetups</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_08_23_20111\">Discussion article for the meetup : <a href=\"/meetups/2g\">West LA Meetup 08-23-2011</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup 08-23-2011", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_08_23_2011", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup 08-23-2011", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_08_23_20111", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YshRbqZHYFoEMqFAu", "RcZCwxFiZzE6X7nsv", "BHYBdijDcAKQ6e45Z", "buixYfcXBah9hbSNZ", "tHFu6kvy2HMvQBEhW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-19T20:35:55.559Z", "modifiedAt": null, "url": null, "title": "Spaced Repetition literature review prize: And the winner is...", "slug": "spaced-repetition-literature-review-prize-and-the-winner-is", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:20.799Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eD6TZm2r25HzYzZzY/spaced-repetition-literature-review-prize-and-the-winner-is", "pageUrlRelative": "/posts/eD6TZm2r25HzYzZzY/spaced-repetition-literature-review-prize-and-the-winner-is", "linkUrl": "https://www.lesswrong.com/posts/eD6TZm2r25HzYzZzY/spaced-repetition-literature-review-prize-and-the-winner-is", "postedAtFormatted": "Friday, August 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Spaced%20Repetition%20literature%20review%20prize%3A%20And%20the%20winner%20is...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASpaced%20Repetition%20literature%20review%20prize%3A%20And%20the%20winner%20is...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeD6TZm2r25HzYzZzY%2Fspaced-repetition-literature-review-prize-and-the-winner-is%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Spaced%20Repetition%20literature%20review%20prize%3A%20And%20the%20winner%20is...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeD6TZm2r25HzYzZzY%2Fspaced-repetition-literature-review-prize-and-the-winner-is", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeD6TZm2r25HzYzZzY%2Fspaced-repetition-literature-review-prize-and-the-winner-is", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<p>The <a href=\"/lw/69p/prize_new_contest_for_spaced_repetition/\">Spaced Repetition literature review&nbsp;prize</a>&nbsp;for the best new review of the evidence on&nbsp;<a href=\"http://en.wikipedia.org/wiki/Spaced_repetition\">Spaced Repetition</a> has ended and the judging panel has made its decision.&nbsp;The prize attracted entries from <a href=\"/user/Duke\">Duke</a>&nbsp;(<a href=\"/lw/64k/memory_spaced_repetition_and_life/\">entry</a>)&nbsp;and <a href=\"/user/Gwern\">Gwern</a>&nbsp;(<a href=\"/lw/718/spaced_repetition_review_my_entry/\">entry</a>). After reviewing the submissions separately and then discussing them together, the judging panel&nbsp;unanimously judged Gwern's entry to be the best.</p>\n<p>With great pleasure, we now award Gwern the prize of $385.&nbsp;</p>\n<p>Anki cards for Gwern's review are available as the shared deck \"Gwern Spaced Repetition Lit Review\". If you have improvements or alternate decks, post them in the comments.</p>\n<p>The three judges,&nbsp;<a href=\"/user/BenLowell/\">BenLowell</a>, <a href=\"/user/GuySrinivasan\">Guy Srinivasan</a>&nbsp;and <a href=\"/user/jsalvatier/\">John Salvatier</a> (me), are&nbsp;Seattle LessWrongers who&nbsp;volunteered to judge the contest.&nbsp;</p>\n<p>We thank Duke and Gwern for their submissions, as well as users <a href=\"/user/randomwalker/\">randomwalker</a>,&nbsp;<a href=\"/user/Antisuji/\">Antisuji</a>, <a href=\"/user/Dr_Manhattan/\">Dr_Manhattan</a>, <a href=\"/user/Benquo/\">Benquo</a>, <a href=\"/user/Nick_Roy/\">Nick_Roy</a>&nbsp;for contributing to the prize fund.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2, "khReijeucXJTnsyMT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eD6TZm2r25HzYzZzY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 37, "extendedScore": null, "score": 7.57323388660126e-07, "legacy": true, "legacyId": "9305", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uR4r3eZZqLmjZDqFj", "3r4GETDPMf335HfpA", "ZCRtnwGbcbpYP8t6h"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-19T20:38:39.818Z", "modifiedAt": null, "url": null, "title": "Meetup : Austin, TX", "slug": "meetup-austin-tx-8", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:05.468Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/agKJLg582SBCGmk73/meetup-austin-tx-8", "pageUrlRelative": "/posts/agKJLg582SBCGmk73/meetup-austin-tx-8", "linkUrl": "https://www.lesswrong.com/posts/agKJLg582SBCGmk73/meetup-austin-tx-8", "postedAtFormatted": "Friday, August 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Austin%2C%20TX&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Austin%2C%20TX%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FagKJLg582SBCGmk73%2Fmeetup-austin-tx-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Austin%2C%20TX%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FagKJLg582SBCGmk73%2Fmeetup-austin-tx-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FagKJLg582SBCGmk73%2Fmeetup-austin-tx-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2h'>Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 August 2011 01:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I'm back after missing three meetups! I hope you'll be there. Once again, at Caffe Medici, starting at 1:30PM.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2h'>Austin, TX</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "agKJLg582SBCGmk73", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.573244238168294e-07, "legacy": true, "legacyId": "9306", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Austin__TX\">Discussion article for the meetup : <a href=\"/meetups/2h\">Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 August 2011 01:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I'm back after missing three meetups! I hope you'll be there. Once again, at Caffe Medici, starting at 1:30PM.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Austin__TX1\">Discussion article for the meetup : <a href=\"/meetups/2h\">Austin, TX</a></h2>", "sections": [{"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX", "level": 1}, {"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-19T20:52:21.114Z", "modifiedAt": null, "url": null, "title": "Good resource for marketing research?", "slug": "good-resource-for-marketing-research", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:20.946Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mercurial", "createdAt": "2011-04-21T03:59:51.257Z", "isAdmin": false, "displayName": "Mercurial"}, "userId": "2dGsX6cZSR9PmQyBq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xJc6HtfJuBvvYnmmp/good-resource-for-marketing-research", "pageUrlRelative": "/posts/xJc6HtfJuBvvYnmmp/good-resource-for-marketing-research", "linkUrl": "https://www.lesswrong.com/posts/xJc6HtfJuBvvYnmmp/good-resource-for-marketing-research", "postedAtFormatted": "Friday, August 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Good%20resource%20for%20marketing%20research%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGood%20resource%20for%20marketing%20research%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxJc6HtfJuBvvYnmmp%2Fgood-resource-for-marketing-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Good%20resource%20for%20marketing%20research%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxJc6HtfJuBvvYnmmp%2Fgood-resource-for-marketing-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxJc6HtfJuBvvYnmmp%2Fgood-resource-for-marketing-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 460, "htmlBody": "<p>I'm planning on starting a business in a few years. &nbsp;I realize that I have absolutely zero entrepreneurial experience and really don't know what I'm doing when it comes to appealing to my target audience. &nbsp;I know a number of psychological tricks that should <em>in theory</em> work, but theory and practice often differ and my understanding of the theory isn't robust enough to make up for the difference.</p>\n<p>So, what I need is some kind of rational direction to look in so that I can start digging into the basic marketing research I'll need to do before I plunk money down on a business. &nbsp;At this point my expertise is limited to having read Michael Gerber's book <a href=\"http://www.amazon.com/-Myth-Revisited-Small-Businesses-About/dp/0887307280/ref=sr_1_1?ie=UTF8&amp;qid=1313786199&amp;sr=8-1\" target=\"_blank\">The E-Myth Revisited</a>, which in my naivety seems like a good way to address the question of how to avoid being a victim of the stunning failure rate of private businesses. &nbsp;But I really wouldn't be surprised to discover that Gerber is just dead wrong and that I don't have the background needed to understand that.</p>\n<p>So, my question is: Where can I look to get rational guidance on this? &nbsp;I prefer a book or a series or something like that since I don't have the money to hire a professional yet, and it's still a bit early to start plunking money down on the seed of a business idea.</p>\n<p>I doubt the nature of the business I'm thinking of is all that relevant since I'm looking for pretty basic, general information on marketing. &nbsp;But as I could easily be wrong: This is for a school. &nbsp;My hope is to get an alternative high school set up that trains students to be meaningfully ready for the modern world, dropping the stuff that's there purely due to cultural momentum. &nbsp;By and large (but not completely), this means focusing on metacognitive skills rather than primarily on subject content. &nbsp;I'm an education researcher and know of a number of psychological tools that are extremely promising and have proven effective in a wide range of cases but simply haven't been adopted for use in classrooms on any relevant scale, so part of my work between now and when I start this school is going to be aimed at doing research to hammer out the pragmatics. &nbsp;But that's the idea in a very tiny nutshell: use findings in psychology and sociology to teach stuff to teenagers that they'll actually find useful throughout their lives and that society will be glad that they know. &nbsp;I'm interested in making this a business primarily because I need it to provide me with enough of an income that I don't have to distract myself with some other job in order to support my family.</p>\n<p>So: rational marketing research methods. &nbsp;Anyone have a good direction for me?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xJc6HtfJuBvvYnmmp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 7.573288210508606e-07, "legacy": true, "legacyId": "9307", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-19T21:15:05.845Z", "modifiedAt": null, "url": null, "title": "A Crash Course in the Neuroscience of Human Motivation", "slug": "a-crash-course-in-the-neuroscience-of-human-motivation", "viewCount": null, "lastCommentedAt": "2017-03-13T16:53:47.039Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hN2aRnu798yas5b2k/a-crash-course-in-the-neuroscience-of-human-motivation", "pageUrlRelative": "/posts/hN2aRnu798yas5b2k/a-crash-course-in-the-neuroscience-of-human-motivation", "linkUrl": "https://www.lesswrong.com/posts/hN2aRnu798yas5b2k/a-crash-course-in-the-neuroscience-of-human-motivation", "postedAtFormatted": "Friday, August 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Crash%20Course%20in%20the%20Neuroscience%20of%20Human%20Motivation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Crash%20Course%20in%20the%20Neuroscience%20of%20Human%20Motivation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhN2aRnu798yas5b2k%2Fa-crash-course-in-the-neuroscience-of-human-motivation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Crash%20Course%20in%20the%20Neuroscience%20of%20Human%20Motivation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhN2aRnu798yas5b2k%2Fa-crash-course-in-the-neuroscience-of-human-motivation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhN2aRnu798yas5b2k%2Fa-crash-course-in-the-neuroscience-of-human-motivation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 11263, "htmlBody": "<p><small>[<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Muehlhauser-A-Crash-Course-in-the-Neuroscience-of-Human-Motivation-08-20-2011.pdf\">PDF</a> of this article updated Aug. 23, 2011]</small></p>\n<p><small>[<a href=\"#preface\">skip to preface</a>] </small></p>\n<p>Whenever I write a new article for Less Wrong, I'm pulled in two opposite directions.</p>\n<p>One force pulls me toward writing short, exciting posts with lots of brain candy and just <em>one main point</em>. Eliezer has done that kind of thing very well many times: see <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">Making Beliefs Pay Rent</a>, <a href=\"/lw/im/hindsight_devalues_science/\">Hindsight Devalues Science</a>, <a href=\"/lw/oj/probability_is_in_the_mind/\">Probability is in the Mind</a>,&nbsp;<a href=\"/lw/nu/taboo_your_words/\">Taboo Your Words</a>, <a href=\"/lw/oi/mind_projection_fallacy/%20%20%20%20\">Mind Projection Fallacy</a>,&nbsp;<a href=\"/lw/iq/guessing_the_teachers_password/\">Guessing the Teacher's Password</a>, <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">Hold Off on Proposing Solutions</a>, <a href=\"/lw/jb/applause_lights/\">Applause Lights</a>, <a href=\"/lw/of/dissolving_the_question/\">Dissolving the Question</a>, and <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">many</a> <a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">more</a>.</p>\n<p>Another force pulls me toward writing long, factually dense posts that fill in as many of the pieces of a particular argument in one fell swoop as possible. This is largely because I want to write about the cutting edge of human knowledge but I keep realizing that the inferential gap is <a href=\"/lw/kg/expecting_short_inferential_distances\">larger than I had anticipated</a>, and I want to fill in that inferential gap quickly so I can get to the cutting edge.</p>\n<p>For example, I had to draw on dozens of Eliezer's posts just to <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">say</a> I was <em>heading toward</em>&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">my metaethics sequence</a>. I've also published 21 new posts (many of them quite long and heavily researched) written specifically because I need to refer to them in my metaethics sequence.<sup>1</sup>&nbsp;I tried to make these posts interesting and useful on their own, but my primary motivation for writing them was that I need them for my metaethics sequence.</p>\n<p>And now I've written only&nbsp;<em>four posts</em><sup>2</sup>&nbsp;in my metaethics sequence and already the inferential gap to my next post in that sequence is huge again. :(</p>\n<p>So I'd like to try an experiment. I won't do it often, but I want to try it at least once. Instead of writing 20 more short posts between now and the next post in my metaethics sequence, I'll attempt to fill in a big chunk of the inferential gap to my next metaethics post in one fell swoop by writing a long tutorial post (<em>a la</em>&nbsp;Eliezer's tutorials on <a href=\"http://yudkowsky.net/rational/bayes\">Bayes' Theorem</a> and <a href=\"http://yudkowsky.net/rational/technical\">technical explanation</a>).<sup>3</sup></p>\n<p>So if you're not up for a 20-page tutorial on human motivation, this post isn't for you, but I hope you're glad I bothered to write it for the sake of others. If you <em>are</em>&nbsp;in the mood for a 20-page tutorial on human motivation, please proceed.</p>\n<p><a id=\"more\"></a></p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/HumanMotivation_title.png\" alt=\"\" /></p>\n<p>&nbsp;</p>\n<blockquote>\n<p>Who knows what I want to do? Who knows what anyone wants to do? How can you&nbsp;be sure about something like that? Isn&rsquo;t it all a question of brain chemistry, signals&nbsp;going back and forth, electrical energy in the cortex? How do you know whether&nbsp;something is really what you want to do or just some kind of nerve impulse in the&nbsp;brain. Some minor little activity takes place somewhere in this unimportant place in&nbsp;one of the brain hemispheres and suddenly I want to go to Montana or I don&rsquo;t want&nbsp;to go to Montana.</p>\n</blockquote>\n<p align=\"right\">- Don DeLillo, <em><a href=\"http://www.amazon.com/White-Noise-Penguin-Classics-Deluxe/dp/0143105981/\">White Noise</a></em></p>\n<h4><a name=\"Preface\"></a>Preface</h4>\n<p>How do we value things, and choose between options? Philosophers, economists, and psychologists have long tried to answer these questions. But human behavior continues to defy our most subtle models of it, and the algorithms producing our behavior remained hidden in a black box.</p>\n<p>But now, neuroscientists are directly measuring the neurons whose firing rates encode value and produce our choices. We know a lot more about the neuroscience of human motivation than you might think. Now we can peer directly into the black box of human motivation, and begin (dimly) to read our own source code.</p>\n<p>The neuroscience of human motivation has implications for philosophy of mind and action, for <a href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">scientific self-help</a>, and for <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">metaethics</a> and <a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">Friendly AI</a>. (We <a href=\"/lw/5sk/inferring_our_desires/\">don't really know what we want</a>, and looking directly at the algorithms that produce human wanting might help in solving this mystery.)</p>\n<p>So, I wrote a crash course in the neuroscience of human motivation.</p>\n<p>The purpose of this document is not to <em>argue</em>&nbsp;for any of the conclusions presented within it. That would require not a long blog post but instead a couple 500-page books &mdash; say, <em><a href=\"http://www.amazon.com/Foundations-Neuroeconomic-Analysis-Paul-Glimcher/dp/0199744254/\">Foundations of Neuroeconomic Analysis</a></em>&nbsp;and <em><a href=\"http://www.amazon.com/Handbook-Reward-Decision-Making-Jean-Claude/dp/0123746205/\">Handbook of Reward and Decision Making</a></em>&nbsp;(my two greatest sources for this post).<sup>4</sup></p>\n<p>Instead, I merely want to summarize the current mainstream scientific picture on the neuroscience of human motivation, explain some of the concepts it uses, and tell a few stories about how our current picture of human motivation developed.</p>\n<p>As you read this, I hope that many questions and objections will come to mind, because it's <em>not</em>&nbsp;the full story. That's why I went to the trouble of linking to PDFs of almost all my sources (see <a href=\"#References\">References</a>): so you can check the full data and the full arguments yourself if you like.</p>\n<p>This document is long. You may prefer to read it in sections.</p>\n<p>&nbsp;</p>\n<h4>Contents:</h4>\n<ol>\n<li><a href=\"#FolkPsychology\">Folk Psychology</a></li>\n<li><a href=\"#NeoclassicalEconomics\">Neoclassical Economics</a></li>\n<li><a href=\"#BehaviorismAnd\">Behaviorism and Reinforcement Learning</a></li>\n<li><a href=\"#ReinforcementLearning\">Reinforcement Learning and Decision Theory</a></li>\n<li><a href=\"#TheTurn\">The Turn to the Brain</a></li>\n<li><a href=\"#HebbianLearning\">Hebbian Learning</a></li>\n<li><a href=\"#ExpectedUtility\">Expected Utility in Neurons</a></li>\n<li><a href=\"#RealTime\">Real-Time Updates to Expected Utility</a></li>\n<li><a href=\"#ArgmaxAnd\">Argmax and Reservation Price</a></li>\n<li><a href=\"#RandomUtility\">Random Utility</a></li>\n<li><a href=\"#Discounting\">Discounting</a></li>\n<li><a href=\"#RelativeAnd\">Relative and Absolute Utility</a></li>\n<li><a href=\"#Normalization\">Normalization</a></li>\n<li><a href=\"#AreActions\">Are Actions Choices?</a></li>\n<li><a href=\"#ThePrimate\">The Primate Choice Mechanism: A Brief Review</a></li>\n<li><a href=\"#MarginalUtility\">Marginal Utility and Reference Dependence</a></li>\n<li><a href=\"#ValuationIn\">Valuation in the Brain</a></li>\n<li><a href=\"#SummaryAnd\">Summary and Research Directions</a></li>\n</ol> \n<ul>\n<li><a href=\"#Notes\">Notes</a></li>\n<li><a href=\"#References\">References</a></li>\n</ul>\n<p>&nbsp;</p>\n<h4><a name=\"FolkPsychology\"></a>Folk Psychology</h4>\n<p>There are these things called 'humans' on planet Earth. They undergo metabolism and cell growth. They produce waste. They maintain homeostasis. They reproduce. They move. They communicate. Sometimes they have <a href=\"http://i.imgur.com/WLYH1.jpg\">pillow fights</a>.</p>\n<p>Some of these human processes are 'automatic', like cell growth and breathing. Other processes are 'intentional' or 'willed', like moving and communicating and having pillow fights. We call these latter processes&nbsp;<em>intentional actions</em>, or simply <em>actions</em>. Sometimes we're not sure <a href=\"/lw/o0/where_to_draw_the_boundary/\">where to draw the line</a> between automatic processes and actions, but this should become clearer as we learn more. In the meantime, we ask...</p>\n<p>How can we <a href=\"http://yudkowsky.net/rational/technical\">explain</a>&nbsp;human actions?</p>\n<p>One popular explanation is '<a href=\"http://en.wikipedia.org/wiki/Folk_psychology\">folk psychology</a>.' Folk psychology posits that we humans have beliefs and desires, and that <em>we are motivated to do what we believe will fulfill our desires</em>.</p>\n<p>I desire to eat a cookie. I believe I can fulfill that desire if I walk to the kitchen and put one of the cookies there into my mouth. So I am motivated to walk to the kitchen and put a cookie in my mouth.</p>\n<p>Of course there are complications. For example I have multiple desires. Suppose I desire to eat a cookie and believe there are cookies in the kitchen. But I also desire to remain sitting comfortably in the living room. Can I satisfy <em>both</em>&nbsp;desires? I also believe that if I nicely ask my friend in the kitchen to bring me a cookie, she will. So I ask her to bring me a cookie and I begin to eat it, without having to leave the comfy living room sofa. We still explain my behavior with constructs like 'beliefs' and 'desires', but we consider more than one of each to do so.</p>\n<p>Most of us use folk psychology every day to successfully predict human behavior. I believe that my friend <em>desires</em>&nbsp;to do nice things for me on occasion if they're not too much trouble, and I believe that my friend, once I tell her I want a cookie, will&nbsp;<em>believe</em>&nbsp;she can be nice to me without much trouble if she brings me a cookie from the kitchen. So, I predict that my friend will bring me a cookie when I ask her. So I ask her, and <em>behold</em>!&nbsp;My prediction was correct. I am happily eating a cookie on the sofa.</p>\n<p>But folk psychology (FP) faces some problems.<sup>5</sup> Consider its context in history:</p>\n<blockquote>\n<p>The presumed domain of FP used to be much larger than it is now. In primitive cultures, the behavior of most of the elements of nature were understood in intentional terms. The wind could know anger, the moon jealousy, the river generosity&hellip; These were not metaphors&hellip; the animistic approach to nature has dominated our history, and it is only in the last two or three thousand years that we have restricted FP&rsquo;s literal application to the domain of the higher animals.</p>\n<p>[Even still,] the FP of the Greeks is essentially the FP we uses today&hellip; This is a very long period of stagnation and infertility for any theory to display, especially when faced with such an enormous backlog of anomalies and mysteries in its own explanatory domain&hellip; To use Imre Lakatos&rsquo; terms, FP is a stagnant or degenerating research program, and has been for millennia.</p>\n</blockquote>\n<p>Consider also its prospects for inter-theoretic reduction:</p>\n<blockquote>\n<p>If we approach homo sapiens from the perspective of natural history and the physical sciences, we can tell a coherent story of its constitution, development, and behavioral capacities which encompasses particle physics, atomic and molecular theory, organic chemistry, evolutionary theory, biology, physiology, and materialistic neuroscience. The story, though still radically incomplete, is already extremely powerful, outperforming FP at many points even in its own domain. And it is deliberately&hellip; coherent with the rest of our developing world picture. In short, the greatest theoretical synthesis in [history] is currently in our hands&hellip;</p>\n<p>But FP is no part of this growing synthesis. Its intentional categories stand magnificently alone, without visible prospect of reduction to that larger corpus. A successful reduction cannot be ruled out, in my view, but FP&rsquo;s explanatory impotence and long stagnation inspire little faith that its categories will find themselves neatly reflected in the framework of neuroscience. On the contrary, one is reminded of how alchemy must have looked as elemental chemistry was taking form, how Aristotelean cosmology must have looked as classical mechanics was being articulated, or how the vitalist conception of life must have looked as organic chemistry marched forward.</p>\n</blockquote>\n<p>Finally, consider the problem of <em>habit</em>. I sit at my computer and want to type my name, 'Luke.' However, I have just used a special program to switch the function of the keys labeled L and P so that they will input the <em>other</em>&nbsp;character instead (so that I can play a prank on my friend, who will be using my computer shortly). I <em>believe</em>&nbsp;that typing the key labeled L will input P instead, but nevertheless when I type my name my fingers fall into their familiar habit and I end up typing my name as 'Puke.' My act of typing was intentional, and yet I <em>didn't</em> do what I believed would fulfill my desire to type my name.</p>\n<p>Folk psychology faces both successes and failures in explaining human action. Hopefully we can do better.</p>\n<p>&nbsp;</p>\n<h4><a name=\"NeoclassicalEconomics\"></a>Neoclassical Economics</h4>\n<p>Folk psychology was updated and quantified by <a href=\"http://www.econlib.org/library/Enc1/NeoclassicalEconomics.html\">neoclassical economics</a>. To summarize:</p>\n<blockquote>\n<p>One [assumption of] neoclassical economics is \"rationality,\" in which individuals are said to choose alternatives that maximize expected utilities. In particular, the neoclassical view is that individuals rank all possible alternatives according to how much satisfaction they will bring and then choose the alternative that [they expect] will bring the most satisfaction or utility...<sup>6</sup></p>\n</blockquote>\n<p>Let's review this notion of maximizing expected utility. Suppose I can choose one of two boxes sitting before me, red and blue. There is a 10% chance the red box contains a million dollars, and a 90% chance it contains nothing. As for the blue box, I am certain it contains $10,000. The 'expected value' of choosing the red box is (0.1 &times; $1,000,000) + (0.9 &times; $0), which is equal to $100,000. The expected value of choosing the blue box is !1&nbsp;&times; $10,000), or $10,000. An agent that chose whatever had the highest expected value would choose the red box, which has 10 times the expected value of the blue box ($100,000 vs. $10,000).</p>\n<p>But humans don't value things only according to their dollar value. A million dollars might have 10 times the <em>objective</em>&nbsp;value&nbsp;of $100,000, but it might have less than 10 times the <em>subjective</em>&nbsp;value&nbsp;of $100,000 because after $100,000 you only care a little how much <em>more</em>&nbsp;wealthy you are.</p>\n<p>Or, you might be <a href=\"http://en.wikipedia.org/wiki/Risk_aversion\">risk averse</a>. You might prefer a sure thing to something that is uncertain. So a 10% chance of a million dollars might be worth <em>less</em> &mdash; in subjective value &mdash; than a 100% chance of $10,000. If you are risk averse you might choose the blue box because it has higher expected <em>subjective</em> value even though it has lower expected <em>objective</em> value.</p>\n<p>We call objective value simply 'value'. We call subjective value 'utility.'</p>\n<p>Neoclassical economics quantifies folk psychology by measuring the strength of belief with <em>probability</em> and by measuring the strength of desire with <em>utility</em>. It then says that humans act so as to maximize <em>expected utility</em>, a measure that combines the utility of particular thing with your subjective probability of getting it.<sup>7</sup></p>\n<p>This neoclassical model of human behavior has faced many challenges, and is regularly revised in the face of new evidence.<sup>8</sup> For example, Loewenstein (1987) found that if students were asked to place a value on the opportunity to kiss a celebrity of their choice 1-5 days in the future, they placed the highest value on a kiss in 3 days. This didn't fit any existing neoclassical models of utility, but was explained in 2001 when Caplin &amp; Leahy (2001) incorporated \"anticipatory feeling\" into the neoclassical model, explaining that the students got some utility from&nbsp;<em>anticipating</em>&nbsp;the kiss with the celebrity (but also, as usual, discounted the utility of a reward the further away it was in the future), and this is why they didn't want the kiss right away.</p>\n<p>Keep in mind that economists don't argue that we <em>actually</em>&nbsp;compute the expected utility of each option before us and then choose the best one, but that we always act \"as if\" we were doing that.<sup>9</sup></p>\n<p>But sometimes we don't even act \"as if\" we are obeying the axioms of neoclassical economics. For example, the <a name=\"independence\"></a>independence axiom of expected utility theory <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/4o7t\">says</a> that if you prefer an apple over an orange, then you must prefer the Gamble A (72% chance you get an apple, otherwise you get a cat) over the Gamble B (72% chance you get an orange, otherwise you get a cat). But Allais (1953) found that subjects <em>do</em>&nbsp;violate this basic assumption under some conditions.</p>\n<p>Such violations of the basic axioms of neoclassical economics led to the development of <a href=\"http://en.wikipedia.org/wiki/Behavioral_economics\">behavioral economics</a>&nbsp;and theories like Kahneman and Tversky's (1979) <a href=\"/lw/6kf/prospect_theory_a_framework_for_understanding/\">prospect theory</a>,<sup>10</sup>&nbsp;which transcends some assumptions of the neoclassical model. But these new theories don't fit the data perfectly, either.<sup>11</sup></p>\n<p>The models of human motivation we've surveyed so far are conceptually related to <a href=\"http://en.wikipedia.org/wiki/Decision_theory\">decision theory</a>&nbsp;(beliefs and desires, or probabilities and utilities), so I'll call them 'decision-theoretic models' of human motivation. We'll discuss decision-theoretic models again when we finally get to the topic of neuroscience, but for now I want to discuss a different approach to motivation.</p>\n<p>&nbsp;</p>\n<h4><a name=\"BehaviorismAnd\"></a>Behaviorism and Reinforcement Learning</h4>\n<p>While neoclassical economists formulated expected utility theory, behaviorist psychologists developed a different set of explanations for human action. Though behaviorists were <a href=\"/lw/sr/the_comedy_of_behaviorism/\">wrong</a>&nbsp;when they said that science can't talk about mental activity or mental states, you <a href=\"/lw/6i5/behaviorism_beware_anthropomorphizing_humans/\">can charitably think of</a> behaviorists as playing a game of <a href=\"/lw/nu/taboo_your_words/\">Rationalist's Taboo</a> with constructs of folk psychology like \"want\" or \"fear\" in order to get at phenomena more appropriate for quantification in&nbsp;<a href=\"http://yudkowsky.net/rational/technical\">technical explanation</a>. Also, the behaviorist approach led to 'reinforcement learning', an important concept in the neuroscience of human motivation.</p>\n<p>Before I explain reinforcement learning, let's recall&nbsp;<a href=\"/lw/6iu/basics_of_animal_reinforcement/\">operant conditioning</a>:</p>\n<blockquote>\n<p>Stick a pigeon in a box with a lever and some associated machinery (a \"Skinner box\"). The pigeon wanders around, does various things, and eventually hits the lever. Delicious sugar water squirts out. The pigeon continues wandering about and eventually hits the lever again. Another squirt of delicious sugar water. Eventually it percolates into its tiny pigeon brain that maybe pushing this lever makes sugar water squirt out. It starts pushing the lever more and more, each push continuing to convince it that yes, this is a good idea.</p>\n<p>Consider a second, less lucky pigeon. It, too, wanders about in a box and eventually finds a lever. It pushes the lever and gets an electric shock. Eh, maybe it was a fluke. It pushes the lever again and gets another electric shock. It starts thinking \"Maybe I should stop pressing that lever.\" The pigeon continues wandering about the box doing anything and everything other than pushing the shock lever.</p>\n<p>The basic concept of operant conditioning is that an animal will repeat behaviors that give it reward, but avoid behaviors that give it punishment.</p>\n</blockquote>\n<p>Behaviorism <a href=\"http://commonsenseatheism.com/?p=13607#history\">died</a> in the wake of cognitive psychology, but its approach to&nbsp;motivation turned out to be very useful in the field of artificial intelligence, where it is called <a href=\"http://www.scholarpedia.org/article/Reinforcement_learning\">reinforcement learning</a>:</p>\n<blockquote>\n<p>Reinforcement learning is learning what to do &mdash; how to map situations to actions &mdash; so as to maximize a numerical reward signal. The learner is not told which actions to take, as in most forms of machine learning, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward, but also the next situation and, through that, all subsequent rewards. These two characteristics &mdash; trial-and-error search and delayed reward &mdash; are the two most important distinguishing features of reinforcement learning.</p>\n<p>To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has tried in the past and found to&nbsp;be effective in producing reward. But to discover such actions it has to try actions that it has not selected before. The agent has to <em>exploit</em> what it already knows in order to obtain reward, but it also has to <em>explore</em> in order to make better action selections in the future. The dilemma is that neither exploitation nor exploration can be pursued exclusively without failing at the task. The agent must try a variety of actions and progressively favor those that appear to be best..<sup>12</sup></p>\n</blockquote>\n<p>In addition to the agent and its environment, there are four major components of a reinforcement learning system:</p>\n<blockquote>\n<p>...a <em>policy</em>, a <em>reward function</em>, a <em>value function</em>, and, optionally, a <em>model</em> of the environment.</p>\n<p>A <em>policy</em> defines the learning agent's way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states...</p>\n<p>A <em>reward function</em> defines the goal in a reinforcement learning problem. Roughly speaking, it maps perceived states (or state-action pairs) of the environment to a single number, a reward, indicating the intrinsic desirability of the state. A reinforcement-learning agent's sole objective is to maximize the total reward it receives in the long run. ...[A reward function may] be used as a basis for changing the policy. For example, if an action selected by the policy is followed by low reward, then the policy may be changed to select some other action in that situation in the future...</p>\n<p>Whereas a reward function indicates what is good in an immediate sense, a <em>value function</em> specifies what is good in the long run. Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future starting from that state. Whereas rewards determine the immediate, intrinsic desirability of environmental states, values indicate the long-term desirability of states after taking into account the states that are likely to follow, and the rewards available in those states. For example, a&nbsp;state might always yield a low immediate reward, but still have a high value because it is regularly followed by other states that yield high rewards. Or the reverse could be true...</p>\n<p>Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary. Without rewards there could be no values, and the only purpose of estimating values is to achieve more reward. Nevertheless, it is values with which we are most concerned when making and evaluating decisions. Action choices are made on the basis of value judgments. We seek actions that bring about states of highest value, not highest reward, because these actions obtain for us the greatest amount of reward over the long run...</p>\n<p>...The fourth and final element of some reinforcement learning systems is a <em>model</em> of the environment. This is something that mimics the behavior of the environment. For example, given a state and action, the model might predict the resultant next state and next reward. Models are used for planning, by which we mean any way of deciding on a course of action by considering possible future situations before they are actually experienced.</p>\n</blockquote>\n<p>Want an example? Here is how a reinforcement learning agent would learn to play <a href=\"http://en.wikipedia.org/wiki/Tic-tac-toe\">Tic-Tac-Toe</a>:</p>\n<blockquote>\n<p>First we set up a table of numbers, one for each possible state of the game. Each number will be the latest estimate of the probability of our winning from that state. We treat this estimate as the state's current value, and the whole table is the learned value function. State A has higher value than state B, or is considered 'better' than state B, if the current estimate of the probability of our winning from A is higher than it is from B. Assuming we always play Xs, then for all states with three Xs in a row the probability of winning is 1, because we have already won. Similarly, for all states with three &Oslash;s in a row... the correct probability is 0, as we cannot win from them. We set the initial values of all the other states, the <em>nonterminals</em>, to 0.5, representing an informed guess that we have a 50% chance of winning.</p>\n<p>Now we play many games against the opponent. To select our moves we examine the states that would result from each of our possible moves (one for each blank space on the board) and look up their current values in the table. Most of the time we move <em>greedily</em>, selecting the move that leads to the state with greatest value, that is, with the highest estimated probability of winning. Occasionally, however, we select randomly from one of the other moves instead; these are called <em>exploratory</em>&nbsp;moves because they cause us to experience states that we might otherwise never see.</p>\n</blockquote>\n<p>A sequence of Tic-Tac-Toe moves might look like this:<sup>13</sup></p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/tic-tac-toe.png\" alt=\"\" /></p>\n<p>Solid lines are the moves our reinforcement learning agent made, and dotted lines are moves it considered but did <em>not</em>&nbsp;make.&nbsp;The second move was an exploratory move: it was taken even though another sibling move, that leading to e*, was ranked higher.</p>\n<p>While playing, the agent changes the values assigned to the states it finds itself in. To improve its estimates concerning the probability of winning from various states, it 'backs up' the value of state after each greedy move to the state <em>before</em>&nbsp;the move (as suggested by the arrows.) What this means is that the value of the earlier state is adjusted to be closer to the value of the later state.</p>\n<blockquote>\n<p>If we let s denote the state before the greedy move, and s' the state after, then the update to the estimated value of s, denoted V(s), can be written:</p>\n<p>V(s) &lt;- V(s) + &alpha;[V(s') - V(s)]</p>\n<p>where &alpha; is a small positive fraction called the <em>step-size parameter</em>, which influences the rate of learning. The update rule is an example of a <em>temporal difference</em>&nbsp;learning method, so called because its changes are based on a difference... between [value] estimates at two different times.</p>\n<p>...if the step-size parameter is reduced properly over time, this method converges, for any [unchanging] opponent, to the true probabilities of winning from each state give optimal play by the agent.</p>\n</blockquote>\n<p>And that's how a simple version of temporal difference (TD) reinforcement learning works.</p>\n<p>&nbsp;</p>\n<h4><a name=\"ReinforcementLearning\"></a>Reinforcement Learning and Decision Theory</h4>\n<p>You may have noticed a key advantage of reinforcement learning: an agent using it can be 'dumber' than a decision-theoretic agent. It can just start with guesses (\"What the hell; let's try 50%!\") for the value of various states, and then it learns their <em>true</em> values by running through <em>many, many&nbsp;trials</em>.</p>\n<p>But what if you don't <em>have</em>&nbsp;many trials to run through, and you need to make an important decision right now?</p>\n<p>Then you have to be smart. You need to have a good model of the world and use decision theory to choose the action with the highest expected utility.</p>\n<p><span>This is precisely what rationality&nbsp;</span>&mdash;<span>&nbsp;being <em>good</em>&nbsp;at building correct models of the world&nbsp;</span>&mdash;<span>&nbsp;<a href=\"/lw/34a/goals_for_which_less_wrong_does_and_doesnt_help/\">is especially good for</a>:</span></p>\n<blockquote>\n<p>For some tasks, the world provides rich, inexpensive empirical feedback. In these tasks you hardly need reasoning. &nbsp;Just try the task many ways... and take care to notice what is and isn&rsquo;t giving you results.</p>\n<p>Thus, if you want to learn to sculpt, [studying rationality] is a bad way to go about it. Better to find some clay and a hands-on sculpting course. The situation is similar for small talk, cooking, selling, programming, and many other useful skills.</p>\n<p>Unfortunately, most of us also have goals for which we can obtain no such ready success/failure data. For example, if you want to know whether cryonics is a good buy, you can&rsquo;t just try buying it and not-buying it and see which works better. &nbsp;If you miss your first bet, you&rsquo;re out for good.</p>\n</blockquote>\n<p>Reinforcement learning can be a good strategy if you have time to learn from many trials. If you've only got <em>one shot</em> at a problem, you'd better build up a really accurate model of the world first and then try to maximize expected utility.</p>\n<p>Now, back to our story.</p>\n<p>It turns out that reinforcement learning seems to underlie many of our mental processes. (More on this later.)</p>\n<p>The lesson Yvain <a href=\"/lw/6i5/behaviorism_beware_anthropomorphizing_humans/\">drew</a>&nbsp;from this discovery was:</p>\n<blockquote>\n<p>Reinforcement learning is <a href=\"/lw/l2/protein_reinforement_and_dna_onsequentialism/\">evolution writ small</a>; behaviors propagate or die out based on their consequences to reinforcement in a mind, just as mutations propagate or die out based on their consequences to reproduction in an organism. In the behaviorist model, our mind is not an agent, but a flourishing ecosystem of behaviors both physical and mental, all scrabbling for supremacy and mutating into more effective versions of themselves.</p>\n<p>Just as evolving organisms are <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">adaptation-executors and not fitness-maximizers</a>, so minds are behavior-executors and not utility-maximizers.</p>\n</blockquote>\n<p>But things are a bit more complicated than that, as we'll now see.</p>\n<p>&nbsp;</p>\n<h4><a name=\"TheTurn\"></a>The Turn to the Brain</h4>\n<blockquote>\n<p>I hesitate to say that men will ever have the means&nbsp;of measuring directly the feelings of the human&nbsp;heart. It is from the quantitative effects of the&nbsp;feelings that we must estimate their comparative&nbsp;amounts.</p>\n</blockquote>\n<p align=\"right\">William Jevons (1871)</p>\n<p>It turns out that Jevons was wrong. Modern neuroscience allows us to peer into the black box of the human value system and measure directly \"the feelings of the human heart.\"<sup>14</sup></p>\n<p>We'll begin with the experiments of <a href=\"http://www.pdn.cam.ac.uk/staff/schultz/\">Wolfram Shultz</a>. Schultz recorded the activity of single dopamine neurons in monkeys who sat in front of a water spout. At irregular intervals, a speaker played a tone and a drop of water dropped from the spout.<sup>15</sup>&nbsp;The monkeys' dopamine neurons normally fired at the baseline rate, but responded with a burst of activity when water was delivered. Over time, though, the neurons responded less and less to the water and more and more to the tone.</p>\n<p>But if Schultz delivered water without first giving the tone, then the dopamine neurons responded with a burst of activity again. And if he played the tone and <em>didn't</em>&nbsp;provide water, the neurons reduced their firing rates <em>below</em>&nbsp;the baseline. The neurons weren't responding to the water itself but to a difference between expected reward and actual reward &mdash; a reward prediction error (RPE).</p>\n<p>Two other researchers, <a href=\"http://www.hnl.bcm.tmc.edu/faculty.html\">Read Montague</a> and <a href=\"http://www.gatsby.ucl.ac.uk/~dayan/\">Peter Dayan</a>, noticed that these patterns of neuronal activity were exactly predicted by TD reinforcement learning theory from computer science.<sup>16</sup>&nbsp;In particular, the RPE observed in neurons appeared to play the same role in monkey learning as the difference between value estimates at two different times did in TD reinforcement learning theory.</p>\n<p>Since then, researchers have done many more single-neuron recording studies to test particular versions of TD reinforcement learning and revise the theory until it predicts more and more behavior while also predicting novel experimental discoveries.</p>\n<p>Caplin &amp; Dean<sup>17</sup> provided another way to test the hypothesis that dopamine neurons encoded RPE in a TD-class model. They showed that all existing RPE-models could be reduced to three axiomatic statements. If a system violated one of these axioms, it could not be an RPE system. Later, Caplin et al. (2010) tested the axioms on actual brain activity to see if they held up. They did. This is another reason why so many scientists working in this field believe the current 'dopamine hypothesis' &mdash; that dopamine neurons encode RPE in a TD-class reinforcement learning system in the brain.</p>\n<p>TD-class reinforcement learning works in computers by updating numbers that represent the values of states. How does reinforcement learning work when using nerve cells?</p>\n<p>&nbsp;</p>\n<h4><a name=\"HebbianLearning\"></a>Hebbian Learning</h4>\n<p>By <a href=\"http://en.wikipedia.org/wiki/Hebbian_theory\">Hebbian learning</a>, of course. \"Cells that fire together, wire together.\"</p>\n<p>Imagine a neural pathway (in one of Pavlov's dogs) that connects the neural circuits that sense the ringing of a bell to the neural circuits for salivation. This is a weak connection at first, which is why the bell doesn't initially elicit salivation.</p>\n<p>Also imagine a third neuron that connects the salivation circuit to a circuit that detects food. This is a strong connection, and that's why food <em>does</em>&nbsp;elicit salivation right away:<sup>18</sup></p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Hebbian-circuits.png\" alt=\"\" /></p>\n<p>Donald Hebb proposed:</p>\n<blockquote>\n<p>When an axon of cell A is near enough to excite cell B and repeatedly or persistently take part in firing it, a growth process of metabolic change takes place in one or both cells such that A's efficacy, as one of the cells firing B, is increased.<sup>19</sup></p>\n</blockquote>\n<p>In short, whenever two connected cells are active at the same time, the synapses connecting them are strengthened.</p>\n<p>Consider Pavlov's experiment. At first, the Bell cell will fire whenever bells ring, but probably not when the salivation cells happen to be active. So, the connection between the Bell cell and the Salivation cell remains weak. But then, Pavlov intervenes and causes the Bell cell and the Salivation cell to fire at the same time by ringing the bell and presenting food at the same time (the Food detector cell already has a strong connection to the Salivation cell). Whenever the Bell cell and the Salivation cell happen to fire at the same time, the synapse between them is strengthened. Once the connection is strong enough, the Bell cell can cause the Salivation cell to fire on its own, just like the Food detector cell can.</p>\n<p>It was a fine theory, but it wasn't observed until Bliss &amp; Lomo (1973) observed Hebb's mechanism at work in the rabbit hippocampus. Today, we know how some forms of Hebb's mechanism work at the molecular level.<sup>20</sup></p>\n<p>Later, Wickens (1993) proposed a similar mechanism called the <em>three-factor rule</em>, according to which some synapses are strengthened whenever presynaptic and postsynaptic activity occurred <em>in the presence of dopamine</em>. These same synapses might be weakened when activity occurred in the <em>absence</em>&nbsp;of dopamine. Later studies confirmed this hypothesis.<sup>21</sup></p>\n<p>Suppose a monkey receives an unexpected reward and encodes a large positive RPE. Glimcher explains:</p>\n<blockquote>\n<p>The TD model tells us that under these conditions we want to increment the value attributed to all actions or sensations that have just occurred. Under these conditions, we know that the dopamine neurons release dopamine throughout the frontocortical-basal ganglia loops, and do so in a highly homogenous manner. That means we can think of any neuron equipped with dopamine receptors as 'primed' for synaptic strengthening. When this happens, any segment of the frontocortical-basal ganglia loop that is already active will have its synapses strengthened.<sup>22</sup></p>\n</blockquote>\n<p>We will return to the dopamine system later, but for now let us back up and pursue the neoclassical economic path into the brain.</p>\n<p>&nbsp;</p>\n<h4><a name=\"ExpectedUtility\"></a>Expected Utility in Neurons</h4>\n<p>Ever since Friedman (1953), economists have insisted that humans only behave <em>as if</em>&nbsp;they are utility maximizers, not that they <em>actually</em>&nbsp;compute expected utility and try to maximize it.</p>\n<p>It was a surprise, then, when neuroscientists stumbled upon the neurons that were encoding expected utility in their firing rates.</p>\n<p>Tanji &amp; Evarts (1976) did their experiments with rhesus monkeys because they are our closest relative besides the apes, and this kind of work is usually forbidden on apes for ethical reasons (we need to implant a recording electrode in the brain).</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/rhesus-monkey.jpg\" alt=\"\" /></p>\n<p>The monkeys were trained to know that a colored light on the screen meant they would soon be offered a reward (a drop of water) either for pushing or pulling, but not for both. This was the &lsquo;ready&rsquo; cue. A second later, researchers gave a &lsquo;direction&rsquo; cue that told the monkeys which action &mdash; push or pull &mdash; was going to be rewarded. The third cue was the 'go' signal: if the monkey made the previously indicated movement, it was rewarded.</p>\n<p>This is what they saw:</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/push-pull-smaller.png\" alt=\"\" /></p>\n<p>At the &lsquo;ready&rsquo; cue, the neurons associated with a pushing motion became weakly active (but fired above the baseline rate), and so did the neurons associated with a pulling motion. When the &lsquo;direction&rsquo; cue was given, the neurons associated with the to-be-rewarded motion doubled their firing rate, and the neurons associated with the opposite motion fell back to the baseline rate. Then at the &lsquo;go&rsquo; cue, the neurons associated with the to-be-rewarded movement increased again rapidly, up past the threshhold required to produce movement, and the movement was produced shortly thereafter.&nbsp;</p>\n<p>One tempting explanation of the data is that after the &lsquo;ready&rsquo; cue, the monkey&rsquo;s brain 'decides' there&rsquo;s a 50% chance that pulling will get the reward, and a 50% chance that pushing will get the reward. That&rsquo;s why we see the neuron firing rates associated with those two actions each jump to slightly less than 50% of the movement threshold when the &lsquo;ready&rsquo; cue is given. But then, when the &lsquo;direction&rsquo; cue is given, those expectations shift to 100%/0% or 0%/100%, depending on which action is about to be rewarded according to the &lsquo;direction&rsquo; cue. That&rsquo;s why activity in the circuit associated with the to-be-rewarded action doubles and the other one drops to baseline. And then the &lsquo;go&rsquo; cue is delivered and firing rates blast past the movement threshold, and movement is produced.</p>\n<p>Let's jump ahead to Basso &amp; Wurtz (1997), who did a similar experiment except that they used voluntary eye movements (called &lsquo;saccades&rsquo;) instead of voluntary arm movements. And this time, they presented each monkey with one, two, four, or eight possible targets, instead of just two targets (push and pull) like Tanji &amp; Evarts did.</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/monkey-saccade.png\" alt=\"\" /></p>\n<p>What they found was that as more potential targets were presented, the magnitude of the preparatory activity associated with each target systematically decreased. And again, once the &lsquo;direction&rsquo; and &lsquo;go&rsquo; cues were presented, the activity associated with those other potential targets dropped rapidly and activity burst rapidly in neurons associated with the to-be-rewarded movement. It was as though the monkeys&rsquo; brains were distributing their probability mass evenly across the potentially rewarded actions, and then once they knew which action should in fact be rewarded, they moved all their probability mass to that action and performed the action and got the reward.</p>\n<p>&nbsp;</p>\n<h4><a name=\"RealTime\"></a>Real-Time Expected Utility Updates</h4>\n<p>Other researchers showed monkeys a black screen with flickering white dots on it. In each frame of the video, the computer moved each dot in a random direction. The independent variable was a measure called 'coherence.' In a 100% leftward coherence condition, all dots moved to the left. In a 60% rightward condition, 60% of the dots move rightward while the rest moved randomly. And so on.</p>\n<p>In a typical experiment, the researchers would identify a neuron in a monkey's brain that increased its firing rate in response to rightward coherence of the dots, and decreased its firing rate in response to leftward coherent of the dots. Then they would present the monkey with a sequence (in random order) of every possible leftward and rightward coherence condition.</p>\n<p>A leftward coherence (of any magnitude) meant the monkey would be rewarded for leftward eye movement, and a rightward coherence meant the monkey would be rewarded for rightward eye movement. But, the monkey had to wait two seconds before being rewarded.</p>\n<p>In this experiment, the probabilities always started at 50% but then updated continuously. A 100% rightward coherence condition allowed the monkey to very quickly know which voluntary eye movement would be rewarded, but in a 5% rightward coherence condition the expected utility of the rightward target grew more slowly.</p>\n<p>The results? The greater the coherence of rightward motion of the dots, the faster the neurons associated with rightward eye movement increased their firing rate. (A higher coherence meant the monkey was able to update its probabilities more quickly.)</p>\n<p>&nbsp;</p>\n<h4><a name=\"ArgmaxAnd\"></a>Argmax and Reservation Price</h4>\n<p>Many studies show that the brain controls movement by way of a 'winner take all' mechanism that is isomorphic to the <a href=\"http://en.wikipedia.org/wiki/Arg_max\">argmax</a> operation from economics.<sup>23</sup> That is, there are many possibilities competing for your final choice, but just before your choice the single strongest signal remains after all the others are inhibited.</p>\n<p>This <em>choice mechanism</em>&nbsp;was investigated in more detail by <a href=\"http://depts.washington.edu/pbiopage/people_fac_page.php?fac_ID=28\">Michael Shadlen</a> and others.<sup>24</sup>&nbsp;Shadlen gave monkeys the same eye movement task as above, except that the monkeys could make their choice at any time instead of waiting for two seconds. He found that:</p>\n<ol>\n<li>When the direction of the dots is unambiguous, monkeys make their choices quickly.</li>\n<li>As the direction of the dots becomes more ambiguous, they take longer to make their choices.</li>\n<li>Throughout the experiment, the firing rates of neurons in the LIP (part of the 'final common path' for generating eye movement) grew toward a specific threshold level.</li>\n</ol>\n<p>The threshold level acts as a kind of criterion of choice. Once the criterion is met, action is taken. Or in economic terms, the monkeys seemed to set a <em>reservation price</em> on making certain movements.<sup>25</sup></p>\n<p>&nbsp;</p>\n<h4><a name=\"RandomUtility\"></a>Random Utility</h4>\n<p>When deciding between goods of different expected utilities, humans exhibit a <em>stochastic transfer function</em>:</p>\n<blockquote>\n<p>Consider a human subject choosing between two objects of highly different expected utilities, such as a first lottery with a 50% chance of winning $5 and a second lottery with a 25% chance of winning $5. We observe highly deterministic behavior under these conditions: basically all subjects always choose the 5o% chance of winning $5. But what happens when we increment the value of the 25% lottery? As the amount one stands to win from that lottery is incremented, individual subjects eventually switch their preference. Exactly when they make that switch depends on their idiosyncratic degree of risk aversion. What is most interesting about this behavior for these purposes, though, is that actual human subjects, when presented with this kind of choice repeatedly, are never completely deterministic. As the value of the 25% lottery increases, they begin to show probabilistic behavior &mdash; selecting the 25% lottery sometimes, but not always.<sup>26</sup></p>\n</blockquote>\n<p>Our behavior has an element of randomness in it. <a href=\"http://elsa.berkeley.edu/~mcfadden/\">Daniel McFadden</a> won a Nobel Prize in economics for capturing such behavior using a <em>random utility model</em>.<sup>27</sup> The way he did it is to suppose that when a chooser asks himself what a thing is worth, he doesn't get a fixed answer but a variable one. That is, there is actual variation <em>in his preferences</em>. Thus, his expected utility for a particular lottery is drawn from a <em>distribution</em> of possible utilities, usually one with a Gaussian variance.<sup>28</sup></p>\n<p>This behavior makes sense when we think about the human choice mechanism at the neuronal level, because neuron firing rates are&nbsp;stochastic.<sup>29</sup> When a neurobiologist says \"The neuron was firing at 200 Hz,\" what she means is that the mean firing rate of the neuron over a long time and stable conditions would have been close to 200 Hz. So the neurons that encode utility (wherever they are) will exhibit stochasticity, and thereby introduce some randomness into our choices. In this way, neurobiological data <em>constrains</em>&nbsp;our economic models of human behavior. An economic model without some randomness in it will have difficulty capturing human choices for as long as humans run on neurons.<sup>30</sup></p>\n<p>&nbsp;</p>\n<h4><a name=\"Discounting\"></a>Discounting</h4>\n<p>Louie &amp; Glimcher (2010) examined temporal discounting in the brain. The two monkeys in this study were repeatedly asked to choose between a small, immediately available reward and a larger reward available after a small delay. For example, on one day they were asked to choose between 0.13 millileters of juice right now, or else 0.2 millileters of juice available after a delay of 2, 4, 8, or 12 seconds. A monkey might be willing to wait 2, 4, or 8 seconds for the larger reward, but not 12 seconds.</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/monkey-discounting-smaller.png\" alt=\"\" /></p>\n<p>After many, many measurements of this kind, Louie and Glimcher were able to describe the discounting function being used by each monkey. (One of them was more impatient than the other.)</p>\n<p>Moreover, the neurons in the relevant section of the brain fired at rates that reflected each monkey&rsquo;s discounting function. If 0.2 millileters of juice was offered with no delay, the neurons were highly active. If the same reward was offered at a delay of 2 seconds, they were slightly less active. If the same reward was offered after 4 seconds, the neurons were less active still. And so on. As it turned out, the discounting function that captured their choices was identical to the discounting function that captured the firing rates of these neurons.</p>\n<p>This shouldn't be a surprise at this point, but just to confirm: Yes, we can observe discounting in the firing rates of neurons involved in the choice-making process.</p>\n<p>&nbsp;</p>\n<h4><a name=\"RelativeAnd\"></a>Relative and Absolute Utility</h4>\n<p>Dorris &amp; Glimcher (2004) observed monkeys and their choice mechanism neurons while the monkeys engaged in repeated plays of the <a href=\"http://www.maths.lse.ac.uk/Personal/stengel/TEXTE/insp.pdf\">inspection game</a>. The study is too involved for me to explain here, but the results suggested that choice mechanism neurons encode <em>relative</em>&nbsp;expected utilities (relative to other actions under consideration) rather than <em>absolute</em>&nbsp;expected utilities.</p>\n<p>Tobler et al. (2005) suggested that the brain <em>only</em>&nbsp;encodes relative expected utilities. But there is reason to suspect this can't be right. If we stored only relative expected utilities, then we would routinely violate the axiom of transitivity (if you prefer A to B and B to C, you can't also prefer C to A). To see why this is the case, consider Glimcher's example (he says 'expected value' instead of 'utility'):</p>\n<blockquote>\n<p>...consider a subject trained to choose between objects A and B, where A is $1,000,000 worth of goods and B is $100,000 worth of goods... A system that represented only the relative expected subjective value of A and B would represent SV(A) &gt; SV(B). Next, consider training the same subject to choose between C and D, where C is $1,000 worth of goods and D is $100 worth of goods. Such a system would represent SV(C) &gt; SV(D). What happens when we ask a chooser to select between B and C? For a chooser who represents only relative expected subjective value, the choice should be C: she should pick $1,000 worth of goods over $100,000 worth of goods because it has a higher learned relative expected subjective value. In order for our chooser to... construct transitive preferences across choice sets (and to obey the continuity axiom)... it is required that somewhere in the brain she represent the absolute subjective values of her choices.<sup>31</sup></p>\n</blockquote>\n<p>And we mostly&nbsp;<em>do</em>&nbsp;seem to obey the axiom of transitivity.</p>\n<p>So if the choice mechanism neurons <em>do</em>&nbsp;represent relative utilities, then some other neurons elsewhere must encode a more absolute form of utility. Other implications of this are explored in the next section.</p>\n<p>&nbsp;</p>\n<h4><a name=\"Normalization\"></a>Normalization</h4>\n<p><a href=\"http://www.cns.nyu.edu/~david/\">David Heeger</a> showed<sup>32</sup> that the firing rates of 'feature detector' neurons in the visual cortex captured a response to a feature in the visual field divided by the sum of the activation rates of nearby neurons sensitive to the same image. Thus, these neurons encode not only whether they 'see' the feature they are built to detect, but also how unique it is in the visual field.</p>\n<p>The effect of this is that neurons reacting to the <em>edge</em>&nbsp;of a visual object fire more actively than others do. Behold! Edge detection!</p>\n<p>It's also an efficient way to encode information about the world. Consider a world where orange dots are ubiquitous. For an animal in that world, it would be wasteful to fire action potentials to represent orange dots. Better to represent the <em>absence</em>&nbsp;of orange dots, or the transition from orange dots to something else. An optimally efficient encoding method would be sensitive not to the 'alphabet' of <em>all possible inputs</em>, but to a smaller alphabet of the inputs that <em>actually</em>&nbsp;appear in the world. This insight was mathematically formalized by Schwartz &amp; Simoncelli (2001).</p>\n<p>The efficiency of this&nbsp;<em>normalization</em>&nbsp;technique may explain why we've discovered it at work in so many different places in the brain.<sup>33</sup> And given that we've found it almost everywhere we've looked for it, it wouldn't be a surprise to see it show up in our choice-making circuits. Indeed, Simoncelli &amp; Schwartz's normalization equation may be what our brains use to encode expected utilities that are relative to the other choices under consideration.</p>\n<p>One implication of their equation is that a chooser's errors become more frequent as the size of the choice set grows. Thus, behavioral errors on small choice sets should be rarer than might be predicted by most random utility models, but error rates will increase rapidly with choice set size (and beyond a certain choice set size, choices will appear random).</p>\n<p>Preliminary evidence that choice set size effects error rates has arrived from behavioral economics. For example, consider Iyengar &amp; Lepper's (2000) study of supermarket shoppers. They set up a table showing either 6 or 24 flavors of jams, allowing shoppers to sample as many as they wanted. Customers who saw 24 flavors had a 3% chance of buying a jar, while those who saw only 6 flavors had a 30% chance!</p>\n<p>In another experiment, Iyengar &amp; Lepper let subjects choose one of either 6 or 30 different chocolates. Those who chose from among only 6 options were more satisfied with their selection than those who had been presented with 30 different chocolates.</p>\n<p>These data fit our expectation that as the choice set grows, the frequency of errors in our behavior rises and the likelihood that an option will rise above the threshold for purchase drops. When Louie &amp; Glimcher (2010) investigated this phenomena in monkey choice mechanism neurons, they found it at work there, too. But the process of choice-set editing is still poorly understood, and some recent studies have failed to replicate Iyengar &amp; Lepper's results (Scheibehenne et al. 2010).</p>\n<p>Perhaps the most surprising implication of these findings is that because of neuronal stochasticity, and because errors increase as the choice set grows, we should expect <em>stochastic violations of the <a href=\"#independence\">independence axiom</a></em>, and that <em>when choosers face very large choice sets they will essentially ignore the independence axiom</em>.</p>\n<p>This is a prediction about human behavior not made by earlier models from neoclassical economics, but it is suggested by looking at the neurons involved in human choice-making.</p>\n<p>&nbsp;</p>\n<h4><a name=\"AreActions\"></a>Are Actions Choices?</h4>\n<p>But all these data come from experiments where the choices are <em>actions</em>, and from our knowledge of the brain's \"final common path\" for producing actions. How do actions map on to choices about lovers and smartphones?</p>\n<p>Studies by <a href=\"http://faculty.washington.edu/ghorwitz/wordpress/\">Greg Horowitz</a> have provided some relevant data, because monkeys had to choose options identified by color rather than by action.<sup>34</sup>&nbsp;For example in one trial, a 'red' option might offer one reward and a 'green' option might offer a different reward. On each trial, the red and green options would appear at random places on the computer screen, and the monkey could choose a reward with a voluntary eye movement. The key here is that rewards were chosen by color and not by a (particular) action.</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/monkey-decide-red-or-green.png\" alt=\"\" /></p>\n<p>Horowitz found that the choice mechanism neurons showed the same pattern of activation under these conditions as was the case under action-based choice tasks.</p>\n<p>So, it looks like the valuation circuits can store the value of a colored target, and these valuations can be mapped to the choice mechanism. But we don't know much about how this works, yet.</p>\n<p>&nbsp;</p>\n<h4><a name=\"ThePrimate\"></a>The Primate Choice Mechanism: A Brief Review</h4>\n<p>Thus far, we have mostly discussed the primate brain's choice mechanism. To review:</p>\n<ol>\n<li>The choice circuit resides in the final common pathway for action.</li>\n<li>It takes as its input a signal that encodes stochastic expected utility, a concept aligned to the random utility term in economic models proposed by McFadden (2005) and Gul &amp; Pesendorfer (2006).</li>\n<li>This input signal is represented by a <em>normalized</em> firing rate (with Poisson variance, like all neurons).</li>\n<li>As the choice set size grows, so does the error rate.</li>\n<li>Final choice is implemented by an argmax function or a reservation price mechanism. (A single circuit can achieve both modes.<sup>35</sup>)</li>\n</ol>\n<p>But how are probability and utility calculated such that they can be fed into the expected utility representations of the choice mechanism? I won't discuss how the brain forms probabilistic beliefs in this article,<sup>36</sup> so let us turn to the study of how utility is calculated in the brain: the question of <em>valuation</em>.</p>\n<p>&nbsp;</p>\n<h4><a name=\"MarginalUtility\"></a>Marginal Utility and Reference Dependence</h4>\n<p>Consider the following story:</p>\n<blockquote>\n<p>Imagine an animal exploring a novel environment from a nest on a day when both (1) its blood concentration is dilute (and thus its need for water is low) and (2) its blood sugar level is low (and thus its need for food is high). The animal travels west one kilometer from the nest and emerges from the undergrowth into an open clearing at the shores of a large lake. Not very thirsty,&nbsp;the animal bends down to sample the water and finds it... unpalatable... the next day the same animal leaves its nest in the same metabolic state and travels one kilometer to the east, where it discovers a grove of trees that yield a dry but nutritious fruit, a grove of dried apricot trees. It samples the fruit and finds it sweet and highly palatable.</p>\n<p>What has the animal actually learned about the value of going west and the value of going east? It has had a weakly negative experience, in the psychological sense, when going west and a very positive experience when going east. Do these subjective properties of its experience influence what it has learned? Do the stored representations derived from these experiences encode the actual objective values of going west and east, or do they encode the subjective experiences? That is a critical question about what the animal has learned, because it determines what it does when it wakes up thirsty. When it wakes up thirsty it should, in a normative sense, go west towards the... lake, despite the fact that its previous visit west was a negative experience.<sup>37</sup></p>\n</blockquote>\n<p>Economists have known this problem for a long time, and solved it with an idea called&nbsp;<em>marginal utility</em>.</p>\n<p>In neoclassical economics, we view the animal as having two kinds of 'wealth': a sugar wealth and a water wealth (the total store of sugar and water in the animal's body at a given time). A piece of fruit or a sip of water is an <em>increment</em>&nbsp;in the animal's total sugar or water wealth. The utility of a piece of fruit or a sip of water, then, depends on its current levels of sugar and water wealth.</p>\n<p>On day one, the animal's need for water is low and its need for sugar is high. On that day, the marginal utility of a piece of fruit is greater than the marginal utility of a sip of water. But suppose during the next week the animal has a high blood sugar level. At that time, the marginal utility of a piece of fruit is low. Thus, the marginal utility of a consumable resource depends on wealth. The wealthier the chooser, the lower the marginal utility provided by a fixed amount of gain ('diminishing marginal utility').</p>\n<p>In neoclassical economics, the animal faced with the option of going east or west in the morning would first estimate how much the water and the fruit would change its objective wealth level, and then it would estimate how much those objective changes in wealth would change its utility. That is, it would use objective values to compute its marginal (subjective) utility. If it only had access to the subjective experiences in our story, it couldn't compute a new marginal utilities when it finds itself unexpectedly thirsty.</p>\n<p>The problem with this solution is that the brain does not appear to encode the objective values of stimuli, and humans <em>behaviorally</em>&nbsp;don't seem to respect the objective values of options either,&nbsp;as discussed <a href=\"/lw/6da/do_humans_want_things/\">here</a>.</p>\n<p>In response to the behavioral evidence, Kahneman &amp; Tversky (1979) developed a <em>reference dependent</em>&nbsp;utility function to describe human behavior: <a href=\"/lw/6kf/prospect_theory_a_framework_for_understanding/\">prospect theory</a>. Their suggestion was, basically:</p>\n<blockquote>\n<p>Rather than computing marginal utilities against [objective] wealth as in [standard neoclassical economic models], <em>utilities</em>&nbsp;(not marginal utilities) could be computed directly as deviations from a baseline level of wealth, and then choices could be based on direct comparisons of these utilities rather than on comparisons of marginal utilities. Their idea was to begin with something like the chooser's <em>status quo</em>, how much wealth he thinks he has. Each gamble is then represented as the chance of winning or losing utilities relative to that status-quo-like reference point.<sup>38</sup></p>\n</blockquote>\n<p>This fits with the neurobiological fact that we encode signals from external stimuli relative to reference points, and don't have access to the objective values of stimuli.</p>\n<p>The advantage of the neoclassical economic model is that it keeps a chooser's choices consistent. The advantage of the reference-dependent approach is that it better fits human behavior and human neurobiology.</p>\n<p>Most neoclassical economists seem to ignore the problems for their theories that are presented by reference dependence in human behavior and human neurobiology, but two neoclassical economists at Berkeley, <a href=\"http://elsa.berkeley.edu/~rabin/\">Matthew Rabin</a> and <a href=\"http://elsa.berkeley.edu/~botond/\">Botond Koszegi</a>, have begun to take reference dependence seriously. As they put it:</p>\n<blockquote>\n<p>...while&nbsp;an unexpected monetary windfall in the lab may be assessed as a gain, a salary of $5o,000 to an employee who expected $60,000 will not be assessed as a large gain relative to status-quo wealth, but rather as a loss relative to&nbsp;expectations of wealth. And in nondurable consumption &mdash; where there is no object with which the person can be endowed &mdash; a status-quo-based theory cannot capture the role of reference dependence at all: it would predict, for instance, that a person who misses a concert she expected to attend would feel no differently than somebody who never expected to see the concert.<sup>39</sup></p>\n</blockquote>\n<p>Their reference-dependent model makes particular predictions:</p>\n<blockquote>\n<p>[Our theory] shows that a consumer's willingness to pay a given price for shoes depends on the probability with which she expected to buy them and the price she expected to pay. On the one hand, an increase in the likelihood of buying increases a consumer's sense of loss of shoes if she does not buy, creating an \"attachment effect\" that increases her willingness to pay. Hence, the greater the likelihood she thought prices would be low enough to induce purchase, the greater is her willingness to buy at higher prices. On the other hand, holding the probability of getting the shoes fixed, a decrease in the price a consumer expected to pay makes paying a higher price feel like more of a loss, creating a \"comparison effect\" that lowers her willingness to pay the high price. Hence, the lower the prices she expected among those prices that induce purchase, the lower is her willingness to buy at higher prices.</p>\n</blockquote>\n<p>Thus, the cost of accepting the human fact of reference-dependence is that we have to admit that humans are irrational (in the sense of 'rationality' defined by <a href=\"http://en.wikipedia.org/wiki/Revealed_preference\">the axioms of revealed preference</a>):</p>\n<blockquote>\n<p>The fact that a consumer will pay more for shoes she expected to buy than for shoes she did not expect to buy, or that an animal would prefer inferior fruit it expected to eat over superior fruit it did not expect to eat, is exactly the kind of irrational behavior that we might hope the pressures of evolution would preclude. What observations tell us, however, is that these behaviors do occur. The neuroscience of sensory encoding tells us that these behaviors are an inescapable product of the fundamental structure of our brains.<sup>40</sup></p>\n</blockquote>\n<p>But really, shouldn't it have been obvious all along that humans are irrational? Perhaps it is, to everyone but neoclassical economists and Aristoteleans. (Okay, enough teasing...)</p>\n<p>One thing to keep in mind is that the brain encodes information about the external world in a reference-dependent way because that method makes a more efficient use of neurons. So evolution traded away some rationality for greater efficiency in the encoding mechanism.&nbsp;</p>\n<p>&nbsp;</p>\n<h4><a name=\"ValuationIn\"></a>Valuation in the Brain</h4>\n<p>Back to dopamine. <a href=\"#TheTurn\">Earlier</a>, we learned that the brain learns the values of their actions with a dopaminergic reward system that uses something like temporal difference (TD) reinforcement learning. This reward system updates the stored values for actions by generating a reward prediction error (RPE) from the difference between expected reward and experience reward, and propagating this learning throughout relevant structures of the brain using the neurotransmitter dopamine. In particular, some&nbsp;synapses are strengthened whenever presynaptic and postsynaptic activity occur <em>in the presence of dopamine</em>, as proposed by Wickens (1993).</p>\n<p>But we haven't yet discussed how utilities for actions are generated in the first place, or how they are stored (independent of the&nbsp;<em>expected</em>&nbsp;utilities represented during the choice process). It feels like I generally want ice cream a little bit and hot sex a lot more. Where is that information stored?</p>\n<p>Dozens<sup>41</sup> of fMRI studies show that two brain regions in particular are correlated with subjective value: the ventral striatum and the medial prefrontal cortex. Other studies suggest that at least five more brain regions probably also contribute to the valuation process: the orbitofrontal cortex, the dorsolateral prefrontal cortex, the amygdala, the insula, and the anterior cingulate cortex.</p>\n<p>There are many theories about how the human brain generates and stores utilities, but these theories are far more speculative and in their infancy than everything else I've presented in this tutorial, so I won't discuss them here. Instead, let us conclude with a summary of what neuroscientists know about the human brain's motivational system, and what some of the greatest open questions are.</p>\n<p>&nbsp;</p>\n<p><span style=\"font-size: 14px; font-weight: bold;\"><a name=\"SummaryAnd\"></a><a name=\"SummaryAnd\"></a>Summary and Research Directions</span></p>\n<p>Here's what we've learned:</p>\n<ul>\n<li>Utilities are real numbers ranging from 0 to 1,000 that take action potentials per second as their natural units. (By 'utility' here I don't mean what's usually meant by the term, I just mean 'utility' for the purpose of predicting choice by measuring the firing rates of certain populations of neurons <em>in the final common path of the choice circuit in the human brain</em>.)</li>\n<li>Mean utilities are mean firing rates of specific populations of neurons in the final common path of human choice circuits.</li>\n<li>Mean utilities predict choice stochastically, similar to random utility models from economics.</li>\n<li>Utilities are encoded cardinally in firing rates relative to neuronal baseline firing rates. (This is opposed to post-Pareto, ordinal notions of utility.)</li>\n<li>The choice circuit takes as its input a firing rate that encodes relative (normalized) stochastic expected utility.</li>\n<li>As the choice set size grows, so does the error rate.</li>\n<li>Final choice is implemented by an argmax function or a reservation price mechanism.</li>\n</ul>\n<p><a href=\"http://www.cns.nyu.edu/~glimcher/\">Paul Glimcher</a> lists<sup>42</sup> the greatest open questions in the field as:</p>\n<ol>\n<li>Where is utility stored and how does it get to the choice mechanism?</li>\n<li>How does the brain decide when it's time to choose?</li>\n<li>What is the neural mechanism that allows us to substitute between two goods at a certain point?</li>\n<li>How are probabilistic beliefs represented in the brain?</li>\n<li>Utility functions are state-dependent, so how do state and utility function interact?</li>\n</ol>\n<p>Later, we'll explore the implications of our findings for <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">metaethics</a>. As of August 2011, if you've read this then you probably know more about how human values <em>actually</em>&nbsp;work than almost every professional metaethicist on Earth. The general lesson here is that you can often out-pace <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">most philosophers</a> simply by reading what today's leading&nbsp;<em>scientists</em>&nbsp;have to say about a given topic instead of reading what <em>philosophers</em>&nbsp;say about it.</p>\n<p>&nbsp;</p>\n<h4><a name=\"Notes\"></a>Notes</h4>\n<p><small><sup>1</sup> They are: <a href=\"/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/\">Less Wrong Rationality and Mainstream Philosophy</a>, <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">Philosophy: A Diseased Discipline</a>, <a href=\"/lw/5i7/on_being_okay_with_the_truth/\">On Being Okay with the Truth</a>,&nbsp;<a href=\"/lw/4yq/the_neuroscience_of_pleasure/\">The Neuroscience of Pleasure</a>,&nbsp;<a href=\"/lw/4z7/the_neuroscience_of_desire/\">The Neuroscience of Desire</a>, <a href=\"/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">How You Make Judgments: The Elephant and its Rider</a>, <a href=\"/lw/5ee/being_wrong_about_your_own_subjective_experience/\">Being Wrong About Your Own Subjective Experience</a>, <a href=\"/lw/59v/intuition_and_unconscious_learning/\">Intuition and Unconscious Learning</a>, <a href=\"/lw/5sk/inferring_our_desires/\">Inferring Our Desires</a>, <a href=\"http://commonsenseatheism.com/?p=15072\">Wrong About Our Own Desires</a>,&nbsp;<a href=\"/lw/6da/do_humans_want_things/\">Do Humans Want Things?</a>, <a href=\"/lw/65w/not_for_the_sake_of_pleasure_alone/\">Not for the Sake of Pleasure Alone</a>, <a href=\"/lw/6dz/not_for_the_sake_of_selfishness_alone/\">Not for the Sake of Selfishness Alone</a>,&nbsp;<a href=\"/lw/5bw/your_evolved_intuitions/\">Your Evolved Intuitions</a>, <a href=\"/lw/4vs/when_intuitions_are_useful/\">When Intuitions Are Useful</a>, <a href=\"http://commonsenseatheism.com/?p=15213\">Cornell Realism</a>, <a href=\"http://commonsenseatheism.com/?p=15253\">Railton's Moral Reductionism (Part 1)</a>, <a href=\"http://commonsenseatheism.com/?p=15264\">Railton's Moral Reductionism (Part 2)</a>, <a href=\"http://commonsenseatheism.com/?p=15267\">Jackson's Moral Functionalism</a>,&nbsp;<a href=\"http://commonsenseatheism.com/?p=15336\">Moral Reductionism and Moore's Open Question Argument</a>, and <a href=\"/lw/74f/are_deontological_moral_judgments_rationalizations/\">Are Deontological Moral Judgments Rationalizations?</a></small></p>\n<p><small><sup>2</sup> <a href=\"/lw/54p/heading_toward_nononsense_metaethics/\">Heading Toward: No-Nonsense Metaethics</a>, <a href=\"/lw/5eh/what_is_metaethics/\">What is Metaethics?</a>, <a href=\"/lw/5kn/conceptual_analysis_and_moral_theory/\">Conceptual Analysis and Moral Theory</a>, and <a href=\"/lw/5u2/pluralistic_moral_reductionism/\">Pluralistic Moral Reductionism</a>.</small></p>\n<p><small><sup>3</sup> I tried something similar before, with <a href=\"http://commonsenseatheism.com/?p=13607\">Cognitive Science in One Lesson</a>.</small></p>\n<p><small><sup>4</sup>&nbsp;Glimcher (2010) offers the best coverage of the topic in a single book. Tobler &amp; Kobayashi (2009) offer the best coverage in a single article.</small></p>\n<p><small><sup>5</sup>&nbsp;The quotes in this section are from Churchland (1981).</small></p>\n<p><small><sup>6</sup> Allen &amp; Ng (2004).</small></p>\n<p><small><sup>7</sup>&nbsp;This perspective goes back at least as far back as Arnauld (1662), who wrote:</small></p>\n<blockquote>\n<p><small>To judge what one must do to obtain a good or avoid an evil, it is necessary to consider not only the good and the evil in itself, but also the probability that it happens or does not happen: and to view geometrically the proportion that all these things have together.</small></p>\n</blockquote>\n<p><small><sup>8</sup>&nbsp;In addition to Caplin &amp; Leahy (2001), see Kreps &amp; Porteus'&nbsp;(1978, 1979)&nbsp;incroporation of the \"utility of knowing\", Loomes &amp; Sugden's (1982) incorporation of \"regret\",&nbsp;Gul &amp; Pesendorfer's (2001) incorporation of \"the cost of self-control\", and&nbsp;Koszegi &amp; Rabin's (2007, 2009) incorporation of the \"reference point\".</small></p>\n<p><small><sup>9</sup> Friedman (1953).</small></p>\n<p><small><sup>10</sup> See a review in Fox &amp; Poldrack (2009).</small></p>\n<p><small><sup>11</sup> For one difficulty with prospect theory, see Laury &amp; Holt (2008).</small></p>\n<p><small><sup>12</sup> Sutton &amp; Barto (2008), p. 3. All quotes from this section are from the early pages of this book.</small></p>\n<p><small><sup>13</sup>&nbsp;From Sutton &amp; Barto (2008).</small></p>\n<p><small><sup>14</sup>&nbsp;Much of the rest of this post is basically a summary and paraphrase of Glimcher (2010).</small></p>\n<p><small><sup>15</sup> Mirenowicz &amp; Schultz (1994).</small></p>\n<p><small><sup>16</sup>&nbsp;Schultz et al. (1997).</small></p>\n<p><small><sup>17</sup> Caplin &amp; Dean (2007)</small></p>\n<p><small><sup>18</sup> From Glimcher (2010).</small></p>\n<p><small><sup>19</sup> Hebb (1949).</small></p>\n<p><small><sup>20</sup> Malenka &amp; Bear (2004).</small></p>\n<p><small><sup>21</sup> Reynolds &amp; Wickens (2002).</small></p>\n<p><small><sup>22</sup> Glimcher (2010), p. 341.</small></p>\n<p><small><sup>23</sup>&nbsp;Edelman &amp; Keller (1996); Van Gisbergen et al. (1987).</small></p>\n<p><small><sup>24</sup>&nbsp;Gold and Shadlen (2007); Roitman and Shadlen (2002).</small></p>\n<p><small><sup>25</sup> Simon (1957).</small></p>\n<p><small><sup>26</sup>&nbsp;Glimcher (2010), p. 215.</small></p>\n<p><small><sup>27</sup>&nbsp;McFadden (2000). The behavior of gradually transitioning between two choices is described by Selten (1975).</small></p>\n<p><small><sup>28</sup>&nbsp;For a probably improved random utility model, see Gul &amp; Pesendorfer (2006).</small></p>\n<p><small><sup>29</sup> Dean (1983); Werner &amp; Mountcastle (1963).</small></p>\n<p><small><sup>30</sup>&nbsp;Unless some other feature of the brain turns out to 'smooth out' the stochasticity of neurons involved in valuation and choice-making.</small></p>\n<p><small><sup>31</sup> Glimcher (2010).</small></p>\n<p><small><sup>32</sup> Heeger (1992, 1993); Carandini &amp; Heeger (1994); Simoncelli &amp; Heeger (1998).</small></p>\n<p><small><sup>33</sup> Carandini &amp; Heeger (1994); Britten &amp; Heuer (1999); Zoccolan et al. (2005); Louie &amp; Glimcher (2010).</small></p>\n<p><small><sup>34</sup> Horowitz &amp; Newsome (2001a, 2001b, 2004).</small></p>\n<p><small><sup>35</sup> Liu &amp; Wang (2008).</small></p>\n<p><small><sup>36</sup>&nbsp;But, see Deneve (2009).</small></p>\n<p><small><sup>37</sup> Glimcher (2010), p. 281.</small></p>\n<p><small><sup>38</sup> Glimcher (2010), p. 283.</small></p>\n<p><small><sup>39</sup>&nbsp;This quote and the next quote are from Koszegi &amp; Rabin (2006).</small></p>\n<p><small><sup>40</sup> Glimcher (2010), p. 292.</small></p>\n<p><small><sup>41</sup> I won't list them all here. For an overview, see Glimcher (2010), ch. 14.</small></p>\n<p><small><sup>42</sup> Glimcher (2010), ch. 17. I've paraphrased his open questions. I also excluded his 6th question:&nbsp;What Is the Neural Organ for Representing Money?</small></p>\n<p><small><span style=\"font-size: 11px;\"><br /></span></small></p>\n<h4><a name=\"References\"></a>References</h4>\n<p><small>Allais (1953). <a href=\"http://www.ericchiang.org/files/Allais_1953_Econometrica.pdf\">Le comportement de l'homme rationel devant le risque. Critique des postulates et axiomes de l'ecole americaine</a>. <em>Econometrica, 21</em>: 503-546.</small></p>\n<p><small>Allen &amp; Ng (2004). Economic behavior. In Spielberger (ed.), <em>Encyclopedia of Applied Psychology, Vol. 1</em>&nbsp;(pp. 661-666). Academic Press.</small></p>\n<p><small>Arnauld (1662). <em><a href=\"http://www.amazon.com/Port-Royal-Logic-Pierre-Nicole/dp/1142071251/\">Port-Royal Logic</a></em>.&nbsp;</small></p>\n<p><small>Basso &amp; Wurtz (1997). <a href=\"http://neuro.cjb.net/content/18/18/7519.full.pdf\">Modulation of neuronal activity in superior colliculus by changes in target probability</a>. <em>Journal of Neuroscience, 18</em>: 7519-7534.</small></p>\n<p><small>Britten &amp; Heuer (1999). <a href=\"http://neuro.cjb.net/content/19/12/5074.full.pdf\">Spatial summation in the receptive fields of MT neurons</a>. <em>Journal of Neuroscience, 19</em>: 5074-5084.</small></p>\n<p><small>Caplin &amp; Dean (2007). <a href=\"http://cess.nyu.edu/caplin/wp-content/uploads/2010/02/Axiomatic-Neuroeconomics.pdf\">Axiomatic neuroeconomics</a>.</small></p>\n<p><small>Caplin, Dean, Glimcher, &amp; Rutledge (2010). <a href=\"http://www.yorkshire-exile.co.uk/Beliefs.pdf\">Measuring beliefs and rewards: a neuroeconomic approach</a>. <em>Quarterly Journal of Economics, 125</em>: 3.</small></p>\n<p><small>Caplin &amp; Leahy (2001). <a href=\"http://pages.stern.nyu.edu/~dbackus/Exotic/1Other/CaplinLeahy%20antic%20QJE%2001.pdf\">Psychological expected utility theory and anticipatory feelings</a>. <em>Quarterly Journal of Economics, 116</em>: 55-79.</small></p>\n<p><small>Carandini &amp; Heeger (1994). <a href=\"http://redwood.berkeley.edu/vs265/carandini-heeger.pdf\">Summation and devision by neurons in primate visual cortex</a>. <em>Science, 264</em>: 1333-1336.</small></p>\n<p><small>Churchland (1981).&nbsp;<a href=\"http://philosophy.wisc.edu/Shapiro/Phil554/PAPERS/Churchland.pdf\">Eliminative materialism and the propositional attitudes</a>. <em>The Journal of Philosophy, 78</em>: 67-90.</small></p>\n<p><small>Dean (1983). Adaptation-induced alteration of the relation between response amplitude and contrast in cat striate cortical neurons. <em>Vision Research, 23</em>: 249-256.</small></p>\n<p><small>Deneve (2009).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Deneve-Bayesian-decision-making-in-two-alternative-forced-choices.pdf\">Bayesian decision making&nbsp;in two-alternative forced choices</a>.&nbsp;In Dreher &amp; Tremblay (eds.), <em>Handbook of Reward and Decision Making</em> (pp. 441-458). Academic Press.</small></p>\n<p><small>Dorris &amp; Glimcher (2004). <a href=\"http://monkeybiz.stanford.edu/nbio220/DorrisMC-Glimcher.pdf\">Activity in posterior parietal cortex is correlated with the subjective desireability of an action</a>. <em>Neuron, 44</em>: 365-378.</small></p>\n<p><small>Edelman &amp; Keller (1996). Activity of visuomotor burst neurons in the superior colliculus accompanying express saccades. <em>Journal of Neurophysiology, 76</em>: 908-926.</small></p>\n<p><small>Fox &amp; Poldrack (2009). <a href=\"http://cnl.salk.edu/~terry/BGGN/CH011.pdf\">Prospect theory and the brain</a>. In Glimcher, Camerer, Fehr, &amp; Poldrack (eds.), <em>Neuroeconomics: Decision Making and the Brain</em>&nbsp;(pp. 145-173). Academic Press.</small></p>\n<p><small>Friedman (1953). <em><a href=\"http://www.amazon.com/Essays-Positive-Economics-Phoenix-Books/dp/0226264033/\">Essays in Positive Economics</a></em>. University of Chicago Press.</small></p>\n<p><small>Glimcher (2010). <em><a href=\"http://www.amazon.com/Foundations-Neuroeconomic-Analysis-Paul-Glimcher/dp/0199744254/\">Foundations of Neuroeconomic Analysis</a></em>. Oxford University Press.</small></p>\n<p><small>Gold and Shadlen (2007). <a href=\"http://homepages.inf.ed.ac.uk/pseries/CCN/gold_shadlen_review.pdf\">The neural basis of decision making</a>. <em>Annual Review of Neuroscience, 30</em>: 535-574.</small></p>\n<p><small>Gul &amp; Pesendorfer (2001). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.160.133&amp;rep=rep1&amp;type=pdf\">Temptation and self-control</a>. <em>Econometrica, 69</em>: 1403-1435.</small></p>\n<p><small>Gul &amp; Pesendorfer (2006). <a href=\"http://www.princeton.edu/~pesendor/random.pdf\">Random expected utility</a>. <em>Econometrica, 74</em>: 121-146.</small></p>\n<p><small>Hebb (1949). <em><a href=\"http://www.amazon.com/Organization-Behavior-Neuropsychological-Theory/dp/0805843000/\">The organization of behavior</a></em>. Wiley &amp; Sons.</small></p>\n<p><small>Heeger (1992). <a href=\"http://www.cns.nyu.edu/labs/heegerlab/content/publications/Heeger-VisNeurosci1992a.pdf\">Normalization of cell responses in cat striate cortex</a>. <em>Visual Neuroscience, 9</em>: 181-197.</small></p>\n<p><small>Heeger (1993). <a href=\"http://www.cns.nyu.edu/labs/heegerlab/content/publications/Heeger-JNeurophysiol1993.pdf\">Modeling simple-cell direction selectivity with normalized, half-squared linear operators</a>. <em>Journal of Neurophysiology, 70</em>: 1885-1898.</small></p>\n<p><small>Horowitz &amp; Newsome (2001a). <a href=\"http://jn.physiology.org/content/86/5/2527.full.pdf\">Target selection for saccadic eye movements: direction selective visual responses in the superior colliculus induced by behavioral training</a>. <em>Journal of Neurophysiology, 86</em>: 2527-2542.</small></p>\n<p><small>Horowitz &amp; Newsome (2001b). <a href=\"http://jn.physiology.org/content/86/5/2543.full.pdf\">Target selection for saccadic eye movements: prelude activity in the superior colliculus during a direction discrimination task</a>. <em>Journal of Neurophysiology, 86</em>: 2543-2558.</small></p>\n<p><small>Iyengar &amp; Lepper (2000). <a href=\"http://www.columbia.edu/~ss957/articles/Choice_is_Demotivating.pdf\">When choice is demotivating: Can one desire too much of a good thing?</a> <em>Journal of Personality and Social Psychology, 79</em>: 995-1006.</small></p>\n<p><small>Jevons (1871).&nbsp;<em><a href=\"http://www.amazon.com/Theory-Political-Economy-William-Stanley/dp/1165162105/\">The Theory of Political&nbsp;Economy</a></em>. Macmillan and Co.</small></p>\n<p><small>Kahneman &amp; Tversky (1979). <a href=\"http://paper.blog.bbiq.jp/Kahneman_and_Tversky_1979.pdf\">Prospect theory: An analysis of decision under risk</a>. <em>Econometrica, 47</em>: 263-291.</small></p>\n<p><small>Koszegi &amp; Rabin (2006). <a href=\"http://webserver1.pugetsound.edu/facultypages/gmilam/courses/econ291/readings/01-KozegiRabin.pdf\">A model of reference-dependent preferences</a>. <em>Quarterly Journal of Economics, 121</em>: 1133-1165.</small></p>\n<p><small>Koszegi &amp; Rabin (2007). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.61.4476&amp;rep=rep1&amp;type=pdf\">Reference-dependent risk attitudes</a>. <em>American Economic Review, 97</em>: 1047-1073.</small></p>\n<p><small>Koszegi &amp; Rabin (2009). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.153.2065&amp;rep=rep1&amp;type=pdf\">Reference-dependent consumption plans</a>. <em>American Economic Review, 99</em>: 909-936.</small></p>\n<p><small>Kreps &amp; Porteus (1978). <a href=\"http://teaching.ust.hk/~bee/papers/040918/1978-Kreps_Porteus-dynamic_choice_theory.pdf\">Temporal resolution of uncertainty and dynamic choice theory</a>. <em>Econometrica, 46</em>: 185-200.</small></p>\n<p><small>Kreps &amp; Porteus (1979). <a href=\"http://hassler-j.iies.su.se/Courses/NewPrefs/Papers/TimeandRisk/KrepsPorteus%20DP%20Ec%20Jan%2079.pdf\">Dynamic choice theory and dynamic programming</a>. <em>Econometrica, 47</em>: 91-100.</small></p>\n<p><small>Laury &amp; Holt (2008). <a href=\"http://www2.gsu.edu/~ecoskl/lotteryhbk.pdf\">Payoff scale effects and risk preference under real and hypothetical conditions</a>. In Plott &amp; Smith (eds.), <em>Handbook of Experimental Economic Results, Vol. 1</em> (pp. 1047-1053). Elsevier Press.&nbsp;</small></p>\n<p><small>Loewenstein (1987). <a href=\"http://sds.hss.cmu.edu/media/pdfs/loewenstein/AnticipationValuDelayed.pdf\">Anticipation and the valuation of delayed consumption</a>. <em>Economic Journal, 97</em>: 666-684.</small></p>\n<p><small>Liu &amp; Wang (2008). <a href=\"http://biophy.nju.edu.cn/papers/Liu_PLoS_253_08.pdf\">A common cortical circuit mechanism for perceptual categorical discrimination and veridical judgment</a>. <em>PLOS Computational Biology, 4</em>: 1-14.</small></p>\n<p><small>Loomes &amp; Sugden (1982). <a href=\"http://teaching.ust.hk/~bee/papers/misc/Regret%20Theory%20An%20Alternative%20Theory%20of%20Rational%20Choice%20Under%20Uncertainty.pdf\">Regret theory: An alternative theory of rational choice under uncertainty</a>. <em>Economic Journal, 92</em>: 805-824.</small></p>\n<p><small>Louie &amp; Glimcher (2010). <a href=\"http://www.jneurosci.org/content/30/16/5498.full.pdf\">Separating value from choice: delay discounting activity in the lateral intraparietal area</a>. <em>Journal of Neuroscience, 30</em>: 5498-5507.</small></p>\n<p><small>Malenka &amp; Bear (2004). LTP and LTD: an embarrassment of riches. <em>Neuron, 44</em>: 5&ndash;21.</small></p>\n<p><small>Mirenowicz &amp; Schultz (1994). Importance of unpredictability for reward responses in primate dopamine neurons. <em>Journal of Neurophysiology, 72</em>: 1024-1027.</small></p>\n<p><small>Reynolds &amp; Wickens (2002). <a href=\"http://cnl.salk.edu/~terry/BGGN/reynolds.NN.02.pdf\">Dopamine-dependent plasticity of corticostriatal synapses</a>. <em>Neural Networks, 15</em>: 507-521.</small></p>\n<p><small>Roitman and Shadlen (2002). <a href=\"http://neuro.cjb.net/content/22/21/9475.full.pdf\">Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction time task</a>. <em>Nature Neuroscience, 22</em>: 9475-9489.</small></p>\n<p><small>Scheibehenne,&nbsp;Greifeneder, &amp; Todd (2010). Can there ever be too many options? A meta-analytic review of choice overload.&nbsp;<em>Journal of Consumer Research, 37</em>: 409-425.</small></p>\n<p><small>Schultz, Dayan, &amp; Montague (1997). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.124.5997&amp;rep=rep1&amp;type=pdf\">A neural substrate of prediction and reward</a>. <em>Science, 275</em>: 1593&ndash;1599.</small></p>\n<p><small>Schwartz &amp; Simoncelli (2001). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.29.3508&amp;rep=rep1&amp;type=pdf\">Natural signal statistics and sensory gain control</a>. <em>Nature Neuroscience, 4</em>: 819-825.</small></p>\n<p><small>Selten (1975). Reexamination of perfectness concept for equilibrium points in extensive games. <em>International Journal of Game Theory, 4</em>: 25-55.</small></p>\n<p><small>Simoncelli &amp; Heeger (1998). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.35.5397&amp;rep=rep1&amp;type=pdf\">A model of neuronal responses in visual area MT</a>. <em>Vision Research, 38</em>: 743-761.</small></p>\n<p><small>Sutton &amp; Barto (2008). <em><a href=\"http://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981/\">Reinforcement Learning: An Introduction</a></em>. MIT Press.</small></p>\n<p><small>Tanji &amp; Evarts (1976). <a href=\"http://jacknife.med.yale.edu/spikeclub/TanjiAnticipatoryJNeurophys1976.pdf\">Anticipatory activity of motor cortex neurons in relation to direction of an intended movement</a>. <em>Journal of Neurophysiology, 39</em>: 1062-1068.</small></p>\n<p><small>Tobler &amp; Kobayashi (2009). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Tobler-Kobayashi-Electrophysiological-correlates-of-reward-processing-in-dopamine-neurons.pdf\">Electrophysiological correlates of reward processing in dopamine neurons</a>. In Dreher &amp; Tremblay (eds.), <em>Handbook of Reward and Decision Making</em>&nbsp;(pp. 29-50). Academic Press.</small></p>\n<p><small>Van Gisbergen, Opstal, &amp; Tax (1987). Collicular ensemble coding of saccades based on vector summation. <em>Neuroscience, 21</em>: 651.</small></p>\n<p><small>Werner &amp; Mountcastle (1963). The variability of central neural activity in a sensory system, and its implications for central reflection of sensory events. <em>Journal of Neurophysiology, 26</em>: 958-977.</small></p>\n<p><small>Wickens (1993). <em><a href=\"http://www.amazon.com/Theory-Striatum-J-Wickens/dp/0080422780/\">A Theory of the Striatum</a></em>. Pergamon Press.</small></p>\n<p><small>Zoccolan, Cox, &amp; DiCarlo (2005). <a href=\"http://www.jneurosci.org/content/25/36/8150.full.pdf\">Multiple object response normalization in monkey inferotemporal cortex</a>. <em>Journal of Neuroscience, 25</em>: 8150-8164.</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"iP2X4jQNHMWHRNPne": 7, "Ng8Gice9KNkncxqcj": 2, "3uE2pXvbcnS9nnZRE": 19, "Wi3EopKJ2aNdtxSWg": 3, "zQw5d37qwzdpgQs5P": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hN2aRnu798yas5b2k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 148, "baseScore": 185, "extendedScore": null, "score": 0.000353, "legacy": true, "legacyId": "9141", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 186, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>[<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Muehlhauser-A-Crash-Course-in-the-Neuroscience-of-Human-Motivation-08-20-2011.pdf\">PDF</a> of this article updated Aug. 23, 2011]</small></p>\n<p><small>[<a href=\"#preface\">skip to preface</a>] </small></p>\n<p>Whenever I write a new article for Less Wrong, I'm pulled in two opposite directions.</p>\n<p>One force pulls me toward writing short, exciting posts with lots of brain candy and just <em>one main point</em>. Eliezer has done that kind of thing very well many times: see <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">Making Beliefs Pay Rent</a>, <a href=\"/lw/im/hindsight_devalues_science/\">Hindsight Devalues Science</a>, <a href=\"/lw/oj/probability_is_in_the_mind/\">Probability is in the Mind</a>,&nbsp;<a href=\"/lw/nu/taboo_your_words/\">Taboo Your Words</a>, <a href=\"/lw/oi/mind_projection_fallacy/%20%20%20%20\">Mind Projection Fallacy</a>,&nbsp;<a href=\"/lw/iq/guessing_the_teachers_password/\">Guessing the Teacher's Password</a>, <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">Hold Off on Proposing Solutions</a>, <a href=\"/lw/jb/applause_lights/\">Applause Lights</a>, <a href=\"/lw/of/dissolving_the_question/\">Dissolving the Question</a>, and <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">many</a> <a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">more</a>.</p>\n<p>Another force pulls me toward writing long, factually dense posts that fill in as many of the pieces of a particular argument in one fell swoop as possible. This is largely because I want to write about the cutting edge of human knowledge but I keep realizing that the inferential gap is <a href=\"/lw/kg/expecting_short_inferential_distances\">larger than I had anticipated</a>, and I want to fill in that inferential gap quickly so I can get to the cutting edge.</p>\n<p>For example, I had to draw on dozens of Eliezer's posts just to <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">say</a> I was <em>heading toward</em>&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">my metaethics sequence</a>. I've also published 21 new posts (many of them quite long and heavily researched) written specifically because I need to refer to them in my metaethics sequence.<sup>1</sup>&nbsp;I tried to make these posts interesting and useful on their own, but my primary motivation for writing them was that I need them for my metaethics sequence.</p>\n<p>And now I've written only&nbsp;<em>four posts</em><sup>2</sup>&nbsp;in my metaethics sequence and already the inferential gap to my next post in that sequence is huge again. :(</p>\n<p>So I'd like to try an experiment. I won't do it often, but I want to try it at least once. Instead of writing 20 more short posts between now and the next post in my metaethics sequence, I'll attempt to fill in a big chunk of the inferential gap to my next metaethics post in one fell swoop by writing a long tutorial post (<em>a la</em>&nbsp;Eliezer's tutorials on <a href=\"http://yudkowsky.net/rational/bayes\">Bayes' Theorem</a> and <a href=\"http://yudkowsky.net/rational/technical\">technical explanation</a>).<sup>3</sup></p>\n<p>So if you're not up for a 20-page tutorial on human motivation, this post isn't for you, but I hope you're glad I bothered to write it for the sake of others. If you <em>are</em>&nbsp;in the mood for a 20-page tutorial on human motivation, please proceed.</p>\n<p><a id=\"more\"></a></p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/HumanMotivation_title.png\" alt=\"\"></p>\n<p>&nbsp;</p>\n<blockquote>\n<p>Who knows what I want to do? Who knows what anyone wants to do? How can you&nbsp;be sure about something like that? Isn\u2019t it all a question of brain chemistry, signals&nbsp;going back and forth, electrical energy in the cortex? How do you know whether&nbsp;something is really what you want to do or just some kind of nerve impulse in the&nbsp;brain. Some minor little activity takes place somewhere in this unimportant place in&nbsp;one of the brain hemispheres and suddenly I want to go to Montana or I don\u2019t want&nbsp;to go to Montana.</p>\n</blockquote>\n<p align=\"right\">- Don DeLillo, <em><a href=\"http://www.amazon.com/White-Noise-Penguin-Classics-Deluxe/dp/0143105981/\">White Noise</a></em></p>\n<h4 id=\"Preface\"><a name=\"Preface\"></a>Preface</h4>\n<p>How do we value things, and choose between options? Philosophers, economists, and psychologists have long tried to answer these questions. But human behavior continues to defy our most subtle models of it, and the algorithms producing our behavior remained hidden in a black box.</p>\n<p>But now, neuroscientists are directly measuring the neurons whose firing rates encode value and produce our choices. We know a lot more about the neuroscience of human motivation than you might think. Now we can peer directly into the black box of human motivation, and begin (dimly) to read our own source code.</p>\n<p>The neuroscience of human motivation has implications for philosophy of mind and action, for <a href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">scientific self-help</a>, and for <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">metaethics</a> and <a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">Friendly AI</a>. (We <a href=\"/lw/5sk/inferring_our_desires/\">don't really know what we want</a>, and looking directly at the algorithms that produce human wanting might help in solving this mystery.)</p>\n<p>So, I wrote a crash course in the neuroscience of human motivation.</p>\n<p>The purpose of this document is not to <em>argue</em>&nbsp;for any of the conclusions presented within it. That would require not a long blog post but instead a couple 500-page books \u2014 say, <em><a href=\"http://www.amazon.com/Foundations-Neuroeconomic-Analysis-Paul-Glimcher/dp/0199744254/\">Foundations of Neuroeconomic Analysis</a></em>&nbsp;and <em><a href=\"http://www.amazon.com/Handbook-Reward-Decision-Making-Jean-Claude/dp/0123746205/\">Handbook of Reward and Decision Making</a></em>&nbsp;(my two greatest sources for this post).<sup>4</sup></p>\n<p>Instead, I merely want to summarize the current mainstream scientific picture on the neuroscience of human motivation, explain some of the concepts it uses, and tell a few stories about how our current picture of human motivation developed.</p>\n<p>As you read this, I hope that many questions and objections will come to mind, because it's <em>not</em>&nbsp;the full story. That's why I went to the trouble of linking to PDFs of almost all my sources (see <a href=\"#References\">References</a>): so you can check the full data and the full arguments yourself if you like.</p>\n<p>This document is long. You may prefer to read it in sections.</p>\n<p>&nbsp;</p>\n<h4 id=\"Contents_\">Contents:</h4>\n<ol>\n<li><a href=\"#FolkPsychology\">Folk Psychology</a></li>\n<li><a href=\"#NeoclassicalEconomics\">Neoclassical Economics</a></li>\n<li><a href=\"#BehaviorismAnd\">Behaviorism and Reinforcement Learning</a></li>\n<li><a href=\"#ReinforcementLearning\">Reinforcement Learning and Decision Theory</a></li>\n<li><a href=\"#TheTurn\">The Turn to the Brain</a></li>\n<li><a href=\"#HebbianLearning\">Hebbian Learning</a></li>\n<li><a href=\"#ExpectedUtility\">Expected Utility in Neurons</a></li>\n<li><a href=\"#RealTime\">Real-Time Updates to Expected Utility</a></li>\n<li><a href=\"#ArgmaxAnd\">Argmax and Reservation Price</a></li>\n<li><a href=\"#RandomUtility\">Random Utility</a></li>\n<li><a href=\"#Discounting\">Discounting</a></li>\n<li><a href=\"#RelativeAnd\">Relative and Absolute Utility</a></li>\n<li><a href=\"#Normalization\">Normalization</a></li>\n<li><a href=\"#AreActions\">Are Actions Choices?</a></li>\n<li><a href=\"#ThePrimate\">The Primate Choice Mechanism: A Brief Review</a></li>\n<li><a href=\"#MarginalUtility\">Marginal Utility and Reference Dependence</a></li>\n<li><a href=\"#ValuationIn\">Valuation in the Brain</a></li>\n<li><a href=\"#SummaryAnd\">Summary and Research Directions</a></li>\n</ol> \n<ul>\n<li><a href=\"#Notes\">Notes</a></li>\n<li><a href=\"#References\">References</a></li>\n</ul>\n<p>&nbsp;</p>\n<h4 id=\"Folk_Psychology\"><a name=\"FolkPsychology\"></a>Folk Psychology</h4>\n<p>There are these things called 'humans' on planet Earth. They undergo metabolism and cell growth. They produce waste. They maintain homeostasis. They reproduce. They move. They communicate. Sometimes they have <a href=\"http://i.imgur.com/WLYH1.jpg\">pillow fights</a>.</p>\n<p>Some of these human processes are 'automatic', like cell growth and breathing. Other processes are 'intentional' or 'willed', like moving and communicating and having pillow fights. We call these latter processes&nbsp;<em>intentional actions</em>, or simply <em>actions</em>. Sometimes we're not sure <a href=\"/lw/o0/where_to_draw_the_boundary/\">where to draw the line</a> between automatic processes and actions, but this should become clearer as we learn more. In the meantime, we ask...</p>\n<p>How can we <a href=\"http://yudkowsky.net/rational/technical\">explain</a>&nbsp;human actions?</p>\n<p>One popular explanation is '<a href=\"http://en.wikipedia.org/wiki/Folk_psychology\">folk psychology</a>.' Folk psychology posits that we humans have beliefs and desires, and that <em>we are motivated to do what we believe will fulfill our desires</em>.</p>\n<p>I desire to eat a cookie. I believe I can fulfill that desire if I walk to the kitchen and put one of the cookies there into my mouth. So I am motivated to walk to the kitchen and put a cookie in my mouth.</p>\n<p>Of course there are complications. For example I have multiple desires. Suppose I desire to eat a cookie and believe there are cookies in the kitchen. But I also desire to remain sitting comfortably in the living room. Can I satisfy <em>both</em>&nbsp;desires? I also believe that if I nicely ask my friend in the kitchen to bring me a cookie, she will. So I ask her to bring me a cookie and I begin to eat it, without having to leave the comfy living room sofa. We still explain my behavior with constructs like 'beliefs' and 'desires', but we consider more than one of each to do so.</p>\n<p>Most of us use folk psychology every day to successfully predict human behavior. I believe that my friend <em>desires</em>&nbsp;to do nice things for me on occasion if they're not too much trouble, and I believe that my friend, once I tell her I want a cookie, will&nbsp;<em>believe</em>&nbsp;she can be nice to me without much trouble if she brings me a cookie from the kitchen. So, I predict that my friend will bring me a cookie when I ask her. So I ask her, and <em>behold</em>!&nbsp;My prediction was correct. I am happily eating a cookie on the sofa.</p>\n<p>But folk psychology (FP) faces some problems.<sup>5</sup> Consider its context in history:</p>\n<blockquote>\n<p>The presumed domain of FP used to be much larger than it is now. In primitive cultures, the behavior of most of the elements of nature were understood in intentional terms. The wind could know anger, the moon jealousy, the river generosity\u2026 These were not metaphors\u2026 the animistic approach to nature has dominated our history, and it is only in the last two or three thousand years that we have restricted FP\u2019s literal application to the domain of the higher animals.</p>\n<p>[Even still,] the FP of the Greeks is essentially the FP we uses today\u2026 This is a very long period of stagnation and infertility for any theory to display, especially when faced with such an enormous backlog of anomalies and mysteries in its own explanatory domain\u2026 To use Imre Lakatos\u2019 terms, FP is a stagnant or degenerating research program, and has been for millennia.</p>\n</blockquote>\n<p>Consider also its prospects for inter-theoretic reduction:</p>\n<blockquote>\n<p>If we approach homo sapiens from the perspective of natural history and the physical sciences, we can tell a coherent story of its constitution, development, and behavioral capacities which encompasses particle physics, atomic and molecular theory, organic chemistry, evolutionary theory, biology, physiology, and materialistic neuroscience. The story, though still radically incomplete, is already extremely powerful, outperforming FP at many points even in its own domain. And it is deliberately\u2026 coherent with the rest of our developing world picture. In short, the greatest theoretical synthesis in [history] is currently in our hands\u2026</p>\n<p>But FP is no part of this growing synthesis. Its intentional categories stand magnificently alone, without visible prospect of reduction to that larger corpus. A successful reduction cannot be ruled out, in my view, but FP\u2019s explanatory impotence and long stagnation inspire little faith that its categories will find themselves neatly reflected in the framework of neuroscience. On the contrary, one is reminded of how alchemy must have looked as elemental chemistry was taking form, how Aristotelean cosmology must have looked as classical mechanics was being articulated, or how the vitalist conception of life must have looked as organic chemistry marched forward.</p>\n</blockquote>\n<p>Finally, consider the problem of <em>habit</em>. I sit at my computer and want to type my name, 'Luke.' However, I have just used a special program to switch the function of the keys labeled L and P so that they will input the <em>other</em>&nbsp;character instead (so that I can play a prank on my friend, who will be using my computer shortly). I <em>believe</em>&nbsp;that typing the key labeled L will input P instead, but nevertheless when I type my name my fingers fall into their familiar habit and I end up typing my name as 'Puke.' My act of typing was intentional, and yet I <em>didn't</em> do what I believed would fulfill my desire to type my name.</p>\n<p>Folk psychology faces both successes and failures in explaining human action. Hopefully we can do better.</p>\n<p>&nbsp;</p>\n<h4 id=\"Neoclassical_Economics\"><a name=\"NeoclassicalEconomics\"></a>Neoclassical Economics</h4>\n<p>Folk psychology was updated and quantified by <a href=\"http://www.econlib.org/library/Enc1/NeoclassicalEconomics.html\">neoclassical economics</a>. To summarize:</p>\n<blockquote>\n<p>One [assumption of] neoclassical economics is \"rationality,\" in which individuals are said to choose alternatives that maximize expected utilities. In particular, the neoclassical view is that individuals rank all possible alternatives according to how much satisfaction they will bring and then choose the alternative that [they expect] will bring the most satisfaction or utility...<sup>6</sup></p>\n</blockquote>\n<p>Let's review this notion of maximizing expected utility. Suppose I can choose one of two boxes sitting before me, red and blue. There is a 10% chance the red box contains a million dollars, and a 90% chance it contains nothing. As for the blue box, I am certain it contains $10,000. The 'expected value' of choosing the red box is (0.1 \u00d7 $1,000,000) + (0.9 \u00d7 $0), which is equal to $100,000. The expected value of choosing the blue box is !1&nbsp;\u00d7 $10,000), or $10,000. An agent that chose whatever had the highest expected value would choose the red box, which has 10 times the expected value of the blue box ($100,000 vs. $10,000).</p>\n<p>But humans don't value things only according to their dollar value. A million dollars might have 10 times the <em>objective</em>&nbsp;value&nbsp;of $100,000, but it might have less than 10 times the <em>subjective</em>&nbsp;value&nbsp;of $100,000 because after $100,000 you only care a little how much <em>more</em>&nbsp;wealthy you are.</p>\n<p>Or, you might be <a href=\"http://en.wikipedia.org/wiki/Risk_aversion\">risk averse</a>. You might prefer a sure thing to something that is uncertain. So a 10% chance of a million dollars might be worth <em>less</em> \u2014 in subjective value \u2014 than a 100% chance of $10,000. If you are risk averse you might choose the blue box because it has higher expected <em>subjective</em> value even though it has lower expected <em>objective</em> value.</p>\n<p>We call objective value simply 'value'. We call subjective value 'utility.'</p>\n<p>Neoclassical economics quantifies folk psychology by measuring the strength of belief with <em>probability</em> and by measuring the strength of desire with <em>utility</em>. It then says that humans act so as to maximize <em>expected utility</em>, a measure that combines the utility of particular thing with your subjective probability of getting it.<sup>7</sup></p>\n<p>This neoclassical model of human behavior has faced many challenges, and is regularly revised in the face of new evidence.<sup>8</sup> For example, Loewenstein (1987) found that if students were asked to place a value on the opportunity to kiss a celebrity of their choice 1-5 days in the future, they placed the highest value on a kiss in 3 days. This didn't fit any existing neoclassical models of utility, but was explained in 2001 when Caplin &amp; Leahy (2001) incorporated \"anticipatory feeling\" into the neoclassical model, explaining that the students got some utility from&nbsp;<em>anticipating</em>&nbsp;the kiss with the celebrity (but also, as usual, discounted the utility of a reward the further away it was in the future), and this is why they didn't want the kiss right away.</p>\n<p>Keep in mind that economists don't argue that we <em>actually</em>&nbsp;compute the expected utility of each option before us and then choose the best one, but that we always act \"as if\" we were doing that.<sup>9</sup></p>\n<p>But sometimes we don't even act \"as if\" we are obeying the axioms of neoclassical economics. For example, the <a name=\"independence\"></a>independence axiom of expected utility theory <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/4o7t\">says</a> that if you prefer an apple over an orange, then you must prefer the Gamble A (72% chance you get an apple, otherwise you get a cat) over the Gamble B (72% chance you get an orange, otherwise you get a cat). But Allais (1953) found that subjects <em>do</em>&nbsp;violate this basic assumption under some conditions.</p>\n<p>Such violations of the basic axioms of neoclassical economics led to the development of <a href=\"http://en.wikipedia.org/wiki/Behavioral_economics\">behavioral economics</a>&nbsp;and theories like Kahneman and Tversky's (1979) <a href=\"/lw/6kf/prospect_theory_a_framework_for_understanding/\">prospect theory</a>,<sup>10</sup>&nbsp;which transcends some assumptions of the neoclassical model. But these new theories don't fit the data perfectly, either.<sup>11</sup></p>\n<p>The models of human motivation we've surveyed so far are conceptually related to <a href=\"http://en.wikipedia.org/wiki/Decision_theory\">decision theory</a>&nbsp;(beliefs and desires, or probabilities and utilities), so I'll call them 'decision-theoretic models' of human motivation. We'll discuss decision-theoretic models again when we finally get to the topic of neuroscience, but for now I want to discuss a different approach to motivation.</p>\n<p>&nbsp;</p>\n<h4 id=\"Behaviorism_and_Reinforcement_Learning\"><a name=\"BehaviorismAnd\"></a>Behaviorism and Reinforcement Learning</h4>\n<p>While neoclassical economists formulated expected utility theory, behaviorist psychologists developed a different set of explanations for human action. Though behaviorists were <a href=\"/lw/sr/the_comedy_of_behaviorism/\">wrong</a>&nbsp;when they said that science can't talk about mental activity or mental states, you <a href=\"/lw/6i5/behaviorism_beware_anthropomorphizing_humans/\">can charitably think of</a> behaviorists as playing a game of <a href=\"/lw/nu/taboo_your_words/\">Rationalist's Taboo</a> with constructs of folk psychology like \"want\" or \"fear\" in order to get at phenomena more appropriate for quantification in&nbsp;<a href=\"http://yudkowsky.net/rational/technical\">technical explanation</a>. Also, the behaviorist approach led to 'reinforcement learning', an important concept in the neuroscience of human motivation.</p>\n<p>Before I explain reinforcement learning, let's recall&nbsp;<a href=\"/lw/6iu/basics_of_animal_reinforcement/\">operant conditioning</a>:</p>\n<blockquote>\n<p>Stick a pigeon in a box with a lever and some associated machinery (a \"Skinner box\"). The pigeon wanders around, does various things, and eventually hits the lever. Delicious sugar water squirts out. The pigeon continues wandering about and eventually hits the lever again. Another squirt of delicious sugar water. Eventually it percolates into its tiny pigeon brain that maybe pushing this lever makes sugar water squirt out. It starts pushing the lever more and more, each push continuing to convince it that yes, this is a good idea.</p>\n<p>Consider a second, less lucky pigeon. It, too, wanders about in a box and eventually finds a lever. It pushes the lever and gets an electric shock. Eh, maybe it was a fluke. It pushes the lever again and gets another electric shock. It starts thinking \"Maybe I should stop pressing that lever.\" The pigeon continues wandering about the box doing anything and everything other than pushing the shock lever.</p>\n<p>The basic concept of operant conditioning is that an animal will repeat behaviors that give it reward, but avoid behaviors that give it punishment.</p>\n</blockquote>\n<p>Behaviorism <a href=\"http://commonsenseatheism.com/?p=13607#history\">died</a> in the wake of cognitive psychology, but its approach to&nbsp;motivation turned out to be very useful in the field of artificial intelligence, where it is called <a href=\"http://www.scholarpedia.org/article/Reinforcement_learning\">reinforcement learning</a>:</p>\n<blockquote>\n<p>Reinforcement learning is learning what to do \u2014 how to map situations to actions \u2014 so as to maximize a numerical reward signal. The learner is not told which actions to take, as in most forms of machine learning, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward, but also the next situation and, through that, all subsequent rewards. These two characteristics \u2014 trial-and-error search and delayed reward \u2014 are the two most important distinguishing features of reinforcement learning.</p>\n<p>To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has tried in the past and found to&nbsp;be effective in producing reward. But to discover such actions it has to try actions that it has not selected before. The agent has to <em>exploit</em> what it already knows in order to obtain reward, but it also has to <em>explore</em> in order to make better action selections in the future. The dilemma is that neither exploitation nor exploration can be pursued exclusively without failing at the task. The agent must try a variety of actions and progressively favor those that appear to be best..<sup>12</sup></p>\n</blockquote>\n<p>In addition to the agent and its environment, there are four major components of a reinforcement learning system:</p>\n<blockquote>\n<p>...a <em>policy</em>, a <em>reward function</em>, a <em>value function</em>, and, optionally, a <em>model</em> of the environment.</p>\n<p>A <em>policy</em> defines the learning agent's way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states...</p>\n<p>A <em>reward function</em> defines the goal in a reinforcement learning problem. Roughly speaking, it maps perceived states (or state-action pairs) of the environment to a single number, a reward, indicating the intrinsic desirability of the state. A reinforcement-learning agent's sole objective is to maximize the total reward it receives in the long run. ...[A reward function may] be used as a basis for changing the policy. For example, if an action selected by the policy is followed by low reward, then the policy may be changed to select some other action in that situation in the future...</p>\n<p>Whereas a reward function indicates what is good in an immediate sense, a <em>value function</em> specifies what is good in the long run. Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future starting from that state. Whereas rewards determine the immediate, intrinsic desirability of environmental states, values indicate the long-term desirability of states after taking into account the states that are likely to follow, and the rewards available in those states. For example, a&nbsp;state might always yield a low immediate reward, but still have a high value because it is regularly followed by other states that yield high rewards. Or the reverse could be true...</p>\n<p>Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary. Without rewards there could be no values, and the only purpose of estimating values is to achieve more reward. Nevertheless, it is values with which we are most concerned when making and evaluating decisions. Action choices are made on the basis of value judgments. We seek actions that bring about states of highest value, not highest reward, because these actions obtain for us the greatest amount of reward over the long run...</p>\n<p>...The fourth and final element of some reinforcement learning systems is a <em>model</em> of the environment. This is something that mimics the behavior of the environment. For example, given a state and action, the model might predict the resultant next state and next reward. Models are used for planning, by which we mean any way of deciding on a course of action by considering possible future situations before they are actually experienced.</p>\n</blockquote>\n<p>Want an example? Here is how a reinforcement learning agent would learn to play <a href=\"http://en.wikipedia.org/wiki/Tic-tac-toe\">Tic-Tac-Toe</a>:</p>\n<blockquote>\n<p>First we set up a table of numbers, one for each possible state of the game. Each number will be the latest estimate of the probability of our winning from that state. We treat this estimate as the state's current value, and the whole table is the learned value function. State A has higher value than state B, or is considered 'better' than state B, if the current estimate of the probability of our winning from A is higher than it is from B. Assuming we always play Xs, then for all states with three Xs in a row the probability of winning is 1, because we have already won. Similarly, for all states with three \u00d8s in a row... the correct probability is 0, as we cannot win from them. We set the initial values of all the other states, the <em>nonterminals</em>, to 0.5, representing an informed guess that we have a 50% chance of winning.</p>\n<p>Now we play many games against the opponent. To select our moves we examine the states that would result from each of our possible moves (one for each blank space on the board) and look up their current values in the table. Most of the time we move <em>greedily</em>, selecting the move that leads to the state with greatest value, that is, with the highest estimated probability of winning. Occasionally, however, we select randomly from one of the other moves instead; these are called <em>exploratory</em>&nbsp;moves because they cause us to experience states that we might otherwise never see.</p>\n</blockquote>\n<p>A sequence of Tic-Tac-Toe moves might look like this:<sup>13</sup></p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/tic-tac-toe.png\" alt=\"\"></p>\n<p>Solid lines are the moves our reinforcement learning agent made, and dotted lines are moves it considered but did <em>not</em>&nbsp;make.&nbsp;The second move was an exploratory move: it was taken even though another sibling move, that leading to e*, was ranked higher.</p>\n<p>While playing, the agent changes the values assigned to the states it finds itself in. To improve its estimates concerning the probability of winning from various states, it 'backs up' the value of state after each greedy move to the state <em>before</em>&nbsp;the move (as suggested by the arrows.) What this means is that the value of the earlier state is adjusted to be closer to the value of the later state.</p>\n<blockquote>\n<p>If we let s denote the state before the greedy move, and s' the state after, then the update to the estimated value of s, denoted V(s), can be written:</p>\n<p>V(s) &lt;- V(s) + \u03b1[V(s') - V(s)]</p>\n<p>where \u03b1 is a small positive fraction called the <em>step-size parameter</em>, which influences the rate of learning. The update rule is an example of a <em>temporal difference</em>&nbsp;learning method, so called because its changes are based on a difference... between [value] estimates at two different times.</p>\n<p>...if the step-size parameter is reduced properly over time, this method converges, for any [unchanging] opponent, to the true probabilities of winning from each state give optimal play by the agent.</p>\n</blockquote>\n<p>And that's how a simple version of temporal difference (TD) reinforcement learning works.</p>\n<p>&nbsp;</p>\n<h4 id=\"Reinforcement_Learning_and_Decision_Theory\"><a name=\"ReinforcementLearning\"></a>Reinforcement Learning and Decision Theory</h4>\n<p>You may have noticed a key advantage of reinforcement learning: an agent using it can be 'dumber' than a decision-theoretic agent. It can just start with guesses (\"What the hell; let's try 50%!\") for the value of various states, and then it learns their <em>true</em> values by running through <em>many, many&nbsp;trials</em>.</p>\n<p>But what if you don't <em>have</em>&nbsp;many trials to run through, and you need to make an important decision right now?</p>\n<p>Then you have to be smart. You need to have a good model of the world and use decision theory to choose the action with the highest expected utility.</p>\n<p><span>This is precisely what rationality&nbsp;</span>\u2014<span>&nbsp;being <em>good</em>&nbsp;at building correct models of the world&nbsp;</span>\u2014<span>&nbsp;<a href=\"/lw/34a/goals_for_which_less_wrong_does_and_doesnt_help/\">is especially good for</a>:</span></p>\n<blockquote>\n<p>For some tasks, the world provides rich, inexpensive empirical feedback. In these tasks you hardly need reasoning. &nbsp;Just try the task many ways... and take care to notice what is and isn\u2019t giving you results.</p>\n<p>Thus, if you want to learn to sculpt, [studying rationality] is a bad way to go about it. Better to find some clay and a hands-on sculpting course. The situation is similar for small talk, cooking, selling, programming, and many other useful skills.</p>\n<p>Unfortunately, most of us also have goals for which we can obtain no such ready success/failure data. For example, if you want to know whether cryonics is a good buy, you can\u2019t just try buying it and not-buying it and see which works better. &nbsp;If you miss your first bet, you\u2019re out for good.</p>\n</blockquote>\n<p>Reinforcement learning can be a good strategy if you have time to learn from many trials. If you've only got <em>one shot</em> at a problem, you'd better build up a really accurate model of the world first and then try to maximize expected utility.</p>\n<p>Now, back to our story.</p>\n<p>It turns out that reinforcement learning seems to underlie many of our mental processes. (More on this later.)</p>\n<p>The lesson Yvain <a href=\"/lw/6i5/behaviorism_beware_anthropomorphizing_humans/\">drew</a>&nbsp;from this discovery was:</p>\n<blockquote>\n<p>Reinforcement learning is <a href=\"/lw/l2/protein_reinforement_and_dna_onsequentialism/\">evolution writ small</a>; behaviors propagate or die out based on their consequences to reinforcement in a mind, just as mutations propagate or die out based on their consequences to reproduction in an organism. In the behaviorist model, our mind is not an agent, but a flourishing ecosystem of behaviors both physical and mental, all scrabbling for supremacy and mutating into more effective versions of themselves.</p>\n<p>Just as evolving organisms are <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">adaptation-executors and not fitness-maximizers</a>, so minds are behavior-executors and not utility-maximizers.</p>\n</blockquote>\n<p>But things are a bit more complicated than that, as we'll now see.</p>\n<p>&nbsp;</p>\n<h4 id=\"The_Turn_to_the_Brain\"><a name=\"TheTurn\"></a>The Turn to the Brain</h4>\n<blockquote>\n<p>I hesitate to say that men will ever have the means&nbsp;of measuring directly the feelings of the human&nbsp;heart. It is from the quantitative effects of the&nbsp;feelings that we must estimate their comparative&nbsp;amounts.</p>\n</blockquote>\n<p align=\"right\">William Jevons (1871)</p>\n<p>It turns out that Jevons was wrong. Modern neuroscience allows us to peer into the black box of the human value system and measure directly \"the feelings of the human heart.\"<sup>14</sup></p>\n<p>We'll begin with the experiments of <a href=\"http://www.pdn.cam.ac.uk/staff/schultz/\">Wolfram Shultz</a>. Schultz recorded the activity of single dopamine neurons in monkeys who sat in front of a water spout. At irregular intervals, a speaker played a tone and a drop of water dropped from the spout.<sup>15</sup>&nbsp;The monkeys' dopamine neurons normally fired at the baseline rate, but responded with a burst of activity when water was delivered. Over time, though, the neurons responded less and less to the water and more and more to the tone.</p>\n<p>But if Schultz delivered water without first giving the tone, then the dopamine neurons responded with a burst of activity again. And if he played the tone and <em>didn't</em>&nbsp;provide water, the neurons reduced their firing rates <em>below</em>&nbsp;the baseline. The neurons weren't responding to the water itself but to a difference between expected reward and actual reward \u2014 a reward prediction error (RPE).</p>\n<p>Two other researchers, <a href=\"http://www.hnl.bcm.tmc.edu/faculty.html\">Read Montague</a> and <a href=\"http://www.gatsby.ucl.ac.uk/~dayan/\">Peter Dayan</a>, noticed that these patterns of neuronal activity were exactly predicted by TD reinforcement learning theory from computer science.<sup>16</sup>&nbsp;In particular, the RPE observed in neurons appeared to play the same role in monkey learning as the difference between value estimates at two different times did in TD reinforcement learning theory.</p>\n<p>Since then, researchers have done many more single-neuron recording studies to test particular versions of TD reinforcement learning and revise the theory until it predicts more and more behavior while also predicting novel experimental discoveries.</p>\n<p>Caplin &amp; Dean<sup>17</sup> provided another way to test the hypothesis that dopamine neurons encoded RPE in a TD-class model. They showed that all existing RPE-models could be reduced to three axiomatic statements. If a system violated one of these axioms, it could not be an RPE system. Later, Caplin et al. (2010) tested the axioms on actual brain activity to see if they held up. They did. This is another reason why so many scientists working in this field believe the current 'dopamine hypothesis' \u2014 that dopamine neurons encode RPE in a TD-class reinforcement learning system in the brain.</p>\n<p>TD-class reinforcement learning works in computers by updating numbers that represent the values of states. How does reinforcement learning work when using nerve cells?</p>\n<p>&nbsp;</p>\n<h4 id=\"Hebbian_Learning\"><a name=\"HebbianLearning\"></a>Hebbian Learning</h4>\n<p>By <a href=\"http://en.wikipedia.org/wiki/Hebbian_theory\">Hebbian learning</a>, of course. \"Cells that fire together, wire together.\"</p>\n<p>Imagine a neural pathway (in one of Pavlov's dogs) that connects the neural circuits that sense the ringing of a bell to the neural circuits for salivation. This is a weak connection at first, which is why the bell doesn't initially elicit salivation.</p>\n<p>Also imagine a third neuron that connects the salivation circuit to a circuit that detects food. This is a strong connection, and that's why food <em>does</em>&nbsp;elicit salivation right away:<sup>18</sup></p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Hebbian-circuits.png\" alt=\"\"></p>\n<p>Donald Hebb proposed:</p>\n<blockquote>\n<p>When an axon of cell A is near enough to excite cell B and repeatedly or persistently take part in firing it, a growth process of metabolic change takes place in one or both cells such that A's efficacy, as one of the cells firing B, is increased.<sup>19</sup></p>\n</blockquote>\n<p>In short, whenever two connected cells are active at the same time, the synapses connecting them are strengthened.</p>\n<p>Consider Pavlov's experiment. At first, the Bell cell will fire whenever bells ring, but probably not when the salivation cells happen to be active. So, the connection between the Bell cell and the Salivation cell remains weak. But then, Pavlov intervenes and causes the Bell cell and the Salivation cell to fire at the same time by ringing the bell and presenting food at the same time (the Food detector cell already has a strong connection to the Salivation cell). Whenever the Bell cell and the Salivation cell happen to fire at the same time, the synapse between them is strengthened. Once the connection is strong enough, the Bell cell can cause the Salivation cell to fire on its own, just like the Food detector cell can.</p>\n<p>It was a fine theory, but it wasn't observed until Bliss &amp; Lomo (1973) observed Hebb's mechanism at work in the rabbit hippocampus. Today, we know how some forms of Hebb's mechanism work at the molecular level.<sup>20</sup></p>\n<p>Later, Wickens (1993) proposed a similar mechanism called the <em>three-factor rule</em>, according to which some synapses are strengthened whenever presynaptic and postsynaptic activity occurred <em>in the presence of dopamine</em>. These same synapses might be weakened when activity occurred in the <em>absence</em>&nbsp;of dopamine. Later studies confirmed this hypothesis.<sup>21</sup></p>\n<p>Suppose a monkey receives an unexpected reward and encodes a large positive RPE. Glimcher explains:</p>\n<blockquote>\n<p>The TD model tells us that under these conditions we want to increment the value attributed to all actions or sensations that have just occurred. Under these conditions, we know that the dopamine neurons release dopamine throughout the frontocortical-basal ganglia loops, and do so in a highly homogenous manner. That means we can think of any neuron equipped with dopamine receptors as 'primed' for synaptic strengthening. When this happens, any segment of the frontocortical-basal ganglia loop that is already active will have its synapses strengthened.<sup>22</sup></p>\n</blockquote>\n<p>We will return to the dopamine system later, but for now let us back up and pursue the neoclassical economic path into the brain.</p>\n<p>&nbsp;</p>\n<h4 id=\"Expected_Utility_in_Neurons\"><a name=\"ExpectedUtility\"></a>Expected Utility in Neurons</h4>\n<p>Ever since Friedman (1953), economists have insisted that humans only behave <em>as if</em>&nbsp;they are utility maximizers, not that they <em>actually</em>&nbsp;compute expected utility and try to maximize it.</p>\n<p>It was a surprise, then, when neuroscientists stumbled upon the neurons that were encoding expected utility in their firing rates.</p>\n<p>Tanji &amp; Evarts (1976) did their experiments with rhesus monkeys because they are our closest relative besides the apes, and this kind of work is usually forbidden on apes for ethical reasons (we need to implant a recording electrode in the brain).</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/rhesus-monkey.jpg\" alt=\"\"></p>\n<p>The monkeys were trained to know that a colored light on the screen meant they would soon be offered a reward (a drop of water) either for pushing or pulling, but not for both. This was the \u2018ready\u2019 cue. A second later, researchers gave a \u2018direction\u2019 cue that told the monkeys which action \u2014 push or pull \u2014 was going to be rewarded. The third cue was the 'go' signal: if the monkey made the previously indicated movement, it was rewarded.</p>\n<p>This is what they saw:</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/push-pull-smaller.png\" alt=\"\"></p>\n<p>At the \u2018ready\u2019 cue, the neurons associated with a pushing motion became weakly active (but fired above the baseline rate), and so did the neurons associated with a pulling motion. When the \u2018direction\u2019 cue was given, the neurons associated with the to-be-rewarded motion doubled their firing rate, and the neurons associated with the opposite motion fell back to the baseline rate. Then at the \u2018go\u2019 cue, the neurons associated with the to-be-rewarded movement increased again rapidly, up past the threshhold required to produce movement, and the movement was produced shortly thereafter.&nbsp;</p>\n<p>One tempting explanation of the data is that after the \u2018ready\u2019 cue, the monkey\u2019s brain 'decides' there\u2019s a 50% chance that pulling will get the reward, and a 50% chance that pushing will get the reward. That\u2019s why we see the neuron firing rates associated with those two actions each jump to slightly less than 50% of the movement threshold when the \u2018ready\u2019 cue is given. But then, when the \u2018direction\u2019 cue is given, those expectations shift to 100%/0% or 0%/100%, depending on which action is about to be rewarded according to the \u2018direction\u2019 cue. That\u2019s why activity in the circuit associated with the to-be-rewarded action doubles and the other one drops to baseline. And then the \u2018go\u2019 cue is delivered and firing rates blast past the movement threshold, and movement is produced.</p>\n<p>Let's jump ahead to Basso &amp; Wurtz (1997), who did a similar experiment except that they used voluntary eye movements (called \u2018saccades\u2019) instead of voluntary arm movements. And this time, they presented each monkey with one, two, four, or eight possible targets, instead of just two targets (push and pull) like Tanji &amp; Evarts did.</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/monkey-saccade.png\" alt=\"\"></p>\n<p>What they found was that as more potential targets were presented, the magnitude of the preparatory activity associated with each target systematically decreased. And again, once the \u2018direction\u2019 and \u2018go\u2019 cues were presented, the activity associated with those other potential targets dropped rapidly and activity burst rapidly in neurons associated with the to-be-rewarded movement. It was as though the monkeys\u2019 brains were distributing their probability mass evenly across the potentially rewarded actions, and then once they knew which action should in fact be rewarded, they moved all their probability mass to that action and performed the action and got the reward.</p>\n<p>&nbsp;</p>\n<h4 id=\"Real_Time_Expected_Utility_Updates\"><a name=\"RealTime\"></a>Real-Time Expected Utility Updates</h4>\n<p>Other researchers showed monkeys a black screen with flickering white dots on it. In each frame of the video, the computer moved each dot in a random direction. The independent variable was a measure called 'coherence.' In a 100% leftward coherence condition, all dots moved to the left. In a 60% rightward condition, 60% of the dots move rightward while the rest moved randomly. And so on.</p>\n<p>In a typical experiment, the researchers would identify a neuron in a monkey's brain that increased its firing rate in response to rightward coherence of the dots, and decreased its firing rate in response to leftward coherent of the dots. Then they would present the monkey with a sequence (in random order) of every possible leftward and rightward coherence condition.</p>\n<p>A leftward coherence (of any magnitude) meant the monkey would be rewarded for leftward eye movement, and a rightward coherence meant the monkey would be rewarded for rightward eye movement. But, the monkey had to wait two seconds before being rewarded.</p>\n<p>In this experiment, the probabilities always started at 50% but then updated continuously. A 100% rightward coherence condition allowed the monkey to very quickly know which voluntary eye movement would be rewarded, but in a 5% rightward coherence condition the expected utility of the rightward target grew more slowly.</p>\n<p>The results? The greater the coherence of rightward motion of the dots, the faster the neurons associated with rightward eye movement increased their firing rate. (A higher coherence meant the monkey was able to update its probabilities more quickly.)</p>\n<p>&nbsp;</p>\n<h4 id=\"Argmax_and_Reservation_Price\"><a name=\"ArgmaxAnd\"></a>Argmax and Reservation Price</h4>\n<p>Many studies show that the brain controls movement by way of a 'winner take all' mechanism that is isomorphic to the <a href=\"http://en.wikipedia.org/wiki/Arg_max\">argmax</a> operation from economics.<sup>23</sup> That is, there are many possibilities competing for your final choice, but just before your choice the single strongest signal remains after all the others are inhibited.</p>\n<p>This <em>choice mechanism</em>&nbsp;was investigated in more detail by <a href=\"http://depts.washington.edu/pbiopage/people_fac_page.php?fac_ID=28\">Michael Shadlen</a> and others.<sup>24</sup>&nbsp;Shadlen gave monkeys the same eye movement task as above, except that the monkeys could make their choice at any time instead of waiting for two seconds. He found that:</p>\n<ol>\n<li>When the direction of the dots is unambiguous, monkeys make their choices quickly.</li>\n<li>As the direction of the dots becomes more ambiguous, they take longer to make their choices.</li>\n<li>Throughout the experiment, the firing rates of neurons in the LIP (part of the 'final common path' for generating eye movement) grew toward a specific threshold level.</li>\n</ol>\n<p>The threshold level acts as a kind of criterion of choice. Once the criterion is met, action is taken. Or in economic terms, the monkeys seemed to set a <em>reservation price</em> on making certain movements.<sup>25</sup></p>\n<p>&nbsp;</p>\n<h4 id=\"Random_Utility\"><a name=\"RandomUtility\"></a>Random Utility</h4>\n<p>When deciding between goods of different expected utilities, humans exhibit a <em>stochastic transfer function</em>:</p>\n<blockquote>\n<p>Consider a human subject choosing between two objects of highly different expected utilities, such as a first lottery with a 50% chance of winning $5 and a second lottery with a 25% chance of winning $5. We observe highly deterministic behavior under these conditions: basically all subjects always choose the 5o% chance of winning $5. But what happens when we increment the value of the 25% lottery? As the amount one stands to win from that lottery is incremented, individual subjects eventually switch their preference. Exactly when they make that switch depends on their idiosyncratic degree of risk aversion. What is most interesting about this behavior for these purposes, though, is that actual human subjects, when presented with this kind of choice repeatedly, are never completely deterministic. As the value of the 25% lottery increases, they begin to show probabilistic behavior \u2014 selecting the 25% lottery sometimes, but not always.<sup>26</sup></p>\n</blockquote>\n<p>Our behavior has an element of randomness in it. <a href=\"http://elsa.berkeley.edu/~mcfadden/\">Daniel McFadden</a> won a Nobel Prize in economics for capturing such behavior using a <em>random utility model</em>.<sup>27</sup> The way he did it is to suppose that when a chooser asks himself what a thing is worth, he doesn't get a fixed answer but a variable one. That is, there is actual variation <em>in his preferences</em>. Thus, his expected utility for a particular lottery is drawn from a <em>distribution</em> of possible utilities, usually one with a Gaussian variance.<sup>28</sup></p>\n<p>This behavior makes sense when we think about the human choice mechanism at the neuronal level, because neuron firing rates are&nbsp;stochastic.<sup>29</sup> When a neurobiologist says \"The neuron was firing at 200 Hz,\" what she means is that the mean firing rate of the neuron over a long time and stable conditions would have been close to 200 Hz. So the neurons that encode utility (wherever they are) will exhibit stochasticity, and thereby introduce some randomness into our choices. In this way, neurobiological data <em>constrains</em>&nbsp;our economic models of human behavior. An economic model without some randomness in it will have difficulty capturing human choices for as long as humans run on neurons.<sup>30</sup></p>\n<p>&nbsp;</p>\n<h4 id=\"Discounting\"><a name=\"Discounting\"></a>Discounting</h4>\n<p>Louie &amp; Glimcher (2010) examined temporal discounting in the brain. The two monkeys in this study were repeatedly asked to choose between a small, immediately available reward and a larger reward available after a small delay. For example, on one day they were asked to choose between 0.13 millileters of juice right now, or else 0.2 millileters of juice available after a delay of 2, 4, 8, or 12 seconds. A monkey might be willing to wait 2, 4, or 8 seconds for the larger reward, but not 12 seconds.</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/monkey-discounting-smaller.png\" alt=\"\"></p>\n<p>After many, many measurements of this kind, Louie and Glimcher were able to describe the discounting function being used by each monkey. (One of them was more impatient than the other.)</p>\n<p>Moreover, the neurons in the relevant section of the brain fired at rates that reflected each monkey\u2019s discounting function. If 0.2 millileters of juice was offered with no delay, the neurons were highly active. If the same reward was offered at a delay of 2 seconds, they were slightly less active. If the same reward was offered after 4 seconds, the neurons were less active still. And so on. As it turned out, the discounting function that captured their choices was identical to the discounting function that captured the firing rates of these neurons.</p>\n<p>This shouldn't be a surprise at this point, but just to confirm: Yes, we can observe discounting in the firing rates of neurons involved in the choice-making process.</p>\n<p>&nbsp;</p>\n<h4 id=\"Relative_and_Absolute_Utility\"><a name=\"RelativeAnd\"></a>Relative and Absolute Utility</h4>\n<p>Dorris &amp; Glimcher (2004) observed monkeys and their choice mechanism neurons while the monkeys engaged in repeated plays of the <a href=\"http://www.maths.lse.ac.uk/Personal/stengel/TEXTE/insp.pdf\">inspection game</a>. The study is too involved for me to explain here, but the results suggested that choice mechanism neurons encode <em>relative</em>&nbsp;expected utilities (relative to other actions under consideration) rather than <em>absolute</em>&nbsp;expected utilities.</p>\n<p>Tobler et al. (2005) suggested that the brain <em>only</em>&nbsp;encodes relative expected utilities. But there is reason to suspect this can't be right. If we stored only relative expected utilities, then we would routinely violate the axiom of transitivity (if you prefer A to B and B to C, you can't also prefer C to A). To see why this is the case, consider Glimcher's example (he says 'expected value' instead of 'utility'):</p>\n<blockquote>\n<p>...consider a subject trained to choose between objects A and B, where A is $1,000,000 worth of goods and B is $100,000 worth of goods... A system that represented only the relative expected subjective value of A and B would represent SV(A) &gt; SV(B). Next, consider training the same subject to choose between C and D, where C is $1,000 worth of goods and D is $100 worth of goods. Such a system would represent SV(C) &gt; SV(D). What happens when we ask a chooser to select between B and C? For a chooser who represents only relative expected subjective value, the choice should be C: she should pick $1,000 worth of goods over $100,000 worth of goods because it has a higher learned relative expected subjective value. In order for our chooser to... construct transitive preferences across choice sets (and to obey the continuity axiom)... it is required that somewhere in the brain she represent the absolute subjective values of her choices.<sup>31</sup></p>\n</blockquote>\n<p>And we mostly&nbsp;<em>do</em>&nbsp;seem to obey the axiom of transitivity.</p>\n<p>So if the choice mechanism neurons <em>do</em>&nbsp;represent relative utilities, then some other neurons elsewhere must encode a more absolute form of utility. Other implications of this are explored in the next section.</p>\n<p>&nbsp;</p>\n<h4 id=\"Normalization\"><a name=\"Normalization\"></a>Normalization</h4>\n<p><a href=\"http://www.cns.nyu.edu/~david/\">David Heeger</a> showed<sup>32</sup> that the firing rates of 'feature detector' neurons in the visual cortex captured a response to a feature in the visual field divided by the sum of the activation rates of nearby neurons sensitive to the same image. Thus, these neurons encode not only whether they 'see' the feature they are built to detect, but also how unique it is in the visual field.</p>\n<p>The effect of this is that neurons reacting to the <em>edge</em>&nbsp;of a visual object fire more actively than others do. Behold! Edge detection!</p>\n<p>It's also an efficient way to encode information about the world. Consider a world where orange dots are ubiquitous. For an animal in that world, it would be wasteful to fire action potentials to represent orange dots. Better to represent the <em>absence</em>&nbsp;of orange dots, or the transition from orange dots to something else. An optimally efficient encoding method would be sensitive not to the 'alphabet' of <em>all possible inputs</em>, but to a smaller alphabet of the inputs that <em>actually</em>&nbsp;appear in the world. This insight was mathematically formalized by Schwartz &amp; Simoncelli (2001).</p>\n<p>The efficiency of this&nbsp;<em>normalization</em>&nbsp;technique may explain why we've discovered it at work in so many different places in the brain.<sup>33</sup> And given that we've found it almost everywhere we've looked for it, it wouldn't be a surprise to see it show up in our choice-making circuits. Indeed, Simoncelli &amp; Schwartz's normalization equation may be what our brains use to encode expected utilities that are relative to the other choices under consideration.</p>\n<p>One implication of their equation is that a chooser's errors become more frequent as the size of the choice set grows. Thus, behavioral errors on small choice sets should be rarer than might be predicted by most random utility models, but error rates will increase rapidly with choice set size (and beyond a certain choice set size, choices will appear random).</p>\n<p>Preliminary evidence that choice set size effects error rates has arrived from behavioral economics. For example, consider Iyengar &amp; Lepper's (2000) study of supermarket shoppers. They set up a table showing either 6 or 24 flavors of jams, allowing shoppers to sample as many as they wanted. Customers who saw 24 flavors had a 3% chance of buying a jar, while those who saw only 6 flavors had a 30% chance!</p>\n<p>In another experiment, Iyengar &amp; Lepper let subjects choose one of either 6 or 30 different chocolates. Those who chose from among only 6 options were more satisfied with their selection than those who had been presented with 30 different chocolates.</p>\n<p>These data fit our expectation that as the choice set grows, the frequency of errors in our behavior rises and the likelihood that an option will rise above the threshold for purchase drops. When Louie &amp; Glimcher (2010) investigated this phenomena in monkey choice mechanism neurons, they found it at work there, too. But the process of choice-set editing is still poorly understood, and some recent studies have failed to replicate Iyengar &amp; Lepper's results (Scheibehenne et al. 2010).</p>\n<p>Perhaps the most surprising implication of these findings is that because of neuronal stochasticity, and because errors increase as the choice set grows, we should expect <em>stochastic violations of the <a href=\"#independence\">independence axiom</a></em>, and that <em>when choosers face very large choice sets they will essentially ignore the independence axiom</em>.</p>\n<p>This is a prediction about human behavior not made by earlier models from neoclassical economics, but it is suggested by looking at the neurons involved in human choice-making.</p>\n<p>&nbsp;</p>\n<h4 id=\"Are_Actions_Choices_\"><a name=\"AreActions\"></a>Are Actions Choices?</h4>\n<p>But all these data come from experiments where the choices are <em>actions</em>, and from our knowledge of the brain's \"final common path\" for producing actions. How do actions map on to choices about lovers and smartphones?</p>\n<p>Studies by <a href=\"http://faculty.washington.edu/ghorwitz/wordpress/\">Greg Horowitz</a> have provided some relevant data, because monkeys had to choose options identified by color rather than by action.<sup>34</sup>&nbsp;For example in one trial, a 'red' option might offer one reward and a 'green' option might offer a different reward. On each trial, the red and green options would appear at random places on the computer screen, and the monkey could choose a reward with a voluntary eye movement. The key here is that rewards were chosen by color and not by a (particular) action.</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/monkey-decide-red-or-green.png\" alt=\"\"></p>\n<p>Horowitz found that the choice mechanism neurons showed the same pattern of activation under these conditions as was the case under action-based choice tasks.</p>\n<p>So, it looks like the valuation circuits can store the value of a colored target, and these valuations can be mapped to the choice mechanism. But we don't know much about how this works, yet.</p>\n<p>&nbsp;</p>\n<h4 id=\"The_Primate_Choice_Mechanism__A_Brief_Review\"><a name=\"ThePrimate\"></a>The Primate Choice Mechanism: A Brief Review</h4>\n<p>Thus far, we have mostly discussed the primate brain's choice mechanism. To review:</p>\n<ol>\n<li>The choice circuit resides in the final common pathway for action.</li>\n<li>It takes as its input a signal that encodes stochastic expected utility, a concept aligned to the random utility term in economic models proposed by McFadden (2005) and Gul &amp; Pesendorfer (2006).</li>\n<li>This input signal is represented by a <em>normalized</em> firing rate (with Poisson variance, like all neurons).</li>\n<li>As the choice set size grows, so does the error rate.</li>\n<li>Final choice is implemented by an argmax function or a reservation price mechanism. (A single circuit can achieve both modes.<sup>35</sup>)</li>\n</ol>\n<p>But how are probability and utility calculated such that they can be fed into the expected utility representations of the choice mechanism? I won't discuss how the brain forms probabilistic beliefs in this article,<sup>36</sup> so let us turn to the study of how utility is calculated in the brain: the question of <em>valuation</em>.</p>\n<p>&nbsp;</p>\n<h4 id=\"Marginal_Utility_and_Reference_Dependence\"><a name=\"MarginalUtility\"></a>Marginal Utility and Reference Dependence</h4>\n<p>Consider the following story:</p>\n<blockquote>\n<p>Imagine an animal exploring a novel environment from a nest on a day when both (1) its blood concentration is dilute (and thus its need for water is low) and (2) its blood sugar level is low (and thus its need for food is high). The animal travels west one kilometer from the nest and emerges from the undergrowth into an open clearing at the shores of a large lake. Not very thirsty,&nbsp;the animal bends down to sample the water and finds it... unpalatable... the next day the same animal leaves its nest in the same metabolic state and travels one kilometer to the east, where it discovers a grove of trees that yield a dry but nutritious fruit, a grove of dried apricot trees. It samples the fruit and finds it sweet and highly palatable.</p>\n<p>What has the animal actually learned about the value of going west and the value of going east? It has had a weakly negative experience, in the psychological sense, when going west and a very positive experience when going east. Do these subjective properties of its experience influence what it has learned? Do the stored representations derived from these experiences encode the actual objective values of going west and east, or do they encode the subjective experiences? That is a critical question about what the animal has learned, because it determines what it does when it wakes up thirsty. When it wakes up thirsty it should, in a normative sense, go west towards the... lake, despite the fact that its previous visit west was a negative experience.<sup>37</sup></p>\n</blockquote>\n<p>Economists have known this problem for a long time, and solved it with an idea called&nbsp;<em>marginal utility</em>.</p>\n<p>In neoclassical economics, we view the animal as having two kinds of 'wealth': a sugar wealth and a water wealth (the total store of sugar and water in the animal's body at a given time). A piece of fruit or a sip of water is an <em>increment</em>&nbsp;in the animal's total sugar or water wealth. The utility of a piece of fruit or a sip of water, then, depends on its current levels of sugar and water wealth.</p>\n<p>On day one, the animal's need for water is low and its need for sugar is high. On that day, the marginal utility of a piece of fruit is greater than the marginal utility of a sip of water. But suppose during the next week the animal has a high blood sugar level. At that time, the marginal utility of a piece of fruit is low. Thus, the marginal utility of a consumable resource depends on wealth. The wealthier the chooser, the lower the marginal utility provided by a fixed amount of gain ('diminishing marginal utility').</p>\n<p>In neoclassical economics, the animal faced with the option of going east or west in the morning would first estimate how much the water and the fruit would change its objective wealth level, and then it would estimate how much those objective changes in wealth would change its utility. That is, it would use objective values to compute its marginal (subjective) utility. If it only had access to the subjective experiences in our story, it couldn't compute a new marginal utilities when it finds itself unexpectedly thirsty.</p>\n<p>The problem with this solution is that the brain does not appear to encode the objective values of stimuli, and humans <em>behaviorally</em>&nbsp;don't seem to respect the objective values of options either,&nbsp;as discussed <a href=\"/lw/6da/do_humans_want_things/\">here</a>.</p>\n<p>In response to the behavioral evidence, Kahneman &amp; Tversky (1979) developed a <em>reference dependent</em>&nbsp;utility function to describe human behavior: <a href=\"/lw/6kf/prospect_theory_a_framework_for_understanding/\">prospect theory</a>. Their suggestion was, basically:</p>\n<blockquote>\n<p>Rather than computing marginal utilities against [objective] wealth as in [standard neoclassical economic models], <em>utilities</em>&nbsp;(not marginal utilities) could be computed directly as deviations from a baseline level of wealth, and then choices could be based on direct comparisons of these utilities rather than on comparisons of marginal utilities. Their idea was to begin with something like the chooser's <em>status quo</em>, how much wealth he thinks he has. Each gamble is then represented as the chance of winning or losing utilities relative to that status-quo-like reference point.<sup>38</sup></p>\n</blockquote>\n<p>This fits with the neurobiological fact that we encode signals from external stimuli relative to reference points, and don't have access to the objective values of stimuli.</p>\n<p>The advantage of the neoclassical economic model is that it keeps a chooser's choices consistent. The advantage of the reference-dependent approach is that it better fits human behavior and human neurobiology.</p>\n<p>Most neoclassical economists seem to ignore the problems for their theories that are presented by reference dependence in human behavior and human neurobiology, but two neoclassical economists at Berkeley, <a href=\"http://elsa.berkeley.edu/~rabin/\">Matthew Rabin</a> and <a href=\"http://elsa.berkeley.edu/~botond/\">Botond Koszegi</a>, have begun to take reference dependence seriously. As they put it:</p>\n<blockquote>\n<p>...while&nbsp;an unexpected monetary windfall in the lab may be assessed as a gain, a salary of $5o,000 to an employee who expected $60,000 will not be assessed as a large gain relative to status-quo wealth, but rather as a loss relative to&nbsp;expectations of wealth. And in nondurable consumption \u2014 where there is no object with which the person can be endowed \u2014 a status-quo-based theory cannot capture the role of reference dependence at all: it would predict, for instance, that a person who misses a concert she expected to attend would feel no differently than somebody who never expected to see the concert.<sup>39</sup></p>\n</blockquote>\n<p>Their reference-dependent model makes particular predictions:</p>\n<blockquote>\n<p>[Our theory] shows that a consumer's willingness to pay a given price for shoes depends on the probability with which she expected to buy them and the price she expected to pay. On the one hand, an increase in the likelihood of buying increases a consumer's sense of loss of shoes if she does not buy, creating an \"attachment effect\" that increases her willingness to pay. Hence, the greater the likelihood she thought prices would be low enough to induce purchase, the greater is her willingness to buy at higher prices. On the other hand, holding the probability of getting the shoes fixed, a decrease in the price a consumer expected to pay makes paying a higher price feel like more of a loss, creating a \"comparison effect\" that lowers her willingness to pay the high price. Hence, the lower the prices she expected among those prices that induce purchase, the lower is her willingness to buy at higher prices.</p>\n</blockquote>\n<p>Thus, the cost of accepting the human fact of reference-dependence is that we have to admit that humans are irrational (in the sense of 'rationality' defined by <a href=\"http://en.wikipedia.org/wiki/Revealed_preference\">the axioms of revealed preference</a>):</p>\n<blockquote>\n<p>The fact that a consumer will pay more for shoes she expected to buy than for shoes she did not expect to buy, or that an animal would prefer inferior fruit it expected to eat over superior fruit it did not expect to eat, is exactly the kind of irrational behavior that we might hope the pressures of evolution would preclude. What observations tell us, however, is that these behaviors do occur. The neuroscience of sensory encoding tells us that these behaviors are an inescapable product of the fundamental structure of our brains.<sup>40</sup></p>\n</blockquote>\n<p>But really, shouldn't it have been obvious all along that humans are irrational? Perhaps it is, to everyone but neoclassical economists and Aristoteleans. (Okay, enough teasing...)</p>\n<p>One thing to keep in mind is that the brain encodes information about the external world in a reference-dependent way because that method makes a more efficient use of neurons. So evolution traded away some rationality for greater efficiency in the encoding mechanism.&nbsp;</p>\n<p>&nbsp;</p>\n<h4 id=\"Valuation_in_the_Brain\"><a name=\"ValuationIn\"></a>Valuation in the Brain</h4>\n<p>Back to dopamine. <a href=\"#TheTurn\">Earlier</a>, we learned that the brain learns the values of their actions with a dopaminergic reward system that uses something like temporal difference (TD) reinforcement learning. This reward system updates the stored values for actions by generating a reward prediction error (RPE) from the difference between expected reward and experience reward, and propagating this learning throughout relevant structures of the brain using the neurotransmitter dopamine. In particular, some&nbsp;synapses are strengthened whenever presynaptic and postsynaptic activity occur <em>in the presence of dopamine</em>, as proposed by Wickens (1993).</p>\n<p>But we haven't yet discussed how utilities for actions are generated in the first place, or how they are stored (independent of the&nbsp;<em>expected</em>&nbsp;utilities represented during the choice process). It feels like I generally want ice cream a little bit and hot sex a lot more. Where is that information stored?</p>\n<p>Dozens<sup>41</sup> of fMRI studies show that two brain regions in particular are correlated with subjective value: the ventral striatum and the medial prefrontal cortex. Other studies suggest that at least five more brain regions probably also contribute to the valuation process: the orbitofrontal cortex, the dorsolateral prefrontal cortex, the amygdala, the insula, and the anterior cingulate cortex.</p>\n<p>There are many theories about how the human brain generates and stores utilities, but these theories are far more speculative and in their infancy than everything else I've presented in this tutorial, so I won't discuss them here. Instead, let us conclude with a summary of what neuroscientists know about the human brain's motivational system, and what some of the greatest open questions are.</p>\n<p>&nbsp;</p>\n<p><span style=\"font-size: 14px; font-weight: bold;\"><a name=\"SummaryAnd\"></a><a name=\"SummaryAnd\"></a>Summary and Research Directions</span></p>\n<p>Here's what we've learned:</p>\n<ul>\n<li>Utilities are real numbers ranging from 0 to 1,000 that take action potentials per second as their natural units. (By 'utility' here I don't mean what's usually meant by the term, I just mean 'utility' for the purpose of predicting choice by measuring the firing rates of certain populations of neurons <em>in the final common path of the choice circuit in the human brain</em>.)</li>\n<li>Mean utilities are mean firing rates of specific populations of neurons in the final common path of human choice circuits.</li>\n<li>Mean utilities predict choice stochastically, similar to random utility models from economics.</li>\n<li>Utilities are encoded cardinally in firing rates relative to neuronal baseline firing rates. (This is opposed to post-Pareto, ordinal notions of utility.)</li>\n<li>The choice circuit takes as its input a firing rate that encodes relative (normalized) stochastic expected utility.</li>\n<li>As the choice set size grows, so does the error rate.</li>\n<li>Final choice is implemented by an argmax function or a reservation price mechanism.</li>\n</ul>\n<p><a href=\"http://www.cns.nyu.edu/~glimcher/\">Paul Glimcher</a> lists<sup>42</sup> the greatest open questions in the field as:</p>\n<ol>\n<li>Where is utility stored and how does it get to the choice mechanism?</li>\n<li>How does the brain decide when it's time to choose?</li>\n<li>What is the neural mechanism that allows us to substitute between two goods at a certain point?</li>\n<li>How are probabilistic beliefs represented in the brain?</li>\n<li>Utility functions are state-dependent, so how do state and utility function interact?</li>\n</ol>\n<p>Later, we'll explore the implications of our findings for <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">metaethics</a>. As of August 2011, if you've read this then you probably know more about how human values <em>actually</em>&nbsp;work than almost every professional metaethicist on Earth. The general lesson here is that you can often out-pace <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">most philosophers</a> simply by reading what today's leading&nbsp;<em>scientists</em>&nbsp;have to say about a given topic instead of reading what <em>philosophers</em>&nbsp;say about it.</p>\n<p>&nbsp;</p>\n<h4 id=\"Notes\"><a name=\"Notes\"></a>Notes</h4>\n<p><small><sup>1</sup> They are: <a href=\"/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/\">Less Wrong Rationality and Mainstream Philosophy</a>, <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">Philosophy: A Diseased Discipline</a>, <a href=\"/lw/5i7/on_being_okay_with_the_truth/\">On Being Okay with the Truth</a>,&nbsp;<a href=\"/lw/4yq/the_neuroscience_of_pleasure/\">The Neuroscience of Pleasure</a>,&nbsp;<a href=\"/lw/4z7/the_neuroscience_of_desire/\">The Neuroscience of Desire</a>, <a href=\"/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">How You Make Judgments: The Elephant and its Rider</a>, <a href=\"/lw/5ee/being_wrong_about_your_own_subjective_experience/\">Being Wrong About Your Own Subjective Experience</a>, <a href=\"/lw/59v/intuition_and_unconscious_learning/\">Intuition and Unconscious Learning</a>, <a href=\"/lw/5sk/inferring_our_desires/\">Inferring Our Desires</a>, <a href=\"http://commonsenseatheism.com/?p=15072\">Wrong About Our Own Desires</a>,&nbsp;<a href=\"/lw/6da/do_humans_want_things/\">Do Humans Want Things?</a>, <a href=\"/lw/65w/not_for_the_sake_of_pleasure_alone/\">Not for the Sake of Pleasure Alone</a>, <a href=\"/lw/6dz/not_for_the_sake_of_selfishness_alone/\">Not for the Sake of Selfishness Alone</a>,&nbsp;<a href=\"/lw/5bw/your_evolved_intuitions/\">Your Evolved Intuitions</a>, <a href=\"/lw/4vs/when_intuitions_are_useful/\">When Intuitions Are Useful</a>, <a href=\"http://commonsenseatheism.com/?p=15213\">Cornell Realism</a>, <a href=\"http://commonsenseatheism.com/?p=15253\">Railton's Moral Reductionism (Part 1)</a>, <a href=\"http://commonsenseatheism.com/?p=15264\">Railton's Moral Reductionism (Part 2)</a>, <a href=\"http://commonsenseatheism.com/?p=15267\">Jackson's Moral Functionalism</a>,&nbsp;<a href=\"http://commonsenseatheism.com/?p=15336\">Moral Reductionism and Moore's Open Question Argument</a>, and <a href=\"/lw/74f/are_deontological_moral_judgments_rationalizations/\">Are Deontological Moral Judgments Rationalizations?</a></small></p>\n<p><small><sup>2</sup> <a href=\"/lw/54p/heading_toward_nononsense_metaethics/\">Heading Toward: No-Nonsense Metaethics</a>, <a href=\"/lw/5eh/what_is_metaethics/\">What is Metaethics?</a>, <a href=\"/lw/5kn/conceptual_analysis_and_moral_theory/\">Conceptual Analysis and Moral Theory</a>, and <a href=\"/lw/5u2/pluralistic_moral_reductionism/\">Pluralistic Moral Reductionism</a>.</small></p>\n<p><small><sup>3</sup> I tried something similar before, with <a href=\"http://commonsenseatheism.com/?p=13607\">Cognitive Science in One Lesson</a>.</small></p>\n<p><small><sup>4</sup>&nbsp;Glimcher (2010) offers the best coverage of the topic in a single book. Tobler &amp; Kobayashi (2009) offer the best coverage in a single article.</small></p>\n<p><small><sup>5</sup>&nbsp;The quotes in this section are from Churchland (1981).</small></p>\n<p><small><sup>6</sup> Allen &amp; Ng (2004).</small></p>\n<p><small><sup>7</sup>&nbsp;This perspective goes back at least as far back as Arnauld (1662), who wrote:</small></p>\n<blockquote>\n<p><small>To judge what one must do to obtain a good or avoid an evil, it is necessary to consider not only the good and the evil in itself, but also the probability that it happens or does not happen: and to view geometrically the proportion that all these things have together.</small></p>\n</blockquote>\n<p><small><sup>8</sup>&nbsp;In addition to Caplin &amp; Leahy (2001), see Kreps &amp; Porteus'&nbsp;(1978, 1979)&nbsp;incroporation of the \"utility of knowing\", Loomes &amp; Sugden's (1982) incorporation of \"regret\",&nbsp;Gul &amp; Pesendorfer's (2001) incorporation of \"the cost of self-control\", and&nbsp;Koszegi &amp; Rabin's (2007, 2009) incorporation of the \"reference point\".</small></p>\n<p><small><sup>9</sup> Friedman (1953).</small></p>\n<p><small><sup>10</sup> See a review in Fox &amp; Poldrack (2009).</small></p>\n<p><small><sup>11</sup> For one difficulty with prospect theory, see Laury &amp; Holt (2008).</small></p>\n<p><small><sup>12</sup> Sutton &amp; Barto (2008), p. 3. All quotes from this section are from the early pages of this book.</small></p>\n<p><small><sup>13</sup>&nbsp;From Sutton &amp; Barto (2008).</small></p>\n<p><small><sup>14</sup>&nbsp;Much of the rest of this post is basically a summary and paraphrase of Glimcher (2010).</small></p>\n<p><small><sup>15</sup> Mirenowicz &amp; Schultz (1994).</small></p>\n<p><small><sup>16</sup>&nbsp;Schultz et al. (1997).</small></p>\n<p><small><sup>17</sup> Caplin &amp; Dean (2007)</small></p>\n<p><small><sup>18</sup> From Glimcher (2010).</small></p>\n<p><small><sup>19</sup> Hebb (1949).</small></p>\n<p><small><sup>20</sup> Malenka &amp; Bear (2004).</small></p>\n<p><small><sup>21</sup> Reynolds &amp; Wickens (2002).</small></p>\n<p><small><sup>22</sup> Glimcher (2010), p. 341.</small></p>\n<p><small><sup>23</sup>&nbsp;Edelman &amp; Keller (1996); Van Gisbergen et al. (1987).</small></p>\n<p><small><sup>24</sup>&nbsp;Gold and Shadlen (2007); Roitman and Shadlen (2002).</small></p>\n<p><small><sup>25</sup> Simon (1957).</small></p>\n<p><small><sup>26</sup>&nbsp;Glimcher (2010), p. 215.</small></p>\n<p><small><sup>27</sup>&nbsp;McFadden (2000). The behavior of gradually transitioning between two choices is described by Selten (1975).</small></p>\n<p><small><sup>28</sup>&nbsp;For a probably improved random utility model, see Gul &amp; Pesendorfer (2006).</small></p>\n<p><small><sup>29</sup> Dean (1983); Werner &amp; Mountcastle (1963).</small></p>\n<p><small><sup>30</sup>&nbsp;Unless some other feature of the brain turns out to 'smooth out' the stochasticity of neurons involved in valuation and choice-making.</small></p>\n<p><small><sup>31</sup> Glimcher (2010).</small></p>\n<p><small><sup>32</sup> Heeger (1992, 1993); Carandini &amp; Heeger (1994); Simoncelli &amp; Heeger (1998).</small></p>\n<p><small><sup>33</sup> Carandini &amp; Heeger (1994); Britten &amp; Heuer (1999); Zoccolan et al. (2005); Louie &amp; Glimcher (2010).</small></p>\n<p><small><sup>34</sup> Horowitz &amp; Newsome (2001a, 2001b, 2004).</small></p>\n<p><small><sup>35</sup> Liu &amp; Wang (2008).</small></p>\n<p><small><sup>36</sup>&nbsp;But, see Deneve (2009).</small></p>\n<p><small><sup>37</sup> Glimcher (2010), p. 281.</small></p>\n<p><small><sup>38</sup> Glimcher (2010), p. 283.</small></p>\n<p><small><sup>39</sup>&nbsp;This quote and the next quote are from Koszegi &amp; Rabin (2006).</small></p>\n<p><small><sup>40</sup> Glimcher (2010), p. 292.</small></p>\n<p><small><sup>41</sup> I won't list them all here. For an overview, see Glimcher (2010), ch. 14.</small></p>\n<p><small><sup>42</sup> Glimcher (2010), ch. 17. I've paraphrased his open questions. I also excluded his 6th question:&nbsp;What Is the Neural Organ for Representing Money?</small></p>\n<p><small><span style=\"font-size: 11px;\"><br></span></small></p>\n<h4 id=\"References\"><a name=\"References\"></a>References</h4>\n<p><small>Allais (1953). <a href=\"http://www.ericchiang.org/files/Allais_1953_Econometrica.pdf\">Le comportement de l'homme rationel devant le risque. Critique des postulates et axiomes de l'ecole americaine</a>. <em>Econometrica, 21</em>: 503-546.</small></p>\n<p><small>Allen &amp; Ng (2004). Economic behavior. In Spielberger (ed.), <em>Encyclopedia of Applied Psychology, Vol. 1</em>&nbsp;(pp. 661-666). Academic Press.</small></p>\n<p><small>Arnauld (1662). <em><a href=\"http://www.amazon.com/Port-Royal-Logic-Pierre-Nicole/dp/1142071251/\">Port-Royal Logic</a></em>.&nbsp;</small></p>\n<p><small>Basso &amp; Wurtz (1997). <a href=\"http://neuro.cjb.net/content/18/18/7519.full.pdf\">Modulation of neuronal activity in superior colliculus by changes in target probability</a>. <em>Journal of Neuroscience, 18</em>: 7519-7534.</small></p>\n<p><small>Britten &amp; Heuer (1999). <a href=\"http://neuro.cjb.net/content/19/12/5074.full.pdf\">Spatial summation in the receptive fields of MT neurons</a>. <em>Journal of Neuroscience, 19</em>: 5074-5084.</small></p>\n<p><small>Caplin &amp; Dean (2007). <a href=\"http://cess.nyu.edu/caplin/wp-content/uploads/2010/02/Axiomatic-Neuroeconomics.pdf\">Axiomatic neuroeconomics</a>.</small></p>\n<p><small>Caplin, Dean, Glimcher, &amp; Rutledge (2010). <a href=\"http://www.yorkshire-exile.co.uk/Beliefs.pdf\">Measuring beliefs and rewards: a neuroeconomic approach</a>. <em>Quarterly Journal of Economics, 125</em>: 3.</small></p>\n<p><small>Caplin &amp; Leahy (2001). <a href=\"http://pages.stern.nyu.edu/~dbackus/Exotic/1Other/CaplinLeahy%20antic%20QJE%2001.pdf\">Psychological expected utility theory and anticipatory feelings</a>. <em>Quarterly Journal of Economics, 116</em>: 55-79.</small></p>\n<p><small>Carandini &amp; Heeger (1994). <a href=\"http://redwood.berkeley.edu/vs265/carandini-heeger.pdf\">Summation and devision by neurons in primate visual cortex</a>. <em>Science, 264</em>: 1333-1336.</small></p>\n<p><small>Churchland (1981).&nbsp;<a href=\"http://philosophy.wisc.edu/Shapiro/Phil554/PAPERS/Churchland.pdf\">Eliminative materialism and the propositional attitudes</a>. <em>The Journal of Philosophy, 78</em>: 67-90.</small></p>\n<p><small>Dean (1983). Adaptation-induced alteration of the relation between response amplitude and contrast in cat striate cortical neurons. <em>Vision Research, 23</em>: 249-256.</small></p>\n<p><small>Deneve (2009).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Deneve-Bayesian-decision-making-in-two-alternative-forced-choices.pdf\">Bayesian decision making&nbsp;in two-alternative forced choices</a>.&nbsp;In Dreher &amp; Tremblay (eds.), <em>Handbook of Reward and Decision Making</em> (pp. 441-458). Academic Press.</small></p>\n<p><small>Dorris &amp; Glimcher (2004). <a href=\"http://monkeybiz.stanford.edu/nbio220/DorrisMC-Glimcher.pdf\">Activity in posterior parietal cortex is correlated with the subjective desireability of an action</a>. <em>Neuron, 44</em>: 365-378.</small></p>\n<p><small>Edelman &amp; Keller (1996). Activity of visuomotor burst neurons in the superior colliculus accompanying express saccades. <em>Journal of Neurophysiology, 76</em>: 908-926.</small></p>\n<p><small>Fox &amp; Poldrack (2009). <a href=\"http://cnl.salk.edu/~terry/BGGN/CH011.pdf\">Prospect theory and the brain</a>. In Glimcher, Camerer, Fehr, &amp; Poldrack (eds.), <em>Neuroeconomics: Decision Making and the Brain</em>&nbsp;(pp. 145-173). Academic Press.</small></p>\n<p><small>Friedman (1953). <em><a href=\"http://www.amazon.com/Essays-Positive-Economics-Phoenix-Books/dp/0226264033/\">Essays in Positive Economics</a></em>. University of Chicago Press.</small></p>\n<p><small>Glimcher (2010). <em><a href=\"http://www.amazon.com/Foundations-Neuroeconomic-Analysis-Paul-Glimcher/dp/0199744254/\">Foundations of Neuroeconomic Analysis</a></em>. Oxford University Press.</small></p>\n<p><small>Gold and Shadlen (2007). <a href=\"http://homepages.inf.ed.ac.uk/pseries/CCN/gold_shadlen_review.pdf\">The neural basis of decision making</a>. <em>Annual Review of Neuroscience, 30</em>: 535-574.</small></p>\n<p><small>Gul &amp; Pesendorfer (2001). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.160.133&amp;rep=rep1&amp;type=pdf\">Temptation and self-control</a>. <em>Econometrica, 69</em>: 1403-1435.</small></p>\n<p><small>Gul &amp; Pesendorfer (2006). <a href=\"http://www.princeton.edu/~pesendor/random.pdf\">Random expected utility</a>. <em>Econometrica, 74</em>: 121-146.</small></p>\n<p><small>Hebb (1949). <em><a href=\"http://www.amazon.com/Organization-Behavior-Neuropsychological-Theory/dp/0805843000/\">The organization of behavior</a></em>. Wiley &amp; Sons.</small></p>\n<p><small>Heeger (1992). <a href=\"http://www.cns.nyu.edu/labs/heegerlab/content/publications/Heeger-VisNeurosci1992a.pdf\">Normalization of cell responses in cat striate cortex</a>. <em>Visual Neuroscience, 9</em>: 181-197.</small></p>\n<p><small>Heeger (1993). <a href=\"http://www.cns.nyu.edu/labs/heegerlab/content/publications/Heeger-JNeurophysiol1993.pdf\">Modeling simple-cell direction selectivity with normalized, half-squared linear operators</a>. <em>Journal of Neurophysiology, 70</em>: 1885-1898.</small></p>\n<p><small>Horowitz &amp; Newsome (2001a). <a href=\"http://jn.physiology.org/content/86/5/2527.full.pdf\">Target selection for saccadic eye movements: direction selective visual responses in the superior colliculus induced by behavioral training</a>. <em>Journal of Neurophysiology, 86</em>: 2527-2542.</small></p>\n<p><small>Horowitz &amp; Newsome (2001b). <a href=\"http://jn.physiology.org/content/86/5/2543.full.pdf\">Target selection for saccadic eye movements: prelude activity in the superior colliculus during a direction discrimination task</a>. <em>Journal of Neurophysiology, 86</em>: 2543-2558.</small></p>\n<p><small>Iyengar &amp; Lepper (2000). <a href=\"http://www.columbia.edu/~ss957/articles/Choice_is_Demotivating.pdf\">When choice is demotivating: Can one desire too much of a good thing?</a> <em>Journal of Personality and Social Psychology, 79</em>: 995-1006.</small></p>\n<p><small>Jevons (1871).&nbsp;<em><a href=\"http://www.amazon.com/Theory-Political-Economy-William-Stanley/dp/1165162105/\">The Theory of Political&nbsp;Economy</a></em>. Macmillan and Co.</small></p>\n<p><small>Kahneman &amp; Tversky (1979). <a href=\"http://paper.blog.bbiq.jp/Kahneman_and_Tversky_1979.pdf\">Prospect theory: An analysis of decision under risk</a>. <em>Econometrica, 47</em>: 263-291.</small></p>\n<p><small>Koszegi &amp; Rabin (2006). <a href=\"http://webserver1.pugetsound.edu/facultypages/gmilam/courses/econ291/readings/01-KozegiRabin.pdf\">A model of reference-dependent preferences</a>. <em>Quarterly Journal of Economics, 121</em>: 1133-1165.</small></p>\n<p><small>Koszegi &amp; Rabin (2007). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.61.4476&amp;rep=rep1&amp;type=pdf\">Reference-dependent risk attitudes</a>. <em>American Economic Review, 97</em>: 1047-1073.</small></p>\n<p><small>Koszegi &amp; Rabin (2009). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.153.2065&amp;rep=rep1&amp;type=pdf\">Reference-dependent consumption plans</a>. <em>American Economic Review, 99</em>: 909-936.</small></p>\n<p><small>Kreps &amp; Porteus (1978). <a href=\"http://teaching.ust.hk/~bee/papers/040918/1978-Kreps_Porteus-dynamic_choice_theory.pdf\">Temporal resolution of uncertainty and dynamic choice theory</a>. <em>Econometrica, 46</em>: 185-200.</small></p>\n<p><small>Kreps &amp; Porteus (1979). <a href=\"http://hassler-j.iies.su.se/Courses/NewPrefs/Papers/TimeandRisk/KrepsPorteus%20DP%20Ec%20Jan%2079.pdf\">Dynamic choice theory and dynamic programming</a>. <em>Econometrica, 47</em>: 91-100.</small></p>\n<p><small>Laury &amp; Holt (2008). <a href=\"http://www2.gsu.edu/~ecoskl/lotteryhbk.pdf\">Payoff scale effects and risk preference under real and hypothetical conditions</a>. In Plott &amp; Smith (eds.), <em>Handbook of Experimental Economic Results, Vol. 1</em> (pp. 1047-1053). Elsevier Press.&nbsp;</small></p>\n<p><small>Loewenstein (1987). <a href=\"http://sds.hss.cmu.edu/media/pdfs/loewenstein/AnticipationValuDelayed.pdf\">Anticipation and the valuation of delayed consumption</a>. <em>Economic Journal, 97</em>: 666-684.</small></p>\n<p><small>Liu &amp; Wang (2008). <a href=\"http://biophy.nju.edu.cn/papers/Liu_PLoS_253_08.pdf\">A common cortical circuit mechanism for perceptual categorical discrimination and veridical judgment</a>. <em>PLOS Computational Biology, 4</em>: 1-14.</small></p>\n<p><small>Loomes &amp; Sugden (1982). <a href=\"http://teaching.ust.hk/~bee/papers/misc/Regret%20Theory%20An%20Alternative%20Theory%20of%20Rational%20Choice%20Under%20Uncertainty.pdf\">Regret theory: An alternative theory of rational choice under uncertainty</a>. <em>Economic Journal, 92</em>: 805-824.</small></p>\n<p><small>Louie &amp; Glimcher (2010). <a href=\"http://www.jneurosci.org/content/30/16/5498.full.pdf\">Separating value from choice: delay discounting activity in the lateral intraparietal area</a>. <em>Journal of Neuroscience, 30</em>: 5498-5507.</small></p>\n<p><small>Malenka &amp; Bear (2004). LTP and LTD: an embarrassment of riches. <em>Neuron, 44</em>: 5\u201321.</small></p>\n<p><small>Mirenowicz &amp; Schultz (1994). Importance of unpredictability for reward responses in primate dopamine neurons. <em>Journal of Neurophysiology, 72</em>: 1024-1027.</small></p>\n<p><small>Reynolds &amp; Wickens (2002). <a href=\"http://cnl.salk.edu/~terry/BGGN/reynolds.NN.02.pdf\">Dopamine-dependent plasticity of corticostriatal synapses</a>. <em>Neural Networks, 15</em>: 507-521.</small></p>\n<p><small>Roitman and Shadlen (2002). <a href=\"http://neuro.cjb.net/content/22/21/9475.full.pdf\">Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction time task</a>. <em>Nature Neuroscience, 22</em>: 9475-9489.</small></p>\n<p><small>Scheibehenne,&nbsp;Greifeneder, &amp; Todd (2010). Can there ever be too many options? A meta-analytic review of choice overload.&nbsp;<em>Journal of Consumer Research, 37</em>: 409-425.</small></p>\n<p><small>Schultz, Dayan, &amp; Montague (1997). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.124.5997&amp;rep=rep1&amp;type=pdf\">A neural substrate of prediction and reward</a>. <em>Science, 275</em>: 1593\u20131599.</small></p>\n<p><small>Schwartz &amp; Simoncelli (2001). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.29.3508&amp;rep=rep1&amp;type=pdf\">Natural signal statistics and sensory gain control</a>. <em>Nature Neuroscience, 4</em>: 819-825.</small></p>\n<p><small>Selten (1975). Reexamination of perfectness concept for equilibrium points in extensive games. <em>International Journal of Game Theory, 4</em>: 25-55.</small></p>\n<p><small>Simoncelli &amp; Heeger (1998). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.35.5397&amp;rep=rep1&amp;type=pdf\">A model of neuronal responses in visual area MT</a>. <em>Vision Research, 38</em>: 743-761.</small></p>\n<p><small>Sutton &amp; Barto (2008). <em><a href=\"http://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981/\">Reinforcement Learning: An Introduction</a></em>. MIT Press.</small></p>\n<p><small>Tanji &amp; Evarts (1976). <a href=\"http://jacknife.med.yale.edu/spikeclub/TanjiAnticipatoryJNeurophys1976.pdf\">Anticipatory activity of motor cortex neurons in relation to direction of an intended movement</a>. <em>Journal of Neurophysiology, 39</em>: 1062-1068.</small></p>\n<p><small>Tobler &amp; Kobayashi (2009). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Tobler-Kobayashi-Electrophysiological-correlates-of-reward-processing-in-dopamine-neurons.pdf\">Electrophysiological correlates of reward processing in dopamine neurons</a>. In Dreher &amp; Tremblay (eds.), <em>Handbook of Reward and Decision Making</em>&nbsp;(pp. 29-50). Academic Press.</small></p>\n<p><small>Van Gisbergen, Opstal, &amp; Tax (1987). Collicular ensemble coding of saccades based on vector summation. <em>Neuroscience, 21</em>: 651.</small></p>\n<p><small>Werner &amp; Mountcastle (1963). The variability of central neural activity in a sensory system, and its implications for central reflection of sensory events. <em>Journal of Neurophysiology, 26</em>: 958-977.</small></p>\n<p><small>Wickens (1993). <em><a href=\"http://www.amazon.com/Theory-Striatum-J-Wickens/dp/0080422780/\">A Theory of the Striatum</a></em>. Pergamon Press.</small></p>\n<p><small>Zoccolan, Cox, &amp; DiCarlo (2005). <a href=\"http://www.jneurosci.org/content/25/36/8150.full.pdf\">Multiple object response normalization in monkey inferotemporal cortex</a>. <em>Journal of Neuroscience, 25</em>: 8150-8164.</small></p>", "sections": [{"title": "Preface", "anchor": "Preface", "level": 1}, {"title": "Contents:", "anchor": "Contents_", "level": 1}, {"title": "Folk Psychology", "anchor": "Folk_Psychology", "level": 1}, {"title": "Neoclassical Economics", "anchor": "Neoclassical_Economics", "level": 1}, {"title": "Behaviorism and Reinforcement Learning", "anchor": "Behaviorism_and_Reinforcement_Learning", "level": 1}, {"title": "Reinforcement Learning and Decision Theory", "anchor": "Reinforcement_Learning_and_Decision_Theory", "level": 1}, {"title": "The Turn to the Brain", "anchor": "The_Turn_to_the_Brain", "level": 1}, {"title": "Hebbian Learning", "anchor": "Hebbian_Learning", "level": 1}, {"title": "Expected Utility in Neurons", "anchor": "Expected_Utility_in_Neurons", "level": 1}, {"title": "Real-Time Expected Utility Updates", "anchor": "Real_Time_Expected_Utility_Updates", "level": 1}, {"title": "Argmax and Reservation Price", "anchor": "Argmax_and_Reservation_Price", "level": 1}, {"title": "Random Utility", "anchor": "Random_Utility", "level": 1}, {"title": "Discounting", "anchor": "Discounting", "level": 1}, {"title": "Relative and Absolute Utility", "anchor": "Relative_and_Absolute_Utility", "level": 1}, {"title": "Normalization", "anchor": "Normalization", "level": 1}, {"title": "Are Actions Choices?", "anchor": "Are_Actions_Choices_", "level": 1}, {"title": "The Primate Choice Mechanism: A Brief Review", "anchor": "The_Primate_Choice_Mechanism__A_Brief_Review", "level": 1}, {"title": "Marginal Utility and Reference Dependence", "anchor": "Marginal_Utility_and_Reference_Dependence", "level": 1}, {"title": "Valuation in the Brain", "anchor": "Valuation_in_the_Brain", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "91 comments"}], "headingsCount": 23}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 91, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a7n8GdKiAZRX86T5A", "WnheMGAka4fL99eae", "f6ZLxEWaankRZ2Crv", "WBdvyyHLdxZSAMmoz", "NMoLJuDJEms7Ku9XS", "uHYYA32CKgKT3FagE", "dLbkrPu5STNCBLRjr", "Mc6QcrsbH5NRXbCRX", "yA4gF5KrboK2m2Xu7", "AdYdLP2sRqPMoe8fb", "HLqWn5LASfhhArZ7w", "33KewgYhNSxFpbpXg", "2G7AH92pHyj3nC32T", "d5NyJ2Lf6N22AD9PB", "LQp9cZPzJncFKh5c8", "9fpWoXpNv83BAHJdc", "EgDpZS4HHeh5vqJPe", "EMJ3egz48BtZS8Pws", "7dRGYDqA2z6Zt7Q4h", "gTNB9CQd5hnbkMxAG", "XPErvb8m9FapXCjhA", "nBdaTGoDAYxHePSDa", "FwiPfF8Woe5JrzqEu", "oTX2LXHqXqYg2u4g6", "3v24wGePcdSGB3i8a", "zThWT5Zvifo5qYaca", "48DTJkBH58JbBNSFH", "du395YvCnQXBPSJax", "J55XeCNeF7wNwgCj9", "6Cc3TWZjAnrNWokWY", "87mdaCvCyo5bkk8hE", "CkW3ambqR9MRaspw7", "WTS4ZbEwvKrcrnaaN", "myLSqHNgi6BumABvE", "62p74DvwNHgQXCXcH", "SFnfhJkGsBQk8jakK", "s4Mcg9aLMeRwdW7fh", "2YPbdHgcjt7g5ZaFN", "3zDX3f3QTepNeZHGc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-19T23:26:35.422Z", "modifiedAt": null, "url": null, "title": "Table of biases, the normative models they violate, and their explanations", "slug": "table-of-biases-the-normative-models-they-violate-and-their", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:05.782Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mPbrjPKv28s7y6ZzS/table-of-biases-the-normative-models-they-violate-and-their", "pageUrlRelative": "/posts/mPbrjPKv28s7y6ZzS/table-of-biases-the-normative-models-they-violate-and-their", "linkUrl": "https://www.lesswrong.com/posts/mPbrjPKv28s7y6ZzS/table-of-biases-the-normative-models-they-violate-and-their", "postedAtFormatted": "Friday, August 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Table%20of%20biases%2C%20the%20normative%20models%20they%20violate%2C%20and%20their%20explanations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATable%20of%20biases%2C%20the%20normative%20models%20they%20violate%2C%20and%20their%20explanations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmPbrjPKv28s7y6ZzS%2Ftable-of-biases-the-normative-models-they-violate-and-their%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Table%20of%20biases%2C%20the%20normative%20models%20they%20violate%2C%20and%20their%20explanations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmPbrjPKv28s7y6ZzS%2Ftable-of-biases-the-normative-models-they-violate-and-their", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmPbrjPKv28s7y6ZzS%2Ftable-of-biases-the-normative-models-they-violate-and-their", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 12, "htmlBody": "<p>The title says it all: <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Table-of-biases.pdf\">PDF</a>. From Baron's&nbsp;<em><a href=\"http://www.amazon.com/Thinking-Deciding-Jonathan-Baron/dp/0521680433/\">Thinking and Deciding, 4th edition</a></em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mPbrjPKv28s7y6ZzS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 24, "extendedScore": null, "score": 7.573783719596001e-07, "legacy": true, "legacyId": "9309", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-20T02:12:33.019Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Why is the Future So Absurd?", "slug": "seq-rerun-why-is-the-future-so-absurd", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:05.732Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/spje9qfAKxeDoXgff/seq-rerun-why-is-the-future-so-absurd", "pageUrlRelative": "/posts/spje9qfAKxeDoXgff/seq-rerun-why-is-the-future-so-absurd", "linkUrl": "https://www.lesswrong.com/posts/spje9qfAKxeDoXgff/seq-rerun-why-is-the-future-so-absurd", "postedAtFormatted": "Saturday, August 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Why%20is%20the%20Future%20So%20Absurd%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Why%20is%20the%20Future%20So%20Absurd%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fspje9qfAKxeDoXgff%2Fseq-rerun-why-is-the-future-so-absurd%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Why%20is%20the%20Future%20So%20Absurd%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fspje9qfAKxeDoXgff%2Fseq-rerun-why-is-the-future-so-absurd", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fspje9qfAKxeDoXgff%2Fseq-rerun-why-is-the-future-so-absurd", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 221, "htmlBody": "<p>Today's post, <a href=\"/lw/j6/why_is_the_future_so_absurd/\">Why is the Future So Absurd?</a> was originally published on 07 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>New technologies and social changes have consistently happened at a rate that would seem absurd and impossible to people only a few decades before they happen. Hindsight bias causes us to see the past as obvious and as a series of changes towards the \"normalcy\" of the present; availability biases make it hard for us to imagine changes greater than those we've already encountered, or the effects of multiple changes. The future will be stranger than we think.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/769/seq_rerun_availability/\">Availability</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "spje9qfAKxeDoXgff", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 7.574316949178407e-07, "legacy": true, "legacyId": "9312", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ga2HSwf9iQe64JwAa", "yrYBNKWTmKZQZ4cAG", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-20T03:52:30.709Z", "modifiedAt": null, "url": null, "title": "Meetup : Irvine Meetup Wednesday August 24", "slug": "meetup-irvine-meetup-wednesday-august-24", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:05.643Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JGWeissman", "createdAt": "2009-04-01T04:43:56.740Z", "isAdmin": false, "displayName": "JGWeissman"}, "userId": "Mw8rsM7m7E8nnEFEp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TsYyag63XSoTtqKCg/meetup-irvine-meetup-wednesday-august-24", "pageUrlRelative": "/posts/TsYyag63XSoTtqKCg/meetup-irvine-meetup-wednesday-august-24", "linkUrl": "https://www.lesswrong.com/posts/TsYyag63XSoTtqKCg/meetup-irvine-meetup-wednesday-august-24", "postedAtFormatted": "Saturday, August 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Irvine%20Meetup%20Wednesday%20August%2024&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Irvine%20Meetup%20Wednesday%20August%2024%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTsYyag63XSoTtqKCg%2Fmeetup-irvine-meetup-wednesday-august-24%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Irvine%20Meetup%20Wednesday%20August%2024%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTsYyag63XSoTtqKCg%2Fmeetup-irvine-meetup-wednesday-august-24", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTsYyag63XSoTtqKCg%2Fmeetup-irvine-meetup-wednesday-august-24", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 93, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2i'>Irvine Meetup Wednesday August 24</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 August 2011 06:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">4187 Campus Dr, University Center, Irvine, CA 92612</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This continues the weekly meetups in Irvine. As always the meetup at the outdoor food court in the <a href=\"http://maps.google.com/maps?ie=UTF8&amp;ll=33.650288,-117.838666&amp;spn=0.001684,0.002363&amp;t=h&amp;z=19\" rel=\"nofollow\">University Center near UCI</a>, from 6:00 to 8:00 (or whenever we actually decide to leave). Look for the sign with <a href=\"http://lesswrong.com/lw/nn/neural_categories/\">naive neural classifiers for bleggs and rubes</a>. See also the <a href=\"http://groups.google.com/group/LW-SoCal-Announce?pli=1\" rel=\"nofollow\">email group</a> and <a href=\"https://www.google.com/calendar/embed?src=h57ej586rdo3jmld14hrk51m1c%40group.calendar.google.com&amp;ctz=America/Los_Angeles\" rel=\"nofollow\">calendar</a> for the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California Meetup Group</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2i'>Irvine Meetup Wednesday August 24</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TsYyag63XSoTtqKCg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 7.574638157132667e-07, "legacy": true, "legacyId": "9318", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Irvine_Meetup_Wednesday_August_24\">Discussion article for the meetup : <a href=\"/meetups/2i\">Irvine Meetup Wednesday August 24</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 August 2011 06:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">4187 Campus Dr, University Center, Irvine, CA 92612</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This continues the weekly meetups in Irvine. As always the meetup at the outdoor food court in the <a href=\"http://maps.google.com/maps?ie=UTF8&amp;ll=33.650288,-117.838666&amp;spn=0.001684,0.002363&amp;t=h&amp;z=19\" rel=\"nofollow\">University Center near UCI</a>, from 6:00 to 8:00 (or whenever we actually decide to leave). Look for the sign with <a href=\"http://lesswrong.com/lw/nn/neural_categories/\">naive neural classifiers for bleggs and rubes</a>. See also the <a href=\"http://groups.google.com/group/LW-SoCal-Announce?pli=1\" rel=\"nofollow\">email group</a> and <a href=\"https://www.google.com/calendar/embed?src=h57ej586rdo3jmld14hrk51m1c%40group.calendar.google.com&amp;ctz=America/Los_Angeles\" rel=\"nofollow\">calendar</a> for the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California Meetup Group</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Irvine_Meetup_Wednesday_August_241\">Discussion article for the meetup : <a href=\"/meetups/2i\">Irvine Meetup Wednesday August 24</a></h2>", "sections": [{"title": "Discussion article for the meetup : Irvine Meetup Wednesday August 24", "anchor": "Discussion_article_for_the_meetup___Irvine_Meetup_Wednesday_August_24", "level": 1}, {"title": "Discussion article for the meetup : Irvine Meetup Wednesday August 24", "anchor": "Discussion_article_for_the_meetup___Irvine_Meetup_Wednesday_August_241", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yFDKvfN6D87Tf5J9f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-20T09:50:23.686Z", "modifiedAt": null, "url": null, "title": "What a practical plan for Friendly AI looks like", "slug": "what-a-practical-plan-for-friendly-ai-looks-like", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:06.043Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kv7gvQ9AkDisCL2kg/what-a-practical-plan-for-friendly-ai-looks-like", "pageUrlRelative": "/posts/kv7gvQ9AkDisCL2kg/what-a-practical-plan-for-friendly-ai-looks-like", "linkUrl": "https://www.lesswrong.com/posts/kv7gvQ9AkDisCL2kg/what-a-practical-plan-for-friendly-ai-looks-like", "postedAtFormatted": "Saturday, August 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20a%20practical%20plan%20for%20Friendly%20AI%20looks%20like&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20a%20practical%20plan%20for%20Friendly%20AI%20looks%20like%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkv7gvQ9AkDisCL2kg%2Fwhat-a-practical-plan-for-friendly-ai-looks-like%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20a%20practical%20plan%20for%20Friendly%20AI%20looks%20like%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkv7gvQ9AkDisCL2kg%2Fwhat-a-practical-plan-for-friendly-ai-looks-like", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkv7gvQ9AkDisCL2kg%2Fwhat-a-practical-plan-for-friendly-ai-looks-like", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1485, "htmlBody": "<p>I have seen too many discussions of Friendly AI, here and elsewhere (e.g. in comments at Michael Anissimov's blog), detached from any concrete idea of how to do it. Sometimes the issue is the lack of code, demos, or a practical plan from SIAI. SIAI is seen as a source of wishful thinking about magic machines that will solve all our problems for us, or as a place engaged in a forever quest for nebulous mathematical vaporware such as \"reflective decision theory\". You will get singularity enthusiasts who say, it's great that SIAI has given the concept of FAI visibility, but enough with the philosophy, let's get coding! ... does anyone know where to start? And you will get singularity skeptics who say, unfriendly AI is a bedtime ghost story for credulous SF fans, wake me up when SIAI actually ships a product. Or, within this subculture of rationalist altruists who want to do the optimal thing, you'll get people saying, I don't know if I should donate, because I don't see how any of this is supposed to happen.</p>\n<p>So in this post I want to sketch what a \"practical\" plan for Friendly AI looks like. I'm not here to <em>advocate</em> this plan - I'm not saying this is the right way to do it. I'm just providing an example of a plan that could be pursued in the real world. Perhaps it will also allow people to better understand SIAI's indirect approach.</p>\n<p>I won't go into the details of financial or technical logistics. If we were talking about how to get to the moon from Earth, then the following plan is along the lines of \"Make a chemical-powered rocket big enough to get you there.\" Once you have that concept, you still have a lot of work to do, but you are at least on the right track - compared to people who want to make a teleportation device or a balloon that goes really high. But I will make one remark about how the idea of Friendly AI is framed. At present, it is discussed in conjunction with a whole cornucopia of science fiction notions such as: immortality, conquering the galaxy, omnipresent wish-fulfilling super-AIs, good and bad Jupiter-brains, mind uploads in heaven and hell, and so on. Similarly, we have all these thought-experiments: guessing games with omniscient aliens, decision problems in a branching multiverse, \"torture versus dust specks\". Whatever the ultimate relevance of such ideas, it is clearly possible to divorce the notion of Friendly AI from all of them. If a FAI project was trying to garner mass support, it first needs to be comprehensible, and the simple approach would be to say it is simply an exercise in creating artificial intelligence that does the right thing. Nothing about utopia; nothing about <em>dystopia</em> caused by unfriendly AI; nothing about godlike superintelligence; just the scenario, already familiar in popular culture, of robots, androids, computers you can talk with. All that is coming, says the practical FAI project, and we are here to design these new beings so they will be good citizens, a positive rather than a negative addition to the world.</p>\n<p>So much for how the project describes itself to the world at large. What are its guiding technical conceptions? What's the specific proposal which will allow educated skeptics to conclude that this might get off the ground? Remember that there are two essential challenges to overcome: the project has to create intelligence, and it has to create <em>ethical</em> intelligence; what we call, in our existing discussions, \"AGI\" - artificial general intelligence - and \"FAI\" - friendly artificial intelligence.</p>\n<p>There is a very simple approach which - like the idea of a chemical-powered rocket which gets you to the moon - should be sufficient to get you to FAI, when sufficiently elaborated. It can be seen by stripping away some of the complexities peculiar to SIAI's strategy, complexities which tend to dominate the discussion. The basic idea should also be thoroughly familiar. We are to conceive of the AI as having two parts, a goal system and a problem-solving system. AGI is achieved by creating a problem-solving system of sufficient power and universality; FAI is achieved by specifying the right goal system.</p>\n<p>SIAI, in discussing the quest for the right goal system, emphasizes the difficulties of this process and the unreliability of human judgment. Their idea of a solution is to use artificial intelligence to neuroscientifically deduce the actual algorithmic structure of human decision-making, and to then employ a presently nonexistent branch of decision theory to construct a goal system embodying ideals implicit in the unknown human cognitive algorithms.</p>\n<p>The practical approach would not bother with this attempt to outsource the task of designing the AI's morality, to a presently nonexistent neuromathematical cognitive bootstrap process. While fully cognizant of the fact that value is complex, as eloquently attested by Eliezer in many speeches, the practical FAI project would nonetheless choose the AI's goal system in the old-fashioned way, by human deliberation and consensus. You would get a team of professional ethicists, some worldly people like managers, some legal experts in the formulation of contracts, and together you would hammer out a mission statement for the AI. Then you would get your programmers and your cognitive scientists to implement that goal condition in a way such that the symbols have the meanings that they are supposed to have. End of story.</p>\n<p>So far, all we've done is to make a wish. We've decided, after appropriate deliberation, what to wish for, and we have found a way to represent it in symbols. All that means nothing if we can't create AGI, the problem solver with at least a human level of intelligence. Here again, SIAI comes in for a lot of criticism, from two angles: it's said to have no ideas about how to create AGI, and it's said to actively discourage work on AGI, on the grounds that we need to solve the FAI problem first. Instead, it only discusses hopelessly impractical models of cognition like AIXI and exact Bayesian inference, that are mostly of theoretical interest.</p>\n<p>Our practical FAI project has \"solved\" FAI by simply coming to an agreement on what to wish for, and by studying with legalistic care how to avoid pitfalls and loopholes in the finer details of the wish; but what is its approach to the hard technical problem of AGI? The answer is, first of all, heuristics and incremental improvement. Projects like Lenat's Cyc are on the right track. A newborn AI has to be seeded with useful knowledge, including useful knowledge of problem-solving methods. It doesn't have time to discover such things entirely unaided. We should not imagine AGI developing just from a simple architecture, like Schmidhuber's G&ouml;del machine, but from a basic architecture plus a large helping of facts and heuristics which are meant to give it a head start.</p>\n<p>So fine, the practical approach to AGI isn't a search for a single killer concept, it's a matter of incrementally increasing the power of a general-purpose problem solver with many diverse ingredients in its design, so that it becomes more and more capable and independent. Ben Goertzel's approach to AGI exhibits the sort of eclectic pluralism that I have in mind. Still, we do need a selling point, something which shows that we're different, that we're aiming for the stars and we have a plan to get there.</p>\n<p>Here, I want to use Steve Omohundro's paper <a href=\"http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/\">\"The Basic AI Drives\"</a> in a slightly unusual way. The paper lists a number of behaviors that should be exhibited by a sufficiently sophisticated AI: it will try to model its own operation, clarify its goals, protect them from modification, protect itself from destruction, acquire resources and use them efficiently... The twist I propose is that Omohundro's list of drives should be used as a design specification. If your goal is AGI, then you <em>want</em> a cognitive architecture that will exhibit these emergent behaviors. They offer a series of milestones for your theorists and developers: a criterion of progress, and a set of intermediate goals sufficient to bridge the gap between a blank-slate beginning and an open-ended problem solver.</p>\n<p>That's the whole plan. It's an anticlimax, I know, for anyone who might have imagined that there was a magic formula for superintelligence coming at the end of this post. But I do claim that what I have described is the skeleton of a plan which can be fleshed out, and which, if it was fleshed out and pursued, would produce goal-directed AGI. Whether the project as I have described it would really produce \"friendly\" AI is another matter. Anyone versed in the folk wisdom about FAI should be able to point out multiple points of potential failure. But I hope this makes it a little clearer, to people who just don't see how FAI is supposed to happen at all, how it might be pursued in the real world.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kv7gvQ9AkDisCL2kg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": -1, "extendedScore": null, "score": 7.575788348151338e-07, "legacy": true, "legacyId": "9326", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-20T18:00:43.174Z", "modifiedAt": null, "url": null, "title": "London meetup, Sunday 2011-08-21 14:00, near Holborn", "slug": "london-meetup-sunday-2011-08-21-14-00-near-holborn", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6npWLhhe3pEoZi7rx/london-meetup-sunday-2011-08-21-14-00-near-holborn", "pageUrlRelative": "/posts/6npWLhhe3pEoZi7rx/london-meetup-sunday-2011-08-21-14-00-near-holborn", "linkUrl": "https://www.lesswrong.com/posts/6npWLhhe3pEoZi7rx/london-meetup-sunday-2011-08-21-14-00-near-holborn", "postedAtFormatted": "Saturday, August 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20London%20meetup%2C%20Sunday%202011-08-21%2014%3A00%2C%20near%20Holborn&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALondon%20meetup%2C%20Sunday%202011-08-21%2014%3A00%2C%20near%20Holborn%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6npWLhhe3pEoZi7rx%2Flondon-meetup-sunday-2011-08-21-14-00-near-holborn%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=London%20meetup%2C%20Sunday%202011-08-21%2014%3A00%2C%20near%20Holborn%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6npWLhhe3pEoZi7rx%2Flondon-meetup-sunday-2011-08-21-14-00-near-holborn", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6npWLhhe3pEoZi7rx%2Flondon-meetup-sunday-2011-08-21-14-00-near-holborn", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 23, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">We're meeting up in London tomorrow</span><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">.&nbsp;Sunday 21st August, at 2pm, in the&nbsp;</span><a style=\"color: #6a8a6b; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" href=\"http://maps.google.co.uk/maps?f=q&amp;source=s_q&amp;hl=en&amp;geocode=&amp;q=shakespeare's+head&amp;sll=51.42559,-0.130394&amp;sspn=0.011854,0.020943&amp;ie=UTF8&amp;hq=shakespeare's+head&amp;hnear=&amp;layer=c&amp;cbll=51.516734,-0.119933&amp;panoid=kXPwAeowAo9LzJJA34agOw&amp;cbp=11,76.42,,1,-1.06&amp;ll=51.516728,-0.124025&amp;spn=0.005622,0.022488&amp;z=16\">Shakespeares Head</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;(</span><a style=\"color: #6a8a6b; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" href=\"http://www.jdwetherspoon.co.uk/home/pubs/shakespeares-head\">official page</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">) on Kingsway near Holborn Tube station. See you there!</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6npWLhhe3pEoZi7rx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.577364693104069e-07, "legacy": true, "legacyId": "9329", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-21T02:56:13.957Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Anchoring and Adjustment", "slug": "seq-rerun-anchoring-and-adjustment", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/D4ujrpEvXNbXTzWYt/seq-rerun-anchoring-and-adjustment", "pageUrlRelative": "/posts/D4ujrpEvXNbXTzWYt/seq-rerun-anchoring-and-adjustment", "linkUrl": "https://www.lesswrong.com/posts/D4ujrpEvXNbXTzWYt/seq-rerun-anchoring-and-adjustment", "postedAtFormatted": "Sunday, August 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Anchoring%20and%20Adjustment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Anchoring%20and%20Adjustment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD4ujrpEvXNbXTzWYt%2Fseq-rerun-anchoring-and-adjustment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Anchoring%20and%20Adjustment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD4ujrpEvXNbXTzWYt%2Fseq-rerun-anchoring-and-adjustment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD4ujrpEvXNbXTzWYt%2Fseq-rerun-anchoring-and-adjustment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<p>Today's post, <a href=\"/lw/j7/anchoring_and_adjustment/\">Anchoring and Adjustment</a> was originally published on 07 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Exposure to numbers affects guesses on estimation problems by anchoring your mind to an given estimate, even if it's wildly off base. Be aware of the effect random numbers have on your estimation ability.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/76o/seq_rerun_why_is_the_future_so_absurd/\">Why is the Future So Absurd?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "D4ujrpEvXNbXTzWYt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 7.579086976646281e-07, "legacy": true, "legacyId": "9333", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bMkCEZoBNhgRBtzoj", "spje9qfAKxeDoXgff", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-21T10:37:50.151Z", "modifiedAt": null, "url": null, "title": "For fiction: How could alien minds differ from human minds?", "slug": "for-fiction-how-could-alien-minds-differ-from-human-minds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:07.252Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Solvent", "createdAt": "2011-07-19T07:12:44.132Z", "isAdmin": false, "displayName": "Solvent"}, "userId": "a3sBsZXtAQacMDHfK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r6jcRKjvFP4LZzBWg/for-fiction-how-could-alien-minds-differ-from-human-minds", "pageUrlRelative": "/posts/r6jcRKjvFP4LZzBWg/for-fiction-how-could-alien-minds-differ-from-human-minds", "linkUrl": "https://www.lesswrong.com/posts/r6jcRKjvFP4LZzBWg/for-fiction-how-could-alien-minds-differ-from-human-minds", "postedAtFormatted": "Sunday, August 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20For%20fiction%3A%20How%20could%20alien%20minds%20differ%20from%20human%20minds%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFor%20fiction%3A%20How%20could%20alien%20minds%20differ%20from%20human%20minds%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr6jcRKjvFP4LZzBWg%2Ffor-fiction-how-could-alien-minds-differ-from-human-minds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=For%20fiction%3A%20How%20could%20alien%20minds%20differ%20from%20human%20minds%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr6jcRKjvFP4LZzBWg%2Ffor-fiction-how-could-alien-minds-differ-from-human-minds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr6jcRKjvFP4LZzBWg%2Ffor-fiction-how-could-alien-minds-differ-from-human-minds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 224, "htmlBody": "<p>One of the most important points raised by the sequences is that not all minds are like humans. In quite a few places, people have discussed minds with slight changes from human minds, which seem altogether different. However, a lot of this discussion has been related to AI, as opposed to minds created by evolution. I'm trying to think of ways that minds which evolved, and are effective enough to start a civilization, could differ from humans'.</p>\n<p>Three Worlds Collide would seem like an excellent starting point, but isn't actually very useful. As far as I recall, the Babyeaters might have learned their baby eating habits as a result of societal pressure. The main difference in their society seemed to be the assumption that people who disagreed with you were simply mistaken: this contrasts to humans' tendency to form rival groups, and assume everyone in the rival groups is evil. The Super-Happies had self modified, and so don't provide an example of an evolved mind.</p>\n<p>So here are my ideas so far.</p>\n<p>\n<ul>\n<li>A species could have the same neural pathways for wanting and liking. This would lead to far less akrasia.</li>\n<li>A species could have a different set of standards for boredom. This seems to be one of the most precarious values in the human mind.</li>\n</ul>\n<div>What other ways can you think of?</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NGtNzdS88JtEQdRP4": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r6jcRKjvFP4LZzBWg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 13, "extendedScore": null, "score": 7.580572112118309e-07, "legacy": true, "legacyId": "9334", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-21T12:35:48.430Z", "modifiedAt": "2020-04-10T17:47:03.821Z", "url": null, "title": "Please do not downvote every comment or post someone has ever made as a retaliation tactic.", "slug": "please-do-not-downvote-every-comment-or-post-someone-has", "viewCount": null, "lastCommentedAt": "2013-06-09T17:50:22.762Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JrsbFTp5fHXFJXFdQ/please-do-not-downvote-every-comment-or-post-someone-has", "pageUrlRelative": "/posts/JrsbFTp5fHXFJXFdQ/please-do-not-downvote-every-comment-or-post-someone-has", "linkUrl": "https://www.lesswrong.com/posts/JrsbFTp5fHXFJXFdQ/please-do-not-downvote-every-comment-or-post-someone-has", "postedAtFormatted": "Sunday, August 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Please%20do%20not%20downvote%20every%20comment%20or%20post%20someone%20has%20ever%20made%20as%20a%20retaliation%20tactic.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APlease%20do%20not%20downvote%20every%20comment%20or%20post%20someone%20has%20ever%20made%20as%20a%20retaliation%20tactic.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJrsbFTp5fHXFJXFdQ%2Fplease-do-not-downvote-every-comment-or-post-someone-has%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Please%20do%20not%20downvote%20every%20comment%20or%20post%20someone%20has%20ever%20made%20as%20a%20retaliation%20tactic.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJrsbFTp5fHXFJXFdQ%2Fplease-do-not-downvote-every-comment-or-post-someone-has", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJrsbFTp5fHXFJXFdQ%2Fplease-do-not-downvote-every-comment-or-post-someone-has", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 147, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; background-color: #f7f7f8;\"> </span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">People who go back and downvote every post or comment a Less Wrong user has ever made, please, stop doing that. It's a clever way to pull information cascades in your direction but it is clearly an abuse of the content filtering system. It's also highly dishonorable. If you truly must use such tactics then downvoting a few of your enemy's top level posts is much less evil; your enemy loses the karma and takes the hint without your severely biasing the public perception of Less Wrong's discourse.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">(I just lost over 200 karma in a few minutes and that'll probably continue for awhile. This happens to me every few weeks. Edit: I mean it's been happening every few weeks for a few months for a total of only three or four. Between 400 and 700 karma lost total I think? I don't mean to overstate the problem.)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JrsbFTp5fHXFJXFdQ", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 66, "baseScore": 56, "extendedScore": null, "score": 0.000121, "legacy": true, "legacyId": "9335", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 130, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2011-08-21T12:35:48.430Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-21T19:44:37.093Z", "modifiedAt": null, "url": null, "title": "synapse renormalization - another reason to sleep more than minimum-REM", "slug": "synapse-renormalization-another-reason-to-sleep-more-than", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:46.133Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jonathan_Graehl", "createdAt": "2009-02-27T23:21:15.671Z", "isAdmin": false, "displayName": "Jonathan_Graehl"}, "userId": "eKsWtKKceoRYwcc7s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C7QvtSJgYzFxnB2sz/synapse-renormalization-another-reason-to-sleep-more-than", "pageUrlRelative": "/posts/C7QvtSJgYzFxnB2sz/synapse-renormalization-another-reason-to-sleep-more-than", "linkUrl": "https://www.lesswrong.com/posts/C7QvtSJgYzFxnB2sz/synapse-renormalization-another-reason-to-sleep-more-than", "postedAtFormatted": "Sunday, August 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20synapse%20renormalization%20-%20another%20reason%20to%20sleep%20more%20than%20minimum-REM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Asynapse%20renormalization%20-%20another%20reason%20to%20sleep%20more%20than%20minimum-REM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC7QvtSJgYzFxnB2sz%2Fsynapse-renormalization-another-reason-to-sleep-more-than%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=synapse%20renormalization%20-%20another%20reason%20to%20sleep%20more%20than%20minimum-REM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC7QvtSJgYzFxnB2sz%2Fsynapse-renormalization-another-reason-to-sleep-more-than", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC7QvtSJgYzFxnB2sz%2Fsynapse-renormalization-another-reason-to-sleep-more-than", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<p>(not yet studied in mammals)</p>\n<p>The ratio of the strength of a synapse between neurons and the total potential (from all incoming synapses) needed to activate a neuron may be all that figures; the absolute values may not be important (this is the basis for computer&nbsp;<a href=\"http://en.wikipedia.org/wiki/Neural_network\">neural networks</a>, though temporal effects, firing rates, and who knows what else also matter in real brains).</p>\n<p>So you can renormalize (multiply by some constant 1) and see almost no difference except for perhaps greater susceptibility to noise. But at least the amount of physical material needed is smaller, and the energy needed is smaller. It's more efficient.</p>\n<p>In flies and other simple animals studied so far, this definitely happens during sleep. Maybe it happens in humans also. (remains to be studied).</p>\n<p>In any case, be careful committing to some REM-sleep only 3hr/day-with-naps sleep schedule, just because you may feel fine at first, when the exact utility of non-REM sleep isn't completely known.</p>\n<p><a href=\"http://neuroskeptic.blogspot.com/2011/08/is-sleep-brain-defragmentation.html\">Via.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C7QvtSJgYzFxnB2sz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 7.582331964232657e-07, "legacy": true, "legacyId": "9338", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-21T21:29:31.173Z", "modifiedAt": null, "url": null, "title": "A Valuable Asset in Your Intellectual Portfolio is Not the Same as a Good Guide", "slug": "a-valuable-asset-in-your-intellectual-portfolio-is-not-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:06.337Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_J_Balan", "createdAt": "2009-06-21T18:40:19.064Z", "isAdmin": false, "displayName": "David_J_Balan"}, "userId": "LkRB9E4GWd7hRa7zt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/47b7Gp22x4yK9rTLT/a-valuable-asset-in-your-intellectual-portfolio-is-not-the", "pageUrlRelative": "/posts/47b7Gp22x4yK9rTLT/a-valuable-asset-in-your-intellectual-portfolio-is-not-the", "linkUrl": "https://www.lesswrong.com/posts/47b7Gp22x4yK9rTLT/a-valuable-asset-in-your-intellectual-portfolio-is-not-the", "postedAtFormatted": "Sunday, August 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Valuable%20Asset%20in%20Your%20Intellectual%20Portfolio%20is%20Not%20the%20Same%20as%20a%20Good%20Guide&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Valuable%20Asset%20in%20Your%20Intellectual%20Portfolio%20is%20Not%20the%20Same%20as%20a%20Good%20Guide%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F47b7Gp22x4yK9rTLT%2Fa-valuable-asset-in-your-intellectual-portfolio-is-not-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Valuable%20Asset%20in%20Your%20Intellectual%20Portfolio%20is%20Not%20the%20Same%20as%20a%20Good%20Guide%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F47b7Gp22x4yK9rTLT%2Fa-valuable-asset-in-your-intellectual-portfolio-is-not-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F47b7Gp22x4yK9rTLT%2Fa-valuable-asset-in-your-intellectual-portfolio-is-not-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 432, "htmlBody": "<p>Haven't posted in quite a while.</p>\n<p>&nbsp;</p>\n<p>Suppose you have a big, complicated question that you're not sure of the answer to, and you want to seek an adviser to guide you. One kind of adviser is someone whose opinion, by your lights, constitutes strong evidence regarding the answer; on the basis of that opinion alone you are prepared to substantially update your beliefs. Of course you may profit from further discussion beyond just hearing the adviser's opinion on the big question: since the question is complicated, hearing his or her reasoning or evidence on different elements of the big question may be valuable, but the point is that there are some advisers for whom just knowing their ultimate judgment moves the needle a lot for you. Such people might be termed \"good guides.\"<br /><br />But there may be other potential advisers whose ultimate opinion on the big question you don't credit much at all, but who you think might still have valuable insight into some important element of the question. A good example for me is \"Chicago School\" Industrial Organization Economics. It's members had some insights that are absolutely true and important (\"one monopoly profit\" and related ideas), and that the people who I would have regarded as my \"good guides\" had I been around at the time did not have before them. No analyst who does not understand those insights can be a good analyst, and no analysis that ignores them can be correct. But simply knowing what an orthodox Chicago School economist thinks about some big question would move me very little. They are a valuable part of my \"intellectual portfolio\" (to use a phrase favored by <a href=\"http://www.google.com/#pq=potato&amp;hl=en&amp;cp=36&amp;gs_id=2t&amp;xhr=t&amp;q=Brad+DeLong+%22intellectual+portfolio%22&amp;qe=QnJhZCBEZUxvbmcgImludGVsbGVjdHVhbCBwb3J0Zm9saW8i&amp;qesig=byvKLDHl5MWzdx1N8dwA0g&amp;pkc=AFgZ2tkzRd_3TYManJdk8bmK61qIzPikIPvsad6xou-5HsHfp_UJKLKb1lOAiE2kE1G463im01jUlNsbpo9xcyPn5SfnaaZpLw&amp;pf=p&amp;sclient=psy&amp;source=hp&amp;pbx=1&amp;oq=Brad+DeLong+%22intellectual+portfolio%22&amp;aq=f&amp;aqi=&amp;aql=&amp;gs_sm=&amp;gs_upl=&amp;bav=on.2,or.r_gc.r_pw.&amp;fp=d31e9716c2482842&amp;biw=853&amp;bih=462\">Brad DeLong</a>) and I would be a fool to dismiss them. But they are only providers of valuable input, not good guides.<br /><br />I think the distinction between these two types of advisers is often missed. If you believe my example (if not, substitute one of your own, the point of this post is not to debate IO), there are a bunch of expert economists (Chicago School types) who should have fancy prestigious professorships, and whose arguments should be given careful consideration; and there are another bunch of expert economists who should have fancy prestigious professorships, whose arguments should be given careful consideration, <strong>and whose advice should be heeded</strong>. Leave aside the practical difficulty of knowing which is which if you are, say, a reporter or a policy-maker. The point is that there should be two buckets for two different types of prestigious advice-giver, but we only really have one.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "47b7Gp22x4yK9rTLT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 7, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "9331", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-22T02:23:49.667Z", "modifiedAt": null, "url": null, "title": "The basic questions of rationality", "slug": "the-basic-questions-of-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:08.858Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "odm3FHPzgbiGpWsSg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FdHMGBwoHCJ9i6xsy/the-basic-questions-of-rationality", "pageUrlRelative": "/posts/FdHMGBwoHCJ9i6xsy/the-basic-questions-of-rationality", "linkUrl": "https://www.lesswrong.com/posts/FdHMGBwoHCJ9i6xsy/the-basic-questions-of-rationality", "postedAtFormatted": "Monday, August 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20basic%20questions%20of%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20basic%20questions%20of%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFdHMGBwoHCJ9i6xsy%2Fthe-basic-questions-of-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20basic%20questions%20of%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFdHMGBwoHCJ9i6xsy%2Fthe-basic-questions-of-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFdHMGBwoHCJ9i6xsy%2Fthe-basic-questions-of-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 467, "htmlBody": "<p>I've been on Less Wrong since its inception, around March 2009. I've read a lot and contributed a lot, and so now I'm more familiar with our jargon, I know of a few more scientific studies, and I might know a couple of useful tricks. Despite all my reading, however, I feel like I'm a far cry from <em>learning rationality</em>. I'm still a wannabe, not an amateur. Less Wrong has tons of information, but I feel like I haven't yet learned the answers to the basic questions of rationality.</p>\n<p>I, personally, am a fan of the top-down approach to learning things. Whereas Less Wrong contains tons of useful facts that could, potentially, be put together to answer life's important questions, I really would find it easier if we started with the important questions, and then broke those down into smaller pieces that can be answered more easily.</p>\n<p>And so, that's precisely what I'm going to do. Here are, as far as I can tell, the basic questions of rationality&mdash;the questions we're actually trying to answer here&mdash;along with what answers I've found:</p>\n<p><strong>Q: Given a question, how should we go about answering it? </strong>A: By gathering evidence effectively, and correctly applying reason and intuition.</p>\n<ul>\n<li><strong>Q: How can we effectively gather relevant evidence? </strong>A: I don't know. (Controlled experiments? Asking people?)</li>\n<li><strong>Q: How can we correctly apply reason?</strong>&nbsp;A: If you have infinite computational resources available, use probability theory. \n<ul>\n<li><strong>Q: We don't have infinite computational resources available, so what now?</strong>&nbsp;A: I don't know. (Apply Bayes' rule anyway? Just try to emulate what a hypercomputer would do?)</li>\n</ul>\n</li>\n<li><strong>Q: How can we successfully apply intuition?</strong>&nbsp;A: By repairing our biases, and developing habits that point us in the right direction under specific circumstances. \n<ul>\n<li><strong>Q: How can we find our biases? </strong>A: I don't know. (Read Less Wrong? What about our personal quirks? How can we notice those?)</li>\n<li><strong>Q: Once we find a bias, how can we fix it?</strong>&nbsp;A: I don't know. (Apply a correction, test, repeat? Figure out how the bias feels?)</li>\n<li><strong>Q: How can we find out what habits would be useful to develop?</strong>&nbsp;A: I don't know. (Examine our past successes and rationalize them?)</li>\n<li><strong>Q: Once we decide on a habit, how can we develop it?</strong>&nbsp;A: I don't know. (Sheer practice?)</li>\n</ul>\n</li>\n</ul>\n<div>We could answer some of these questions ourselves, though simple practice and straightforward methods. The method \"apply a correction, test, repeat\", for example, is so generally useful that it deserves to be called the Fundamental Algorithm of Control. Nevertheless, since Less Wrong is devoted to developing human rationality, surely it contains answers to these questions somewhere. Where are they?</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FdHMGBwoHCJ9i6xsy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 39, "extendedScore": null, "score": 7.9e-05, "legacy": true, "legacyId": "9283", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-22T05:32:10.596Z", "modifiedAt": null, "url": null, "title": "A Sketch of an Anti-Realist Metaethics", "slug": "a-sketch-of-an-anti-realist-metaethics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:38.095Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jack", "createdAt": "2009-02-27T15:27:14.891Z", "isAdmin": false, "displayName": "Jack"}, "userId": "GwetakMQqsGCf7ZQv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4bNpEBW4dwBuzf4TS/a-sketch-of-an-anti-realist-metaethics", "pageUrlRelative": "/posts/4bNpEBW4dwBuzf4TS/a-sketch-of-an-anti-realist-metaethics", "linkUrl": "https://www.lesswrong.com/posts/4bNpEBW4dwBuzf4TS/a-sketch-of-an-anti-realist-metaethics", "postedAtFormatted": "Monday, August 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Sketch%20of%20an%20Anti-Realist%20Metaethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Sketch%20of%20an%20Anti-Realist%20Metaethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4bNpEBW4dwBuzf4TS%2Fa-sketch-of-an-anti-realist-metaethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Sketch%20of%20an%20Anti-Realist%20Metaethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4bNpEBW4dwBuzf4TS%2Fa-sketch-of-an-anti-realist-metaethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4bNpEBW4dwBuzf4TS%2Fa-sketch-of-an-anti-realist-metaethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1971, "htmlBody": "<p>Below is a sketch of a moral anti-realist position based on the map-territory distinction, Hume and studies of psychopaths. Hopefully it is productive.</p>\n<h4>The Map is Not the Territory Reviewed<br /></h4>\n<p>Consider the founding metaphor of Less Wrong: <a href=\"http://wiki.lesswrong.com/wiki/Map_and_territory\">the map-territory distinction</a>. Beliefs are to reality as maps are to territory. As the wiki says:</p>\n<blockquote>\n<p>Since our predictions don't always come true, we need different words to describe the thingy that generates our predictions and the thingy that generates our experimental results. The first thingy is called \"belief\", the second thingy \"reality\".</p>\n</blockquote>\n<p>Of course <a href=\"http://wiki.lesswrong.com/wiki/The_map_is_not_the_territory\">the map is not the territory</a>.</p>\n<p>Here is Albert Einstein making much the same analogy:</p>\n<blockquote>\n<p>Physical concepts are free creations of the human mind and are not, however it may seem, uniquely determined by the external world. In our endeavor to understand reality we are somewhat like a man trying to understand the mechanism of a closed watch. He sees the face and the moving hands, even hears its ticking, but he has no way of opening the case. If he is ingenious he may form some picture of a mechanism which could be responsible for all the things he observes, but he may never be quite sure his picture is the only one which could explain his observations. He will never be able to compare his picture with the real mechanism and cannot even imagine the possibility or the meaning of such a comparison. But he certainly believes that, as his knowledge increases, his picture of reality will become simpler and simpler and will explain a wider and wider range of his sensuous impressions. He may also believe in the existence of the ideal limit of knowledge and that it is approached by the human mind. He may call this ideal limit the objective truth.</p>\n</blockquote>\n<p>The above notions about beliefs involve pictorial analogs, but we can also imagine other ways the same information could be contained. If the ideal map is turned into a series of sentences we can define a 'fact' as any sentence in the ideal map (IM). The moral realist position can then be stated as follows:</p>\n<p>Moral Realism: &exist;x(x &sub; IM) &amp; (x = M)</p>\n<p>In English: there is some set of sentences x such that all the sentences are part of the ideal map and x provides a complete account of morality.</p>\n<p>Moral anti-realism simply negates the above.&nbsp; &not;(&exist;x(x &sub; IM) &amp; (x = M)).</p>\n<p><a id=\"more\"></a></p>\n<p>Now it might seem that, as long as our concept of morality doesn't require the existence of entities like non-natural gods, which don't appear to figure into an ideal map, moral realism must be true (where else but the territory could morality be?). The problem of ethics then, is chiefly one of finding a satisfactory reduction of moral language into sentences we are confident of finding in the IM. Moreover, the 'folk' meta-ethics certainly seems to be a realist one. People routinely use moral predicates and speak of having moral beliefs. \"Stealing that money was wrong\", \"I believe abortion is immoral\", \"Hitler was a bad person\". In other words, in the maps people *actually have right now*&nbsp; a moral code seems to exist.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<h4>Beliefs vs. Preferences</h4>\n<p>But we don't think talking about belief networks is sufficient for modeling an agent's behavior. To predict what other agents will do we need to know both their beliefs and their preferences (or call them goals, desires, affect or utility function). And when we're making our own choices we don't think we're responding merely to beliefs about the external world. Rather, it seems like we're also responding to an internal algorithm that helps us decide between actions according to various criteria, many of which reference the external world.</p>\n<p>The distinction between belief function and utility function shouldn't be new to anyone here. I bring it up because the <a href=\"http://en.wikipedia.org/wiki/Argument_from_queerness\">queer</a> thing about moral statements is that they seem to be self-motivating. They're not merely descriptive, they're prescriptive. So we have a good reason to think that they call our utility function. One way of phrasing a <a href=\"http://plato.stanford.edu/entries/moral-cognitivism/\">moral non-cognitivist</a> position is to say that moral statements are properly thought of as expressions of an individual's utility function rather than sentences describing the world.</p>\n<p>Note that 'expressions of an individual's utility function' is not the same as 'sentences describing an individual's utility function'. The latter is something like 'I prefer chocolate to vanilla' the former is something like 'Mmmm chocolate!'. It's how the utility function feels from the inside. And the way a utility function feels from the inside appears to be, or at least involve, emotion.</p>\n<h4>\n<hr />\nProjectivism and Psychopathy<br /></h4>\n<p>That our brains might routinely turn expressions of our utility function into properties of the external world <a href=\"/lw/oi/mind_projection_fallacy/\">shouldn't be surprising</a>. This was essentially Hume's position. From the <a href=\"http://plato.stanford.edu/entries/moral-anti-realism/projectivism-quasi-realism.html\">Stanford Encyclopedia of Philosophy</a>.</p>\n<blockquote>\n<p>Projectivism is best thought of as a causal account of moral experience. Consider a straightforward, observation-based moral judgment: Jane sees two youths hurting a cat and thinks &ldquo;That is impermissible.&rdquo; The causal story begins with a real event in the world: two youth performing actions, a suffering cat, etc. Then there is Jane's sensory perception of this event (she sees the youths, hears the cat's howls, etc.). Jane may form certain inferential beliefs concerning, say, the youths' intentions, the cats' pain, etc. All this prompts in Jane an emotion: <em>She</em> <em>disapproves</em> (say). She then &ldquo;projects&rdquo; this emotion onto her experience of the world, which results in her judging the action to be <em>impermissible</em>. In David Hume's words: &ldquo;taste [as opposed to reason] has a productive faculty, and gilding and staining all natural objects with the colours, borrowed from internal sentiment, raises in a manner a new creation&rdquo; (Hume [1751] 1983: 88). Here, <em>impermissibility</em> is the &ldquo;new creation.&rdquo; This is not to say that Jane &ldquo;sees&rdquo; the action to instantiate impermissibility in the same way as she sees the cat to instantiate brownness; but she judges the world to contain a certain quality, and her doing so is not the product of her tracking a real feature of the world, but is, rather, prompted by an emotional experience.</p>\n</blockquote>\n<p>This account has a surface plausibility. Moreover, it has substantial support in psychological literature. In particular, the behavior of psychopaths closely matches what we would expect if the projectivist thesis were true. The distinctive neurobiological feature of psychopathy is <a href=\"http://bjp.rcpsych.org/content/182/1/5.full\">impaired function of the amygdala</a>. The amygdala mainly associated with emotional processing and memory. Obviously, as a group psychopaths tend toward moral deficiency. But more importantly psychopaths fail to make the normal human distinction between <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/7587017\">morality and convention</a>. Thus a plausible account of a moral judgment is that it requires both social convention and emotional reaction. See the work of Shaun Nichols, in particular <a href=\"http://www.hum.utah.edu/philosophy/faculty/nichols/Papers/PsychopathsFinal.htm\">this</a> for an extended discussion of the implications of psychopathy on metaethics and his <a href=\"http://www.amazon.com/Sentimental-Rules-Natural-Foundations-Judgment/dp/0195314204\">book</a> for a broader, empirically informed account of sentimentalist morality. Auditory learners might benefit from <a href=\"http://bloggingheads.tv/diavlogs/11517\">this bloggingheads he did</a>.</p>\n<p>If the projectivist account is right the difference between <a href=\"http://en.wikipedia.org/wiki/Non-cognitivism\">non-cognitivism</a> and <a href=\"http://en.wikipedia.org/wiki/Moral_skepticism#Moral_Error_Theory\">error theory</a> is essentially one of emphasis. If you want to call moral judgments beliefs based on the above account then you are an error theorist. If you think they're a kind of pseudo-belief then you're a non-cognitivist.</p>\n<hr />\n<h4>But utility functions are part of the territory described by the map!<br /></h4>\n<p>Modeling reality has a recursive element which tends to generate considerable confusion over multiple domains. The issue is that somewhere in any good map of the territory will be a description of the agent doing the mapping. So agents end up with beliefs about what they believe and beliefs about what they desire. Thus, we might think there could be a set of sentences in IM that make up our morality so long as some of those sentences describe our utility function. That is, the motivational aspect of morality can be accounted for by including in the reduction both a) a sentence which describes what conditions are to be preferred to others and b) a statement which says that the agent prefers such conditions.&nbsp;</p>\n<p>The problem is, our morality doesn't seem completely responsive to hypothetical and counter-factual shifts in what our utility function is. That is, *if* I thought causing suffering in others was something I should do and I got good feelings from doing it that&nbsp; *wouldn't* make causing suffering moral (though Sadist Jack might think it was). In other words, changing one's morality function isn't a way to change what is moral (perhaps this judgment is uncommon, we should test it).</p>\n<p>&nbsp;This does not mean the morality subroutine of your utility function isn't responsive to changes in other parts of the utility function. If you think fulfilling your own non-moral desires is a moral good then which actions are moral will depend on how your non-moral desires change. But hypothetical changes in our morality subroutine don't change our moral judgments about our actions in the hypothetical. This is because when we make moral judgments we *don't* look at our map of the world to find our what our morality says, rather we have an emotional reaction to a set of facts and that emotional reaction generates the moral belief. Below is a diagram that somewhat messily describes what I'm talking about.</p>\n<p>&nbsp;</p>\n<p><img src=\"http://i.imgur.com/l1loG.png\" alt=\"\" width=\"739\" height=\"311\" /></p>\n<p>On the left we have the external world which generates the sensory inputs our agent uses to form beliefs. Those beliefs are then input into the utility function, a subroutine of which is morality. The utility function outputs the action the agent chooses. On the right we have zoomed in on the green Map circle from the left. Here we see that the map includes moral 'beliefs' (note that this isn't an<em> ideal map</em>) which have been projected from the morality subroutine in the utility function. Then we have, also within the Map, the self-representation of the agent which in turn includes her algorithms and mental states. Note that altering morality of the self-representation won't change the output of the morality subroutine of the first level of the model. Of course, in an ideal map the self-representation would match the first level but that doesn't change the causal or phenomenal story of how moral judgments are made.</p>\n<p>Observe how <em>easy</em> it is to make category errors if this model is accurate. Since we're projecting our moral subroutine onto our map and we're depicting ourselves in the map it is very easy to think that morality is something we're learning about from the external world (if not from sensory input then from a priori reflection!). Of course, morality <em>is</em> in the external world in a meaningful sense since our brains are in the external world. But learning what is in our brains is not motivating in the way moral judgments are supposed to be. This diagram explains why: the facts about our moral code in our self-representation are not directly connected to our choice circuits which cause us to perform actions. Simply stating what our brains are like will not activate our utility function and so the expressive content of moral language will be left out. This is Hume's is-ought distinction- 'ought' sentences can't be derived from 'is' sentences because ought sentences involve the activation of the utility function at the first level of the diagram, whereas 'is' sentences are exclusively part of the map.</p>\n<p>And of course since agents can have different morality functions there are no <a href=\"/lw/rn/no_universally_compelling_arguments/\">universally compelling arguments</a>.</p>\n<p>The above is the anti-realist position given in terms I think Less Wrong is comfortable with. It has the following things in it's favor: it does not posit any 'queer' moral properties as having objective existence and it fully accounts for the motivating, prescriptive aspect of moral language. It is psychologically sound, albeit simplistic. It is naturalistic while providing an account of why meta-ethics is so confusing to begin with. It explains our naive moral realism and dissolves the difference between the two prominent anti-realist camps.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LnEEs8xGooYmQ8iLA": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4bNpEBW4dwBuzf4TS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 26, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "9339", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Below is a sketch of a moral anti-realist position based on the map-territory distinction, Hume and studies of psychopaths. Hopefully it is productive.</p>\n<h4 id=\"The_Map_is_Not_the_Territory_Reviewed\">The Map is Not the Territory Reviewed<br></h4>\n<p>Consider the founding metaphor of Less Wrong: <a href=\"http://wiki.lesswrong.com/wiki/Map_and_territory\">the map-territory distinction</a>. Beliefs are to reality as maps are to territory. As the wiki says:</p>\n<blockquote>\n<p>Since our predictions don't always come true, we need different words to describe the thingy that generates our predictions and the thingy that generates our experimental results. The first thingy is called \"belief\", the second thingy \"reality\".</p>\n</blockquote>\n<p>Of course <a href=\"http://wiki.lesswrong.com/wiki/The_map_is_not_the_territory\">the map is not the territory</a>.</p>\n<p>Here is Albert Einstein making much the same analogy:</p>\n<blockquote>\n<p>Physical concepts are free creations of the human mind and are not, however it may seem, uniquely determined by the external world. In our endeavor to understand reality we are somewhat like a man trying to understand the mechanism of a closed watch. He sees the face and the moving hands, even hears its ticking, but he has no way of opening the case. If he is ingenious he may form some picture of a mechanism which could be responsible for all the things he observes, but he may never be quite sure his picture is the only one which could explain his observations. He will never be able to compare his picture with the real mechanism and cannot even imagine the possibility or the meaning of such a comparison. But he certainly believes that, as his knowledge increases, his picture of reality will become simpler and simpler and will explain a wider and wider range of his sensuous impressions. He may also believe in the existence of the ideal limit of knowledge and that it is approached by the human mind. He may call this ideal limit the objective truth.</p>\n</blockquote>\n<p>The above notions about beliefs involve pictorial analogs, but we can also imagine other ways the same information could be contained. If the ideal map is turned into a series of sentences we can define a 'fact' as any sentence in the ideal map (IM). The moral realist position can then be stated as follows:</p>\n<p>Moral Realism: \u2203x(x \u2282 IM) &amp; (x = M)</p>\n<p>In English: there is some set of sentences x such that all the sentences are part of the ideal map and x provides a complete account of morality.</p>\n<p>Moral anti-realism simply negates the above.&nbsp; \u00ac(\u2203x(x \u2282 IM) &amp; (x = M)).</p>\n<p><a id=\"more\"></a></p>\n<p>Now it might seem that, as long as our concept of morality doesn't require the existence of entities like non-natural gods, which don't appear to figure into an ideal map, moral realism must be true (where else but the territory could morality be?). The problem of ethics then, is chiefly one of finding a satisfactory reduction of moral language into sentences we are confident of finding in the IM. Moreover, the 'folk' meta-ethics certainly seems to be a realist one. People routinely use moral predicates and speak of having moral beliefs. \"Stealing that money was wrong\", \"I believe abortion is immoral\", \"Hitler was a bad person\". In other words, in the maps people *actually have right now*&nbsp; a moral code seems to exist.</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<h4 id=\"Beliefs_vs__Preferences\">Beliefs vs. Preferences</h4>\n<p>But we don't think talking about belief networks is sufficient for modeling an agent's behavior. To predict what other agents will do we need to know both their beliefs and their preferences (or call them goals, desires, affect or utility function). And when we're making our own choices we don't think we're responding merely to beliefs about the external world. Rather, it seems like we're also responding to an internal algorithm that helps us decide between actions according to various criteria, many of which reference the external world.</p>\n<p>The distinction between belief function and utility function shouldn't be new to anyone here. I bring it up because the <a href=\"http://en.wikipedia.org/wiki/Argument_from_queerness\">queer</a> thing about moral statements is that they seem to be self-motivating. They're not merely descriptive, they're prescriptive. So we have a good reason to think that they call our utility function. One way of phrasing a <a href=\"http://plato.stanford.edu/entries/moral-cognitivism/\">moral non-cognitivist</a> position is to say that moral statements are properly thought of as expressions of an individual's utility function rather than sentences describing the world.</p>\n<p>Note that 'expressions of an individual's utility function' is not the same as 'sentences describing an individual's utility function'. The latter is something like 'I prefer chocolate to vanilla' the former is something like 'Mmmm chocolate!'. It's how the utility function feels from the inside. And the way a utility function feels from the inside appears to be, or at least involve, emotion.</p>\n<h4 id=\"_Projectivism_and_Psychopathy\">\n<hr>\nProjectivism and Psychopathy<br></h4>\n<p>That our brains might routinely turn expressions of our utility function into properties of the external world <a href=\"/lw/oi/mind_projection_fallacy/\">shouldn't be surprising</a>. This was essentially Hume's position. From the <a href=\"http://plato.stanford.edu/entries/moral-anti-realism/projectivism-quasi-realism.html\">Stanford Encyclopedia of Philosophy</a>.</p>\n<blockquote>\n<p>Projectivism is best thought of as a causal account of moral experience. Consider a straightforward, observation-based moral judgment: Jane sees two youths hurting a cat and thinks \u201cThat is impermissible.\u201d The causal story begins with a real event in the world: two youth performing actions, a suffering cat, etc. Then there is Jane's sensory perception of this event (she sees the youths, hears the cat's howls, etc.). Jane may form certain inferential beliefs concerning, say, the youths' intentions, the cats' pain, etc. All this prompts in Jane an emotion: <em>She</em> <em>disapproves</em> (say). She then \u201cprojects\u201d this emotion onto her experience of the world, which results in her judging the action to be <em>impermissible</em>. In David Hume's words: \u201ctaste [as opposed to reason] has a productive faculty, and gilding and staining all natural objects with the colours, borrowed from internal sentiment, raises in a manner a new creation\u201d (Hume [1751] 1983: 88). Here, <em>impermissibility</em> is the \u201cnew creation.\u201d This is not to say that Jane \u201csees\u201d the action to instantiate impermissibility in the same way as she sees the cat to instantiate brownness; but she judges the world to contain a certain quality, and her doing so is not the product of her tracking a real feature of the world, but is, rather, prompted by an emotional experience.</p>\n</blockquote>\n<p>This account has a surface plausibility. Moreover, it has substantial support in psychological literature. In particular, the behavior of psychopaths closely matches what we would expect if the projectivist thesis were true. The distinctive neurobiological feature of psychopathy is <a href=\"http://bjp.rcpsych.org/content/182/1/5.full\">impaired function of the amygdala</a>. The amygdala mainly associated with emotional processing and memory. Obviously, as a group psychopaths tend toward moral deficiency. But more importantly psychopaths fail to make the normal human distinction between <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/7587017\">morality and convention</a>. Thus a plausible account of a moral judgment is that it requires both social convention and emotional reaction. See the work of Shaun Nichols, in particular <a href=\"http://www.hum.utah.edu/philosophy/faculty/nichols/Papers/PsychopathsFinal.htm\">this</a> for an extended discussion of the implications of psychopathy on metaethics and his <a href=\"http://www.amazon.com/Sentimental-Rules-Natural-Foundations-Judgment/dp/0195314204\">book</a> for a broader, empirically informed account of sentimentalist morality. Auditory learners might benefit from <a href=\"http://bloggingheads.tv/diavlogs/11517\">this bloggingheads he did</a>.</p>\n<p>If the projectivist account is right the difference between <a href=\"http://en.wikipedia.org/wiki/Non-cognitivism\">non-cognitivism</a> and <a href=\"http://en.wikipedia.org/wiki/Moral_skepticism#Moral_Error_Theory\">error theory</a> is essentially one of emphasis. If you want to call moral judgments beliefs based on the above account then you are an error theorist. If you think they're a kind of pseudo-belief then you're a non-cognitivist.</p>\n<hr>\n<h4 id=\"But_utility_functions_are_part_of_the_territory_described_by_the_map_\">But utility functions are part of the territory described by the map!<br></h4>\n<p>Modeling reality has a recursive element which tends to generate considerable confusion over multiple domains. The issue is that somewhere in any good map of the territory will be a description of the agent doing the mapping. So agents end up with beliefs about what they believe and beliefs about what they desire. Thus, we might think there could be a set of sentences in IM that make up our morality so long as some of those sentences describe our utility function. That is, the motivational aspect of morality can be accounted for by including in the reduction both a) a sentence which describes what conditions are to be preferred to others and b) a statement which says that the agent prefers such conditions.&nbsp;</p>\n<p>The problem is, our morality doesn't seem completely responsive to hypothetical and counter-factual shifts in what our utility function is. That is, *if* I thought causing suffering in others was something I should do and I got good feelings from doing it that&nbsp; *wouldn't* make causing suffering moral (though Sadist Jack might think it was). In other words, changing one's morality function isn't a way to change what is moral (perhaps this judgment is uncommon, we should test it).</p>\n<p>&nbsp;This does not mean the morality subroutine of your utility function isn't responsive to changes in other parts of the utility function. If you think fulfilling your own non-moral desires is a moral good then which actions are moral will depend on how your non-moral desires change. But hypothetical changes in our morality subroutine don't change our moral judgments about our actions in the hypothetical. This is because when we make moral judgments we *don't* look at our map of the world to find our what our morality says, rather we have an emotional reaction to a set of facts and that emotional reaction generates the moral belief. Below is a diagram that somewhat messily describes what I'm talking about.</p>\n<p>&nbsp;</p>\n<p><img src=\"http://i.imgur.com/l1loG.png\" alt=\"\" width=\"739\" height=\"311\"></p>\n<p>On the left we have the external world which generates the sensory inputs our agent uses to form beliefs. Those beliefs are then input into the utility function, a subroutine of which is morality. The utility function outputs the action the agent chooses. On the right we have zoomed in on the green Map circle from the left. Here we see that the map includes moral 'beliefs' (note that this isn't an<em> ideal map</em>) which have been projected from the morality subroutine in the utility function. Then we have, also within the Map, the self-representation of the agent which in turn includes her algorithms and mental states. Note that altering morality of the self-representation won't change the output of the morality subroutine of the first level of the model. Of course, in an ideal map the self-representation would match the first level but that doesn't change the causal or phenomenal story of how moral judgments are made.</p>\n<p>Observe how <em>easy</em> it is to make category errors if this model is accurate. Since we're projecting our moral subroutine onto our map and we're depicting ourselves in the map it is very easy to think that morality is something we're learning about from the external world (if not from sensory input then from a priori reflection!). Of course, morality <em>is</em> in the external world in a meaningful sense since our brains are in the external world. But learning what is in our brains is not motivating in the way moral judgments are supposed to be. This diagram explains why: the facts about our moral code in our self-representation are not directly connected to our choice circuits which cause us to perform actions. Simply stating what our brains are like will not activate our utility function and so the expressive content of moral language will be left out. This is Hume's is-ought distinction- 'ought' sentences can't be derived from 'is' sentences because ought sentences involve the activation of the utility function at the first level of the diagram, whereas 'is' sentences are exclusively part of the map.</p>\n<p>And of course since agents can have different morality functions there are no <a href=\"/lw/rn/no_universally_compelling_arguments/\">universally compelling arguments</a>.</p>\n<p>The above is the anti-realist position given in terms I think Less Wrong is comfortable with. It has the following things in it's favor: it does not posit any 'queer' moral properties as having objective existence and it fully accounts for the motivating, prescriptive aspect of moral language. It is psychologically sound, albeit simplistic. It is naturalistic while providing an account of why meta-ethics is so confusing to begin with. It explains our naive moral realism and dissolves the difference between the two prominent anti-realist camps.</p>", "sections": [{"title": "The Map is Not the Territory Reviewed", "anchor": "The_Map_is_Not_the_Territory_Reviewed", "level": 1}, {"title": "Beliefs vs. Preferences", "anchor": "Beliefs_vs__Preferences", "level": 1}, {"title": "\nProjectivism and Psychopathy", "anchor": "_Projectivism_and_Psychopathy", "level": 1}, {"title": "But utility functions are part of the territory described by the map!", "anchor": "But_utility_functions_are_part_of_the_territory_described_by_the_map_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "136 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 136, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZTRiSNmeGQK8AkdN2", "PtoQdG7E8MxYJrigu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-22T10:20:06.889Z", "modifiedAt": null, "url": null, "title": "REQ: Latin translation for HPMOR", "slug": "req-latin-translation-for-hpmor", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:39.407Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qpp6ZdwHLNKj6PXRp/req-latin-translation-for-hpmor", "pageUrlRelative": "/posts/qpp6ZdwHLNKj6PXRp/req-latin-translation-for-hpmor", "linkUrl": "https://www.lesswrong.com/posts/qpp6ZdwHLNKj6PXRp/req-latin-translation-for-hpmor", "postedAtFormatted": "Monday, August 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20REQ%3A%20Latin%20translation%20for%20HPMOR&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AREQ%3A%20Latin%20translation%20for%20HPMOR%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqpp6ZdwHLNKj6PXRp%2Freq-latin-translation-for-hpmor%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=REQ%3A%20Latin%20translation%20for%20HPMOR%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqpp6ZdwHLNKj6PXRp%2Freq-latin-translation-for-hpmor", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqpp6ZdwHLNKj6PXRp%2Freq-latin-translation-for-hpmor", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 44, "htmlBody": "<p>If anyone can do non-wrong Latin, I could use a translation of the following for HPMOR.&nbsp; The original is supposed to be circa 1200.</p>\n<p>No rescuer hath the rescuer.<br />No Lord hath the champion,<br />no mother and no father,<br />only nothingness above.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qpp6ZdwHLNKj6PXRp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 21, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "9354", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-22T15:42:36.906Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Crackpot Offer", "slug": "seq-rerun-the-crackpot-offer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:07.729Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sYfoAyjbLFiMprGGN/seq-rerun-the-crackpot-offer", "pageUrlRelative": "/posts/sYfoAyjbLFiMprGGN/seq-rerun-the-crackpot-offer", "linkUrl": "https://www.lesswrong.com/posts/sYfoAyjbLFiMprGGN/seq-rerun-the-crackpot-offer", "postedAtFormatted": "Monday, August 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Crackpot%20Offer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Crackpot%20Offer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsYfoAyjbLFiMprGGN%2Fseq-rerun-the-crackpot-offer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Crackpot%20Offer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsYfoAyjbLFiMprGGN%2Fseq-rerun-the-crackpot-offer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsYfoAyjbLFiMprGGN%2Fseq-rerun-the-crackpot-offer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>Today's post, <a href=\"/lw/j8/the_crackpot_offer/\">The Crackpot Offer</a> was originally published on 08 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If you make a mistake, don't excuse it or pat yourself on the back for thinking originally; acknowledge you made a mistake and move on. If you become invested in your own mistakes, you'll stay stuck on bad ideas.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/779/seq_rerun_anchoring_and_adjustment/\">Anchoring and Adjustment</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sYfoAyjbLFiMprGGN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 7.586190313182243e-07, "legacy": true, "legacyId": "9356", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qRWfvgJG75ESLRNu9", "D4ujrpEvXNbXTzWYt", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-22T17:36:58.725Z", "modifiedAt": null, "url": null, "title": "Meetup : Monthly Berkeley meetup", "slug": "meetup-monthly-berkeley-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YY4athhobyT3JKXZH/meetup-monthly-berkeley-meetup", "pageUrlRelative": "/posts/YY4athhobyT3JKXZH/meetup-monthly-berkeley-meetup", "linkUrl": "https://www.lesswrong.com/posts/YY4athhobyT3JKXZH/meetup-monthly-berkeley-meetup", "postedAtFormatted": "Monday, August 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Monthly%20Berkeley%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Monthly%20Berkeley%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYY4athhobyT3JKXZH%2Fmeetup-monthly-berkeley-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Monthly%20Berkeley%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYY4athhobyT3JKXZH%2Fmeetup-monthly-berkeley-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYY4athhobyT3JKXZH%2Fmeetup-monthly-berkeley-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 75, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2j'>Monthly Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 August 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2128 Oxford Street, Berkeley</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's time for the monthly Berkeley meetup! We'll be meeting up at the Oxford Street Starbucks at 7pm, and then head over to the Free Speech Cafe on campus. Newcomers are very welcome. Feel free to bring your friends!</p>\n\n<p>If you're trying to find us, call me at 734-904-6123.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2j'>Monthly Berkeley meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YY4athhobyT3JKXZH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.586558822322277e-07, "legacy": true, "legacyId": "9357", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Monthly_Berkeley_meetup\">Discussion article for the meetup : <a href=\"/meetups/2j\">Monthly Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 August 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2128 Oxford Street, Berkeley</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's time for the monthly Berkeley meetup! We'll be meeting up at the Oxford Street Starbucks at 7pm, and then head over to the Free Speech Cafe on campus. Newcomers are very welcome. Feel free to bring your friends!</p>\n\n<p>If you're trying to find us, call me at 734-904-6123.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Monthly_Berkeley_meetup1\">Discussion article for the meetup : <a href=\"/meetups/2j\">Monthly Berkeley meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Monthly Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Monthly_Berkeley_meetup", "level": 1}, {"title": "Discussion article for the meetup : Monthly Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Monthly_Berkeley_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-22T17:43:34.685Z", "modifiedAt": null, "url": null, "title": "Do we want more publicity, and if so how?", "slug": "do-we-want-more-publicity-and-if-so-how", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:20.756Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/idD96QGD9Pr8QvFwH/do-we-want-more-publicity-and-if-so-how", "pageUrlRelative": "/posts/idD96QGD9Pr8QvFwH/do-we-want-more-publicity-and-if-so-how", "linkUrl": "https://www.lesswrong.com/posts/idD96QGD9Pr8QvFwH/do-we-want-more-publicity-and-if-so-how", "postedAtFormatted": "Monday, August 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Do%20we%20want%20more%20publicity%2C%20and%20if%20so%20how%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADo%20we%20want%20more%20publicity%2C%20and%20if%20so%20how%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FidD96QGD9Pr8QvFwH%2Fdo-we-want-more-publicity-and-if-so-how%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Do%20we%20want%20more%20publicity%2C%20and%20if%20so%20how%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FidD96QGD9Pr8QvFwH%2Fdo-we-want-more-publicity-and-if-so-how", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FidD96QGD9Pr8QvFwH%2Fdo-we-want-more-publicity-and-if-so-how", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<p>Recently reporters from two major national magazines contacted me in preparation for doing stories on <a href=\"http://en.wikipedia.org/wiki/Bitcoin\">Bitcoin</a>. This reminded me that Wired magazine did a cover story on the <a href=\"http://en.wikipedia.org/wiki/Cypherpunks\">Cypherpunks</a> in its second issue. I think the LessWrong community is already larger and more active than Cypherpunks were back then, and potentially more influential, but there hasn't been much publicity on us. I'm tempted to suggest doing a story on LessWrong to one of the reporters. Is this a good idea, or bad?</p>\n<p>More generally, do we want more publicity, and if so what's the best way to go about getting it?</p>\n<p><strong>ETA</strong>: Would it be bad etiquette to reveal the names of these magazines at this point, or even to say as much as I've said?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "idD96QGD9Pr8QvFwH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 23, "extendedScore": null, "score": 5.3e-05, "legacy": true, "legacyId": "9358", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-22T18:46:07.719Z", "modifiedAt": null, "url": null, "title": "Kill the mind-killer", "slug": "kill-the-mind-killer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:28.960Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bKwjHqNHrMY9rGA2u/kill-the-mind-killer", "pageUrlRelative": "/posts/bKwjHqNHrMY9rGA2u/kill-the-mind-killer", "linkUrl": "https://www.lesswrong.com/posts/bKwjHqNHrMY9rGA2u/kill-the-mind-killer", "postedAtFormatted": "Monday, August 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Kill%20the%20mind-killer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKill%20the%20mind-killer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbKwjHqNHrMY9rGA2u%2Fkill-the-mind-killer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Kill%20the%20mind-killer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbKwjHqNHrMY9rGA2u%2Fkill-the-mind-killer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbKwjHqNHrMY9rGA2u%2Fkill-the-mind-killer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 344, "htmlBody": "<p>The budget stalemate in the US Congress was caused entirely by blocks of voters and representatives that coalesced around strong sets of opinions that few people would have come up with on their own, and by political party leaders forcing representatives in their parties to toe the party line.&nbsp; Politics isn't the mind killer.&nbsp; Political parties are the mind-killer.</p>\n<p>Parties are also notorious for obliterating information in elections, as well as for encouraging voters to vote sans information.&nbsp; If you went to your polling place and saw a list of candidates, none of whom you'd heard of before, you might rightly refrain from voting and polluting the signal with your noise.&nbsp; Knowing party affiliations makes people think they have enough information to vote.</p>\n<p>For discussion:</p>\n<ul>\n<li>What other disadvantages are provided by the existence of political parties?</li>\n<li>Do political parties provide us with any advantages at all?</li>\n<li>If so, do the benefits outweigh the disadvantages?</li>\n<li>How might we go about disenfranchising political parties?</li>\n</ul>\n<p>We want the freedom to form groups that promote political concerns.&nbsp; But it would be possible to keep these groups at a greater distance from elected representatives.&nbsp; Candidates for office could be forbidden from endorsing a particular party.&nbsp; The Congress could be forbidden from basing any procedural rules on party affiliation.&nbsp; Political parties could be forbidden from making large donations to election campaigns, or sponsoring advertising.&nbsp; That's not so different from what we do today with religious groups, which are not much different from political parties.</p>\n<p>Political parties are currently officially part of Congress' operation, even though they're not in the constitution.&nbsp; There are all sorts of Congressional rules specifying how the parties interact, who gets to choose committee members, who runs the House and Senate floors, etc.&nbsp; A party leader can punish a representative who doesn't toe the line with many incentives and disincentives.</p>\n<p>Make that illegal.&nbsp; Make persecuting a representative for party-based reasons have the same legal standing as persecuting a representative for religious reasons.</p>\n<p>I will ignore comments saying \"you're an intellectual dreamer\", for the usual reasons.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bKwjHqNHrMY9rGA2u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": -7, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "9359", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-23T02:03:07.890Z", "modifiedAt": null, "url": null, "title": "Computer Programs Rig Elections", "slug": "computer-programs-rig-elections", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:07.054Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "magfrump", "createdAt": "2009-12-10T20:51:45.065Z", "isAdmin": false, "displayName": "magfrump"}, "userId": "KsYFs5ip5jeiFETJa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3q5wBDQmvj85vvbHb/computer-programs-rig-elections", "pageUrlRelative": "/posts/3q5wBDQmvj85vvbHb/computer-programs-rig-elections", "linkUrl": "https://www.lesswrong.com/posts/3q5wBDQmvj85vvbHb/computer-programs-rig-elections", "postedAtFormatted": "Tuesday, August 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Computer%20Programs%20Rig%20Elections&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComputer%20Programs%20Rig%20Elections%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3q5wBDQmvj85vvbHb%2Fcomputer-programs-rig-elections%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Computer%20Programs%20Rig%20Elections%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3q5wBDQmvj85vvbHb%2Fcomputer-programs-rig-elections", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3q5wBDQmvj85vvbHb%2Fcomputer-programs-rig-elections", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 116, "htmlBody": "<p>I don't know how interested this community would be in this topic, I don't mean to be talking politics so much as technology and decision mechanisms.</p>\n<p>According to this programmer's testimony, voting machine companies requested that their programmers make it possible for the companies to rig elections, while in communication with elected officials.</p>\n<p><a href=\"http://www.youtube.com/watch?v=1thcO_olHas&amp;sns=fb\">http://www.youtube.com/watch?v=1thcO_olHas&amp;sns=fb</a></p>\n<p>If there is a discussion of how worthwhile taking the time to vote is, this may be worth knowing.</p>\n<p>This is something that I expected to be true beforehand, but I am wondering: How reliable is this testimony? &nbsp;What are other LWers' prior and posterior probabilities of elections being rigged in this way? &nbsp;Is it worth trying to do something about this, and if so what?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3q5wBDQmvj85vvbHb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -3, "extendedScore": null, "score": 7.588190157374872e-07, "legacy": true, "legacyId": "9362", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-23T03:59:25.135Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Radical Honesty", "slug": "seq-rerun-radical-honesty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:07.050Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PooMHAsRMeweKiFAx/seq-rerun-radical-honesty", "pageUrlRelative": "/posts/PooMHAsRMeweKiFAx/seq-rerun-radical-honesty", "linkUrl": "https://www.lesswrong.com/posts/PooMHAsRMeweKiFAx/seq-rerun-radical-honesty", "postedAtFormatted": "Tuesday, August 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Radical%20Honesty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Radical%20Honesty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPooMHAsRMeweKiFAx%2Fseq-rerun-radical-honesty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Radical%20Honesty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPooMHAsRMeweKiFAx%2Fseq-rerun-radical-honesty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPooMHAsRMeweKiFAx%2Fseq-rerun-radical-honesty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<p>Today's post, <a href=\"/lw/j9/radical_honesty/\">Radical Honesty</a> was originally published on 10 September 2007. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The Radical Honesty movement requires participants to speak the truth, always, whatever they think. The more competent you grow at avoiding self-deceit, the more of a challenge this would be - but it's an interesting thing to imagine, and perhaps strive for.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/r/discussion/lw/77w/seq_rerun_the_crackpot_offer/\">The Crackpot Offer</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PooMHAsRMeweKiFAx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 10, "extendedScore": null, "score": 7.588565040014703e-07, "legacy": true, "legacyId": "9366", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GMhzDb3uAFYLwmXtY", "sYfoAyjbLFiMprGGN", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-23T09:13:13.807Z", "modifiedAt": null, "url": null, "title": "Antisocial personality traits predict utilitarian responses to moral dilemmas", "slug": "antisocial-personality-traits-predict-utilitarian-responses", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:05.689Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_M", "createdAt": "2010-04-17T17:31:50.226Z", "isAdmin": false, "displayName": "Vladimir_M"}, "userId": "Fj5i6j28HMGcdytoM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9cCT6DYYL4PWXi65n/antisocial-personality-traits-predict-utilitarian-responses", "pageUrlRelative": "/posts/9cCT6DYYL4PWXi65n/antisocial-personality-traits-predict-utilitarian-responses", "linkUrl": "https://www.lesswrong.com/posts/9cCT6DYYL4PWXi65n/antisocial-personality-traits-predict-utilitarian-responses", "postedAtFormatted": "Tuesday, August 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Antisocial%20personality%20traits%20predict%20utilitarian%20responses%20to%20moral%20dilemmas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAntisocial%20personality%20traits%20predict%20utilitarian%20responses%20to%20moral%20dilemmas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9cCT6DYYL4PWXi65n%2Fantisocial-personality-traits-predict-utilitarian-responses%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Antisocial%20personality%20traits%20predict%20utilitarian%20responses%20to%20moral%20dilemmas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9cCT6DYYL4PWXi65n%2Fantisocial-personality-traits-predict-utilitarian-responses", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9cCT6DYYL4PWXi65n%2Fantisocial-personality-traits-predict-utilitarian-responses", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 581, "htmlBody": "<p>So says the title of <a href=\"http://leeds-faculty.colorado.edu/mcgrawp/PDF/BartelsPizarro.2011.pdf\">an interesting recent paper I stumbled on yesterday</a> (ungated link; h/t <a href=\"http://crookedtimber.org/2011/08/22/utilitarian-psychopaths/\">Chris Bertram</a>). Here's the abstract:&nbsp;</p>\n<blockquote>\n<p><em>Researchers have recently argued that utilitarianism is the appropriate framework by which to evaluate moral judgment, and that individuals who endorse non-utilitarian solutions to moral dilemmas (involving active vs. passive harm) are committing an error. We report a study in which participants responded to a battery of personality assessments and a set of dilemmas that pit utilitarian and non-utilitarian options against each other. Participants who indicated greater endorsement of utilitarian solutions had higher scores on measures of Psychopathy, machiavellianism, and life meaninglessness. These results&nbsp;question the widely-used methods by which lay moral judgments are evaluated, as these approaches lead to the counterintuitive conclusion that those individuals who are least prone to moral errors also possess a set of psychological characteristics that many would consider prototypically immoral.</em></p>\n</blockquote>\n<p>This conclusion is very much along the lines of some of my recent LW comments (for example, those I left in <a href=\"/lw/74f/are_deontological_moral_judgments_rationalizations/4nor\">this thread</a>). To me it seems quite obvious that in the space of possible human minds, those that produce on the whole reasonably cooperative and reliably non-threatening behavior are overwhelmingly unlikely to produce utilitarian decisions in trolley-footbridge and similar \"sacrificial\" problems.&nbsp;</p>\n<p>Of course, what people <em>say</em> they would do in situations of this sort is usually determined by signaling rather than a realistic appraisal. Kind and philosophical utilitarians of the sort one meets on LW would be extremely unlikely to act in practice according to the implications of their favored theories in real-life \"sacrificial\" situations, so their views are by themselves not strong evidence of antisocial personality traits.&nbsp;However, actually acting in such ways would be, in my opinion, very strong evidence for such traits, which is correctly reflected in the typical person's fear and revulsion of someone who is known to have acted like that. I would venture to guess that it is in fact the signaling-driven disconnect between people's endorsement of utilitarian actions and the actual decisions they would make that makes the found correlations fairly low. (Assuming also that these tests really are strong indicators of antisocial personalities, of course, which I lack the knowledge to judge.)</p>\n<p>(Also, endorsement of utilitarianism even just for signaling value causes its own problems, since it leads to political and ideological support for all sorts of crazy ideas backed by plausible-sounding utilitarian arguments, but that's a whole different issue.)</p>\n<p>Here is also a full citation for reference:&nbsp;&ldquo;The mismeasure of morals: Antisocial personality traits predict utilitarian responses to moral dilemmas&rdquo;, by Daniel M. Bartels and David A. Pizarro, Cognition 121 (2011), pp. 154-161.</p>\n<hr />\n<p><strong>Edit:</strong> As Wei Dai points out in a comment, I should also add that some of the previous literature cited by Bartels and Pizarro has concluded that, in their words, \"individuals with higher working memory capacity and those who are more deliberative thinkers are... more likely to approve of utilitarian solution.\" One the face of it, taken together with the conclusions of this paper, this would mean that propensity for utilitarian responses may stem from different causes in different individuals (i.e. deliberative thinking versus antisocial traits).</p>\n<p>My own hypothesis, however, is that deliberative thinking leads to verbal utilitarian responses that are likely due to signaling, and that propensity for actual utilitarian \"sacrificial\" acts would have a much weaker link to deliberative thinking and a much stronger link to antisocial traits than mere utilitarian statements. Unfortunately, I don't know how this could be tested empirically in an ethical manner. &nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9cCT6DYYL4PWXi65n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 26, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "9374", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-23T14:49:35.891Z", "modifiedAt": null, "url": null, "title": "[link] Apostles' Creed = Tsuyoku Naritai???", "slug": "link-apostles-creed-tsuyoku-naritai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:07.034Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SilasBarta", "createdAt": "2009-03-01T00:03:34.864Z", "isAdmin": false, "displayName": "SilasBarta"}, "userId": "zDPSZfarhLM7Gehug", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x79rkPxBjbKzSLfwv/link-apostles-creed-tsuyoku-naritai", "pageUrlRelative": "/posts/x79rkPxBjbKzSLfwv/link-apostles-creed-tsuyoku-naritai", "linkUrl": "https://www.lesswrong.com/posts/x79rkPxBjbKzSLfwv/link-apostles-creed-tsuyoku-naritai", "postedAtFormatted": "Tuesday, August 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Apostles'%20Creed%20%3D%20Tsuyoku%20Naritai%3F%3F%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Apostles'%20Creed%20%3D%20Tsuyoku%20Naritai%3F%3F%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx79rkPxBjbKzSLfwv%2Flink-apostles-creed-tsuyoku-naritai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Apostles'%20Creed%20%3D%20Tsuyoku%20Naritai%3F%3F%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx79rkPxBjbKzSLfwv%2Flink-apostles-creed-tsuyoku-naritai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx79rkPxBjbKzSLfwv%2Flink-apostles-creed-tsuyoku-naritai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 256, "htmlBody": "<p>Background: <a href=\"http://en.wikipedia.org/wiki/Apostles%27_Creed\">Apostles' Creed</a>, <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">Tsuyoku Naritai</a></p>\r\n<p>Related to: <a href=\"/lw/fm/a_parable_on_obsolete_ideologies/\">A Parable on Obsolete Ideologies</a></p>\r\n<p>Just something I thought I might add to the annals of cases where someone tries to re-interpret an old religious text to mean something more acceptable to the modern ear, in contradiction to what most people (especially its contemporaries)&nbsp;think the texts mean.&nbsp; And this is not some random person, but Gene Callahan, who makes sure you understand he holds a doctorate in philosophy, and pretty much makes a career out of defending this and anti-reductionist views in general.&nbsp; Here's <a href=\"http://gene-callahan.blogspot.com/2011/08/credo-ut-intelligam.html\">the post</a>:</p>\r\n<blockquote>\r\n<p>I believe in one God, the Father Almighty, Maker of heaven and earth...<br />And in one Lord Jesus Christ, the only-begotten Son of God...<br />And I believe in the Holy Ghost, the Lord and Giver of Life...\"<br /><br />If I say these words, what do I mean? I am asserting that I have some secret knowledge that others do not? Do I believe these things like I believe it will rain tonight?<br /><br />No, I asserting that, by meditating on these symbols, I believe I will come to understand better what I now know only through a glass darkly.<br /><br />I believe that I may understand.</p>\r\n</blockquote>\r\n<p>I suggested that this is not what most people mean when they say the Creed, but (surprise) the comment was deleted.</p>\r\n<p>(Yes I know Tsuyoku Naritai is not quite the same as Callahan's interpretation, but it's the closest short LW term for the general idea.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x79rkPxBjbKzSLfwv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -5, "extendedScore": null, "score": -9e-06, "legacy": true, "legacyId": "9375", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DoLQN5ryZ9XkZjq5h", "Ltey8BS83qSkd9M3u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-24T03:16:37.649Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] We Don't Really Want Your Participation", "slug": "seq-rerun-we-don-t-really-want-your-participation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:08.212Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Nv83MtesQZyyF3L8v/seq-rerun-we-don-t-really-want-your-participation", "pageUrlRelative": "/posts/Nv83MtesQZyyF3L8v/seq-rerun-we-don-t-really-want-your-participation", "linkUrl": "https://www.lesswrong.com/posts/Nv83MtesQZyyF3L8v/seq-rerun-we-don-t-really-want-your-participation", "postedAtFormatted": "Wednesday, August 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20We%20Don't%20Really%20Want%20Your%20Participation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20We%20Don't%20Really%20Want%20Your%20Participation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNv83MtesQZyyF3L8v%2Fseq-rerun-we-don-t-really-want-your-participation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20We%20Don't%20Really%20Want%20Your%20Participation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNv83MtesQZyyF3L8v%2Fseq-rerun-we-don-t-really-want-your-participation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNv83MtesQZyyF3L8v%2Fseq-rerun-we-don-t-really-want-your-participation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p>Today's post, <a href=\"/lw/ja/we_dont_really_want_your_participation/\">We Don't Really Want Your Participation</a> was originally published on 10 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Advocates for the Singularity sometimes call for outreach to artists or poets; we should move away from thinking of people as if their profession is the only thing they can contribute to humanity. Being human is what gives us a stake in the future, not being poets or mathematicians.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/786/seq_rerun_radical_honesty/\">Radical Honesty</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Nv83MtesQZyyF3L8v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 12, "extendedScore": null, "score": 7.59307187302149e-07, "legacy": true, "legacyId": "9381", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RiWPdNaoL8fq7phbY", "PooMHAsRMeweKiFAx", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-24T07:16:45.967Z", "modifiedAt": null, "url": null, "title": "Help Fund Lukeprog at SIAI", "slug": "help-fund-lukeprog-at-siai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:35.420Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WqDGJNtxNMT8fHe37/help-fund-lukeprog-at-siai", "pageUrlRelative": "/posts/WqDGJNtxNMT8fHe37/help-fund-lukeprog-at-siai", "linkUrl": "https://www.lesswrong.com/posts/WqDGJNtxNMT8fHe37/help-fund-lukeprog-at-siai", "postedAtFormatted": "Wednesday, August 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20Fund%20Lukeprog%20at%20SIAI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20Fund%20Lukeprog%20at%20SIAI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWqDGJNtxNMT8fHe37%2Fhelp-fund-lukeprog-at-siai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20Fund%20Lukeprog%20at%20SIAI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWqDGJNtxNMT8fHe37%2Fhelp-fund-lukeprog-at-siai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWqDGJNtxNMT8fHe37%2Fhelp-fund-lukeprog-at-siai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 449, "htmlBody": "<p>Singularity Institute desperately needs someone who is not me who can write cognitive-science-based material. Someone smart, energetic, able to speak to popular audiences, and with an excellent command of the science. If you&rsquo;ve been reading Less Wrong for the last few months, you probably just thought the same thing I did: &ldquo;SIAI should hire Lukeprog!&rdquo; To support Luke Muelhauser becoming a full-time Singularity Institute employee, please <a href=\"http://intelligence.org/donate/\">donate</a> and mention Luke (e.g. &ldquo;Yay for Luke!&rdquo;) in the check memo or the comment field of your donation - or if you donate by a method that doesn&rsquo;t allow you to leave a comment, tell Louie Helm (louie@intelligence.org) your donation was to help fund Luke.</p>\n<p>Note that the <a href=\"http://intelligence.org/2011summerchallenge\">Summer Challenge</a> that doubles all donations will run until August 31st. (We're currently at $31,000 of $125,000.)<a id=\"more\"></a></p>\n<p>During his stint as a Singularity Institute Visiting Fellow, Luke has already:</p>\n<ul>\n<li>Co-organized and taught sessions for a well-received one-week <a href=\"/lw/5ec/minicamp_on_rationality_awesomeness_and/\">Rationality Minicamp</a>, and taught sessions for the nine-week Rationality Boot Camp.</li>\n<li>Written many <a href=\"http://commonsenseatheism.com/?p=13954\">helpful and well-researched</a> articles for Less Wrong on metaethics, rationality theory, and rationality practice, including the 20-page tutorial <em><a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">A Crash Course in the Neuroscience of Human Motivation</a></em>.</li>\n<li>Written a new <a href=\"http://intelligence.org/singularityfaq\">Singularity FAQ</a>.</li>\n<li>Published an intelligence explosion <a href=\"http://intelligenceexplosion.com/\">website</a> for academics.</li>\n<li>...and completed many smaller projects.</li>\n</ul>\n<p>As a full-time Singularity Institute employee, Luke could:</p>\n<ul>\n<li>Author and co-author research papers and outreach papers, including \n<ul>\n<li>A chapter already accepted to Springer&rsquo;s <em><a href=\"http://singularityhypothesis.blogspot.com/\">The Singularity Hypothesis</a></em> volume (co-authored with Louie Helm).</li>\n<li>A paper on existential risk and optimal philanthropy, co-authored with a Columbia University researcher.</li>\n</ul>\n</li>\n<li>Continue to write articles for Less Wrong on the theory and practice of rationality.</li>\n<li>Write a report that summarizes unsolved problems related to Friendly AI.</li>\n<li>Continue to develop his <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">metaethics sequence</a>, the conclusion of which will be a sort of <a href=\"http://en.wikipedia.org/wiki/Timothy_Gowers#Polymath_Project\">Polymath Project</a> for collaboratively solving open problems in metaethics relevant to FAI development.</li>\n<li>Teach courses on rationality and social effectiveness, as he has been doing for the Singularity Institute&rsquo;s <a href=\"/lw/5ec/minicamp_on_rationality_awesomeness_and/\">Rationality Minicamp</a> and <a href=\"/lw/4wm/rationality_boot_camp/\">Rationality Boot Camp</a>.</li>\n<li>Produce introductory materials to help bridge <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential gaps</a>, as he did with the <a href=\"http://intelligence.org/singularityfaq\">Singularity FAQ</a>.</li>\n<li>Raise awareness of AI risk and the uses of rationality by giving talks at universities and technology companies, as he recently did at <a href=\"http://halcyonmolecular.com/\">Halcyon Molecular</a>.</li>\n</ul>\n<p>If you&rsquo;d like to help us fund Luke Muehlhauser to do all that and probably more, please <a href=\"http://intelligence.org/donate/\">donate now</a> and include the word &ldquo;Luke&rdquo; in the comment field. &nbsp;And if you donate before August 31st, your donation will be doubled as part of the <a href=\"http://intelligence.org/2011summerchallenge\">2011 Summer Singularity Challenge</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1, "Z6DgiCrMtpSNxwuYW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WqDGJNtxNMT8fHe37", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": 63, "extendedScore": null, "score": 0.000164, "legacy": true, "legacyId": "9388", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 278, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9vBasHrBtCmC6zAzD", "hN2aRnu798yas5b2k", "s887k4Hcqj28cchYo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-24T11:25:31.217Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne, Ben's house", "slug": "meetup-melbourne-ben-s-house", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:23.772Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bwqig4dFq9bq4Nkia/meetup-melbourne-ben-s-house", "pageUrlRelative": "/posts/bwqig4dFq9bq4Nkia/meetup-melbourne-ben-s-house", "linkUrl": "https://www.lesswrong.com/posts/bwqig4dFq9bq4Nkia/meetup-melbourne-ben-s-house", "postedAtFormatted": "Wednesday, August 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%2C%20Ben's%20house&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%2C%20Ben's%20house%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbwqig4dFq9bq4Nkia%2Fmeetup-melbourne-ben-s-house%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%2C%20Ben's%20house%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbwqig4dFq9bq4Nkia%2Fmeetup-melbourne-ben-s-house", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbwqig4dFq9bq4Nkia%2Fmeetup-melbourne-ben-s-house", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2k'>Melbourne, Ben's house</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 September 2011 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">5/52 Leicester St Carlton 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>6pm for 7pm, at Ben's place. Ben will supply meat/vege burgers and maybe some salad etc. for a BBQ. If you comment here to tell him you're coming he'll get food for you. It's also okay to just turn up, but there might not be enough food. BYO drinks and board games (if you have any that you think we'll enjoy). If you have any problems getting in, you can call the host on 0412 996 288.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2k'>Melbourne, Ben's house</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bwqig4dFq9bq4Nkia", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.594649962594744e-07, "legacy": true, "legacyId": "9391", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne__Ben_s_house\">Discussion article for the meetup : <a href=\"/meetups/2k\">Melbourne, Ben's house</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 September 2011 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">5/52 Leicester St Carlton 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>6pm for 7pm, at Ben's place. Ben will supply meat/vege burgers and maybe some salad etc. for a BBQ. If you comment here to tell him you're coming he'll get food for you. It's also okay to just turn up, but there might not be enough food. BYO drinks and board games (if you have any that you think we'll enjoy). If you have any problems getting in, you can call the host on 0412 996 288.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne__Ben_s_house1\">Discussion article for the meetup : <a href=\"/meetups/2k\">Melbourne, Ben's house</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne, Ben's house", "anchor": "Discussion_article_for_the_meetup___Melbourne__Ben_s_house", "level": 1}, {"title": "Discussion article for the meetup : Melbourne, Ben's house", "anchor": "Discussion_article_for_the_meetup___Melbourne__Ben_s_house1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-24T15:20:08.154Z", "modifiedAt": null, "url": null, "title": "Existential Risk Reduction Career Network", "slug": "existential-risk-reduction-career-network-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/np5Pe6a33rw6JQq2t/existential-risk-reduction-career-network-0", "pageUrlRelative": "/posts/np5Pe6a33rw6JQq2t/existential-risk-reduction-career-network-0", "linkUrl": "https://www.lesswrong.com/posts/np5Pe6a33rw6JQq2t/existential-risk-reduction-career-network-0", "postedAtFormatted": "Wednesday, August 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Existential%20Risk%20Reduction%20Career%20Network&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExistential%20Risk%20Reduction%20Career%20Network%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnp5Pe6a33rw6JQq2t%2Fexistential-risk-reduction-career-network-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Existential%20Risk%20Reduction%20Career%20Network%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnp5Pe6a33rw6JQq2t%2Fexistential-risk-reduction-career-network-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnp5Pe6a33rw6JQq2t%2Fexistential-risk-reduction-career-network-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>Interested in donating to existential risk reduction efforts? Would  you like to exchange career information with like-minded others? Then  you should consider the Existential Risk Reduction Career Network! (\"X  Risk Network\" for those short on time.) From the front page of the  website:</p>\n<p>\"This network is for anyone interested in donating  substantial amounts (relative to income) to non-profit organizations  focused on the reduction of existential risk, such as <a rel=\"nofollow\" href=\"http://intelligence.org/\" target=\"_blank\">SIAI</a>,&nbsp;<a rel=\"nofollow\" href=\"http://www.fhi.ox.ac.uk/\" target=\"_blank\">FHI</a>, and the <a rel=\"nofollow\" href=\"http://lifeboat.com/ex/main\" target=\"_blank\">Lifeboat Foundation</a>.  [...] We are a community of people assisting each other to increase our  resources available for contribution. Members discuss the strengths and  weaknesses of different careers, network, share advice on job  applications and career advancement, assist others with finding  interviews, and occasionally look for qualified individuals to hire from  within the network.\"</p>\n<p>For more details, including on the process of requesting invitations, head on over to the front page at <a href=\"http://www.xrisknetwork.com/\" target=\"_blank\">http://www.xrisknetwork.com/</a></p>\n<p>Keep  in mind that the network is for students as well, not just those  currently on the job market. The network also has discussion of long  term job strategy, school admissions, and intern possibilities.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "np5Pe6a33rw6JQq2t", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 7.595407481219988e-07, "legacy": true, "legacyId": "9392", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-24T15:21:22.071Z", "modifiedAt": null, "url": null, "title": "A failure of conservation of evidence", "slug": "a-failure-of-conservation-of-evidence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:52.514Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mpw2yjKRBhWoE3iJY/a-failure-of-conservation-of-evidence", "pageUrlRelative": "/posts/mpw2yjKRBhWoE3iJY/a-failure-of-conservation-of-evidence", "linkUrl": "https://www.lesswrong.com/posts/mpw2yjKRBhWoE3iJY/a-failure-of-conservation-of-evidence", "postedAtFormatted": "Wednesday, August 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20failure%20of%20conservation%20of%20evidence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20failure%20of%20conservation%20of%20evidence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmpw2yjKRBhWoE3iJY%2Fa-failure-of-conservation-of-evidence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20failure%20of%20conservation%20of%20evidence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmpw2yjKRBhWoE3iJY%2Fa-failure-of-conservation-of-evidence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmpw2yjKRBhWoE3iJY%2Fa-failure-of-conservation-of-evidence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<p>From <a href=\"http://www.bloomberg.com/news/2011-08-23/u-s-stock-futures-rise-on-fed-stimulus-speculation-china-manufacturing.html\">Bloomberg</a>:</p>\n<p>U.S. stocks rallied, driving the Standard &amp; Poor&rsquo;s 500 Index up from the cheapest valuations since 2009, as weaker-than-estimated economic data reinforced optimism the Federal Reserve will act to spur growth.</p>\n<p>The S&amp;P 500 rose 3.4 percent to 1,162.35 at 4 p.m. in New York, for the biggest rally since Aug. 11. All 10 industries in the benchmark gauge rose, with gains ranging between 1.8 percent and 4.6 percent. The Dow Jones Industrial Average added 322.11 points, or 3 percent, to 11,176.76.</p>\n<p>&ldquo;There&rsquo;s plenty of evidence that the economy has slowed,&rdquo; Kevin Caron, market strategist in Florham Park, <a href=\"http://topics.bloomberg.com/new-jersey/\">New Jersey</a>, at Stifel Nicolaus &amp; Co., said in a telephone interview. His firm has more than $115 billion in client assets. &ldquo;The speculation would be that it&rsquo;s possible that the Fed will say something designed to calm markets and provide a bit of encouragement.&rdquo;</p>\n<p>So, bad news about the economy explains the market going both up and down!</p>\n<p>(Presumably, good news about the economy can also explain the market going both up and down.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mpw2yjKRBhWoE3iJY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 0, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "9393", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-24T15:47:24.846Z", "modifiedAt": null, "url": null, "title": "How to be Deader than Dead", "slug": "how-to-be-deader-than-dead", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:17.897Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xM4MLzaTnMA7AQbSM/how-to-be-deader-than-dead", "pageUrlRelative": "/posts/xM4MLzaTnMA7AQbSM/how-to-be-deader-than-dead", "linkUrl": "https://www.lesswrong.com/posts/xM4MLzaTnMA7AQbSM/how-to-be-deader-than-dead", "postedAtFormatted": "Wednesday, August 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20be%20Deader%20than%20Dead&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20be%20Deader%20than%20Dead%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxM4MLzaTnMA7AQbSM%2Fhow-to-be-deader-than-dead%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20be%20Deader%20than%20Dead%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxM4MLzaTnMA7AQbSM%2Fhow-to-be-deader-than-dead", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxM4MLzaTnMA7AQbSM%2Fhow-to-be-deader-than-dead", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 752, "htmlBody": "<p>For your consideration, a psychology study as summarized by <em>The Economist</em> in <a href=\"https://www.economist.com/node/21526321\">\"How dead is dead? Sometimes, those who have died seem more alive than those who have not\"</a>:</p>\n<blockquote>\n<p>\"They first asked 201 people stopped in public in New York and New England to answer questions after reading one of three short stories. In all three, a man called David was involved in a car accident and suffered serious injuries. In one, he recovered fully. In another, he died. In the third, his entire brain was destroyed except for one part that kept him breathing. Although he was technically alive, he would never again wake up.</p>\n<p>...each participant was asked to rate David&rsquo;s mental capacities, including whether he could influence the outcome of events, know right from wrong, remember incidents from his life, be aware of his environment, possess a personality and have emotions. Participants used a seven-point scale to make these ratings, where 3 indicated that they strongly agreed that he could do such things...and -3 indicated that they strongly disagreed.</p>\n<p>...the fully recovered David rated an average of +1.77 and the dead David -0.29. That score for the dead David was surprising enough, suggesting as it did a considerable amount of mental acuity in the dead. What was extraordinary, though, was the result for the vegetative David: -1.73. In the view of the average New Yorker or New Englander, the vegetative David was more dead [-1.73] than the version who was dead [-0.29].</p>\n<p>...they ran a follow-up experiment which had two different descriptions of the dead David. One said he had simply passed away. The other directed the participant&rsquo;s attention to the corpse. It read, &ldquo;After being embalmed at the morgue, he was buried in the local cemetery. David now lies in a coffin underground.&rdquo;...In this follow-up study participants were also asked to rate how religious they were.</p>\n<p>Once again, the vegetative David was seen to have less mind than the David who had &ldquo;passed away&rdquo;. This was equally true, regardless of how religious a participant said he was. However, ratings of the dead David&rsquo;s mind in the story in which his corpse was embalmed and buried varied with the participant&rsquo;s religiosity. Irreligious participants gave the buried corpse about the same mental ratings as the vegetative patient (-1.51 and -1.64 respectively). Religious participants, however, continued to ascribe less mind to the irretrievably unconscious David than they did to his buried corpse (-1.57 and 0.59).</p>\n<p>That those who believe in an afterlife ascribe mental acuity to the dead is hardly surprising. That those who do not are inclined to do so unless heavily prompted not to is curious indeed.\"</p>\n</blockquote>\n<p>The study is <a href=\"http://www.sciencedirect.com/science/article/pii/S0010027711001752\">\"More dead than dead: Perceptions of persons in the persistent vegetative state\": </a></p>\n<blockquote>\n<p>Patients in persistent vegetative state (PVS) may be biologically alive, but these experiments indicate that people see PVS as a state curiously more dead than dead. Experiment 1 found that PVS patients were perceived to have less mental capacity than the dead. Experiment 2 explained this effect as an outgrowth of afterlife beliefs, and the tendency to focus on the bodies of PVS patients at the expense of their minds. Experiment 3 found that PVS is also perceived as &ldquo;worse&rdquo; than death: people deem early death better than being in PVS. These studies suggest that people perceive the minds of PVS patients as less valuable than those of the dead &ndash; ironically, this effect is especially robust for those high in religiosity.</p>\n</blockquote>\n<p><a href=\"http://blogs.discovermagazine.com/notrocketscience/2011/08/02/deader-than-dead-people-in-vegetative-states-are-viewed-as-deader-than-corpses/\">Ed Yong</a> points to another interesting study, the 2004 <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/14979762\">\"The natural emergence of reasoning about the afterlife as a developmental regularity\"</a>:</p>\n<blockquote>\n<p>Participants were interviewed about the biological and psychological functioning of a dead agent. In Experiment 1, even 4- to 6-year-olds stated that biological processes ceased at death, although this trend was more apparent among 6- to 8-year-olds. In Experiment 2, 4- to 12-year-olds were asked about psychological functioning. The youngest children were equally likely to state that both cognitive and psychobiological states continued at death, whereas the oldest children were more likely to state that cognitive states continued. In Experiment 3, children and adults were asked about an array of psychological states. With the exception of preschoolers, who did not differentiate most of the psychological states, older children and adults were likely to attribute epistemic, emotional, and desire states to dead agents. These findings suggest that developmental mechanisms underlie intuitive accounts of dead agents' minds</p>\n</blockquote>\n<p><a href=\"http://news.ycombinator.com/item?id=2920006\">Jach</a> on <em>Hacker News</em> makes the obvious connection with cryonics; see also lukeprog's \"<a href=\"/lw/73r/remind_physicalists_theyre_physicalists/\">Remind Physicalists They're Physicalists</a>\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"E9ihK6bA9YKkmJs2f": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xM4MLzaTnMA7AQbSM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 26, "extendedScore": null, "score": 5.3e-05, "legacy": true, "legacyId": "9394", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tWrTR8JGT9CRShtW2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-24T16:45:17.227Z", "modifiedAt": null, "url": null, "title": "[LINK] Google Talks: Author lectures about the history of Bayes' Theorem.", "slug": "link-google-talks-author-lectures-about-the-history-of-bayes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:07.486Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZvEwMJTjw6tKXjgA7/link-google-talks-author-lectures-about-the-history-of-bayes", "pageUrlRelative": "/posts/ZvEwMJTjw6tKXjgA7/link-google-talks-author-lectures-about-the-history-of-bayes", "linkUrl": "https://www.lesswrong.com/posts/ZvEwMJTjw6tKXjgA7/link-google-talks-author-lectures-about-the-history-of-bayes", "postedAtFormatted": "Wednesday, August 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Google%20Talks%3A%20Author%20lectures%20about%20the%20history%20of%20Bayes'%20Theorem.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Google%20Talks%3A%20Author%20lectures%20about%20the%20history%20of%20Bayes'%20Theorem.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZvEwMJTjw6tKXjgA7%2Flink-google-talks-author-lectures-about-the-history-of-bayes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Google%20Talks%3A%20Author%20lectures%20about%20the%20history%20of%20Bayes'%20Theorem.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZvEwMJTjw6tKXjgA7%2Flink-google-talks-author-lectures-about-the-history-of-bayes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZvEwMJTjw6tKXjgA7%2Flink-google-talks-author-lectures-about-the-history-of-bayes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<p>\n<p class=\"MsoNormal\">Authors@Google, Sharon Bertsch McGrayne discusses her book&nbsp;<span style=\"font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\">\"The Theory That Would Not Die\" How Bayes' Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy\".</span></p>\n<p class=\"MsoNormal\"><a href=\"http://www.youtube.com/watch?v=8oD6eBkjF9o\">http://www.youtube.com/watch?v=8oD6eBkjF9o</a></p>\n<p class=\"MsoNormal\"><span style=\"font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\">She traces its discovery by an amateur mathematician in the 1740s through its development into roughly its modern form by French scientist Pierre Simon Laplace. She reveals why respected statisticians rendered it professionally taboo for 150 years&mdash;at the same time that practitioners relied on it to solve crises involving great uncertainty and scanty information, even breaking Germany's Enigma code during World War II, and explains how the advent of off-the-shelf computer technology in the 1980s proved to be a game-changer.</span></p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZvEwMJTjw6tKXjgA7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "9395", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-24T17:54:41.669Z", "modifiedAt": null, "url": null, "title": "Are there any Lesswrongers in the Waterloo, Ontario area?", "slug": "are-there-any-lesswrongers-in-the-waterloo-ontario-area", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:19.387Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "endoself", "createdAt": "2011-01-02T09:26:40.389Z", "isAdmin": false, "displayName": "endoself"}, "userId": "e4JPxEMj36oRwTALQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wjSPzAfKgxMfbtX5h/are-there-any-lesswrongers-in-the-waterloo-ontario-area", "pageUrlRelative": "/posts/wjSPzAfKgxMfbtX5h/are-there-any-lesswrongers-in-the-waterloo-ontario-area", "linkUrl": "https://www.lesswrong.com/posts/wjSPzAfKgxMfbtX5h/are-there-any-lesswrongers-in-the-waterloo-ontario-area", "postedAtFormatted": "Wednesday, August 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20there%20any%20Lesswrongers%20in%20the%20Waterloo%2C%20Ontario%20area%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20there%20any%20Lesswrongers%20in%20the%20Waterloo%2C%20Ontario%20area%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwjSPzAfKgxMfbtX5h%2Fare-there-any-lesswrongers-in-the-waterloo-ontario-area%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20there%20any%20Lesswrongers%20in%20the%20Waterloo%2C%20Ontario%20area%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwjSPzAfKgxMfbtX5h%2Fare-there-any-lesswrongers-in-the-waterloo-ontario-area", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwjSPzAfKgxMfbtX5h%2Fare-there-any-lesswrongers-in-the-waterloo-ontario-area", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 28, "htmlBody": "<p>I will soon be attending the University of Waterloo and I was wondering if there is interest in a meetup. Please reply if you are in the area.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wjSPzAfKgxMfbtX5h", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 7.59590658725759e-07, "legacy": true, "legacyId": "9396", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-24T22:46:05.072Z", "modifiedAt": null, "url": null, "title": "[Link] Simon Cowell plans to sign up for cryonics", "slug": "link-simon-cowell-plans-to-sign-up-for-cryonics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:22.939Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W6M3RfqQGogGntSed/link-simon-cowell-plans-to-sign-up-for-cryonics", "pageUrlRelative": "/posts/W6M3RfqQGogGntSed/link-simon-cowell-plans-to-sign-up-for-cryonics", "linkUrl": "https://www.lesswrong.com/posts/W6M3RfqQGogGntSed/link-simon-cowell-plans-to-sign-up-for-cryonics", "postedAtFormatted": "Wednesday, August 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Simon%20Cowell%20plans%20to%20sign%20up%20for%20cryonics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Simon%20Cowell%20plans%20to%20sign%20up%20for%20cryonics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW6M3RfqQGogGntSed%2Flink-simon-cowell-plans-to-sign-up-for-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Simon%20Cowell%20plans%20to%20sign%20up%20for%20cryonics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW6M3RfqQGogGntSed%2Flink-simon-cowell-plans-to-sign-up-for-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW6M3RfqQGogGntSed%2Flink-simon-cowell-plans-to-sign-up-for-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 135, "htmlBody": "<p>From a <a href=\"http://www.gq.com/entertainment/celebrities/201109/simon-cowell-interview-september-2011?printable=true&amp;currentPage=1\">GQ interview</a>:</p>\n<blockquote>\n<p>A while ago, a piece of gossip appeared in a British newspaper, alleging that Cowell had declared&mdash;while dining with the British prime minister at the time, Gordon Brown&mdash;that upon his death he plans to be frozen. Cowell tells me he doesn't recall this discussion (\"I had dinner with him a couple of times, but I can't remember talking about that&mdash;that's probably why I wasn't invited back a third time\") but agrees that, although he has yet to make the arrangements, this is indeed his plan.</p>\n<p>\"It's an insurance policy,\" he reasons. \"If it doesn't work, it doesn't work. If it does work, I'll be happy. If it's possible, and I think it will be, why not have a second crack? Does that sound crazy? I think it's a good idea.\"</p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W6M3RfqQGogGntSed", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 7.596847712578544e-07, "legacy": true, "legacyId": "9398", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-24T23:29:26.032Z", "modifiedAt": null, "url": null, "title": "30 Day Karma", "slug": "30-day-karma-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lessdazed", "createdAt": "2011-02-02T05:06:52.010Z", "isAdmin": false, "displayName": "lessdazed"}, "userId": "ehZzKt5ByYBeyCLkz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9WhpfDQEtdqZR4dpf/30-day-karma-0", "pageUrlRelative": "/posts/9WhpfDQEtdqZR4dpf/30-day-karma-0", "linkUrl": "https://www.lesswrong.com/posts/9WhpfDQEtdqZR4dpf/30-day-karma-0", "postedAtFormatted": "Wednesday, August 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%2030%20Day%20Karma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A30%20Day%20Karma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9WhpfDQEtdqZR4dpf%2F30-day-karma-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=30%20Day%20Karma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9WhpfDQEtdqZR4dpf%2F30-day-karma-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9WhpfDQEtdqZR4dpf%2F30-day-karma-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p>Is the karma shown that accumulated from posts and comments that were made within the past 30 days, or from all votes on all posts within the past 30 days, regardless of the age of the post or comment?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9WhpfDQEtdqZR4dpf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "9399", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-24T23:30:37.344Z", "modifiedAt": null, "url": null, "title": "30 day karma", "slug": "30-day-karma", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:08.044Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lessdazed", "createdAt": "2011-02-02T05:06:52.010Z", "isAdmin": false, "displayName": "lessdazed"}, "userId": "ehZzKt5ByYBeyCLkz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5PkjfDAWXLZweKNyB/30-day-karma", "pageUrlRelative": "/posts/5PkjfDAWXLZweKNyB/30-day-karma", "linkUrl": "https://www.lesswrong.com/posts/5PkjfDAWXLZweKNyB/30-day-karma", "postedAtFormatted": "Wednesday, August 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%2030%20day%20karma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A30%20day%20karma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5PkjfDAWXLZweKNyB%2F30-day-karma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=30%20day%20karma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5PkjfDAWXLZweKNyB%2F30-day-karma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5PkjfDAWXLZweKNyB%2F30-day-karma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 20px;\">Is the karma shown that accumulated from posts and comments that were made within the past 30 days, or from all votes on all posts within the past 30 days, regardless of the age of the post or comment?</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5PkjfDAWXLZweKNyB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 22, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "9400", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-25T02:04:52.405Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Applause Lights", "slug": "seq-rerun-applause-lights", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:08.516Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SzwXgkHJaSACEpNwd/seq-rerun-applause-lights", "pageUrlRelative": "/posts/SzwXgkHJaSACEpNwd/seq-rerun-applause-lights", "linkUrl": "https://www.lesswrong.com/posts/SzwXgkHJaSACEpNwd/seq-rerun-applause-lights", "postedAtFormatted": "Thursday, August 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Applause%20Lights&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Applause%20Lights%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSzwXgkHJaSACEpNwd%2Fseq-rerun-applause-lights%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Applause%20Lights%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSzwXgkHJaSACEpNwd%2Fseq-rerun-applause-lights", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSzwXgkHJaSACEpNwd%2Fseq-rerun-applause-lights", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p>Today's post, <a href=\"/lw/jb/applause_lights/\">Applause Lights</a> was originally published on 11 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Words like \"Democracy\" or \"freedom\" are applause lights - no one disapproves of them, so they can be used to signal conformity and hand-wave away difficult problems. If you hear people talking about the importance of \"balancing risks and opportunities\" or of solving problems \"through a collaborative process\" that aren't followed up by any specifics, then the words are applause lights, not real thoughts.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/78l/seq_rerun_we_dont_really_want_your_participation/\">We Don't Really Want Your Participation</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb26b": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SzwXgkHJaSACEpNwd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 7.597489875101827e-07, "legacy": true, "legacyId": "9401", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dLbkrPu5STNCBLRjr", "Nv83MtesQZyyF3L8v", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-25T02:17:00.455Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 8", "slug": "harry-potter-and-the-methods-of-rationality-discussion-10", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:59.551Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Unnamed", "createdAt": "2009-02-27T06:08:10.900Z", "isAdmin": false, "displayName": "Unnamed"}, "userId": "PdzQ73mN7S4SvRMhu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zvXfBqp6TSriNkmbg/harry-potter-and-the-methods-of-rationality-discussion-10", "pageUrlRelative": "/posts/zvXfBqp6TSriNkmbg/harry-potter-and-the-methods-of-rationality-discussion-10", "linkUrl": "https://www.lesswrong.com/posts/zvXfBqp6TSriNkmbg/harry-potter-and-the-methods-of-rationality-discussion-10", "postedAtFormatted": "Thursday, August 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%208&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%208%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzvXfBqp6TSriNkmbg%2Fharry-potter-and-the-methods-of-rationality-discussion-10%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%208%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzvXfBqp6TSriNkmbg%2Fharry-potter-and-the-methods-of-rationality-discussion-10", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzvXfBqp6TSriNkmbg%2Fharry-potter-and-the-methods-of-rationality-discussion-10", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 270, "htmlBody": "<p><strong>Update: Discussion has moved on to a <a href=\"/r/discussion/lw/7jd/harry_potter_and_the_methods_of_rationality/\">new thread</a>.</strong></p>\n<p>The hiatus is over with today's publication of chapter 73, and the previous thread is approaching the 500-comment threshold, so let's start a new <em><a href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em> discussion thread.&nbsp; This is the place to discuss Eliezer Yudkowsky's Harry Potter fanfic and anything related to it.<br /><br />The first 5 discussion threads are on the main page under the <a href=\"/tag/harry_potter/\">harry_potter tag</a>.&nbsp; Threads 6 and on (including this one) are in the <a href=\"/r/discussion/tag/harry_potter/\">discussion section</a> using its separate tag system.&nbsp; Also: <a href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality\">one</a>, <a href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">two</a>, <a href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">three</a>, <a href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality\">four</a>, <a href=\"/lw/30g/harry_potter_and_the_methods_of_rationality\">five</a>, <a href=\"/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">six</a>, <a href=\"/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality/\">seven</a>.&nbsp; The <a href=\"http://www.fanfiction.net/u/2269863/Less_Wrong\">fanfiction.net author page</a> is the central location for information about updates and links to HPMOR-related goodies, and AdeleneDawner has kept an <a href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author's Notes</a>.<br /><br />As a reminder, it's often useful to start your comment by indicating which chapter you are commenting on.<br /><br /><strong>Spoiler Warning</strong>:&nbsp; this thread is full of spoilers.&nbsp; With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13.&nbsp; <a href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote>\n<p>You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).<br /><br />If there is evidence for X in MOR and/or canon then it's fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that \"Eliezer said X is true\" unless you use rot13.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zvXfBqp6TSriNkmbg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 7.597526006630462e-07, "legacy": true, "legacyId": "9403", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 658, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WQ7XMjqvuRRj8nkpu", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-25T06:33:11.922Z", "modifiedAt": null, "url": null, "title": "IntelligenceExplosion.com graphic redesign", "slug": "intelligenceexplosion-com-graphic-redesign", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:08.691Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Toemt78bxPhu4y5C6/intelligenceexplosion-com-graphic-redesign", "pageUrlRelative": "/posts/Toemt78bxPhu4y5C6/intelligenceexplosion-com-graphic-redesign", "linkUrl": "https://www.lesswrong.com/posts/Toemt78bxPhu4y5C6/intelligenceexplosion-com-graphic-redesign", "postedAtFormatted": "Thursday, August 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20IntelligenceExplosion.com%20graphic%20redesign&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligenceExplosion.com%20graphic%20redesign%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FToemt78bxPhu4y5C6%2Fintelligenceexplosion-com-graphic-redesign%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=IntelligenceExplosion.com%20graphic%20redesign%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FToemt78bxPhu4y5C6%2Fintelligenceexplosion-com-graphic-redesign", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FToemt78bxPhu4y5C6%2Fintelligenceexplosion-com-graphic-redesign", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<p>LW user <a href=\"/user/Lightwave/\">Lightwave</a> offered to redesign <a href=\"http://intelligenceexplosion.com/\">IntelligenceExplosion.com</a>, which at that point looked almost identical to <a href=\"http://www.anthropic-principle.com/\">anthropic-principle.com</a> because I don't do web design. Within 9 days of original email contact, Lightwave completed the project and I uploaded the new files. The redesign is a big improvement, so please go to <a href=\"/lw/70q/intelligenceexplosioncom/4mkc\">this comment</a> by Lightwave and give him/her lots of karma!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Toemt78bxPhu4y5C6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 7.598356817241853e-07, "legacy": true, "legacyId": "9415", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-25T07:24:57.735Z", "modifiedAt": null, "url": null, "title": "Meetup : Helsinki meetup", "slug": "meetup-helsinki-meetup-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:21.107Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kellopyy", "createdAt": "2009-02-27T04:17:07.285Z", "isAdmin": false, "displayName": "Kellopyy"}, "userId": "7BGKsJBzf9m8d2zWQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bGR2FhX6pc59zYJfB/meetup-helsinki-meetup-0", "pageUrlRelative": "/posts/bGR2FhX6pc59zYJfB/meetup-helsinki-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/bGR2FhX6pc59zYJfB/meetup-helsinki-meetup-0", "postedAtFormatted": "Thursday, August 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Helsinki%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Helsinki%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbGR2FhX6pc59zYJfB%2Fmeetup-helsinki-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Helsinki%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbGR2FhX6pc59zYJfB%2Fmeetup-helsinki-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbGR2FhX6pc59zYJfB%2Fmeetup-helsinki-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 33, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2l'>Helsinki meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 September 2011 06:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Yliopistonkatu 5, 00100 Helsinki, Finland </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Less Wrong meetup for our Helsinki group.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2l'>Helsinki meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bGR2FhX6pc59zYJfB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.598524081148302e-07, "legacy": true, "legacyId": "9418", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Helsinki_meetup\">Discussion article for the meetup : <a href=\"/meetups/2l\">Helsinki meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 September 2011 06:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Yliopistonkatu 5, 00100 Helsinki, Finland </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Less Wrong meetup for our Helsinki group.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Helsinki_meetup1\">Discussion article for the meetup : <a href=\"/meetups/2l\">Helsinki meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Helsinki meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_meetup", "level": 1}, {"title": "Discussion article for the meetup : Helsinki meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-25T11:29:12.784Z", "modifiedAt": null, "url": null, "title": "[Poll] Who looks better in your eyes?", "slug": "poll-who-looks-better-in-your-eyes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:21.530Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dg6BEMM7MmjaLWypJ/poll-who-looks-better-in-your-eyes", "pageUrlRelative": "/posts/dg6BEMM7MmjaLWypJ/poll-who-looks-better-in-your-eyes", "linkUrl": "https://www.lesswrong.com/posts/dg6BEMM7MmjaLWypJ/poll-who-looks-better-in-your-eyes", "postedAtFormatted": "Thursday, August 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BPoll%5D%20Who%20looks%20better%20in%20your%20eyes%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BPoll%5D%20Who%20looks%20better%20in%20your%20eyes%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdg6BEMM7MmjaLWypJ%2Fpoll-who-looks-better-in-your-eyes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BPoll%5D%20Who%20looks%20better%20in%20your%20eyes%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdg6BEMM7MmjaLWypJ%2Fpoll-who-looks-better-in-your-eyes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdg6BEMM7MmjaLWypJ%2Fpoll-who-looks-better-in-your-eyes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 144, "htmlBody": "<p>This is thread where I'm trying to figure out a few things about <a href=\"http://wiki.lesswrong.com/wiki/Signaling\">signalling</a> on LessWrong and need some information, so please immediately after reading about the two individuals please answer the <a href=\"/r/discussion/lw/79q/poll_who_looks_better_to_you/4p0j\">poll</a>. The two individuals:</p>\n<p><br /><a href=\"/r/discussion/lw/79q/poll_who_looks_better_to_you/4p0m\"><strong>A.</strong></a> Sees that an interpretation of reality shared by others is not correct, but tries to pretend otherwise for personal gain and/or safety.</p>\n<p><a href=\"/r/discussion/lw/79q/poll_who_looks_better_to_you/4p0l\"><strong>B.</strong></a> Fails to see that an interpretation of reality is shared by others is flawed. He is therefore perfectly honest in sharing the interpretation of reality with others. The reward regime for outward behaviour is the same as with A.</p>\n<p>&nbsp;</p>\n<p>To add a <a href=\"http://wiki.lesswrong.com/wiki/Trivial_inconvenience\">trivial inconvenience</a> that matches the inconvenience of answering the poll before reading on, comments on what I think the two individuals signal,what the trade off is and what I speculate the results might be here versus the general population, is behind this <a href=\"/r/discussion/lw/79q/poll_who_looks_better_to_you/4p0s\">link</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dg6BEMM7MmjaLWypJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "9422", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 100, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-25T12:49:22.368Z", "modifiedAt": null, "url": null, "title": "Meetup : Austin, TX", "slug": "meetup-austin-tx-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:08.946Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iezekcSC5CMovT3SF/meetup-austin-tx-0", "pageUrlRelative": "/posts/iezekcSC5CMovT3SF/meetup-austin-tx-0", "linkUrl": "https://www.lesswrong.com/posts/iezekcSC5CMovT3SF/meetup-austin-tx-0", "postedAtFormatted": "Thursday, August 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Austin%2C%20TX&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Austin%2C%20TX%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiezekcSC5CMovT3SF%2Fmeetup-austin-tx-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Austin%2C%20TX%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiezekcSC5CMovT3SF%2Fmeetup-austin-tx-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiezekcSC5CMovT3SF%2Fmeetup-austin-tx-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/2m\">Austin, TX</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">27 August 2011 01:30:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Saturday SATURDAY <em>SATURDAY</em>, come down to Caffe Medici and be AMAZED and ASTOUNDED by the WARM PERSONALITIES and GREAT CONVERSATION on display!</p>\n<p>We typically occupy the little raised stage on the second floor, to the left of the stairs. Come join us!</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/2m\">Austin, TX</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iezekcSC5CMovT3SF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 7.599572507115921e-07, "legacy": true, "legacyId": "9424", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Austin__TX\">Discussion article for the meetup : <a href=\"/meetups/2m\">Austin, TX</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">27 August 2011 01:30:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Saturday SATURDAY <em>SATURDAY</em>, come down to Caffe Medici and be AMAZED and ASTOUNDED by the WARM PERSONALITIES and GREAT CONVERSATION on display!</p>\n<p>We typically occupy the little raised stage on the second floor, to the left of the stairs. Come join us!</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Austin__TX1\">Discussion article for the meetup : <a href=\"/meetups/2m\">Austin, TX</a></h2>", "sections": [{"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX", "level": 1}, {"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-25T14:30:29.507Z", "modifiedAt": null, "url": null, "title": "Prisoner's Dilemma as a Game Theory Laboratory", "slug": "prisoner-s-dilemma-as-a-game-theory-laboratory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:23.296Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "prase", "createdAt": "2009-02-28T10:00:10.260Z", "isAdmin": false, "displayName": "prase"}, "userId": "WAP32wvmNt9QdutSu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vJAFXz6DWYnKq4Mue/prisoner-s-dilemma-as-a-game-theory-laboratory", "pageUrlRelative": "/posts/vJAFXz6DWYnKq4Mue/prisoner-s-dilemma-as-a-game-theory-laboratory", "linkUrl": "https://www.lesswrong.com/posts/vJAFXz6DWYnKq4Mue/prisoner-s-dilemma-as-a-game-theory-laboratory", "postedAtFormatted": "Thursday, August 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Prisoner's%20Dilemma%20as%20a%20Game%20Theory%20Laboratory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrisoner's%20Dilemma%20as%20a%20Game%20Theory%20Laboratory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvJAFXz6DWYnKq4Mue%2Fprisoner-s-dilemma-as-a-game-theory-laboratory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Prisoner's%20Dilemma%20as%20a%20Game%20Theory%20Laboratory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvJAFXz6DWYnKq4Mue%2Fprisoner-s-dilemma-as-a-game-theory-laboratory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvJAFXz6DWYnKq4Mue%2Fprisoner-s-dilemma-as-a-game-theory-laboratory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 895, "htmlBody": "<p>Last year Yvain had <a href=\"/lw/32u/diplomacy_as_a_game_theory_laboratory/\">organised</a> a <a href=\"http://en.wikipedia.org/wiki/Diplomacy_game\">Diplomacy game</a> between LessWrong users to test how well we perform in practical application of game theory. At least <a href=\"/lw/32z/spring_1912_a_new_heaven_and_a_new_earth/\">two</a> <a href=\"/lw/345/rationalist_diplomacy_game_2_game_over/\">games</a> had been played, but as far as I know no analysis was made afterwards. One reason is probably that few games involving complex interactions between players constitute at most anecdotal evidence for whatever hypothesis one may test. The second one is lack of comparison to outside players. Although the games were fun, their value as a game theory experiment remains rather low. Could we test our game theoretic skills in a statistically more significant way?</p>\n<p>Only recently I have learned about Robert Axelrod's <a href=\"http://en.wikipedia.org/wiki/Prisoner%27s_dilemma#Strategy_for_the_classic_prisoner.27s_dilemma_2\">experiment</a> in which he run a competition of different strategies playing <em>iteraded prisoner's dilemma</em>, and got an idea to replicate it. I have already run a similar experiment with five contestants (all being my friends) and now a second run is being prepared, with at least nine strategies in the pool. I am interested in a third run, this time with strategies nominated by LessWrongers. The contestants of the second run which has identical rules are readers of my blog and neither of them is probably familiar with specific LW ideas. Therefore, they would serve as a fairly good control group to test LW's applied rationality skills (or a subset of). After matching the strategies in both groups separately, I plan to put all of them together and see who wins.</p>\n<p>So, if you want to participate in this contest, feel free to send me your strategy. The rules are following.</p>\n<ol>\n<li>By a <em>strategy</em> I mean a program sent by a contestant or coded according to his/her instructions. The strategies compete in iterated prisoner's dilemmas. A single iteration I will call a <em>turn</em>. In each turn each strategy has to choose between <em>cooperating</em> and <em>defecting</em>. The payoffs are: \n<ul>\n<li>if both cooperate, 4 points for each</li>\n<li>if both defect, 1 point for each</li>\n<li>else 7 points for the defector and 0 points for its cooperating opponent</li>\n</ul>\n</li>\n<li>By a <em>match</em> I mean a series of 100 iterations between the same opponents.</li>\n<li>There will be two different competitions, the <em>round-robin tournament</em> and the <em>evolutionary tournament</em>. Two separate final standings will be made, one for each tournament. Any received strategy has to participate in both. \n<ul>\n<li>In the round-robin tournament strategies will play one match against each other (not against a copy of themselves). The winner will be the strategy which acquires the highest total number of points. Number of won or lost matches is disregarded.</li>\n<li>The evolutionary tournament will simulate evolution of a population of strategies. In the beginning, equal number of copies of each strategy is present in the pool. Then the strategies are paired randomly (now a strategy may be paired against a copy of itself) and each pair plays one match. In the next <em>generation</em> pool, the strategies will be represented in numbers proportional to the total number of points won by all its copies in the present generation. The total population will be maintained at a constant level (probably 2,000 strategies, up to rounding errors). Strategies may go extinct. The strategy with the highest number of copies after the 100th generation will be considered a winner.</li>\n</ul>\n</li>\n<li>A strategy has access to results of all previous turns in its current match and the number of current turn. It can decide randomly (a pseudo-random generator will be used). It does <em>not</em> have access to results of other matches (including its own previous matches), the number of current generation, population sizes, number of points won by any strategy in any phase of the tournament (except the number of points already won in the current match which can be calculated from the previous turns results) and its opponent's identity (namely it doesn't know whether it plays against a copy of itself). The strategies obviously can't read their opponent's source code.</li>\n<li>Each person can send only one strategy. Be honest.</li>\n<li>The strategy can be described in any comprehensible language. If I find problems in understanding, which will probably happen if the strategy is described in Lisp or Basque, but can happen even if it is written in English, I will ask.</li>\n<li>You should send your strategies by private message, not in comments to this post. Your opponents shouldn't know what you have prepared.</li>\n<li>The strategy needn't be original, but if I get two identical strategies, I will treat them as one.</li>\n<li>A Fully Random strategy is automatically included in the tournament. It plays defect or cooperate randomly with 50% chance each turn.</li>\n<li>Names of the authors of the strategies will be published by default. If you wish your name excluded, specify it in your message, and your strategy will compete anonymously.</li>\n</ol>\n<p>The simulation will probably not be run before at least eight strategies are collected and before the beginning of September. <em>The competition is closed, no new strategies are accepted at this moment. 21 different strategies were accepted, their implementations are now being tested. Results will be probably posted on Sunday 4th September.<br /></em></p>\n<p>[Edit: Found inconsistency in using words <em>round</em> and <em>turn</em> to denote the same thing. Now <em>turn</em> is used everywhere.]<em></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"be2Mh2bddQ6ZaBcti": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vJAFXz6DWYnKq4Mue", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 22, "extendedScore": null, "score": 7.599899354549932e-07, "legacy": true, "legacyId": "9423", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jkf2YjuH8Z2E7hKBA", "BD6WYC4GT6dnWaJRN", "RZuwnE7htZTgaruBJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-25T18:02:01.019Z", "modifiedAt": null, "url": null, "title": "Meetup : Madison", "slug": "meetup-madison-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:19.597Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/63TWj97qZDmNAbMEE/meetup-madison-2", "pageUrlRelative": "/posts/63TWj97qZDmNAbMEE/meetup-madison-2", "linkUrl": "https://www.lesswrong.com/posts/63TWj97qZDmNAbMEE/meetup-madison-2", "postedAtFormatted": "Thursday, August 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Madison&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Madison%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F63TWj97qZDmNAbMEE%2Fmeetup-madison-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Madison%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F63TWj97qZDmNAbMEE%2Fmeetup-madison-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F63TWj97qZDmNAbMEE%2Fmeetup-madison-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 89, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2n'>Madison</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 August 2011 06:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2100 Winnebago St, Madison, WI 53704</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The third biweekly Madison Meetup, this week at <a href=\"http://www.sector67.org\" rel=\"nofollow\">Sector67</a>.</p>\n\n<p>For rather lighter fare than last time, we'll play at least a few rounds of The Resistance -- in which good play demands practical use of Bayesian reasoning and understanding the causes of your own thoughts. Also, it's quite fun. If you're a reader in the area, sign up on the Less Wrong Madison <a href=\"https://groups.google.com/forum/#!forum/lesswrong-madison\">mailing list</a>!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2n'>Madison</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "63TWj97qZDmNAbMEE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.600583149089001e-07, "legacy": true, "legacyId": "9425", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Madison\">Discussion article for the meetup : <a href=\"/meetups/2n\">Madison</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 August 2011 06:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2100 Winnebago St, Madison, WI 53704</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The third biweekly Madison Meetup, this week at <a href=\"http://www.sector67.org\" rel=\"nofollow\">Sector67</a>.</p>\n\n<p>For rather lighter fare than last time, we'll play at least a few rounds of The Resistance -- in which good play demands practical use of Bayesian reasoning and understanding the causes of your own thoughts. Also, it's quite fun. If you're a reader in the area, sign up on the Less Wrong Madison <a href=\"https://groups.google.com/forum/#!forum/lesswrong-madison\">mailing list</a>!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Madison1\">Discussion article for the meetup : <a href=\"/meetups/2n\">Madison</a></h2>", "sections": [{"title": "Discussion article for the meetup : Madison", "anchor": "Discussion_article_for_the_meetup___Madison", "level": 1}, {"title": "Discussion article for the meetup : Madison", "anchor": "Discussion_article_for_the_meetup___Madison1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-25T19:16:08.631Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Wednesday August 31 7pm", "slug": "meetup-fort-collins-colorado-meetup-wednesday-august-31-7pm", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.821Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kNJ867P2owYXnrTfq/meetup-fort-collins-colorado-meetup-wednesday-august-31-7pm", "pageUrlRelative": "/posts/kNJ867P2owYXnrTfq/meetup-fort-collins-colorado-meetup-wednesday-august-31-7pm", "linkUrl": "https://www.lesswrong.com/posts/kNJ867P2owYXnrTfq/meetup-fort-collins-colorado-meetup-wednesday-august-31-7pm", "postedAtFormatted": "Thursday, August 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wednesday%20August%2031%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wednesday%20August%2031%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkNJ867P2owYXnrTfq%2Fmeetup-fort-collins-colorado-meetup-wednesday-august-31-7pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wednesday%20August%2031%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkNJ867P2owYXnrTfq%2Fmeetup-fort-collins-colorado-meetup-wednesday-august-31-7pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkNJ867P2owYXnrTfq%2Fmeetup-fort-collins-colorado-meetup-wednesday-august-31-7pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2o'>Fort Collins, Colorado Meetup Wednesday August 31 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 August 2011 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Coffee shop chat format, to get to know other rationalists. Look for the LessWrong sign. I'll be there till at least 8pm.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2o'>Fort Collins, Colorado Meetup Wednesday August 31 7pm</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kNJ867P2owYXnrTfq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 7.600822803728092e-07, "legacy": true, "legacyId": "9426", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wednesday_August_31_7pm\">Discussion article for the meetup : <a href=\"/meetups/2o\">Fort Collins, Colorado Meetup Wednesday August 31 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 August 2011 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Coffee shop chat format, to get to know other rationalists. Look for the LessWrong sign. I'll be there till at least 8pm.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wednesday_August_31_7pm1\">Discussion article for the meetup : <a href=\"/meetups/2o\">Fort Collins, Colorado Meetup Wednesday August 31 7pm</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wednesday August 31 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wednesday_August_31_7pm", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wednesday August 31 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wednesday_August_31_7pm1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-25T21:03:12.032Z", "modifiedAt": null, "url": null, "title": "LW September WebDiplomacy Games", "slug": "lw-september-webdiplomacy-games", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:09.669Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TyNtGwW7p6dNeESHY/lw-september-webdiplomacy-games", "pageUrlRelative": "/posts/TyNtGwW7p6dNeESHY/lw-september-webdiplomacy-games", "linkUrl": "https://www.lesswrong.com/posts/TyNtGwW7p6dNeESHY/lw-september-webdiplomacy-games", "postedAtFormatted": "Thursday, August 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%20September%20WebDiplomacy%20Games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%20September%20WebDiplomacy%20Games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTyNtGwW7p6dNeESHY%2Flw-september-webdiplomacy-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%20September%20WebDiplomacy%20Games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTyNtGwW7p6dNeESHY%2Flw-september-webdiplomacy-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTyNtGwW7p6dNeESHY%2Flw-september-webdiplomacy-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 492, "htmlBody": "<p><a href=\"/user/prase/\">prase</a> is starting up a Prisoner's Dilemma Game Theory Lab which you ought to <a href=\"/r/discussion/lw/79r/prisoners_dilemma_as_a_game_theory_laboratory/\">check out</a>, and that revived (my) interest in playing <a href=\"/lw/32u/diplomacy_as_a_game_theory_laboratory/\">Diplomacy with LWers</a>. We've had <a href=\"/lw/32z/spring_1912_a_new_heaven_and_a_new_earth/\">two</a> <a href=\"/lw/345/rationalist_diplomacy_game_2_game_over/\">games</a> run on this site, and <a href=\"http://webdiplomacy.net/board.php?gameID=42765\">one</a> run on WebDiplomacy.</p>\n<p>Diplomacy is easy to learn and, at its heart, a very simple game. Strategy and (surprisingly) diplomacy take center stage; tactics cannot get you very far, and there is no randomness. Players control a great power chosen at random on the eve of WWI, with all powers having roughly the same army size and strength, and so the ability to prevail in conflicts comes almost entirely from creating the right alliances and knowing when to trust (and betray) others. (Also, did you catch the lie in this explanation? Diplomacy involves a lot of lies.)</p>\n<p>WebDiplomacy is a free site you can use to play Diplomacy with people online; <a href=\"http://webdiplomacy.net/register.php\">make an account here</a>. It also seems convenient for having multiple games going on or doing games repeated- Yvain did a heroic job in organizing, administrating, and updating the first game, but it's the sort of job that can be done with less personality by a computer. So, this post exists to help people interested in playing Diplomacy with LWers find other people to play with.</p>\n<p>The games spawned by this post will start <strong>around the beginning of September</strong>, and a typical game lasts somewhere around 20 turns (though many players are eliminated before then). If there's continuing interest, we'll probably have a thread like this once a month, so if you don't know if you can commit to being active during September, but want to make sure a November thread gets posted even if there's not much interest now, reply to the November comment below.</p>\n<p>A downside of Diplomacy is it really only works with 7 people, but we should be able to get at least one game going. It seems natural to break people up by game pace, as that represents significantly different time commitments. I'll be posting a number of comments with different game paces (i.e. times between turns); please <strong>only </strong>reply to the time you would <strong>most prefer</strong>. That'll make it easy to see how many players we have that are interested, and if we need to shuffle people between paces that'll be easy enough to do later. <strong>Signups will close August 31st</strong>, and games may start before then if we have sufficient interest. <strong></strong>An exception is single-day games; those games will only have 15 or 20 minute turns, and thus only last one day (try to have an 8 hour block open) and so shouldn't conflict with longer-running games.</p>\n<p>&nbsp;</p>\n<p>[Edit]The comments below mention that orders must be submitted \"at\" a particular time; what I mean is they need to be submitted <em>by</em> that time. Apologies for any resulting confusion, and hat tip to <a href=\"/user/RobertLumley/\">RobertLumley</a> for pointing that out.</p>\n<p>&nbsp;</p>\n<p>[Edit 9/1: I'm sending out private messages to everyone who signed up pointing them to <a href=\"/r/discussion/lw/7ec/lw_september_webdiplomacy_games_starting_soon/\">this new post</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TyNtGwW7p6dNeESHY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 7.6011689451339e-07, "legacy": true, "legacyId": "9427", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vJAFXz6DWYnKq4Mue", "jkf2YjuH8Z2E7hKBA", "BD6WYC4GT6dnWaJRN", "RZuwnE7htZTgaruBJ", "MW2q5BnfzB85FwcFY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-25T22:03:56.295Z", "modifiedAt": null, "url": null, "title": "Meetup : Irvine Meetup Wednesday August 31", "slug": "meetup-irvine-meetup-wednesday-august-31", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:20.336Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JGWeissman", "createdAt": "2009-04-01T04:43:56.740Z", "isAdmin": false, "displayName": "JGWeissman"}, "userId": "Mw8rsM7m7E8nnEFEp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fKcR28bQYMyWCcPuD/meetup-irvine-meetup-wednesday-august-31", "pageUrlRelative": "/posts/fKcR28bQYMyWCcPuD/meetup-irvine-meetup-wednesday-august-31", "linkUrl": "https://www.lesswrong.com/posts/fKcR28bQYMyWCcPuD/meetup-irvine-meetup-wednesday-august-31", "postedAtFormatted": "Thursday, August 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Irvine%20Meetup%20Wednesday%20August%2031&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Irvine%20Meetup%20Wednesday%20August%2031%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfKcR28bQYMyWCcPuD%2Fmeetup-irvine-meetup-wednesday-august-31%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Irvine%20Meetup%20Wednesday%20August%2031%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfKcR28bQYMyWCcPuD%2Fmeetup-irvine-meetup-wednesday-august-31", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfKcR28bQYMyWCcPuD%2Fmeetup-irvine-meetup-wednesday-august-31", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 93, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2p'>Irvine Meetup Wednesday August 31</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 August 2011 06:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">4187 Campus Dr, University Center, Irvine, CA 92612</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This continues the weekly meetups in Irvine. As always the meetup at the outdoor food court in the <a href=\"http://maps.google.com/maps?ie=UTF8&amp;ll=33.650288,-117.838666&amp;spn=0.001684,0.002363&amp;t=h&amp;z=19\" rel=\"nofollow\">University Center near UCI</a>, from 6:00 to 8:00 (or whenever we actually decide to leave). Look for the sign with <a href=\"http://lesswrong.com/lw/nn/neural_categories/\">naive neural classifiers for bleggs and rubes</a>. See also the <a href=\"http://groups.google.com/group/LW-SoCal-Announce?pli=1\" rel=\"nofollow\">email group</a> and <a href=\"https://www.google.com/calendar/embed?src=h57ej586rdo3jmld14hrk51m1c%40group.calendar.google.com&amp;ctz=America/Los_Angeles\" rel=\"nofollow\">calendar</a> for the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California Meetup Group</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2p'>Irvine Meetup Wednesday August 31</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fKcR28bQYMyWCcPuD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.60136533787376e-07, "legacy": true, "legacyId": "9428", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Irvine_Meetup_Wednesday_August_31\">Discussion article for the meetup : <a href=\"/meetups/2p\">Irvine Meetup Wednesday August 31</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 August 2011 06:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">4187 Campus Dr, University Center, Irvine, CA 92612</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This continues the weekly meetups in Irvine. As always the meetup at the outdoor food court in the <a href=\"http://maps.google.com/maps?ie=UTF8&amp;ll=33.650288,-117.838666&amp;spn=0.001684,0.002363&amp;t=h&amp;z=19\" rel=\"nofollow\">University Center near UCI</a>, from 6:00 to 8:00 (or whenever we actually decide to leave). Look for the sign with <a href=\"http://lesswrong.com/lw/nn/neural_categories/\">naive neural classifiers for bleggs and rubes</a>. See also the <a href=\"http://groups.google.com/group/LW-SoCal-Announce?pli=1\" rel=\"nofollow\">email group</a> and <a href=\"https://www.google.com/calendar/embed?src=h57ej586rdo3jmld14hrk51m1c%40group.calendar.google.com&amp;ctz=America/Los_Angeles\" rel=\"nofollow\">calendar</a> for the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California Meetup Group</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Irvine_Meetup_Wednesday_August_311\">Discussion article for the meetup : <a href=\"/meetups/2p\">Irvine Meetup Wednesday August 31</a></h2>", "sections": [{"title": "Discussion article for the meetup : Irvine Meetup Wednesday August 31", "anchor": "Discussion_article_for_the_meetup___Irvine_Meetup_Wednesday_August_31", "level": 1}, {"title": "Discussion article for the meetup : Irvine Meetup Wednesday August 31", "anchor": "Discussion_article_for_the_meetup___Irvine_Meetup_Wednesday_August_311", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yFDKvfN6D87Tf5J9f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-26T03:22:10.622Z", "modifiedAt": null, "url": null, "title": ".", "slug": "", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:09.639Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "a7xJQpZ55R6SxFTik", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bk5apN7nrKkYYcQFb/", "pageUrlRelative": "/posts/bk5apN7nrKkYYcQFb/", "linkUrl": "https://www.lesswrong.com/posts/bk5apN7nrKkYYcQFb/", "postedAtFormatted": "Friday, August 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbk5apN7nrKkYYcQFb%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbk5apN7nrKkYYcQFb%2F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbk5apN7nrKkYYcQFb%2F", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bk5apN7nrKkYYcQFb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 7.60239449624078e-07, "legacy": true, "legacyId": "9432", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-26T05:01:34.891Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Rationality and the English Language", "slug": "seq-rerun-rationality-and-the-english-language", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:36.835Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Wp5jejbogdLGdehLy/seq-rerun-rationality-and-the-english-language", "pageUrlRelative": "/posts/Wp5jejbogdLGdehLy/seq-rerun-rationality-and-the-english-language", "linkUrl": "https://www.lesswrong.com/posts/Wp5jejbogdLGdehLy/seq-rerun-rationality-and-the-english-language", "postedAtFormatted": "Friday, August 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Rationality%20and%20the%20English%20Language&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Rationality%20and%20the%20English%20Language%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWp5jejbogdLGdehLy%2Fseq-rerun-rationality-and-the-english-language%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Rationality%20and%20the%20English%20Language%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWp5jejbogdLGdehLy%2Fseq-rerun-rationality-and-the-english-language", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWp5jejbogdLGdehLy%2Fseq-rerun-rationality-and-the-english-language", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 200, "htmlBody": "<p>Today's post, <a href=\"/lw/jc/rationality_and_the_english_language/\">Rationality and the English Language</a> was originally published on 12 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>George Orwell's writings on language and totalitarianism are critical to understanding rationality. Orwell was an opponent of the use of words to obscure meaning, or to convey ideas without their emotional impact. Language should get the point across - when the effort to convey information gets lost in the effort to sound authoritative, you are acting irrationally.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/795/seq_rerun_applause_lights/\">Applause Lights</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Wp5jejbogdLGdehLy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 7.602716012586584e-07, "legacy": true, "legacyId": "9435", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Lz64L3yJEtYGkzMzu", "SzwXgkHJaSACEpNwd", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-26T08:14:10.294Z", "modifiedAt": null, "url": null, "title": "Why no archive of refuted research?", "slug": "why-no-archive-of-refuted-research", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:08.903Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kSgQmZ49fLrkfRndu/why-no-archive-of-refuted-research", "pageUrlRelative": "/posts/kSgQmZ49fLrkfRndu/why-no-archive-of-refuted-research", "linkUrl": "https://www.lesswrong.com/posts/kSgQmZ49fLrkfRndu/why-no-archive-of-refuted-research", "postedAtFormatted": "Friday, August 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20no%20archive%20of%20refuted%20research%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20no%20archive%20of%20refuted%20research%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkSgQmZ49fLrkfRndu%2Fwhy-no-archive-of-refuted-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20no%20archive%20of%20refuted%20research%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkSgQmZ49fLrkfRndu%2Fwhy-no-archive-of-refuted-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkSgQmZ49fLrkfRndu%2Fwhy-no-archive-of-refuted-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 556, "htmlBody": "<p>From <em>The Atlantic</em>'s <a href=\"http://www.theatlantic.com/magazine/archive/2010/11/lies-damned-lies-and-medical-science/8269/1/\">Lies, Damned Lies and Medical Science</a>:</p>\n<blockquote>\n<p>Still, Ioannidis anticipated that the community might shrug off his findings: sure, a lot of dubious research makes it into journals, but we researchers and physicians know to ignore it and focus on the good stuff, so what&rsquo;s the big deal? The other paper headed off that claim. He zoomed in on 49 of the most highly regarded research findings in medicine over the previous 13 years, as judged by the science community&rsquo;s two standard measures: the papers had appeared in the journals most widely cited in research articles, and the 49 articles themselves were the most widely cited articles in these journals. These were articles that helped lead to the widespread popularity of treatments such as the use of hormone-replacement therapy for menopausal women, vitamin E to reduce the risk of heart disease, coronary stents to ward off heart attacks, and daily low-dose aspirin to control blood pressure and prevent heart attacks and strokes. Ioannidis was putting his contentions to the test not against run-of-the-mill research, or even merely well-accepted research, but against the absolute tip of the research pyramid. Of the 49 articles, 45 claimed to have uncovered effective interventions. Thirty-four of these claims had been retested, and 14 of these, or 41 percent, had been convincingly shown to be wrong or significantly exaggerated. If between a third and a half of the most acclaimed research in medicine was proving untrustworthy, the scope and impact of the problem were undeniable.<br /><br />[...]<br /><br />But even for medicine&rsquo;s most influential studies, the evidence sometimes remains surprisingly narrow. Of those 45 super-cited studies that Ioannidis focused on, 11 had never been retested. Perhaps worse, Ioannidis found that even when a research error is outed, it typically persists for years or even decades. He looked at three prominent health studies from the 1980s and 1990s that were each later soundly refuted, and discovered that researchers continued to cite the original results as correct more often than as flawed&mdash;in one case for at least 12 years after the results were discredited.</p>\n</blockquote>\n<p>Here's a suggested solution to the problem of refuted research still being cited. Have some respected agency maintain an archive of studies that have failed to replicate or that have otherwise been found lacking. Once such an archive existed, medical journals could adopt a policy of checking all citations in a proposed article against the archive, rejecting submissions that tried to cite refuted research as valid. This might also alleviate the problem of people not doing replications because replications don't get many cites. Once such an archive was established, getting your results there might become quite prestigious.</p>\n<p>The one major problem that I can see with this proposal is that it's not always obvious when a study should be considered refuted. But even erring on the side of only including firmly refuted studies should be much better than nothing at all.</p>\n<p>Such a fix seems obvious and simple to me, and while maintaining the  archive and keeping it up to date would be expensive, it should be  easily affordable for an organization such as a major university or the  NIH. Similar archives could also be used for fields other than medicine. Is there some reason that I'm missing for why this isn't done?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZpG9rheyAkgCoEQea": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kSgQmZ49fLrkfRndu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 37, "extendedScore": null, "score": 8.5e-05, "legacy": true, "legacyId": "9442", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-26T12:54:14.893Z", "modifiedAt": null, "url": null, "title": "Meetup : London Science Museum, Aug. 31", "slug": "meetup-london-science-museum-aug-31", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mRSazdpwhspsxHWXw/meetup-london-science-museum-aug-31", "pageUrlRelative": "/posts/mRSazdpwhspsxHWXw/meetup-london-science-museum-aug-31", "linkUrl": "https://www.lesswrong.com/posts/mRSazdpwhspsxHWXw/meetup-london-science-museum-aug-31", "postedAtFormatted": "Friday, August 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Science%20Museum%2C%20Aug.%2031&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Science%20Museum%2C%20Aug.%2031%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmRSazdpwhspsxHWXw%2Fmeetup-london-science-museum-aug-31%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Science%20Museum%2C%20Aug.%2031%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmRSazdpwhspsxHWXw%2Fmeetup-london-science-museum-aug-31", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmRSazdpwhspsxHWXw%2Fmeetup-london-science-museum-aug-31", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2q'>London Science Museum, Aug. 31</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 August 2011 06:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Exhibition Road, London SW7 2DD</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We have decided to try different meetup formats and the next 'Science Museum Lates' event for adults, on Wednesday 31st of August looks like just the thing.</p>\n\n<p>More information about the event: <a href=\"http://www.sciencemuseum.org.uk/visitmuseum/events/events_for_adults/Lates.aspx\" rel=\"nofollow\">http://www.sciencemuseum.org.uk/visitmuseum/events/events_for_adults/Lates.aspx</a></p>\n\n<p>UPDATE: Meeting time&amp;place under discussion, will be updated as soon as they are decided.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2q'>London Science Museum, Aug. 31</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mRSazdpwhspsxHWXw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 7.6042451466557e-07, "legacy": true, "legacyId": "9448", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Science_Museum__Aug__31\">Discussion article for the meetup : <a href=\"/meetups/2q\">London Science Museum, Aug. 31</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 August 2011 06:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Exhibition Road, London SW7 2DD</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We have decided to try different meetup formats and the next 'Science Museum Lates' event for adults, on Wednesday 31st of August looks like just the thing.</p>\n\n<p>More information about the event: <a href=\"http://www.sciencemuseum.org.uk/visitmuseum/events/events_for_adults/Lates.aspx\" rel=\"nofollow\">http://www.sciencemuseum.org.uk/visitmuseum/events/events_for_adults/Lates.aspx</a></p>\n\n<p>UPDATE: Meeting time&amp;place under discussion, will be updated as soon as they are decided.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_Science_Museum__Aug__311\">Discussion article for the meetup : <a href=\"/meetups/2q\">London Science Museum, Aug. 31</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Science Museum, Aug. 31", "anchor": "Discussion_article_for_the_meetup___London_Science_Museum__Aug__31", "level": 1}, {"title": "Discussion article for the meetup : London Science Museum, Aug. 31", "anchor": "Discussion_article_for_the_meetup___London_Science_Museum__Aug__311", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-26T13:27:35.095Z", "modifiedAt": null, "url": null, "title": "General textbook comparison thread", "slug": "general-textbook-comparison-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:08.263Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gjm", "createdAt": "2009-03-09T01:11:32.668Z", "isAdmin": false, "displayName": "gjm"}, "userId": "977L8MR7JmNrQx6df", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mzsPs7TudmSuTowRz/general-textbook-comparison-thread", "pageUrlRelative": "/posts/mzsPs7TudmSuTowRz/general-textbook-comparison-thread", "linkUrl": "https://www.lesswrong.com/posts/mzsPs7TudmSuTowRz/general-textbook-comparison-thread", "postedAtFormatted": "Friday, August 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20General%20textbook%20comparison%20thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGeneral%20textbook%20comparison%20thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmzsPs7TudmSuTowRz%2Fgeneral-textbook-comparison-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=General%20textbook%20comparison%20thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmzsPs7TudmSuTowRz%2Fgeneral-textbook-comparison-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmzsPs7TudmSuTowRz%2Fgeneral-textbook-comparison-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 314, "htmlBody": "<p>We've already had a lengthy (and still active) <a title=\"&quot;The best textbooks on every subject&quot;, started by lukeprog\" href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">thread</a> attempting to address the question \"What are the best textbooks, and why are they better than their rivals?\". That's excellent, but no one is going to post there unless they're prepared to claim: Textbook X is <em>the best</em> on its subject. But surely many of us have read many texts for which we couldn't say that but could say \"I've read X and Y, and here's how they differ\". A good supply of such comparisons would be extremely useful.</p>\n<p>I propose this thread for that purpose. Rules:</p>\n<ul>\n<li>Each top-level reply should concern two or more texts on a single subject, and provide enough information about how they compare to one another that an interested would-be reader should be able to tell which is likely to be better for his or her purposes.</li>\n<li>Replies to these offering or soliciting further comparisons in the same domain are encouraged.</li>\n<li>At least one book in each comparison should <em>either</em> \n<ul>\n<li>be a very good one, <em>or</em> at least<em><br /></em></li>\n<li>look like a very good one even though it isn't.</li>\n</ul>\n</li>\n</ul>\n<p>If this gets enough responses that simply looking through them becomes tiresome, I'll update the article with (something like) a list of textbooks, arranged by subject and then by author, with links for the comments in which they're compared to other books and a brief summary of what's said about them. (I might include links to comments in Luke's thread too, since anything that deserves its place there would also be acceptable here.)</p>\n<p>See also: <a title=\"&quot;Best Textbook List Expansion&quot;, started by magfrump\" href=\"/r/discussion/lw/715/best_textbook_list_expansion/\">magfrump's request</a> for recommendations of basic science books; <a title=\"&quot;Recommended Rationalist Reading&quot;, started by Eliezer\" href=\"/lw/jv/recommended_rationalist_reading/\">\"Recommended Rationalist Reading\"</a> (narrower subject focus, and without the element of comparison).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mzsPs7TudmSuTowRz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "9449", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xg3hXCYQPJkwHyik2", "MmG5YnQT2bJvxSX3R", "RiQYixgCdvd8eWsjg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-26T15:25:42.166Z", "modifiedAt": null, "url": null, "title": "Weight training", "slug": "weight-training", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:20.514Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexflint", "createdAt": "2009-07-17T10:07:09.115Z", "isAdmin": false, "displayName": "Alex Flint"}, "userId": "ifEGDHySkAejhCFDf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3D9Q82yt9rGYcKc4H/weight-training", "pageUrlRelative": "/posts/3D9Q82yt9rGYcKc4H/weight-training", "linkUrl": "https://www.lesswrong.com/posts/3D9Q82yt9rGYcKc4H/weight-training", "postedAtFormatted": "Friday, August 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weight%20training&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeight%20training%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3D9Q82yt9rGYcKc4H%2Fweight-training%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weight%20training%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3D9Q82yt9rGYcKc4H%2Fweight-training", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3D9Q82yt9rGYcKc4H%2Fweight-training", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<p>I'm looking for resources on effective weight training for the purpose of physique building. It's an area with a particularly poor signal to noise ratio so I would value pointers from other rationalists. The kinds of questions I would like to answer are:</p>\n<ul>\n<li>How to structure work-outs. Should I lift as much weight as possible or do more repetitions at lower weights?</li>\n<li>How should I trade off frequency of gym visits against length of those visits?</li>\n<li>What supplements should I take?</li>\n</ul>\n<p><em>Edit</em>: I'm vegetarian, and I now realise this is rather important to answers to point three. So far the only supplement I've been taking is soy protein.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3D9Q82yt9rGYcKc4H", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 6, "extendedScore": null, "score": 7.604735235811593e-07, "legacy": true, "legacyId": "9450", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-26T17:58:39.387Z", "modifiedAt": null, "url": null, "title": "Schroedinger's cat is always dead", "slug": "schroedinger-s-cat-is-always-dead", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:31.829Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4PpQ2jyE9LxvsrCsc/schroedinger-s-cat-is-always-dead", "pageUrlRelative": "/posts/4PpQ2jyE9LxvsrCsc/schroedinger-s-cat-is-always-dead", "linkUrl": "https://www.lesswrong.com/posts/4PpQ2jyE9LxvsrCsc/schroedinger-s-cat-is-always-dead", "postedAtFormatted": "Friday, August 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Schroedinger's%20cat%20is%20always%20dead&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASchroedinger's%20cat%20is%20always%20dead%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4PpQ2jyE9LxvsrCsc%2Fschroedinger-s-cat-is-always-dead%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Schroedinger's%20cat%20is%20always%20dead%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4PpQ2jyE9LxvsrCsc%2Fschroedinger-s-cat-is-always-dead", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4PpQ2jyE9LxvsrCsc%2Fschroedinger-s-cat-is-always-dead", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 418, "htmlBody": "<p>Suppose you believe in the Copenhagen interpretation of quantum mechanics.&nbsp; Schroedinger puts his cat in a box, with a device that has a 50% chance of releasing a deathly poisonous gas.&nbsp; He will then open the box, and observe a live or dead cat, collapsing that waveform.</p>\n<p>But Schroedinger's cat is lazy, and spends most of its time sleeping.&nbsp; Schroedinger is a pessimist, or else an optimist who hates cats; and so he mistakes a sleeping cat for a dead cat with probability P(M) &gt; 0, but never mistakes a dead cat for a living cat.</p>\n<p>So if the cat is dead with probability P(D) &gt;= .5, Schroedinger <em>observes</em> a dead cat with probability P(D) + P(M)(1-P(D)).</p>\n<p>If observing a dead cat causes the waveform to collapse such that the cat is dead, then P(D) = P(D) + P(M)(1-P(D)).&nbsp; This is possible only if P(D) = 1.</p>\n<p><a id=\"more\"></a>If you don't say that only conscious agents can collapse waveforms, then you have to agree that something in the box collapses the waveform as seen from inside the box, while it's still uncollapsed to Schroedinger. And Schroedinger's opening the box collapses that waveform for him; but it is still uncollapsed for someone outside the room.&nbsp; This seems like it might be equivalent to many worlds - all possibilities already exist; you just haven't chosen which one you're going to access until you open the box.</p>\n<p>But if you do say that only conscious agents can collapse waveforms, then it's something about their mental processes that does the collapsing. This could mean their beliefs matter. And then, the cat is always dead.</p>\n<p>ADDED:&nbsp; People.&nbsp; Read the entire post before responding.&nbsp; I am not claiming that the cat is always dead.&nbsp; I am not claiming that consciousness collapses waveforms.&nbsp; I am claiming that there are only 2 known alternatives:</p>\n<ol>\n<li>Interactions collapse waveforms, regardless of whether conscious entities are involved.&nbsp; It is not possible, under this view, for any waveform to be either collapsed or not collapsed, because there will be some viewpoints from which it has collapsed, and some from which it hasn't.&nbsp; So this appears to be equivalent to many-worlds.</li>\n<li>Consciousness collapses waveforms.&nbsp; Leading to weirdness such as, potentially, the cat always being dead.</li>\n</ol>\n<p>If you can't produce another alternative, and you don't believe in many-worlds, you owe me an upvote.</p>\n<p>Finally, this post is supposed to be fun!&nbsp; You are crushing all whimsy and playfulness on LessWrong when you pile downvotes like bricks on anything playful because it does not provide a complete and satisfactory resolution.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4PpQ2jyE9LxvsrCsc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -18, "extendedScore": null, "score": -3.8e-05, "legacy": true, "legacyId": "9452", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-26T20:34:29.539Z", "modifiedAt": null, "url": null, "title": "New Favicon", "slug": "new-favicon", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:32.251Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/txJxzKMPpcc6h8Fp2/new-favicon", "pageUrlRelative": "/posts/txJxzKMPpcc6h8Fp2/new-favicon", "linkUrl": "https://www.lesswrong.com/posts/txJxzKMPpcc6h8Fp2/new-favicon", "postedAtFormatted": "Friday, August 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Favicon&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Favicon%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtxJxzKMPpcc6h8Fp2%2Fnew-favicon%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Favicon%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtxJxzKMPpcc6h8Fp2%2Fnew-favicon", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtxJxzKMPpcc6h8Fp2%2Fnew-favicon", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 34, "htmlBody": "<p>LW appears to have acquired a new favicon, \"&lt;X\" in place of the prior \"Lw\".&nbsp; This change wasn't announced and I don't know what the new icon means.&nbsp; Can someone explain it to me?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "txJxzKMPpcc6h8Fp2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 7.605734616255158e-07, "legacy": true, "legacyId": "9453", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-26T23:34:13.479Z", "modifiedAt": null, "url": null, "title": "Singularity Institute Strategic Plan 2011", "slug": "singularity-institute-strategic-plan-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:16.066Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelAnissimov", "createdAt": "2009-03-21T20:49:52.763Z", "isAdmin": false, "displayName": "MichaelAnissimov"}, "userId": "tkZmAXciPjSumi4Wk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xDfgrmhzf3uF42dKn/singularity-institute-strategic-plan-2011", "pageUrlRelative": "/posts/xDfgrmhzf3uF42dKn/singularity-institute-strategic-plan-2011", "linkUrl": "https://www.lesswrong.com/posts/xDfgrmhzf3uF42dKn/singularity-institute-strategic-plan-2011", "postedAtFormatted": "Friday, August 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20Institute%20Strategic%20Plan%202011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20Institute%20Strategic%20Plan%202011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDfgrmhzf3uF42dKn%2Fsingularity-institute-strategic-plan-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20Institute%20Strategic%20Plan%202011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDfgrmhzf3uF42dKn%2Fsingularity-institute-strategic-plan-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDfgrmhzf3uF42dKn%2Fsingularity-institute-strategic-plan-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>Thanks to the hard work and cooperation of Singularity Institute staff and volunteers, especially Louie Helm and Luke Muehlhauser (lukeprog), we now have a Strategic Plan, which outlines the near-term goals and vision of the Institute, and concrete actions we can take to fulfill those goals.</p>\n<p><a href=\"http://intelligence.org/blog/2011/08/26/singularity-institute-strategic-plan-2011/\">http://singinst.org/blog/2011/08/26/singularity-institute-strategic-plan-2011/</a></p>\n<p>We welcome your feedback. You can send any comments to <a href=\"mailto:institute@intelligence.org\">institute@intelligence.org</a>.&nbsp;</p>\n<p>The release of this Strategic Plan is part of an overall effort to increase transparency at Singularity Institute.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xDfgrmhzf3uF42dKn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 45, "extendedScore": null, "score": 7.606316416985264e-07, "legacy": true, "legacyId": "9455", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-26T23:43:13.806Z", "modifiedAt": null, "url": null, "title": "Bayesian Reasoning Applied to House Selling: Listing Price", "slug": "bayesian-reasoning-applied-to-house-selling-listing-price", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:25.221Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "byrnema", "createdAt": "2009-03-20T18:02:38.305Z", "isAdmin": false, "displayName": "byrnema"}, "userId": "SCnD6W8NiztYBN39M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZrAaPt6MnMnNZfpHM/bayesian-reasoning-applied-to-house-selling-listing-price", "pageUrlRelative": "/posts/ZrAaPt6MnMnNZfpHM/bayesian-reasoning-applied-to-house-selling-listing-price", "linkUrl": "https://www.lesswrong.com/posts/ZrAaPt6MnMnNZfpHM/bayesian-reasoning-applied-to-house-selling-listing-price", "postedAtFormatted": "Friday, August 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayesian%20Reasoning%20Applied%20to%20House%20Selling%3A%20Listing%20Price&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayesian%20Reasoning%20Applied%20to%20House%20Selling%3A%20Listing%20Price%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZrAaPt6MnMnNZfpHM%2Fbayesian-reasoning-applied-to-house-selling-listing-price%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayesian%20Reasoning%20Applied%20to%20House%20Selling%3A%20Listing%20Price%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZrAaPt6MnMnNZfpHM%2Fbayesian-reasoning-applied-to-house-selling-listing-price", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZrAaPt6MnMnNZfpHM%2Fbayesian-reasoning-applied-to-house-selling-listing-price", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 582, "htmlBody": "<p>Like <a href=\"/lw/7am/rational_home_buying/\">Yvain's parents</a>, I am planning on moving house. Selling a house and buying a house involve making a lot of decisions based on limited information, which I thought would make a set of good exercises for the application of Bayesian reasoning. I need to decide what price to list my house for, determine how much time and money to put into fixing it up, choose a new home and then there's the two poker games of the final negotiations of the sale.</p>\n<p>(I logged onto Less Wrong having just made the decision to consider posting this article, so I was kind of weirded out at first by the title of Yvain's post; but then I was relieved that the topic was somewhat different. I am used to coincidences but on the other hand they push me a little paranoid on my spectrum and I'll feel less stable for a few hours. I already know Google tracks me and who knows what algorithms could be running given a bunch of computer scientists...?)</p>\n<p>&nbsp;</p>\n<p><strong>House Story</strong></p>\n<p>tldr; We're listing at the appraised value +10%.</p>\n<p>A few years ago, we purchased a beautiful house. 'We' is my husband and I and my parents. We purchased the house because it includes a guest house where my parents can retire. However, my mom continues to postpone retirement and in the meantime my husband and I decided we would a) like more light, b) like a shorter commute and c) could purchase two homes we prefer for the price of this one -- my parents would enjoy a house on the water. (Great post and spot on about the features that matter, Yvain!)</p>\n<p>I would be happy to sell the house for +5%, covering real estate fees and new flooring we put in. However, three houses in the cul de sac have sold this year for +10% and so we listed it at that price too. Our house is bigger than theirs but not as nice (they have granite and impressive entrances and we don't). On the other hand, having the guest house makes us special.</p>\n<p>Via agent and potential buyer feedback, we're coming to realize that we might be lucky to sell the house for +5%. At this price level, people prefer a house that is impressive and in perfect condition.</p>\n<p>&nbsp;</p>\n<p><strong>Primary Bayesian Question</strong></p>\n<p>My primary question is the following: how should we decide to modify our listing price as we get more information?</p>\n<p>First, I've read that if a house is priced correctly you'll get an average of one offer every 10 showings. So far we've had 2 showings without an offer. After how many showings should we reduce the price?</p>\n<p>Second, the other three houses sold in 6 or 7 months. After how many months should we reduce the price?</p>\n<p>Keep in mind, we don't have to move and I estimate that I would be willing to stay in this house for about +3% per year. In other words, I would be willing to wait 2 years for a higher offer if I could sell it for +3% more by doing so.</p>\n<p>I anticipate that after posting this I will be embarrassed that it is so pecuniary. On the other hand, this makes it concrete and the problem in general doesn't have too many emotional factors. Any money we make over the first +5% can be used as a down payment for our next house after we pay our parents back. (I did feel embarrassed, so I took out the dollar values and replaced with relative percents.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZrAaPt6MnMnNZfpHM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 7.606345569903707e-07, "legacy": true, "legacyId": "9456", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Like <a href=\"/lw/7am/rational_home_buying/\">Yvain's parents</a>, I am planning on moving house. Selling a house and buying a house involve making a lot of decisions based on limited information, which I thought would make a set of good exercises for the application of Bayesian reasoning. I need to decide what price to list my house for, determine how much time and money to put into fixing it up, choose a new home and then there's the two poker games of the final negotiations of the sale.</p>\n<p>(I logged onto Less Wrong having just made the decision to consider posting this article, so I was kind of weirded out at first by the title of Yvain's post; but then I was relieved that the topic was somewhat different. I am used to coincidences but on the other hand they push me a little paranoid on my spectrum and I'll feel less stable for a few hours. I already know Google tracks me and who knows what algorithms could be running given a bunch of computer scientists...?)</p>\n<p>&nbsp;</p>\n<p><strong id=\"House_Story\">House Story</strong></p>\n<p>tldr; We're listing at the appraised value +10%.</p>\n<p>A few years ago, we purchased a beautiful house. 'We' is my husband and I and my parents. We purchased the house because it includes a guest house where my parents can retire. However, my mom continues to postpone retirement and in the meantime my husband and I decided we would a) like more light, b) like a shorter commute and c) could purchase two homes we prefer for the price of this one -- my parents would enjoy a house on the water. (Great post and spot on about the features that matter, Yvain!)</p>\n<p>I would be happy to sell the house for +5%, covering real estate fees and new flooring we put in. However, three houses in the cul de sac have sold this year for +10% and so we listed it at that price too. Our house is bigger than theirs but not as nice (they have granite and impressive entrances and we don't). On the other hand, having the guest house makes us special.</p>\n<p>Via agent and potential buyer feedback, we're coming to realize that we might be lucky to sell the house for +5%. At this price level, people prefer a house that is impressive and in perfect condition.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Primary_Bayesian_Question\">Primary Bayesian Question</strong></p>\n<p>My primary question is the following: how should we decide to modify our listing price as we get more information?</p>\n<p>First, I've read that if a house is priced correctly you'll get an average of one offer every 10 showings. So far we've had 2 showings without an offer. After how many showings should we reduce the price?</p>\n<p>Second, the other three houses sold in 6 or 7 months. After how many months should we reduce the price?</p>\n<p>Keep in mind, we don't have to move and I estimate that I would be willing to stay in this house for about +3% per year. In other words, I would be willing to wait 2 years for a higher offer if I could sell it for +3% more by doing so.</p>\n<p>I anticipate that after posting this I will be embarrassed that it is so pecuniary. On the other hand, this makes it concrete and the problem in general doesn't have too many emotional factors. Any money we make over the first +5% can be used as a down payment for our next house after we pay our parents back. (I did feel embarrassed, so I took out the dollar values and replaced with relative percents.)</p>", "sections": [{"title": "House Story", "anchor": "House_Story", "level": 1}, {"title": "Primary Bayesian Question", "anchor": "Primary_Bayesian_Question", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "19 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YSWa8rYeD3aDaofSP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-26T23:59:32.786Z", "modifiedAt": null, "url": null, "title": "Alzheimer's vs Cryonics", "slug": "alzheimer-s-vs-cryonics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:09.221Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HPWod9Zrph8CCdR86/alzheimer-s-vs-cryonics", "pageUrlRelative": "/posts/HPWod9Zrph8CCdR86/alzheimer-s-vs-cryonics", "linkUrl": "https://www.lesswrong.com/posts/HPWod9Zrph8CCdR86/alzheimer-s-vs-cryonics", "postedAtFormatted": "Friday, August 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Alzheimer's%20vs%20Cryonics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAlzheimer's%20vs%20Cryonics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHPWod9Zrph8CCdR86%2Falzheimer-s-vs-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Alzheimer's%20vs%20Cryonics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHPWod9Zrph8CCdR86%2Falzheimer-s-vs-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHPWod9Zrph8CCdR86%2Falzheimer-s-vs-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 265, "htmlBody": "<p>The <a href=\"http://msn.foxsports.com/wcbk/story/Tennessee-womens-basketball-coach-Pat-Summitt-diagnosed-with-dementia-082311?ocid=ansfox11\">diagnosis</a> of a legendary women's basketball coach at my school, Pat Summit, with early onset dementia, Alzheimer's type, got me thinking about Cryonics and Alzheimer's. For the purposes of this thought experiment, we will ignore the legal implications of the fact that you can't be frozen until you are legally dead. Let us further assume (which given my knowledge of Alzheimer's, is pretty reasonable) that the damage done by Alzheimer's is complete, and that future technology will be unable to reconstruct the destroyed components.</p>\n<p>&nbsp;</p>\n<p>If you were diagnosed with Alzheimer's, or really any neurodegenerative disorder, at what point in the degradation would you want to be frozen? This would fairly easily prevent further degradation, but might further damage you/all of the other risks associated with cryonics that everyone knows. Obviously, you wouldn't have the agency of mind (or perhaps you still would, depending on when you made the decision) to do it yourself, but suppose you were caring for a loved one, or writing a living will. Assume you operate healthily at the onset of your diagnosis.</p>\n<p>&nbsp;</p>\n<p>Things to consider:</p>\n<p>How would your loved ones react to your being frozen versus coping with you having Alzheimer's? Should this matter in your decision? If it does, what implications does that have for a <a href=\"http://web.utk.edu/~jhardwig/dutydie.htm\">duty to die</a>?</p>\n<p>How much degradation of the mind is acceptable (and the added damage potentially done by cryonics) before one should freeze onesself?</p>\n<p>Would you avoid the risk of being frozen all together because you believe we will or may have a cure for Alzheimer's soon, and waiting for it would do less damage than cryonics?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HPWod9Zrph8CCdR86", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "9451", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-27T00:15:39.695Z", "modifiedAt": null, "url": null, "title": "Rational Home Buying", "slug": "rational-home-buying", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:34.012Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YSWa8rYeD3aDaofSP/rational-home-buying", "pageUrlRelative": "/posts/YSWa8rYeD3aDaofSP/rational-home-buying", "linkUrl": "https://www.lesswrong.com/posts/YSWa8rYeD3aDaofSP/rational-home-buying", "postedAtFormatted": "Saturday, August 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Home%20Buying&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Home%20Buying%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYSWa8rYeD3aDaofSP%2Frational-home-buying%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Home%20Buying%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYSWa8rYeD3aDaofSP%2Frational-home-buying", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYSWa8rYeD3aDaofSP%2Frational-home-buying", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2020, "htmlBody": "<p>My parents are considering moving house. I've had a front-seat window to their decision process as they compare alternatives, and sometimes it isn't pretty.<br /><br />A new house is one of the most important purchases most people will make. Because of the sums involved, the usual pitfalls of decision-making gain new importance, and it becomes especially important to make sure you're thinking rationally. Research in a couple of fields, most importantly positive psychology, offers some potentially helpful tips.</p>\n<p><a id=\"more\"></a></p>\n<p><strong>LOCATION, LOCATION, LOCATION</strong><br /><br />People so consistently under-count the pain of commuting when making choices that the problem has its own name: <a href=\"http://ftp.iza.org/dp1278.pdf\">Commuter's Paradox</a>. The paradox is that, although rational choice theory predicts people should balance commuting against other goods and costs, so that one person might have a longer commute but a nicer (or cheaper) house and so be just as happy overall, this doesn't happen: people who have long commutes are miserable, full stop. A <a href=\"http://www.krueger.princeton.edu/PDF%20of%20Kahneman%20Krueger%20paper.pdf\">separate survey</a> by Kahneman and Krueger found that commuting was the least enjoyable of nineteen daily activities mentioned, and other studies <a href=\"http://www.huffingtonpost.com/kirsten-dirksen/happiness-research-ranks-_b_829591.html\">have found</a> relations between long commutes and poor social lives, poor health, high stress, and various other problems.<br /><br />Psychologists aren't entirely sure why people so consistently under-count the pain of commuting. Maybe it's because it's viewed as \"in-between\" time rather than as an activity on its own; maybe it's because it comes in relatively short and individually bearable chunks repeated over many years, instead of as a single entity. In any case, unless you are mentally atypical you will probably have a tendency to undercount commute time when buying a new home, and may want to adjust for that tendency.<br /><br /><strong>HOUSES COST A LOT OF MONEY</strong><br /><br />One of Kahneman and Tversky's famous bias experiments went like this: imagine you're buying a new shirt. It costs $40 at a nearby store, and it costs $20 at a store that's fifteen minutes away. Do you drive the fifteen minutes to save twenty bucks? Most people would.<br /><br />Now imagine you're buying a new TV which costs $2020 at a nearby store, and $2000 at a store that's fifteen minutes away. Do you drive the fifteen minutes to save twenty bucks? Most people wouldn't.<br /><br />In both cases, the tradeoff is the same - drive fifteen minutes to save twenty bucks - but people were much more willing to do it for the cheap item, because $20 was a higher percentage of its total cost. With the $2000 TV, the $20 vanishes into the total cost like a drop in the ocean and seems insignificant.<br /><br />Nice homes can cost $500,000, $1,000,000, or even more. There doesn't seem to be a big difference in price between $710,000 and $745,000 houses; perhaps if the second home looked even a little nicer in an undefinable way you might be prepared to take it. But $35,000 is $35,000; if those minor advantages don't provide $35,000 worth of value, when measured on the same scale on which you measure the value of of movie tickets, shoes, and college funds, then you should buy the first house and keep the cash.<br /><br />I find purchasing decisions easier when I think about them like this: which would you rather have, the second house, or the first house plus a two-week luxury vacation to anywhere in the world every summer for the next five years? The second house, or the first house plus a brand new Lexus? The second house and dining at home every week, or the first house and eating out at your favorite restaurant every weekend for the rest of your life? (EDIT: gjm <a href=\"/lw/7am/rational_home_buying/4pgh?context=1#4pgh\">points out</a> that it's easier to resell houses than other types of good, so if you expect to resell your house you should really only be considering the extra money involved in the mortgage)<br /><br /><strong>DON'T OVERCOUNT EASILY AVAILABLE DETAILS<br /></strong><br />The <a href=\"http://wiki.lesswrong.com/wiki/Availability_heuristic\">availability heuristic</a> says that people overcount scenarios that are easy and vivid to imagine, and undercount scenarios that don't involve any readily available examples or mental images. For example, most people will assert, when asked, that there are more English words ending with \"-ing\" than with \"-g\". A moment's thought reveals this to be impossible - words ending in \"-ing\" are a subset of those ending in \"-g\" - but thinking specifically of \"-ing\" words makes it easier to bring examples to mind.<br /><br />The real estate version of this fallacy involves exciting opportunities that you will rarely or never use. For example, a house with a pool may bring to mind the opportunity to hold pool parties. But most such plans will probably fall victim to akrasia, and even if they don't, how often can one person throw pool parties without exhausting their friends' interest? Pool parties may be fun to imagine, but they'll probably only affect a few hours every couple of months. Other factors, like the commuting distance and whether your children end up in a nice school, may affect several hours every day.<br /><br />(a classic example here is the \"extra bedroom for Grandma\" - visits from Grandma are easy to imagine, but if she only comes a couple of days a year, spending tens of thousands more dollars for a house with an extra bedroom and bathroom for her is probably pretty stupid. You'd save money - and make her happier - by putting her up in the local five star hotel.)<br /><br /><strong>LIGHT AND NATURE</strong><br /><br />Good illumination and a view of natural beauty aren't just pleasant luxuries, but can make important practical differences in your life.<br /><br />Light, especially daylight, has a strong effect on mood. There are <a href=\"http://www.brightenyourlife.info/ch5.html\">at least fifteen controlled studies</a> showing that bright light reduces symptoms of seasonal and nonseasonal depression by about 10-20% over placebo. This is about equal benefit to some antidepressant drugs, and sufficient that light therapy is a recognized medical treatment for depression. <a href=\"http://www.nrc-cnrc.gc.ca/obj/irc/doc/pubs/nrcc54002.pdf\">Bright light</a> leads to self-reported better mood even in subjects without a diagnosis of depression, and also leads to better sleep and more agreeable social interactions.<br /><br />Light and nature have positive effects on health. Some of the most compelling data comes from hospitals, which have long realized that their patients near windows do better than their more interior counterparts. <a href=\"http://www.hospitalart.com/image/science_article.pdf\">In one study</a>, surgical patients near windows recovered faster (7.9 vs. 8.7 days), received fewer negative comments from nurses (1.1 vs. 4 notes), and needed fewer strong painkillers (1 vs. 2.5 doses) than matched controls without a view. <a href=\"http://greenplantsforgreenbuildings.org/attachments/contentmanagers/25/HealthSettingsUlrich.pdf\">Other studies</a> have compared recovery of physiological indicators of stress (for example, blood pressure) in subjects viewing natural or artificial scenes; the subjects with views of nature consistently have healthier stress reactions. <br /><br />Nature may have <a href=\"http://architecture.mit.edu/house_n/web/resources/articles/education/Boston%20Globe%20Online%20-%20Nation%20%20World%20-%20Study%20says%20natural%20light%20boosts%20learning.htm\">special benefits for children</a>. Experiments with subjects of all ages and levels of mental health have shown nature increases mental functioning and concentration, but some of the most cited work has been in children with Attention Deficit Hyperactivity Disorder. Children who live in greener settings also (independently of wealth) do better on schoolwork and show greater ability to delay gratification. <em>Large</em> <a href=\"http://www.coe.uga.edu/sdpl/research/daylightingstudy.pdf\">studies</a> find with <em>high</em> certainty that students who take standardized tests in better lighting do up to 25% better than their literally dimmer schoolmates, and progress through lessons 15-25% faster.<br /><br />You don't have to live in the Amazon to get a benefit: even children in a concrete building with a tiny \"green island\" boasting a single tree <a href=\"http://web.mac.com/wcsulliv/William_Sullivan/Publications_files/Taylor,%20Kuo,%20Sullivan%202002.pdf\">did better</a> than their peers in a building without such an island.<br /><br /><strong>BETTER FIRST IN A VILLAGE THAN SECOND IN ROME</strong><br /><br />Brains generally encode variables not as absolute values but as differences from an appropriate reference frame. That means that to <a href=\"http://www.outsidethebeltway.com/happiness_is_being_richer_than_your_friends/\">really appreciate</a> your wealth, you've got to be surrounded by people who are poorer than you are.<br /><br />This seems to be empirically the case: <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20503742\">a US study</a> found the happiest Americans were rich people living in poor counties. However, this was true only of rich people living in rich <em>neighborhoods</em> of poor counties. As the study puts it, \"individuals in fact are happier when they live among the poor, as long as the poor do not live too close\". <br /><br />Of course, this doesn't mean that you should move to Somalia for eternal bliss. There are community-wide benefits to living in a wealthy neighborhood, like better schools, and you may be better able to socialize with people from a similar class background as yourself. But given the choice between a neighborhood at the top of your price range and one at the bottom, you may find yourself more satisfied living in an area where it's the Joneses who have to try to keep up with <em>you</em>.<br /><br /><strong>DON'T OVERSHOP AND DON'T OVERTHINK</strong><br /><br />It's easy to confuse \"rationality\" with a tendency to turn all decision-making over to conscious general-purpose reasoning, and in turn to assume that whoever ruminates about a decision the most is most rational. But there are at least two reasons to think that <em>within reason</em> it may be better to worry less over important decisions.<br /><br />One is the finding that \"comparison shopping\" usually leads to less happiness in whatever you buy. Imagine being pretty sure you're going to buy House X until you look at House Y and find out that this one has a granite fireplace, and a pond in the backyard. It may be you don't like House Y at all - but now every time you go back to House X, you're thinking about how it doesn't have a granite fireplace or a pond, two features which you never would have even considered before. Whether you find this explanation plausible or not, the research generally agrees: <a href=\"http://biopsychiatry.com/happiness/choice.html\">too many choices result in less satisfaction</a> with whatever you finally buy.<br /><br />The second is the discovery that attempts to make your reasoning explicit and verbal usually result in worse choices. This includes that favorite of guidance counselors: to write out a list of the pros and cons of all your choices - but it covers any attempt to explain choices in words. In one study, subjects were asked to rate the taste of various jams; an experimental group was also asked to give reasons for their ratings. Ratings from the group that didn't need reasons correlated more closely with the ratings of professional jam experts (which is totally a thing) than those who gave justifications. A similar study found students choosing posters were more likely to still like the poster a month later if they weren't asked to justify their choice (Lehrer, <em>How We Decide</em>, p. 144).<br /><br />The most plausible explanation is that having to verbalize your choices shifts your attention to features that are easy to explain in words (or perhaps which make good signaling value), and these are not necessarily the same features that are really important. In a telling experiment under the same protocol as the ones listed above, people asked to reflect upon their choices were more likely to choose the house with the extra room for Grandma than the house with the shorter commute times, because the extra reflection gave more opportunity for the availability heuristic to come into play.<br /><br /><strong>CONCLUSION</strong><br /><br />Buying a house is one of the biggest decisions a family faces, and so has extra opportunity to be improved by rational thinking. Try to buy a house with good illumination and nearby green space in an area close to your workplace where you'll be relatively high on the social ladder. Carefully consider whether special features have genuine utility or are just highly available small details, and justify the relative differences in cost in absolute, not just relative terms. And, um, try to do all of this while following your gut instincts and not overshopping.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 3, "4R8JYu4QF2FqzJxE5": 1, "5f5c37ee1b5cdee568cfb19c": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YSWa8rYeD3aDaofSP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 130, "baseScore": 163, "extendedScore": null, "score": 0.000316, "legacy": true, "legacyId": "9454", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 163, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 138, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-27T03:43:16.172Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Human Evil and Muddled Thinking", "slug": "seq-rerun-human-evil-and-muddled-thinking", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KtAahgp4WiAZc6q6C/seq-rerun-human-evil-and-muddled-thinking", "pageUrlRelative": "/posts/KtAahgp4WiAZc6q6C/seq-rerun-human-evil-and-muddled-thinking", "linkUrl": "https://www.lesswrong.com/posts/KtAahgp4WiAZc6q6C/seq-rerun-human-evil-and-muddled-thinking", "postedAtFormatted": "Saturday, August 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Human%20Evil%20and%20Muddled%20Thinking&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Human%20Evil%20and%20Muddled%20Thinking%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKtAahgp4WiAZc6q6C%2Fseq-rerun-human-evil-and-muddled-thinking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Human%20Evil%20and%20Muddled%20Thinking%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKtAahgp4WiAZc6q6C%2Fseq-rerun-human-evil-and-muddled-thinking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKtAahgp4WiAZc6q6C%2Fseq-rerun-human-evil-and-muddled-thinking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 195, "htmlBody": "<p>Today's post, <a href=\"/lw/jd/human_evil_and_muddled_thinking/\">Human Evil and Muddled Thinking</a> was originally published on 13 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It's easy to think that rationality and seeking truth is an intellectual exercise, but this ignores the lessons of history. Cognitive biases and muddled thinking allow people to hide from their own mistakes and allow evil to take root. Spreading the truth makes a real difference in defeating evil.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/7a3/seq_rerun_rationality_and_the_english_language/\">Rationality and the English Language</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KtAahgp4WiAZc6q6C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 7.607122714933705e-07, "legacy": true, "legacyId": "9459", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["i8q4vXestDkGTFwsc", "Wp5jejbogdLGdehLy", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-27T19:22:15.046Z", "modifiedAt": null, "url": null, "title": "Decision Theory Paradox: PD with Three Implies Chaos?", "slug": "decision-theory-paradox-pd-with-three-implies-chaos", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:22.962Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HT8jwNJ6vH7p9gaTT/decision-theory-paradox-pd-with-three-implies-chaos", "pageUrlRelative": "/posts/HT8jwNJ6vH7p9gaTT/decision-theory-paradox-pd-with-three-implies-chaos", "linkUrl": "https://www.lesswrong.com/posts/HT8jwNJ6vH7p9gaTT/decision-theory-paradox-pd-with-three-implies-chaos", "postedAtFormatted": "Saturday, August 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Decision%20Theory%20Paradox%3A%20PD%20with%20Three%20Implies%20Chaos%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecision%20Theory%20Paradox%3A%20PD%20with%20Three%20Implies%20Chaos%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHT8jwNJ6vH7p9gaTT%2Fdecision-theory-paradox-pd-with-three-implies-chaos%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Decision%20Theory%20Paradox%3A%20PD%20with%20Three%20Implies%20Chaos%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHT8jwNJ6vH7p9gaTT%2Fdecision-theory-paradox-pd-with-three-implies-chaos", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHT8jwNJ6vH7p9gaTT%2Fdecision-theory-paradox-pd-with-three-implies-chaos", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1085, "htmlBody": "<p><strong>Prerequisites:</strong> Familiarity with decision theories (in particular, <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/\">Eliezer's Timeless Decision Theory</a>) and of course the <a href=\"http://wiki.lesswrong.com/wiki/Prisoner%27s_dilemma\">Prisoner's Dilemma</a>.</p>\n<p><em><strong>Summary:</strong> I show an apparent paradox in a three-agent variant of the Prisoner's Dilemma: despite full knowledge of each others' source codes, TDT agents allow themselves to be exploited by CDT, and lose completely to another simple decision theory. Please read the post and think for yourself about the Exercises and the Problem below before reading the comments; this is an opportunity to become a stronger expert <a href=\"http://www.overcomingbias.com/2007/04/expert_at_versu.html\">at and on</a> decision theory!</em></p>\n<p>We all know that in a world of one-shot Prisoner's Dilemmas with read-access to the other player's source code, it's good to be Timeless Decision Theory. A TDT agent in a one-shot Prisoner's Dilemma will correctly defect against an agent that always cooperates (call this CooperateBot) or always defects (call this DefectBot, and note that CDT trivially reduces to this agent), and it will cooperate against another TDT agent (or any other type of agent whose decision depends on TDT's decision in the appropriate way). In fact, if we run an evolutionary contest <a href=\"http://en.wikipedia.org/wiki/The_Evolution_of_Cooperation#Axelrod.27s_Tournaments\">as Robert Axelrod famously did for the Iterated Prisoner's Dilemma</a>, and again allow players to read the other players' source codes, TDT will annihilate both DefectBot and CooperateBot over the long run, and it beats or ties any other decision theory.<a href=\"/lw/767/decision_theory_paradox_pd_with_three_implies/#foot1\"><sup>1</sup></a> But something interesting happens when we take players in threes...</p>\n<p><a id=\"more\"></a>Consider a population of agents in a simulated world. <a href=\"http://wiki.lesswrong.com/wiki/Omega\">Omega</a>, being the trickster from outside the Matrix that ze is, decides to spend a couple of eons playing the following game with these agents: ze selects three of them at random (call them X, Y and Z), wipes their memories,<a href=\"/lw/767/decision_theory_paradox_pd_with_three_implies/#foot2\"><sup>2</sup></a> gives them each others' source codes, and privately asks each whether they cooperate or defect. If X defects, then Omega will create 2 \"children\" of X (distinct near-copies of X, with the same decision theory as X) and return them to the simulation. If X cooperates, then Omega will return 3 \"children\" of Y and 3 \"children\" of Z to the simulation. Simultaneously, Y and Z make the analogous decisions.</p>\n<p>(Just to reiterate: cooperating gives +3 to each other player, nothing to oneself; defecting gives +2 to oneself, nothing to anyone else. The analogy to the Prisoner's Dilemma should be obvious.)</p>\n<p>Assume maximal selfishness: each agent is motivated solely to maximize its own number of children (the agent itself doesn't get returned!), and doesn't care about the other agents using the same decision theory, or even about its other \"relatives\" in the simulation.<a href=\"/lw/767/decision_theory_paradox_pd_with_three_implies/#foot3\"><sup>3</sup></a> Although we've had to explicitly state quite a few conditions, this seems like a pretty simple and fair evolutionary tournament.</p>\n<p>It's clear that CDT agents will simply defect each time. What about TDT agents?</p>\n<p><strong>Exercise 1:</strong> Prove that if the population consists of TDT agents and DefectBots, then a TDT agent will cooperate precisely when at least one of the other agents is also TDT. <em>(Difficulty: 1 star.)</em></p>\n<p>Notice that we've created a <a href=\"http://en.wikipedia.org/wiki/Free_rider_problem\">free-rider problem</a>. Any DefectBot paired with two TDT agents gets 8 children- even better than the 6 that each of three TDT agents get in their best case! As you might expect, this bonus balances against the fact that three TDTs played together will fare much better than three DefectBots played together, and so it turns out that the population settles into a nontrivial equilibrium:</p>\n<p><strong>Exercise 2:</strong> Prove that if a very large population starts with equal numbers of TDTs and DefectBots, then the expected population growth in TDTs and DefectBots is practically equal. (If Omega samples with replacement&ndash; assuming that the agents don't care about their exact copy's children&ndash; then the expected population growth is precisely equal.) <em>(Difficulty: 2 stars.)</em></p>\n<p><strong>Exercise 3:&nbsp; </strong>Prove that if the initial population consists of TDTs and DefectBots, then the ratio of the two will (with probability 1) tend to 1 over time. <em>(Difficulty: 3 stars.)</em></p>\n<p>This should already perplex the reader who believes that <a href=\"http://wiki.lesswrong.com/wiki/Rationalists_should_win\">rationalists should win</a>, and that in particular TDT should beat the socks off of DefectBots in any fair fight. The DefectBots aren't harmless parasites, either: the TDTs' rate of reproduction in equilibrium with DefectBots is less than 30% of their rate of reproduction in a population of TDTs alone!<em> (Easy to verify if you've done Problem 2.)</em></p>\n<p>And it gets worse, in two ways.</p>\n<p>First, if we adjust the payoff matrix so that defecting gets (+200,+0,+0) and cooperating gets (+0,+201,+201), then any population of TDTs and DefectBots ends up (with probability 1) with the DefectBots outnumbering TDTs by a ratio of 100:1. <em>(Easy to verify if you've done Exercise 3.)</em></p>\n<p>Secondly and more damningly, we can introduce CliqueBots, who cooperate only if both other agents are CliqueBots. These, and not TDTs, are the champions of the three-way Prisoner's Dilemmas:</p>\n<p><strong>Exercise 4: </strong>If the initial population consists of CliqueBots, DefectBots and TDTs<a href=\"/lw/767/decision_theory_paradox_pd_with_three_implies/#foot4\"><sup>4</sup></a> in any proportion, then the ratio of both others to CliqueBots approaches 0 (with probability 1). <em>(Difficulty: 4 stars.)</em></p>\n<p><strong>Problem: </strong>The setup looks perfectly fair for TDT agents. So why do they lose? <em>(Difficulty: 2+3i stars.)</em></p>\n<p><strong>Note:</strong> This problem is solved in <a href=\"/lw/778/consequentialism_need_not_be_nearsighted/\">a more general followup post</a>; but do try and think about it yourself first! Also, I've posted solutions to the exercises <a href=\"/r/discussion/lw/7gs/decision_theory_paradox_answer_key/\">in the discussion section</a>. It's worth noting that I asked Eliezer, Vladimir Nesov, Wei Dai, cousin_it, and other decision-theory heavyweights to avoid posting spoilers on the main problem below, and they obliged; many of the other commenters figured out the problem, but nobody posted solutions to the exercises in the comments (as of 9/5/11).</p>\n<p><strong>Footnotes:</strong></p>\n<p><a name=\"foot1\"></a><strong>1.</strong> What's meant by \"beat or tie\" is sort of complicated- there are decision theories that (with high probability) crush TDT when starting out as a majority, and (with equally high probability) get crushed by TDT when starting out as a minority. Also, it gets intractably messy if you allow populations consisting of three or more decision theories (because then A can decide, not just based on how B will decide to act towards A, but also on how B would act towards C, etc).</p>\n<p><strong><a name=\"foot2\"></a>2.</strong> No playing <a href=\"http://en.wikipedia.org/wiki/Tit_for_tat\">Tit-for-Tat</a> in this game! Omega will <a href=\"http://wiki.lesswrong.com/wiki/Least_convenient_possible_world\">make sure</a> there's no funny business <a href=\"http://en.wikipedia.org/wiki/Steganography\">steganographically</a> hidden in the source codes, either.</p>\n<p><strong><a name=\"foot3\"></a>3.</strong> I'm intentionally blocking the analogue of kin altruism among related populations, in order to exclude any reasons for cooperation besides the purely decision-theoretical ones.</p>\n<p><strong><a name=\"foot4\"></a>4.</strong> Yeah, I know I said that populations with three different decision theories are basically intractable. That's true in general, but here we have the simplifying factor that two of them (CliqueBots and DefectBots) are simple decision theories that don't bother to simulate the other agents.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HT8jwNJ6vH7p9gaTT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 42, "extendedScore": null, "score": 8.7e-05, "legacy": true, "legacyId": "9295", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["szfxvS8nsxTgJLBHs", "prb8raC4XGJiRWs5n", "HxYneuv9XRit4dMRm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-27T21:02:05.105Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Less Wrong Meetup", "slug": "meetup-vancouver-less-wrong-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:09.062Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Arkanj3l", "createdAt": "2011-04-23T03:48:47.569Z", "isAdmin": false, "displayName": "Arkanj3l"}, "userId": "nQmA4dnBdX99WyCt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ApzYWdGQpAoPiA92P/meetup-vancouver-less-wrong-meetup", "pageUrlRelative": "/posts/ApzYWdGQpAoPiA92P/meetup-vancouver-less-wrong-meetup", "linkUrl": "https://www.lesswrong.com/posts/ApzYWdGQpAoPiA92P/meetup-vancouver-less-wrong-meetup", "postedAtFormatted": "Saturday, August 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Less%20Wrong%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Less%20Wrong%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FApzYWdGQpAoPiA92P%2Fmeetup-vancouver-less-wrong-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Less%20Wrong%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FApzYWdGQpAoPiA92P%2Fmeetup-vancouver-less-wrong-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FApzYWdGQpAoPiA92P%2Fmeetup-vancouver-less-wrong-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2r'>Vancouver Less Wrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 August 2011 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Waves Coffee House, 100-900 Howe St. Vancouver, BC V6Z 2M4</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Vancouver Rationalists (<a href=\"https://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">Google Group</a>) have skipped a weekend; hopefully we'll pick up the pace once again and continue to build the community. The meetup is at the usual locale, <a href=\"http://www.wavescoffee.ca/locations/howesmithe.html\" rel=\"nofollow\">Waves Coffee House</a>. Look into one of the meeting rooms near the back.</p>\n\n<p>Currently the prompt for discussion is the following:</p>\n\n<p><strong>If reverse stupidity is not correlated with intelligence, then what does it have to do with increasing intelligence?</strong></p>\n\n<p>... but, as always, it is a guide and not a rule. Intellectual tangents welcome. One possible avenue of discussion could be new places for activity, possible outside as the summer still gives its due (potluck? <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Stanley_Park\" rel=\"nofollow\">Stanley Park</a>?).</p>\n\n<p>Come one, come all: bring yourself, friends, and loved ones interested in rationality.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2r'>Vancouver Less Wrong Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ApzYWdGQpAoPiA92P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 7.610487576581868e-07, "legacy": true, "legacyId": "9466", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Less_Wrong_Meetup\">Discussion article for the meetup : <a href=\"/meetups/2r\">Vancouver Less Wrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 August 2011 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Waves Coffee House, 100-900 Howe St. Vancouver, BC V6Z 2M4</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Vancouver Rationalists (<a href=\"https://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">Google Group</a>) have skipped a weekend; hopefully we'll pick up the pace once again and continue to build the community. The meetup is at the usual locale, <a href=\"http://www.wavescoffee.ca/locations/howesmithe.html\" rel=\"nofollow\">Waves Coffee House</a>. Look into one of the meeting rooms near the back.</p>\n\n<p>Currently the prompt for discussion is the following:</p>\n\n<p><strong id=\"If_reverse_stupidity_is_not_correlated_with_intelligence__then_what_does_it_have_to_do_with_increasing_intelligence_\">If reverse stupidity is not correlated with intelligence, then what does it have to do with increasing intelligence?</strong></p>\n\n<p>... but, as always, it is a guide and not a rule. Intellectual tangents welcome. One possible avenue of discussion could be new places for activity, possible outside as the summer still gives its due (potluck? <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Stanley_Park\" rel=\"nofollow\">Stanley Park</a>?).</p>\n\n<p>Come one, come all: bring yourself, friends, and loved ones interested in rationality.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Less_Wrong_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/2r\">Vancouver Less Wrong Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Less Wrong Meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver_Less_Wrong_Meetup", "level": 1}, {"title": "If reverse stupidity is not correlated with intelligence, then what does it have to do with increasing intelligence?", "anchor": "If_reverse_stupidity_is_not_correlated_with_intelligence__then_what_does_it_have_to_do_with_increasing_intelligence_", "level": 2}, {"title": "Discussion article for the meetup : Vancouver Less Wrong Meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver_Less_Wrong_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-27T21:23:23.095Z", "modifiedAt": null, "url": null, "title": "How much is karma worth, after all?", "slug": "how-much-is-karma-worth-after-all", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:09.585Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ytNR2cG5LdnQTWmEo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6N4kiTjjmMtu6uKEe/how-much-is-karma-worth-after-all", "pageUrlRelative": "/posts/6N4kiTjjmMtu6uKEe/how-much-is-karma-worth-after-all", "linkUrl": "https://www.lesswrong.com/posts/6N4kiTjjmMtu6uKEe/how-much-is-karma-worth-after-all", "postedAtFormatted": "Saturday, August 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20much%20is%20karma%20worth%2C%20after%20all%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20much%20is%20karma%20worth%2C%20after%20all%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6N4kiTjjmMtu6uKEe%2Fhow-much-is-karma-worth-after-all%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20much%20is%20karma%20worth%2C%20after%20all%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6N4kiTjjmMtu6uKEe%2Fhow-much-is-karma-worth-after-all", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6N4kiTjjmMtu6uKEe%2Fhow-much-is-karma-worth-after-all", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 356, "htmlBody": "<p>It's been a couple days since the <a href=\"/lw/78s/help_fund_lukeprog_at_siai/\">funding plea</a>, so I thought I'd like to take this chance to compare self-reported donations to short-term karma gains. Naturally, I voted on none of these comments. Note that after posting this, the karma on these posts will almost definitely change; the values here are for 27/8/11 at around 9:00 GMT.</p>\n<p>So, the data:</p>\n<ul>\n<li><a href=\"/lw/78s/help_fund_lukeprog_at_siai/4pij\">Kaj_Sotala</a> ~172USD, 5 karma</li>\n<li><a href=\"/lw/78s/help_fund_lukeprog_at_siai/4p9a\">Rain</a> 12000USD, 25 karma</li>\n<li><a href=\"/lw/78s/help_fund_lukeprog_at_siai/4p66\">Nisan</a> 100USD, 16 karma</li>\n<li><a href=\"/lw/78s/help_fund_lukeprog_at_siai/4p1x\">pengvado</a> 10000USD, 36 karma</li>\n<li><a href=\"/lw/78s/help_fund_lukeprog_at_siai/4oyu\">JGWeissman</a> 2000USD, 24 karma</li>\n<li><a href=\"/lw/78s/help_fund_lukeprog_at_siai/4oyq\">Benquo</a> 1000USD, 18 karma</li>\n<li><a href=\"/lw/78s/help_fund_lukeprog_at_siai/4ovx\">AlexMennen</a> 285USD, 7 karma; and 30USD, 2 karma</li>\n<li><a href=\"/lw/78s/help_fund_lukeprog_at_siai/4otk\">wmorgan</a> 1000USD, 13 karma</li>\n</ul>\n<p>Note: two people (Kaj_Sotala and Rain) reported monthly commitments, but as far as I understand only the yearly pledge is matched, so for the purposes of this informal study I treat them as reporting X*12 USD donations, instead of X/month.</p>\n<p>There's not enough data for an honest causal analysis (I tried), but there are a few observations one can make. Intuitively one expects karma to be determined by the donation amount, the duration of time since the posting, and some unknown error.</p>\n<p>First observation: the users with the best USD/karma exchange rate made modest contributions early. Nisan came out best, with $6.25/karma &mdash; though some of this karma may be due also to the fantastic signal, on their part, that they overcame a rational hazard to make this donation. (Also, EY responded afterward, confounding the karmic flow with his wake.)</p>\n<p>In this spirit, we now name \"doing the least restrictive, obviously acceptable thing, instead of doing nothing while contemplating alternatives\" Nisan's razor, (\u30cb\u30b5\u30f3\u306e\u5243\u5200, perhaps) unless it happens to have a better, previously-existing name.</p>\n<p>Second observation: Hyperbolic discounting is alive and well. Those reporting monthly donations have karma below comparable one-shot donations, though both monthly data points did come slightly later than their one-shot counterparts.</p>\n<p>Third observation: Large donations are really inefficient at netting karma. pengvado paid $277.48/karma; no one above 1000USD paid less than $50/karma.</p>\n<p>Naturally, there's little point to this analysis. If anyone is trying to maximize net karma by donating to SIAI, something is probably wrong with their priorities.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6N4kiTjjmMtu6uKEe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 10, "extendedScore": null, "score": 7.610556596981506e-07, "legacy": true, "legacyId": "9465", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WqDGJNtxNMT8fHe37"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-28T03:24:54.594Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Doublethink (Choosing to be Biased)", "slug": "seq-rerun-doublethink-choosing-to-be-biased", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:09.565Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gtX9xyEHy93cuzPm6/seq-rerun-doublethink-choosing-to-be-biased", "pageUrlRelative": "/posts/gtX9xyEHy93cuzPm6/seq-rerun-doublethink-choosing-to-be-biased", "linkUrl": "https://www.lesswrong.com/posts/gtX9xyEHy93cuzPm6/seq-rerun-doublethink-choosing-to-be-biased", "postedAtFormatted": "Sunday, August 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Doublethink%20(Choosing%20to%20be%20Biased)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Doublethink%20(Choosing%20to%20be%20Biased)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgtX9xyEHy93cuzPm6%2Fseq-rerun-doublethink-choosing-to-be-biased%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Doublethink%20(Choosing%20to%20be%20Biased)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgtX9xyEHy93cuzPm6%2Fseq-rerun-doublethink-choosing-to-be-biased", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgtX9xyEHy93cuzPm6%2Fseq-rerun-doublethink-choosing-to-be-biased", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>Today's post, <a href=\"/lw/je/doublethink_choosing_to_be_biased/\">Doublethink (Choosing to be Biased)</a> was originally published on 14 September 2007.  A summary :</p>\n<p>&nbsp;</p>\n<blockquote>George Orwell wrote about what he called \"doublethink\", where a person was able to hold two contradictory thoughts in their mind simultaneously. While some argue that self deception can make you happier, doublethink will actually lead only to problems.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/7ar/seq_rerun_human_evil_and_muddled_thinking/\">Human Evil and Muddled Thinking</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gtX9xyEHy93cuzPm6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 10, "extendedScore": null, "score": 7.611728261300584e-07, "legacy": true, "legacyId": "9468", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Hs3ymqypvhgFMkgLb", "KtAahgp4WiAZc6q6C", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-28T07:45:10.649Z", "modifiedAt": null, "url": null, "title": "List of Donors, Fall 2011", "slug": "list-of-donors-fall-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:12.560Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mass_Driver", "createdAt": "2010-03-30T15:48:06.997Z", "isAdmin": false, "displayName": "Mass_Driver"}, "userId": "62rKjNqA2LCJ6RthR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fYN2qxMh4K2LYBNqo/list-of-donors-fall-2011", "pageUrlRelative": "/posts/fYN2qxMh4K2LYBNqo/list-of-donors-fall-2011", "linkUrl": "https://www.lesswrong.com/posts/fYN2qxMh4K2LYBNqo/list-of-donors-fall-2011", "postedAtFormatted": "Sunday, August 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20List%20of%20Donors%2C%20Fall%202011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AList%20of%20Donors%2C%20Fall%202011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfYN2qxMh4K2LYBNqo%2Flist-of-donors-fall-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=List%20of%20Donors%2C%20Fall%202011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfYN2qxMh4K2LYBNqo%2Flist-of-donors-fall-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfYN2qxMh4K2LYBNqo%2Flist-of-donors-fall-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 53, "htmlBody": "<p>This discussion-level article is a handy place for people to share info about their recent donations, especially donations to unusually efficient or effective charities. Feel free to post your one-time donations, your recurring donations, and/or any interesting changes in your donation habits. Gratitude and appreciation for other people's donations is also very welcome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fYN2qxMh4K2LYBNqo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 3, "extendedScore": null, "score": 7.612571958432596e-07, "legacy": true, "legacyId": "9470", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-28T08:35:29.887Z", "modifiedAt": null, "url": null, "title": "Polyhacking", "slug": "polyhacking", "viewCount": null, "lastCommentedAt": "2015-09-21T20:31:31.934Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kLR5H4pbaBjzZxLv6/polyhacking", "pageUrlRelative": "/posts/kLR5H4pbaBjzZxLv6/polyhacking", "linkUrl": "https://www.lesswrong.com/posts/kLR5H4pbaBjzZxLv6/polyhacking", "postedAtFormatted": "Sunday, August 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Polyhacking&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolyhacking%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkLR5H4pbaBjzZxLv6%2Fpolyhacking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Polyhacking%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkLR5H4pbaBjzZxLv6%2Fpolyhacking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkLR5H4pbaBjzZxLv6%2Fpolyhacking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2463, "htmlBody": "<p>This is a post about applied <a href=\"/lw/1xh/living_luminously/\">luminosity</a> in action: how I hacked myself to become polyamorous over (admittedly weak) <a href=\"/lw/2ee/unknown_knowns_why_did_you_choose_to_be_monogamous/274l\">natural monogamous inclinations</a>.&nbsp; It is a case history about me and, given the specific topic, my love life, which means <em>gooey self-disclosure ahoy</em>.&nbsp; As with <a href=\"/lw/20l/ureshiku_naritai/\">the last time I did that</a>, skip the post if it's not a thing you desire to read about.&nbsp; Named partners of mine have given permission to be named.</p>\n<p><strong>1. In Which Motivation is Acquired<br /></strong></p>\n<p>When one is monogamous, one can only date monogamous people.&nbsp; When one is poly, one can only date poly people.<sup>1</sup>&nbsp; Therefore, if one should find oneself with one's top romantic priority being to secure a relationship with a specific individual, it is only practical to adapt to the style of said individual, presuming that's something one can do.&nbsp; I found myself in such a position when <a href=\"/user/MBlume\">MBlume</a>, <a href=\"/lw/1a8/oy_girls_on_lw_want_to_get_together_some_time/1o8e?context=2#comments\">then my ex</a>, asked me from three time zones away if I might want to get back together.&nbsp; Since the breakup he had become polyamorous and had a different girlfriend, who herself juggled multiple partners; I'd moved, twice, and on the way dated a handful of people to no satisfactory clicking/sparking/other sound effects associated with successful romances. So the idea was appealing, if only I could get around the annoying fact that I was not, at that time, wired to be poly.</p>\n<p>Everything went according to plan: I can now comfortably describe myself and the primary relationship I have with MBlume as poly.&nbsp; &lt;bragging&gt;Since moving back to the Bay Area I've been out with four other people too, one of whom he's also seeing; I've been in my primary's presence while he kissed one girl, and when he asked another for her phone number; I've gossiped with a secondary about other persons of romantic interest and accepted his offer to hint to a guy I like that this is the case; I hit on someone at a party right in front of my primary.&nbsp; I haven't suffered a hiccup of drama or a twinge of jealousy to speak of and all evidence (including verbal confirmation) indicates that I've been managing my primary's feelings satisfactorily too.&lt;/bragging&gt;&nbsp; Does this sort of thing appeal to you?&nbsp; Cross your fingers and hope your brain works enough like mine that you can swipe my procedure.<a id=\"more\"></a></p>\n<p><strong>2. In Which I Vivisect a Specimen of Monogamy</strong></p>\n<p>It's easier to get several small things out of the way, or route around them, than to defeat one large thing embedded in several places.&nbsp; Time to <a href=\"/lw/1bj/the_shadow_question/\">ask myself what I wanted</a>.&nbsp; A notable virtue of polyamory is that it's <em>extremely customizable</em>.&nbsp; (Monogamy could be too, in theory, but comes with a strong cultural template that makes it uncomfortably non-default to implement and maintain nonstandard parameters.)&nbsp; If I could take apart what I liked about monogamy, there seemed a good chance that I could get some of those desiderata in an open relationship too (by asking my cooperative would-be primary).&nbsp; The remaining items - the ones that were <a href=\"/lw/wj/is_that_your_true_rejection/\">actually standing between me and <em>polyamory</em></a>, not just my cached stereotype thereof - would be a more manageable hacking target.&nbsp; I determined that I could, post-hack, keep and pursue the following desires:</p>\n<ul>\n<li>I want to be someone's top romantic <em>priority</em>, ideally symmetrically.&nbsp; [This is satisfied by me and MBlume having an explicitly primary relationship instead of each having a bunch of undifferentiated ones.]</li>\n<li>I eventually want to get married.&nbsp; (This one isn't in the works as of this time, but isn't precluded by anything I'm doing now.&nbsp; Open marriages are a thing.)&nbsp; Relatedly, I want to produce spawn within wedlock, and to have reproductive exclusivity (i.e. no generating half-siblings for said spawn on either side of the family).&nbsp; [MBlume was fine with this mattering to me.]</li>\n<li>I want to be able to secure attention on demand - even though I didn't anticipate needing this option routinely.&nbsp; My model of myself indicated that I would feel more comfortable with my primary going off with other girls if I knew that I was entitled to keep him home, for status- and security-related reasons.&nbsp; Actually requiring this of him in practice is rare.&nbsp; [We invented the term \"pairbonding\" to refer to designated periods of time when we are not to be distracted from one another.]</li>\n<li>I want to be suitably paranoid about STIs.&nbsp; [We worked out acceptable standards for this well in advance.]</li>\n</ul>\n<p>These things weren't the sole components of my monogamous inclinations, but what was left was a puny little thing made of <a href=\"/lw/21b/ugh_fields/\">ugh fields</a> and <a href=\"/lw/ux/traditional_capitalist_values/309c?context=1#comments\">aesthetic</a> <a href=\"/lw/6v5/new_post_version_2_please_read_this_only_if_your/4kyd\">tastes</a> and the least portions of the above.&nbsp; (For example, the first bullet point, being someone's top romantic priority, is 95% of the whole wanting to be someone's <em>exclusive</em> romantic priority.&nbsp; That last 5% is not that huge.)</p>\n<p>The vivisection process also revealed that a lot of my monogamous inclinations were composed of <em>the bare fact that monogamy had always been the specified arrangement.</em>&nbsp; Being presumed by the agreed-upon boundaries of my relationships to be monogamous meant that if either party went off and was non-monogamous, this was Breaking A Rule.&nbsp; My brain does not like it when people (including me) Break Rules<sup>2</sup> or try to change them too close to the time of the proposed would-be exception, generally speaking, but doesn't object to rules being different in different contexts.&nbsp; If I entered a relationship where, from the get-go, poly was how it was <em>supposed</em> to work, this entire structure would be silent on the subject of monogamy.&nbsp; Pre-vivisection I would have considered it more closely embedded than that.</p>\n<p><strong>3. In Which I Use My Imagination</strong></p>\n<p>Humans respond to incentives.&nbsp; We do this even when it comes to major decisions that should be significant enough in themselves to swamp said incentives.&nbsp; Encoding the switch to poly as a grand, dramatic sacrifice I was preparing to make for cinematic reasons (advance the plot, make soulful faces at the camera, establish my character to the rapt audience as some sort of long-suffering altruist giving up a Part Of Who I Am for True Love) was admittedly appealing.&nbsp; But it wasn't appealing to the bits of my brain that were doing the heavy lifting, just to the part that generates fiction and applies the templates to real life whenever possible.&nbsp; Better to find ways to cater to the selfish, practical crowd in my internal committee.</p>\n<p>Polyamory has <em>perks</em>.</p>\n<p>So I imagined a model of myself with one modification: the debris of my monogamous inclinations that were still left after I'd pared away the non-intrusive parts were not present in this model.&nbsp; Imaginary Model Alicorn was already finished with her hack and comfortable with plugging into a poly network.&nbsp; Contemplating how she went about her life, I noted the following:</p>\n<ul>\n<li>She got to date MBlume.&nbsp; (This one was important.)</li>\n<li>When I considered who else besides MBlume I might want to date if I lived in the relevant area and was poly, I found that I had a <em>list</em>.&nbsp; In several cases, the people on the list were folks I <em>couldn't</em> date if they were going to be 100% of my significant others or if I was going to be 100% of theirs - some had the wrong gametes or other features for hypothetical future spawn-production, some were already thoroughly poly and weren't about to abandon that (or, where applicable, other partner(s)) for me, some were incompletely satisfactory in other ways that I'd find frustrating if they were my sole partner but could overlook if they were supplemented appropriately.&nbsp; Imaginary Model Alicorn could date these people and wouldn't have to rely on hypotheticals to learn what it would be like.</li>\n<li>She acquired a certain level of status (respect for her mind-hacking skills and the approval that comes with having an approved-of \"sensible\" romantic orientation) within a relevant subculture.&nbsp; She got to write this post to claim said status publicly, and accumulate delicious karma.&nbsp; And she got to make this meta bullet point.</li>\n<li>She had a way to live comfortably in the Bay Area within arm's reach of lots of her friends.</li>\n<li>She had a non-destructive outlet for her appetite for social drama<sup>3</sup>.</li>\n<li>She had firsthand information about both ways to orchestrate her love life, and even if she wanted to go back to monogamy eventually for some reason, she'd be making an informed decision.</li>\n<li>She had to check fewer impulses and restrain fewer urges to remark on the attributes of people around her, because the consequences for being interpreted incorrectly (or correctly) as expressing romantic or sexual interest in arbitrary people weren't as big a deal.</li>\n</ul>\n<p>So I spent some time thinking about Imaginary Model Alicorn.&nbsp; When her life started seeming like a pleasant fantasy, instead of a far-out alternate universe, that was progress; when it sounded like a viable plan for the near future, instead of an implausible flight of fancy, that was progress too.</p>\n<p><strong>4. In Which I Put Some Brainbits in Mothballs</strong></p>\n<p>At this point my interest in being poly was thoroughly motivated and I already had a comfortably broken-in new self-model to move into - if and when I managed the hack.&nbsp; It wasn't done.&nbsp; I still had to get rid of:</p>\n<ul>\n<li>My aesthetic keening for a perfect, pretty, self-contained monogamous setup<sup>4</sup>.</li>\n<li>Resentment that I ought to <em>have</em> to self-modify to get some things I wanted, instead of the universe being set up so I could comfortably retain my factory settings.</li>\n<li>The difference between \"top priority\" and \"exclusive priority\".</li>\n<li>My impulse to retain the right to claim victim status if certain things went wrong (e.g. if I were faithful in a supposedly monogamous relationship, and then I wound up with an STI because my SO slept with someone else, I would be the wronged party and could tremble my lip at my faithless partner and demand the sympathy of my friends, instead of being a casualty of an accident yielded by allowable behaviors and entitled to nothing but a sigh of regret).</li>\n<li>Anxiety about the possibility that my primary would be stolen away by some more appealing secondary.</li>\n<li>Loss aversion, which wanted to restrain me from giving up the potential to date people who would consider <em>ever having been </em>poly a dealbreaker.&nbsp; (Note: I implemented what I believe to be a <em>reversible</em> hack, so I didn't have to worry about not being able to enter a monogamous relationship if that ever seemed called for).</li>\n</ul>\n<p>Respectively, here's what I did to get these brainbits to stop struggling long enough that I could box them up and put them into deep storage (forgive the metaphors in which I appear to make faces at myself.&nbsp; I did not actually need a mirror for any of this; those bits are symbols for the attitudes associated with the mental actions):</p>\n<ul>\n<li>Replacement.&nbsp; Cultivated a new aesthetic according to which polyamory was the \"prettier\" style.&nbsp; (Each aesthetic has the weakness of working primarily when the people around me are all doing the same thing, and I don't know how to fix that yet; but I was going to move into an area and subculture with lots of poly people anyway.)</li>\n<li>Rolled my eyes at myself and listed prior self-modifications I'd undertaken, then asking if those goals were <em>less</em> important to me than getting the benefits of being poly or if I <em>regretted </em>those prior hacks.</li>\n<li>Raised an eyebrow at myself and asked what, exactly, was the added value of exclusivity.&nbsp; Question dissolved on sufficiently skeptical inspection.</li>\n<li>Pointed out that victim status is not actually particularly valuable.&nbsp; I have acquired a better caliber of friends than I had when this brainbit appears to have crystallized, and could reasonably expect sympathy from most of them whether or not I was technically the victim of someone else's wrongdoing.&nbsp; And I can tremble my lip as much as I want, for all the good that will do.</li>\n<li>Weighed the badness of losing an SO <em>to</em> someone vs. just plain losing one due to dissatisfaction; determined difference to be insignificant, at least without more detailed information about the \"someone\" which I could not generate <em>ex hypothesi</em>.&nbsp; Noted that I would hardly <em>improve</em> my odds of retaining an SO by demanding a relationship style dispreferred by said SO.&nbsp; And the relevant individual had indicated his preference to be polyamory.</li>\n<li>\"Who exactly are these people?&nbsp; Do I know any of them?&nbsp; Not any who I'd want to date in any recognizable scenario.&nbsp; Okay then, the class as a whole is to be counted a less valuable opportunity than the class of poly people (which notably includes MBlume).\"</li>\n</ul>\n<p><strong>5. In Which Everything Goes According To Plan And I Am Repeatedly Commended For Having Magical Powers<br /></strong></p>\n<p>Field-testing has confirmed that I'm doing something right: I'm happy and comfortable.&nbsp; (Also, spontaneously all kinds of popular.&nbsp; If I'd known I could get this many people interested by hacking poly I might have done it sooner.)&nbsp; I would reverse the hack if my primary decided he wanted to be monogamous with me, but otherwise don't see a likely reason to want to.</p>\n<p>&nbsp;</p>\n<ul>\n</ul>\n<p><sup>1</sup>I'm counting willingness that one's sole partner have other partners (e.g. being an arm of a V) to be a low-key flavor of being poly oneself, not a variety of tolerant monogamy.&nbsp; I think this is the more reasonable way to divide things up given a two-way division, but if you feel that I mischaracterize the highly simplified taxonomy, do tell.</p>\n<p><sup>2</sup>The details of what my brain considers to be Rules and how it protests when they are broken or self-servingly altered are mildly interesting but irrelevant to this post.</p>\n<p><sup>3</sup>I don't think I'd describe myself as <em>enjoying</em> drama, but it's interesting and I'm drawn to it, and if I don't keep track of this carefully enough I go around starting it without realizing what I'm doing until too late.&nbsp; Generating actual drama is a good way to hurt people, so I was pleasantly surprised to discover that the same appetite appears to be indulged by working out the intricacies of relationship parameters, and keeping track of the structure of a polycule in which I am an atom, even if no drama per se exists.</p>\n<p><sup>4</sup>If the comments I linked when I first mentioned this aesthetic don't adequately explain it to you, perhaps listen to the song \"Somewhere That's Green\" from <em>Little Shop of Horrors</em>.&nbsp; The exact details in the lyrics thereof are not what I ever had in mind (it's designed to highlight and poke fun at the singing character's extremely modest ambitions) but the emotional context - minus the backstory where the character currently has an abusive boyfriend - is just right.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mip7tdAN87Jarkcew": 3, "zcvsZQWJBFK6SxK4K": 1, "AWz5ryH8SpAgTeydh": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kLR5H4pbaBjzZxLv6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 137, "baseScore": 122, "extendedScore": null, "score": 0.000233, "legacy": true, "legacyId": "9429", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 122, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This is a post about applied <a href=\"/lw/1xh/living_luminously/\">luminosity</a> in action: how I hacked myself to become polyamorous over (admittedly weak) <a href=\"/lw/2ee/unknown_knowns_why_did_you_choose_to_be_monogamous/274l\">natural monogamous inclinations</a>.&nbsp; It is a case history about me and, given the specific topic, my love life, which means <em>gooey self-disclosure ahoy</em>.&nbsp; As with <a href=\"/lw/20l/ureshiku_naritai/\">the last time I did that</a>, skip the post if it's not a thing you desire to read about.&nbsp; Named partners of mine have given permission to be named.</p>\n<p><strong id=\"1__In_Which_Motivation_is_Acquired\">1. In Which Motivation is Acquired<br></strong></p>\n<p>When one is monogamous, one can only date monogamous people.&nbsp; When one is poly, one can only date poly people.<sup>1</sup>&nbsp; Therefore, if one should find oneself with one's top romantic priority being to secure a relationship with a specific individual, it is only practical to adapt to the style of said individual, presuming that's something one can do.&nbsp; I found myself in such a position when <a href=\"/user/MBlume\">MBlume</a>, <a href=\"/lw/1a8/oy_girls_on_lw_want_to_get_together_some_time/1o8e?context=2#comments\">then my ex</a>, asked me from three time zones away if I might want to get back together.&nbsp; Since the breakup he had become polyamorous and had a different girlfriend, who herself juggled multiple partners; I'd moved, twice, and on the way dated a handful of people to no satisfactory clicking/sparking/other sound effects associated with successful romances. So the idea was appealing, if only I could get around the annoying fact that I was not, at that time, wired to be poly.</p>\n<p>Everything went according to plan: I can now comfortably describe myself and the primary relationship I have with MBlume as poly.&nbsp; &lt;bragging&gt;Since moving back to the Bay Area I've been out with four other people too, one of whom he's also seeing; I've been in my primary's presence while he kissed one girl, and when he asked another for her phone number; I've gossiped with a secondary about other persons of romantic interest and accepted his offer to hint to a guy I like that this is the case; I hit on someone at a party right in front of my primary.&nbsp; I haven't suffered a hiccup of drama or a twinge of jealousy to speak of and all evidence (including verbal confirmation) indicates that I've been managing my primary's feelings satisfactorily too.&lt;/bragging&gt;&nbsp; Does this sort of thing appeal to you?&nbsp; Cross your fingers and hope your brain works enough like mine that you can swipe my procedure.<a id=\"more\"></a></p>\n<p><strong id=\"2__In_Which_I_Vivisect_a_Specimen_of_Monogamy\">2. In Which I Vivisect a Specimen of Monogamy</strong></p>\n<p>It's easier to get several small things out of the way, or route around them, than to defeat one large thing embedded in several places.&nbsp; Time to <a href=\"/lw/1bj/the_shadow_question/\">ask myself what I wanted</a>.&nbsp; A notable virtue of polyamory is that it's <em>extremely customizable</em>.&nbsp; (Monogamy could be too, in theory, but comes with a strong cultural template that makes it uncomfortably non-default to implement and maintain nonstandard parameters.)&nbsp; If I could take apart what I liked about monogamy, there seemed a good chance that I could get some of those desiderata in an open relationship too (by asking my cooperative would-be primary).&nbsp; The remaining items - the ones that were <a href=\"/lw/wj/is_that_your_true_rejection/\">actually standing between me and <em>polyamory</em></a>, not just my cached stereotype thereof - would be a more manageable hacking target.&nbsp; I determined that I could, post-hack, keep and pursue the following desires:</p>\n<ul>\n<li>I want to be someone's top romantic <em>priority</em>, ideally symmetrically.&nbsp; [This is satisfied by me and MBlume having an explicitly primary relationship instead of each having a bunch of undifferentiated ones.]</li>\n<li>I eventually want to get married.&nbsp; (This one isn't in the works as of this time, but isn't precluded by anything I'm doing now.&nbsp; Open marriages are a thing.)&nbsp; Relatedly, I want to produce spawn within wedlock, and to have reproductive exclusivity (i.e. no generating half-siblings for said spawn on either side of the family).&nbsp; [MBlume was fine with this mattering to me.]</li>\n<li>I want to be able to secure attention on demand - even though I didn't anticipate needing this option routinely.&nbsp; My model of myself indicated that I would feel more comfortable with my primary going off with other girls if I knew that I was entitled to keep him home, for status- and security-related reasons.&nbsp; Actually requiring this of him in practice is rare.&nbsp; [We invented the term \"pairbonding\" to refer to designated periods of time when we are not to be distracted from one another.]</li>\n<li>I want to be suitably paranoid about STIs.&nbsp; [We worked out acceptable standards for this well in advance.]</li>\n</ul>\n<p>These things weren't the sole components of my monogamous inclinations, but what was left was a puny little thing made of <a href=\"/lw/21b/ugh_fields/\">ugh fields</a> and <a href=\"/lw/ux/traditional_capitalist_values/309c?context=1#comments\">aesthetic</a> <a href=\"/lw/6v5/new_post_version_2_please_read_this_only_if_your/4kyd\">tastes</a> and the least portions of the above.&nbsp; (For example, the first bullet point, being someone's top romantic priority, is 95% of the whole wanting to be someone's <em>exclusive</em> romantic priority.&nbsp; That last 5% is not that huge.)</p>\n<p>The vivisection process also revealed that a lot of my monogamous inclinations were composed of <em>the bare fact that monogamy had always been the specified arrangement.</em>&nbsp; Being presumed by the agreed-upon boundaries of my relationships to be monogamous meant that if either party went off and was non-monogamous, this was Breaking A Rule.&nbsp; My brain does not like it when people (including me) Break Rules<sup>2</sup> or try to change them too close to the time of the proposed would-be exception, generally speaking, but doesn't object to rules being different in different contexts.&nbsp; If I entered a relationship where, from the get-go, poly was how it was <em>supposed</em> to work, this entire structure would be silent on the subject of monogamy.&nbsp; Pre-vivisection I would have considered it more closely embedded than that.</p>\n<p><strong id=\"3__In_Which_I_Use_My_Imagination\">3. In Which I Use My Imagination</strong></p>\n<p>Humans respond to incentives.&nbsp; We do this even when it comes to major decisions that should be significant enough in themselves to swamp said incentives.&nbsp; Encoding the switch to poly as a grand, dramatic sacrifice I was preparing to make for cinematic reasons (advance the plot, make soulful faces at the camera, establish my character to the rapt audience as some sort of long-suffering altruist giving up a Part Of Who I Am for True Love) was admittedly appealing.&nbsp; But it wasn't appealing to the bits of my brain that were doing the heavy lifting, just to the part that generates fiction and applies the templates to real life whenever possible.&nbsp; Better to find ways to cater to the selfish, practical crowd in my internal committee.</p>\n<p>Polyamory has <em>perks</em>.</p>\n<p>So I imagined a model of myself with one modification: the debris of my monogamous inclinations that were still left after I'd pared away the non-intrusive parts were not present in this model.&nbsp; Imaginary Model Alicorn was already finished with her hack and comfortable with plugging into a poly network.&nbsp; Contemplating how she went about her life, I noted the following:</p>\n<ul>\n<li>She got to date MBlume.&nbsp; (This one was important.)</li>\n<li>When I considered who else besides MBlume I might want to date if I lived in the relevant area and was poly, I found that I had a <em>list</em>.&nbsp; In several cases, the people on the list were folks I <em>couldn't</em> date if they were going to be 100% of my significant others or if I was going to be 100% of theirs - some had the wrong gametes or other features for hypothetical future spawn-production, some were already thoroughly poly and weren't about to abandon that (or, where applicable, other partner(s)) for me, some were incompletely satisfactory in other ways that I'd find frustrating if they were my sole partner but could overlook if they were supplemented appropriately.&nbsp; Imaginary Model Alicorn could date these people and wouldn't have to rely on hypotheticals to learn what it would be like.</li>\n<li>She acquired a certain level of status (respect for her mind-hacking skills and the approval that comes with having an approved-of \"sensible\" romantic orientation) within a relevant subculture.&nbsp; She got to write this post to claim said status publicly, and accumulate delicious karma.&nbsp; And she got to make this meta bullet point.</li>\n<li>She had a way to live comfortably in the Bay Area within arm's reach of lots of her friends.</li>\n<li>She had a non-destructive outlet for her appetite for social drama<sup>3</sup>.</li>\n<li>She had firsthand information about both ways to orchestrate her love life, and even if she wanted to go back to monogamy eventually for some reason, she'd be making an informed decision.</li>\n<li>She had to check fewer impulses and restrain fewer urges to remark on the attributes of people around her, because the consequences for being interpreted incorrectly (or correctly) as expressing romantic or sexual interest in arbitrary people weren't as big a deal.</li>\n</ul>\n<p>So I spent some time thinking about Imaginary Model Alicorn.&nbsp; When her life started seeming like a pleasant fantasy, instead of a far-out alternate universe, that was progress; when it sounded like a viable plan for the near future, instead of an implausible flight of fancy, that was progress too.</p>\n<p><strong id=\"4__In_Which_I_Put_Some_Brainbits_in_Mothballs\">4. In Which I Put Some Brainbits in Mothballs</strong></p>\n<p>At this point my interest in being poly was thoroughly motivated and I already had a comfortably broken-in new self-model to move into - if and when I managed the hack.&nbsp; It wasn't done.&nbsp; I still had to get rid of:</p>\n<ul>\n<li>My aesthetic keening for a perfect, pretty, self-contained monogamous setup<sup>4</sup>.</li>\n<li>Resentment that I ought to <em>have</em> to self-modify to get some things I wanted, instead of the universe being set up so I could comfortably retain my factory settings.</li>\n<li>The difference between \"top priority\" and \"exclusive priority\".</li>\n<li>My impulse to retain the right to claim victim status if certain things went wrong (e.g. if I were faithful in a supposedly monogamous relationship, and then I wound up with an STI because my SO slept with someone else, I would be the wronged party and could tremble my lip at my faithless partner and demand the sympathy of my friends, instead of being a casualty of an accident yielded by allowable behaviors and entitled to nothing but a sigh of regret).</li>\n<li>Anxiety about the possibility that my primary would be stolen away by some more appealing secondary.</li>\n<li>Loss aversion, which wanted to restrain me from giving up the potential to date people who would consider <em>ever having been </em>poly a dealbreaker.&nbsp; (Note: I implemented what I believe to be a <em>reversible</em> hack, so I didn't have to worry about not being able to enter a monogamous relationship if that ever seemed called for).</li>\n</ul>\n<p>Respectively, here's what I did to get these brainbits to stop struggling long enough that I could box them up and put them into deep storage (forgive the metaphors in which I appear to make faces at myself.&nbsp; I did not actually need a mirror for any of this; those bits are symbols for the attitudes associated with the mental actions):</p>\n<ul>\n<li>Replacement.&nbsp; Cultivated a new aesthetic according to which polyamory was the \"prettier\" style.&nbsp; (Each aesthetic has the weakness of working primarily when the people around me are all doing the same thing, and I don't know how to fix that yet; but I was going to move into an area and subculture with lots of poly people anyway.)</li>\n<li>Rolled my eyes at myself and listed prior self-modifications I'd undertaken, then asking if those goals were <em>less</em> important to me than getting the benefits of being poly or if I <em>regretted </em>those prior hacks.</li>\n<li>Raised an eyebrow at myself and asked what, exactly, was the added value of exclusivity.&nbsp; Question dissolved on sufficiently skeptical inspection.</li>\n<li>Pointed out that victim status is not actually particularly valuable.&nbsp; I have acquired a better caliber of friends than I had when this brainbit appears to have crystallized, and could reasonably expect sympathy from most of them whether or not I was technically the victim of someone else's wrongdoing.&nbsp; And I can tremble my lip as much as I want, for all the good that will do.</li>\n<li>Weighed the badness of losing an SO <em>to</em> someone vs. just plain losing one due to dissatisfaction; determined difference to be insignificant, at least without more detailed information about the \"someone\" which I could not generate <em>ex hypothesi</em>.&nbsp; Noted that I would hardly <em>improve</em> my odds of retaining an SO by demanding a relationship style dispreferred by said SO.&nbsp; And the relevant individual had indicated his preference to be polyamory.</li>\n<li>\"Who exactly are these people?&nbsp; Do I know any of them?&nbsp; Not any who I'd want to date in any recognizable scenario.&nbsp; Okay then, the class as a whole is to be counted a less valuable opportunity than the class of poly people (which notably includes MBlume).\"</li>\n</ul>\n<p><strong id=\"5__In_Which_Everything_Goes_According_To_Plan_And_I_Am_Repeatedly_Commended_For_Having_Magical_Powers\">5. In Which Everything Goes According To Plan And I Am Repeatedly Commended For Having Magical Powers<br></strong></p>\n<p>Field-testing has confirmed that I'm doing something right: I'm happy and comfortable.&nbsp; (Also, spontaneously all kinds of popular.&nbsp; If I'd known I could get this many people interested by hacking poly I might have done it sooner.)&nbsp; I would reverse the hack if my primary decided he wanted to be monogamous with me, but otherwise don't see a likely reason to want to.</p>\n<p>&nbsp;</p>\n<ul>\n</ul>\n<p><sup>1</sup>I'm counting willingness that one's sole partner have other partners (e.g. being an arm of a V) to be a low-key flavor of being poly oneself, not a variety of tolerant monogamy.&nbsp; I think this is the more reasonable way to divide things up given a two-way division, but if you feel that I mischaracterize the highly simplified taxonomy, do tell.</p>\n<p><sup>2</sup>The details of what my brain considers to be Rules and how it protests when they are broken or self-servingly altered are mildly interesting but irrelevant to this post.</p>\n<p><sup>3</sup>I don't think I'd describe myself as <em>enjoying</em> drama, but it's interesting and I'm drawn to it, and if I don't keep track of this carefully enough I go around starting it without realizing what I'm doing until too late.&nbsp; Generating actual drama is a good way to hurt people, so I was pleasantly surprised to discover that the same appetite appears to be indulged by working out the intricacies of relationship parameters, and keeping track of the structure of a polycule in which I am an atom, even if no drama per se exists.</p>\n<p><sup>4</sup>If the comments I linked when I first mentioned this aesthetic don't adequately explain it to you, perhaps listen to the song \"Somewhere That's Green\" from <em>Little Shop of Horrors</em>.&nbsp; The exact details in the lyrics thereof are not what I ever had in mind (it's designed to highlight and poke fun at the singing character's extremely modest ambitions) but the emotional context - minus the backstory where the character currently has an abusive boyfriend - is just right.</p>", "sections": [{"title": "1. In Which Motivation is Acquired", "anchor": "1__In_Which_Motivation_is_Acquired", "level": 1}, {"title": "2. In Which I Vivisect a Specimen of Monogamy", "anchor": "2__In_Which_I_Vivisect_a_Specimen_of_Monogamy", "level": 1}, {"title": "3. In Which I Use My Imagination", "anchor": "3__In_Which_I_Use_My_Imagination", "level": 1}, {"title": "4. In Which I Put Some Brainbits in Mothballs", "anchor": "4__In_Which_I_Put_Some_Brainbits_in_Mothballs", "level": 1}, {"title": "5. In Which Everything Goes According To Plan And I Am Repeatedly Commended For Having Magical Powers", "anchor": "5__In_Which_Everything_Goes_According_To_Plan_And_I_Am_Repeatedly_Commended_For_Having_Magical_Powers", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "603 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 603, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3Cjjem7AbmmZfBs", "xnPFYBuaGhpq869mY", "vEJBc6hfKntnQ2A7C", "TGux5Fhcd7GmTfNGC", "EFQ3F6kmt4WHXRqik"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2011-08-28T08:35:29.887Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-28T20:36:07.644Z", "modifiedAt": null, "url": null, "title": "Public Donation List or Putting Peer Pressure to Good Use", "slug": "public-donation-list-or-putting-peer-pressure-to-good-use", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:09.568Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "drethelin", "createdAt": "2011-03-02T17:38:08.607Z", "isAdmin": false, "displayName": "drethelin"}, "userId": "ZwawHK4dwF53ZDvMM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZC2XQE6ov63LSMdh9/public-donation-list-or-putting-peer-pressure-to-good-use", "pageUrlRelative": "/posts/ZC2XQE6ov63LSMdh9/public-donation-list-or-putting-peer-pressure-to-good-use", "linkUrl": "https://www.lesswrong.com/posts/ZC2XQE6ov63LSMdh9/public-donation-list-or-putting-peer-pressure-to-good-use", "postedAtFormatted": "Sunday, August 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Public%20Donation%20List%20or%20Putting%20Peer%20Pressure%20to%20Good%20Use&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APublic%20Donation%20List%20or%20Putting%20Peer%20Pressure%20to%20Good%20Use%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZC2XQE6ov63LSMdh9%2Fpublic-donation-list-or-putting-peer-pressure-to-good-use%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Public%20Donation%20List%20or%20Putting%20Peer%20Pressure%20to%20Good%20Use%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZC2XQE6ov63LSMdh9%2Fpublic-donation-list-or-putting-peer-pressure-to-good-use", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZC2XQE6ov63LSMdh9%2Fpublic-donation-list-or-putting-peer-pressure-to-good-use", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<p>The recent top level post encouraging people to donate in support of Luke had a lot of a comments saying that it helped remind them to donate at all, simply by bringing it to their attention. I think this is a useful function, and the more people that publicly state that they are donating the stronger the peer pressure on people to donate. I have a recurring monthly donation of 500 dollars, and recently donated 2000 in support of lukeprog and because it is currently getting doubled. Please, if you are willing, comment on this post with donation amounts! Any amount is useful to talk about, even if you think it's less than what other people are donating, because for every person that can donate a large amount there is a large amount of people that can donate a small amount. We should encourage and reinforce donation behavior at all levels.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZC2XQE6ov63LSMdh9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": -1, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "9467", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-28T21:56:42.067Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup 09-06-2011", "slug": "meetup-west-la-meetup-09-06-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.750Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5tupEPao9Xkc5ZiNd/meetup-west-la-meetup-09-06-2011", "pageUrlRelative": "/posts/5tupEPao9Xkc5ZiNd/meetup-west-la-meetup-09-06-2011", "linkUrl": "https://www.lesswrong.com/posts/5tupEPao9Xkc5ZiNd/meetup-west-la-meetup-09-06-2011", "postedAtFormatted": "Sunday, August 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%2009-06-2011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%2009-06-2011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5tupEPao9Xkc5ZiNd%2Fmeetup-west-la-meetup-09-06-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%2009-06-2011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5tupEPao9Xkc5ZiNd%2Fmeetup-west-la-meetup-09-06-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5tupEPao9Xkc5ZiNd%2Fmeetup-west-la-meetup-09-06-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2s'>West LA Meetup 09-06-2011</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 September 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10800 West Pico Blvd, Suite 312, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When</strong>: 7pm - 9pm September 6th</p>\n\n<p><strong>Where</strong>: <a href=\"http://maps.google.com/maps?q=10800+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">The Westside Pavillion</a> - on the bridge, which connects Nordstrom 3rd floor with Barnes &amp; Noble / Landmark Theatres 3rd floor.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p>Whether you're a regular reader or totally new, here for the theoretical musings or the practical things, come by and say hello! The conversation is largely unstructured, and the people are awesome.</p>\n\n<p><em>There will be snacks. I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</em></p>\n\n<p>See also: <a href=\"http://lesswrong.com/r/discussion/lw/6at/west_la_biweekly_meetups/\">West LA Biweekly Meetups</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2s'>West LA Meetup 09-06-2011</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5tupEPao9Xkc5ZiNd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.61533345945746e-07, "legacy": true, "legacyId": "9471", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_09_06_2011\">Discussion article for the meetup : <a href=\"/meetups/2s\">West LA Meetup 09-06-2011</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 September 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10800 West Pico Blvd, Suite 312, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When</strong>: 7pm - 9pm September 6th</p>\n\n<p><strong>Where</strong>: <a href=\"http://maps.google.com/maps?q=10800+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">The Westside Pavillion</a> - on the bridge, which connects Nordstrom 3rd floor with Barnes &amp; Noble / Landmark Theatres 3rd floor.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p>Whether you're a regular reader or totally new, here for the theoretical musings or the practical things, come by and say hello! The conversation is largely unstructured, and the people are awesome.</p>\n\n<p><em>There will be snacks. I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</em></p>\n\n<p>See also: <a href=\"http://lesswrong.com/r/discussion/lw/6at/west_la_biweekly_meetups/\">West LA Biweekly Meetups</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_09_06_20111\">Discussion article for the meetup : <a href=\"/meetups/2s\">West LA Meetup 09-06-2011</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup 09-06-2011", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_09_06_2011", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup 09-06-2011", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_09_06_20111", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tHFu6kvy2HMvQBEhW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-29T03:35:02.344Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Why I'm Blooking", "slug": "seq-rerun-why-i-m-blooking", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RzRSB62HN88b9Edma/seq-rerun-why-i-m-blooking", "pageUrlRelative": "/posts/RzRSB62HN88b9Edma/seq-rerun-why-i-m-blooking", "linkUrl": "https://www.lesswrong.com/posts/RzRSB62HN88b9Edma/seq-rerun-why-i-m-blooking", "postedAtFormatted": "Monday, August 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Why%20I'm%20Blooking&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Why%20I'm%20Blooking%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRzRSB62HN88b9Edma%2Fseq-rerun-why-i-m-blooking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Why%20I'm%20Blooking%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRzRSB62HN88b9Edma%2Fseq-rerun-why-i-m-blooking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRzRSB62HN88b9Edma%2Fseq-rerun-why-i-m-blooking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<p>Today's post, <a href=\"/lw/jf/why_im_blooking/\">Why I'm Blooking</a> was originally published on 15 September 2007.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>Now that Eliezer had posted over 100 blog posts, he created this post to lay out why exactly he was writing this series of posts. Understanding what he was trying to do can assist in understanding what became the sequences.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/7b0/seq_rerun_doublethink_choosing_to_be_biased/#comments\">Doublethink (Choosing to be Biased)</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RzRSB62HN88b9Edma", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 7.616431184613984e-07, "legacy": true, "legacyId": "9480", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vHPrTLnhrgAHA96ko", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-29T05:23:19.638Z", "modifiedAt": null, "url": null, "title": "Book trades with open-minded theists - recommendations?", "slug": "book-trades-with-open-minded-theists-recommendations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:58.019Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X2cbdCpskSAAGF5h3/book-trades-with-open-minded-theists-recommendations", "pageUrlRelative": "/posts/X2cbdCpskSAAGF5h3/book-trades-with-open-minded-theists-recommendations", "linkUrl": "https://www.lesswrong.com/posts/X2cbdCpskSAAGF5h3/book-trades-with-open-minded-theists-recommendations", "postedAtFormatted": "Monday, August 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Book%20trades%20with%20open-minded%20theists%20-%20recommendations%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABook%20trades%20with%20open-minded%20theists%20-%20recommendations%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX2cbdCpskSAAGF5h3%2Fbook-trades-with-open-minded-theists-recommendations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Book%20trades%20with%20open-minded%20theists%20-%20recommendations%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX2cbdCpskSAAGF5h3%2Fbook-trades-with-open-minded-theists-recommendations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX2cbdCpskSAAGF5h3%2Fbook-trades-with-open-minded-theists-recommendations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<p>In an Open Thread <a href=\"/lw/6y9/open_thread_august_2011/4nro\">comment</a> beriukay mentioned that he's reading C.S. Lewis' <a href=\"http://en.wikipedia.org/wiki/Mere_Christianity\">Mere Christianity</a>. I've been reading it too, for interesting reasons.</p>\n<p>In my case it so happened that I started discussing faith with a long-time online friend whose spiritual views I didn't yet know, and he turned out to be a Christian with a high regard for the Bible, who also has an interest in science. As our discussion turned to our readings on spirituality, I acknowledged (I think it was me) that I probably spent more time on books that reinforce my point of view than on books that challenge it, perhaps a case of confirmation bias. (I've been exposed to many poor arguments for Christianity, and dismissed them; but possibly that was largely a function of having started out with that bottom line already written and picking arguments I wouldn't have much trouble refuting.)</p>\n<p>In the spirit of experiment we agreed to a \"trade\" - he would read (thoughtfully and with an open mind) a book of my choosing on reasons to doubt faith, and I'd do the same with a book he chose on Christianity.</p>\n<p>So the idea here is to pick a book that's the \"best argument from the other side\" (as in quote 3 <a href=\"/lw/tm/rationality_quotes_13/\">here</a>).</p>\n<p>I recommended <a href=\"http://en.wikipedia.org/wiki/The_God_Delusion\">The God Delusion</a> - I'm not sure if that's the best choice given the above intent, but it's what came to mind on the spot.</p>\n<p>Would you make a different choice? If so, what?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X2cbdCpskSAAGF5h3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 13, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "9483", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 70, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gm9bg33io9ikaHudN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-29T05:27:31.636Z", "modifiedAt": null, "url": null, "title": "[LINK] How Hard is Artificial Intelligence? The Evolutionary Argument and Observation Selection Effects ", "slug": "link-how-hard-is-artificial-intelligence-the-evolutionary", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:19.949Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZakKKbuqfcSoGvma9/link-how-hard-is-artificial-intelligence-the-evolutionary", "pageUrlRelative": "/posts/ZakKKbuqfcSoGvma9/link-how-hard-is-artificial-intelligence-the-evolutionary", "linkUrl": "https://www.lesswrong.com/posts/ZakKKbuqfcSoGvma9/link-how-hard-is-artificial-intelligence-the-evolutionary", "postedAtFormatted": "Monday, August 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20How%20Hard%20is%20Artificial%20Intelligence%3F%20The%20Evolutionary%20Argument%20and%20Observation%20Selection%20Effects%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20How%20Hard%20is%20Artificial%20Intelligence%3F%20The%20Evolutionary%20Argument%20and%20Observation%20Selection%20Effects%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZakKKbuqfcSoGvma9%2Flink-how-hard-is-artificial-intelligence-the-evolutionary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20How%20Hard%20is%20Artificial%20Intelligence%3F%20The%20Evolutionary%20Argument%20and%20Observation%20Selection%20Effects%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZakKKbuqfcSoGvma9%2Flink-how-hard-is-artificial-intelligence-the-evolutionary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZakKKbuqfcSoGvma9%2Flink-how-hard-is-artificial-intelligence-the-evolutionary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p>If you're interested in evolution, anthropics, and AI timelines -- or in what the Singularity Institute has been producing lately -- you might want to check out this new paper, by SingInst research fellow Carl Shulman and FHI professor Nick Bostrom.</p>\n<p>The paper:</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://www.nickbostrom.com/aievolution.pdf\">How Hard is Artificial Intelligence? The Evolutionary Argument and Observation Selection Effects&nbsp;</a></p>\n<p>The abstract:</p>\n<p style=\"padding-left: 30px;\">Several authors have made the argument that because blind evolutionary processes produced human&nbsp;intelligence on Earth, it should be feasible for clever human engineers to create human-level artificial&nbsp;intelligence in the not-too-distant future. &nbsp;This evolutionary argument, however, has ignored the&nbsp;observation selection effect that guarantees that observers will see intelligent life having arisen on their&nbsp;planet no matter how hard it is for intelligent life to evolve on any given Earth-like planet. &nbsp;We explore&nbsp;how the evolutionary argument might be salvaged from this objection, using a variety of considerations&nbsp;from observation selection theory and analysis of specific timing features and instances of convergent&nbsp;evolution in the terrestrial evolutionary record. &nbsp;We find that a probabilistic version of the evolutionary&nbsp;argument emerges largely intact once appropriate corrections have been made.</p>\n<p>I'd be interested to hear LW-ers' takes on the content; Carl, too, would much appreciate feedback.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZakKKbuqfcSoGvma9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 24, "extendedScore": null, "score": 7.616796210367718e-07, "legacy": true, "legacyId": "9484", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-29T07:04:30.385Z", "modifiedAt": "2020-09-13T02:04:32.360Z", "url": null, "title": "A History of Bayes' Theorem", "slug": "a-history-of-bayes-theorem", "viewCount": null, "lastCommentedAt": "2020-10-21T18:45:55.968Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "lukeprog", "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RTt59BtFLqQbsSiqd/a-history-of-bayes-theorem", "pageUrlRelative": "/posts/RTt59BtFLqQbsSiqd/a-history-of-bayes-theorem", "linkUrl": "https://www.lesswrong.com/posts/RTt59BtFLqQbsSiqd/a-history-of-bayes-theorem", "postedAtFormatted": "Monday, August 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20History%20of%20Bayes'%20Theorem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20History%20of%20Bayes'%20Theorem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRTt59BtFLqQbsSiqd%2Fa-history-of-bayes-theorem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20History%20of%20Bayes'%20Theorem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRTt59BtFLqQbsSiqd%2Fa-history-of-bayes-theorem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRTt59BtFLqQbsSiqd%2Fa-history-of-bayes-theorem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4566, "htmlBody": "<figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3af01042c819c214e42312da1872de6acb082373e826a003.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3af01042c819c214e42312da1872de6acb082373e826a003.png/w_100 100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3af01042c819c214e42312da1872de6acb082373e826a003.png/w_180 180w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3af01042c819c214e42312da1872de6acb082373e826a003.png/w_260 260w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3af01042c819c214e42312da1872de6acb082373e826a003.png/w_340 340w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3af01042c819c214e42312da1872de6acb082373e826a003.png/w_420 420w\"></figure><blockquote><p>Sometime during the 1740s, the Reverend Thomas Bayes made the ingenious discovery that bears his name but then mysteriously abandoned it. It was rediscovered independently by a different and far more renowned man, Pierre Simon Laplace, who gave it its modern mathematical form and scientific application \u2014 and then moved on to other methods. Although Bayes\u2019 rule drew the attention of the greatest statisticians of the twentieth century, some of them vilified both the method and its adherents, crushed it, and declared it dead. Yet at the same time, it solved practical questions that were unanswerable by any other means: the defenders of Captain Dreyfus used it to demonstrate his innocence; insurance actuaries used it to set rates; Alan Turing used it to decode the German Enigma cipher and arguably save the Allies from losing the Second World War; the U.S. Navy used it to search for a missing H-bomb and to locate Soviet subs; RAND Corporation used it to assess the likelihood of a nuclear accident; and Harvard and Chicago researchers used it to verify the authorship of the Federalist Papers. In discovering its value for science, many supporters underwent a near-religious conversion yet had to conceal their use of Bayes\u2019 rule and pretend they employed something else. It&nbsp;was not until the twenty-first century that the method lost its stigma and was widely and enthusiastically embraced.</p></blockquote><p><br>So begins <a href=\"http://www.mcgrayne.com/\">Sharon McGrayne</a>'s fun new book,&nbsp;<a href=\"http://www.amazon.com/Theory-That-Would-Not-Die/dp/0300169698/\"><i>The Theory That Would Not Die</i></a>, a popular history of <a href=\"http://yudkowsky.net/rational/bayes\">Bayes' Theorem</a>. Instead of reviewing the book, I'll summarize some of its content below. I skip the details and many great stories from the book, for example the (Bayesian) search for a lost submarine that inspired <i>Hunt for Red October</i>. Also see McGrayne's Google Talk <a href=\"http://www.youtube.com/watch?v=8oD6eBkjF9o\">here</a>. She will be speaking at the upcoming Singularity Summit, too, which you can register for <a href=\"https://www.singularitysummit.com/registration/\">here</a> (price goes up after August 31st).</p><h1>Origins</h1><p>In the 1700s, when probability theory was just a whiff in the air, the English Reverend <a href=\"http://en.wikipedia.org/wiki/Thomas_bayes\">Thomas Bayes</a> wanted to know how to infer causes from effects. He set up his working problem like this: How could he learn the probability of a future event occurring if he only knew how many times it had occurred or not occurred in the past?</p><p>He needed a number, and it was hard to decide which number to choose. In the end, his solution was to just <i>guess</i>&nbsp;and then improve his guess later as he gathered more information.</p><p>He used a thought experiment to illustrate the process. Imagine that Bayes has his back turned to a table, and he asks his assistant to drop a ball on the table. The table is such that the ball has just as much chance of landing at any&nbsp;one place on the table as anywhere else. Now Bayes has to figure out where the ball is, without looking.</p><p>He asks his assistant to throw another ball on the table and report whether it is to the left or the right of the first ball. If the new ball landed to the left of the first ball, then the first ball is more likely to be on the right side of the table than the left side. He asks his assistant to throw the second ball again. If it again lands to the left of the first ball, then the first ball is even <i>more</i> likely than before to be on the right side of the table. And so on.</p><p>Throw after throw, Bayes is able to narrow down the area in which the first ball probably sits. Each new piece of information constrains the area where the first ball probably is.</p><p>Bayes' system was: Initial Belief + New Data -&gt; Improved Belief.</p><p>Or, as the terms came to be called: Prior + Likelihood of your new observation given competing hypotheses -&gt; Posterior.</p><p>In each new round of belief updating, the most recent posterior becomes the prior for the new calculation.</p><p>There were two enduring criticisms to Bayes' system. First, mathematicians were horrified to see something as whimsical as a <i>guess</i>&nbsp;play a role in rigorous mathematics. Second, Bayes said that if he didn't know what guess to make, he'd just assign all possibilities <i>equal</i>&nbsp;probability to start. For most mathematicians, this <i>problem of priors</i>&nbsp;was insurmountable.</p><p>Bayes never published his discovery, but his friend <a href=\"http://en.wikipedia.org/wiki/Richard_Price\">Richard Price</a> found it among his notes after Bayes' death in 1761, re-edited it, and published it. Unfortunately, virtually no one seems to have read the paper, and Bayes' method lay cold until the arrival of Laplace.</p><p>&nbsp;</p><h1>Laplace</h1><p>By the late 18th century, Europe was awash in scientific data. Astronomers had observations made by the Chinese in 1100 BC, by the Greeks in 200 BC, by the Romans in AD 100, and by the Arabs in AD 1000. The data were not of equal reliability. How could scientists process all their observations and choose the best? Many astronomers simply <i>averaged</i>&nbsp;their three 'best' observations, but this was ad-hoc. The world needed a better way to handle all these data.</p><p><a href=\"http://en.wikipedia.org/wiki/Pierre-Simon_Laplace\">Pierre-Simon Laplace</a>, a brilliant young mathematician, came to believe that probability theory held the key, and he independently rediscovered Bayes' mechanism and published&nbsp;it in 1774. Laplace stated the principle not with an equation, but in words:&nbsp;the probability of a cause (given an event) is proportional to the probability of the event&nbsp;(given its cause). And for the next 40 years, Laplace used, extended, clarified, and proved his new principle.</p><p>In 1781, Richard Price visited Paris, and word of Bayes' earlier discovery eventually reached Laplace. Laplace was now all the more confident that he was on the right track.</p><p>He needed to test his principle, so he turned to the largest data set available: birth records. A few people had noticed that slightly more boys than girls were born, and Laplace wanted to know if this was an anomalous or constant phenomenon. He began by applying equal probability to his hunches, and then updated his belief as he examined data sets from Paris, from London, from Naples, from St. Petersburg, and from rural areas in France. Later he even asked friends for birth data from Egypt and Central America. Finally, by 1812, he was almost certain that the birth of more boys than girls was \"a general law for the human race.\"</p><p>Laplace's friend Bouvard used his method to calculate the masses of Jupiter and Saturn from a wide variety of observations. Laplace was so impressed that he offered his readers a famous bet: 11,000 to 1 odds that Bouvard's results for Saturn were within 1% of the correct answer, and a million to one odds for Jupiter. Nobody seems to have taken Laplace's bet, but today's technology confirms that Laplace should have won both bets.</p><p>Laplace used his principle on the issue of testimony, both in court and in the Bible, and made famous progress in astronomy. When asked by Napoleon who authored the heavens, Laplace replied that natural law could explain the behavior of the heavens. Napoleon asked why Laplace had failed to mention God in his book on the subject. Laplace replied: \"Sire, I have no need of that hypothesis.\"</p><p>The answer became a symbol of the new science: the search for natural laws that produced phenomena without the need to call upon magic in the explanation.&nbsp;</p><p>And then, Laplace invented the <a href=\"http://en.wikipedia.org/wiki/Central_limit_theorem\">central limit theorem</a>, which let him handle almost any kind of data. He soon realized that where large amounts of data were available, both the Bayesian and the frequentist approaches (judging an event's probability by how frequently it occurs among many observations) to probability tended to produce the same results. (Only much later did scientists discover how wildly the two approaches can diverge even given lots of data.)</p><p>And so at age 62, Laplace \u2014 the world's first Bayesian \u2014 converted to&nbsp;frequentism, which he used for the remaining 16 years of his life.</p><p>...though he did finally realize what the general theorem for Bayes' method had to be:</p><blockquote><p>P(C|E) = [ P(E|C)P<sub>prior</sub>(C) ] / [\u03a3P(E|C')P<sub>prior</sub>(C')</p></blockquote><p>Which says that the probability of a hypothesis C <i>given</i>&nbsp;some evidence E equals our initial estimate of the probability <i>times</i>&nbsp;the probability of the evidence given the hypothesis C divided by the sum of the probabilities of the data in all possible hypotheses.</p><p>Basically, Laplace did all the hard work, and he deserves most of the honor for what we call Bayes' Theorem. But historical accidents happen, and the method is named after Bayes.</p><p>&nbsp;</p><h1>The Decline of Bayes' Theorem</h1><p>Empowered by Laplace's central limit theorem, government officials were expected to collect statistics on all sorts of things: cholera victims, the chest sizes of soldiers, the number of Prussian officers killed by kicking horses, and so on. But the idea that probability quantifies our ignorance was gone, replaced by the idea that the new science could not allow for anything 'subjective'. John Stuart Mill denounced probability as \"ignorance... coined into science.\"</p><p>By 1891, the Scottish mathematician George Chrystal urged: \"[Laplace's principle] being dead, [it] should be decently buried out of sight, and not embalmed in text-books and examination papers... The indiscretions of great men should be quietly allowed to be forgotten.\"</p><p>And thus, Bayes' Theorem fell yet again in disuse... at least among theoreticians. A smattering of practitioners continued to find it useful.</p><p><a href=\"http://en.wikipedia.org/wiki/Joseph_Louis_Fran%C3%A7ois_Bertrand\">Joseph Bertrand</a>&nbsp;was convinced that Bayes' Theorem was the only way for artillery officers to correctly deal with a host of uncertainties about the enemies' location, air density, wind direction, and more. From 1890-1935, French and Russian artillery officers used Bertrand's Bayesian textbook to fire their weapons.</p><p>When the French Jew Alfred Dreyfus was falsely accused of having sold a letter to German military expert, France's famous mathematician Henri Poincar\u00e9 was called to the stand.&nbsp;Poincar\u00e9 was a frequentist, but when asked whether Dreyfus had written the letter,&nbsp;Poincar\u00e9 invoked Bayes' Theorem as the only sensible way for a court of law to update a hypothesis with new evidence, and proclaimed that the prosecution's discussion of probability was nonsense. Dreyfus was still convicted, though his sentence was reduced, but the public was outraged and the president issued a pardon two weeks later.</p><p>Statisticians used Bayes' Theorem to set up a functioning Bell phone system, set of up the United States' first working social insurance system, and solve other problems.</p><p>Meanwhile, the biologist&nbsp;<a href=\"http://en.wikipedia.org/wiki/Ronald_Fisher\">R.A. Fisher</a> was pioneering new randomization methods, sampling theory, tests of significant, analyses of variance, and a variety of experimental designs. In 1925 he published his revolutionary manual, <i>Statistical Methods of Research Workers</i>. The success of the book enshrined frequentism and the standard statistical method.</p><p>&nbsp;</p><h1>Jeffreys</h1><p>Even during its decline, a few people made progress on Bayesian theory. At about the same time, three men in three countries \u2014 <a href=\"http://en.wikipedia.org/wiki/%C3%89mile_Borel\">\u00c9mile Borel</a>, <a href=\"http://en.wikipedia.org/wiki/Frank_P._Ramsey\">Frank Ramsey</a>, and <a href=\"http://en.wikipedia.org/wiki/Bruno_de_Finetti\">Bruno de Finetti</a> \u2014&nbsp;independently happened upon the same idea: knowledge <i>is</i>&nbsp;subjective, and we can quantify it with a bet. The amount we wager shows how strongly we believe something.</p><p>And then, the geologist <a href=\"http://en.wikipedia.org/wiki/Harold_Jeffreys\">Harold Jeffreys</a> made Bayes' Theorem useful for scientists, proposing it as an alternative to Fisher's 'p-values' and 'significance tests', which depended on \"imaginary repetitions.\" In contrast, Bayesianism considered data as fixed evidence. Moreover, the p-value is a statement about data, but Jeffreys wanted to know about his hypothesis <i>given</i>&nbsp;the data. He published the monumental <i>Theory of Probability</i>&nbsp;in 1939, which remained for many years the only explanation of how to use Bayes to do science.</p><p>For decades, Fisher and Jeffreys were the world's two greatest statisticians, though both were practicing scientists instead of theoreticians. They traded blows over probability theory in scientific journals and in public. Fisher was louder and bolder, and frequentism was easier to use than Bayesianism.</p><p>&nbsp;</p><h1>Bayes at War</h1><p>In 1941, German U-Boats were devastating allied naval forces. Britain was cut off from its sources of food, and couldn't grow enough on its own soil to feed its citizens. Winston Churchill said the U-boat problem was the scariest part of the war for him.</p><p>The German codes, produced by Enigma machines with customizable wheel positions that allowed the codes to be changed rapidly, were considered unbreakable, so nobody was working on them. This attracted <a href=\"http://en.wikipedia.org/wiki/Alan_Turing\">Alan Turing</a> to the problem, because he liked solitude. He built a machine that could test different code possibilities, but it was slow. The machine might need four days to test all 336 wheel positions on a particular Enigma code. Until more machines could be built, Turing had to find a way for reducing the burden on the machine.&nbsp;</p><p>He used a Bayesian system to guess the letters in an Enigma message, and add more clues as they arrived with new data. With this method he could reduce the number of wheel settings to be tested by his machine from 336 to as few as 18. But soon, Turing realized that he couldn't compare the probabilities of his hunches without a standard unit of measurement. So, he invented the 'ban', defined as \"about the smallest change in weight of evidence that is directly perceptible to human intuition.\" This unit turned out to be very similar to the bit, the measure of information discovered using Bayes' Theorem while working for Bell Telephone.</p><p>Now that he had a unit of measurement, he could target the amount of evidence he needed for a particular hunch and then stop the process when he had that much evidence.</p><p>While Turing was cracking the Enigma codes in Britain, <a href=\"http://en.wikipedia.org/wiki/Andrey_Kolmogorov\">Andrey Kolmogorov</a> was fleeing the German artillery bombardment of Moscow. In 1933 he had showed that probability theory can be derived from basic mathematical axioms, and now Russia's generals were asking him about how best to fire back at the Germans. Though a frequentist, Kolmogorov recommended they used Bertrand's Bayesian firing system in a crisis like this.</p><p>Shortly after this, the British learned that the Germans were now using stronger, faster encryption machines: Lorenz machines. The British team used Turing's Bayesian scoring system and tried a variety of priors to crack the codes.&nbsp;</p><p>Turing visited America and spent time with <a href=\"http://en.wikipedia.org/wiki/Claude_Shannon\">Claude Shannon</a>, whose brilliant insights about information theory came a bit later. He realized that the purpose of information is to reduce uncertainty and the purpose of encryption is to increase it. He was using Bayes for both. Basically, if the posterior in a Bayesian equation is very different from the prior, then much has been learned, but if the posterior is roughly the same as the prior, then the information content is low. Shannon's unit for information was the 'bit'.</p><p>Meanwhile, Allied patrol planes needed to narrow their search for German U-boats. If 7 different listening posts intercepted the same message from the same U-boat, it could be located to somewhere in a circle 236 miles across. That's a lot of uncertainty, and mathematician <a href=\"http://en.wikipedia.org/wiki/Bernard_Koopman\">Bernard Koopman</a> was assigned to solve the problem. He wasn't bashful about Bayes at all. He said: \"Every operation involved in search is beset with uncertainties; it can be understood quantitatively only in terms of... probability. This may now be regarded as a truism, but it seems to have taken the developments in operational research of the Second World War to drive home its practical implications.\"</p><p>Koopman started by assigning 50% probability that a U-boat was inside the 236-mile circle, and then update his probability as more data came in, apportioning plane flyover hours according to the probabilities of U-boat locations.</p><p>And then, a few day's after Germany's surrender, Churchill ordered the destruction of all evidence that decoding has helped win the war, apparently because the British didn't want the Soviets to know they could decrypt Lorenz codes. It wasn't until 1973 that the story of Turing and Bayes began to emerge.</p><p>&nbsp;</p><h1>Revival</h1><p>Its wartime successes classified, Bayes' Theorem remained mostly in the dark after the Second World War. Textbooks self-righteously dismissed Bayes. During the McCarthyism of the 1950s, one government statistician half-jokingly called a colleague \"un-American because [he] was a Bayesian, ...undermining the United States Government.\"</p><p>In 1950, an economist preparing a report asked statistician <a href=\"http://en.wikipedia.org/wiki/David_blackwell\">David Blackwell</a> (not yet a Bayesian) to estimate the probability of another world war in the next five years. Blackwell answered: \"Oh, that question just doesn't make sense. Probability applies to a long sequence of repeatable events, and this is clearly a unique situation. The probability is either 0 or 1, but we won't know for five years.\" The economist replied, \"I was afraid you were going to say that. I've spoken to several other statisticians, and they all told me the same thing.\"</p><p>Still, there were flickers of life. For decades after the war, one of Turing's American colleagues taught Bayes to NSA cryptographers. <a href=\"http://en.wikipedia.org/wiki/I.J._Good\">I.J. Good</a>, one of Turing's statistics assistant, developed Bayesian methods and theory, writing about 900 articles about Bayes.</p><p>And then there was the Bible-quoting business executive Arthur Bailey.</p><p>Bailey was trained in statistics, and when he joined an insurance company he was horrified to see them using Bayesian techniques developed in 1918. They asked not \"What should the new rates be?\" but instead \"How much should the present rates be changed?\" But after a year of trying different things, he realized that the Bayesian actuarial methods worked better than frequentist methods. Bailey \"realized that the hard-shelled underwriters were recognizing certain facts of life neglected by the statistical theorists.\" For example, Fisher's method of maximum likelihood assigned a zero probability to nonevents. But since many businesses don't file insurance claims, Fisher's method produced premiums that were too low to cover future costs.</p><p>Bailey began writing a paper about his change in attitude about Bayes. By 1950 he was vice president of a large insurance company in Chicago. On May 22 he read his famous paper at a black-tie banquet for an actuarial society. The title: '<a href=\"http://www.casact.org/pubs/proceed/proceed50/50007.pdf\">Credibility Procedures: Laplace's Generalization of Bayes' Rule and the Combination of [Prior] Knowledge with Observed Data</a>.'</p><p>Bailey praised his colleagues for standing mostly alone against the statistics establishment. Then he announced that their beloved Credibility formula was actually Bayes Theorem, and in fact that the person who had published Bayes' work, Richard Price, would today be considered an actuary. He used Bayes' ball-and-table thought experiment to attack Fisher and his methods, and ended with a rousing call to put prior knowledge back into probability theory. His speech occupied theorists for years, and actuaries often see Bailey as taking their profession out of its dark ages.</p><p>That same year, I.J. Good published <i>Probability and the Weighing of Evidence</i>, which helped to found Bayes' Theorem into a logical, coherent methodology. Good was smart, quick, and by now perhaps the world's expert on codes. He introduced by holding out his hand and saying \"I am Good.\" When the British finally declassified his cryptanalysis work, allowing him to reveal Bayes' success during WWII, he bought a vanity licensed plate reading 007 IJG.</p><p>In the 1950s, <a href=\"http://en.wikipedia.org/wiki/Dennis_Lindley\">Dennis Lindley</a> and <a href=\"http://en.wikipedia.org/wiki/Leonard_Jimmie_Savage\">Jimmie Savage</a> worked to turn the statistician's hodgepodge of tools into a \"respectable branch of mathematics,\" as Kolmogorov had done for probability in in general in the 1930s. They found some success at putting statistics on a rigorous mathematical footing, and didn't realize at the time that they couldn't get from their theorems to the ad hoc methods of frequentism. Lindley said later, \"We were both fools because we failed completely to recognize the consequences of what we were doing.\"</p><p>In 1954, Savage published <i>Foundations of Statistics</i>, which built on Frank Ramsey's earlier attempts to use Bayes' Theorem not just for making inferences but for making decisions, too. His response to a classic objection to Bayesianism is worth remembering. He was asked, \"If prior opinions can differ from one researcher to the next, what happens to scientific objectivity in data analysis?\" Savage explained that as we gain data, subjectivists move into agreement, just as scientists come to consensus as evidence accumulates about, say, cigarettes causing lung cancer. When they have little data, scientists are subjectivists. When they have tons of data, they agree and become objectivists.</p><p>Savage became a Messianic advocate of Bayesianism, but died suddenly of a heart attack in 1971. I.J. Good was active but working at a small university and was poor at public speaking. David Lindley, however, moved to Britain and almost single-handedly created 10 Bayesian departments in the U.K. \u2014 professorship by professorship, battle by battle, he got Bayesians hired again and again. By 1977 he was exhausted and retired early.</p><p>&nbsp;</p><h1>Medicine</h1><p>In 1951, history major Jerome Cornfield used Bayes' Theorem to solve a puzzle about the chances of a person getting lung cancer. His paper helped epidemiologists to see how patients' histories could help measure the link between a disease and its possible cause. Moreover, he had begun to establish the link between smoking and lung cancer. Later efforts in England and the U.S. confirmed Cornfield's results.</p><p>Fisher and <a href=\"http://en.wikipedia.org/wiki/Neyman\">Neyman</a>, the world's two leading anti-Bayesians, didn't accept the research showing that cigarettes caused lung cancer. Fisher, especially, published many papers. He even developed the hypothesis that, somehow, lung cancer might cause smoking. But in 1959, Cornfield published <a href=\"http://ije.oxfordjournals.org/content/38/5/1175.full.pdf\">a paper</a> that systematically addressed every one of Fisher's arguments, and Fisher ended up looking ridiculous.</p><p>Cornfield went on to be involved in most of the major public health battles involving scientific data and statistics, and in 1974 was elected president of the American Statistical Association despite never having gotten any degree in statistics. He had developed a congenial spirit and&nbsp;infectious&nbsp;laugh, which came in handy when enduring long, bitter battles over health issues.</p><p>In 1979 he was diagnosed with pancreatic cancer, but his humor remained. A friend told him, \"I'm so glad to see you.\" Smiling, Cornfield replied, \"That's nothing compared to how happy <i>I</i>&nbsp;am to be able to see you.\" As he lay dying, he called to his two daughters and told them: \"You spend your whole life practicing humor for the times when you really need it.\"</p><p>&nbsp;</p><h1>Practical Use</h1><p>Frequentist methods worked for repetitive, standardized phenomena like crops, genetics, gambling, and insurance. But business executives needed to make decisions under conditions of uncertainty, without sample data. And frequentism didn't address that problem.</p><p>At Harvard Business School, <a href=\"http://en.wikipedia.org/wiki/Robert_Schlaifer\">Robert Schlaifer</a> thought about the problem. He realized that starting with prior information about demand for a product was better than nothing. From there, he realized that he could update his prior with new evidence, and independently arrived at Bayes' Theorem. Unaware of the literature, he reinvented Bayesian decision theory from scratch and began to teach it confidently. He did not think of it as 'an' approach. It was <i>the</i>&nbsp;approach, and everybody else was wrong, and he could <i>show</i>&nbsp;everybody else why they were wrong.</p><p>Later, he recruited <a href=\"http://en.wikipedia.org/wiki/Howard_Raiffa\">Howard Raiffa</a> to come work with him, because he needed another Bayesian to teach him more math. Together, the two invented the field of Decision-making Under Uncertainty (DUU). Schlaifer wrote the first practical textbook written entirely from a Bayesian perspective: <i>Probability and Statistics for Business Decisions</i> (1959). They introduced useful tools like decision trees, 'tree-flipping', and conjugate priors. They co-authored what would become the standard textbook of Bayesian statistics for two decades: <i>Applied Statistical Decision Theory</i>. Today, Bayesian methods dominate the business decision-making literature but frequentists still have some hold on statistics departments.</p><p>Meanwhile, <a href=\"http://en.wikipedia.org/wiki/Frederick_Mosteller\">Frederick Mosteller</a>&nbsp;spent a decade using early computers and hundreds of volunteers to painstakingly perform a Bayesian analysis of the disputed <i>Federalist Papers</i>, and concluded with high probability that they were all written by Madison, not Hamilton. The work impressed many statisticians, even frequentists.</p><p>Bayes had another chance at fame during the 1960 presidential race between Nixon and Kennedy. The race was too close to call, but the three major TV networks all wanted to be the first to make the correct call. NBC went looking for someone to help them predict the winner, and they found Princeton statistics professor <a href=\"http://en.wikipedia.org/wiki/John_Tukey\">John Tukey</a>. Tukey analyzed huge amounts of voting data, and by 2:30am during the election Tukey and his colleagues were ready to call Kennedy as the winner. The pressure was too much for NBC to make the call, though, so they locked Tukey and his team in a room until 8am when it was clear Kennedy was indeed the winner. NBC immediately asked him to come back for the 1962 election, and Tukey worked with NBC for 18 years.</p><p>But Tukey publicly denied Bayesianism. When working on the NBC projects, he said he wasn't using Bayes, instead he was \"borrowing strength.\" He didn't allow anybody on his team to talk about their methods, either, saying it was proprietary information.</p><p>In 1980 NBC soon switched to exit polling to predict elections. Exit polling was more visual, chatty, and fun than equations. It would be 28 years before someone used Bayes to predict presidential election results.&nbsp;When Nate Silver of <a href=\"http://fivethirtyeight.blogs.nytimes.com/\">FiveThirtyEight.com</a> used Bayes to predict results of the November 2008 race, he correctly predicted the winner in 49 states, an unmatched record among pollsters.</p><p>When the U.S. Atomic Energy Commission ordered a safety study of nuclear power plants, they hired Norman Rasmussen. At the time, there had never been a nuclear power plant accident. He couldn't use frequentist methods to estimate the probability of something that had never happened. So he looked to two sources: equipment failure rates, and expert opinion. But how could he combine those two types of evidence?</p><p>Bayes' Theorem, of course. But Rasmussen knew that Bayes was so out of favor that his results would be dismissed by the statistics community if he used the word 'Bayes'. So he used Raiffa's decision trees, instead. They were grounded in Bayes, but this way he didn't have to use the word 'Bayes.'</p><p>Alas, the report's subjectivist approach to statistics was roundly damned, and the U.S. Nuclear Regulatory Commission withdrew its support for the study five years later. And two months after they did so, the <a href=\"http://en.wikipedia.org/wiki/Three_Mile_Island_accident\">Three Mile Island accident</a>&nbsp;occurred.</p><p>Previous experts had said the odds of severe core damage were extremely low, but the effects would be catastrophic. Instead, the Rasmussen report had concluded that the probability of core damage was higher than anticipated, but the consequences wouldn't be catastrophic. The report also identified two important sources of the problem: human error and radioactivity outside the building. In the eyes of many, the report had been vindicated.</p><p>Finally, in 1983 the US Air Force sponsored a review of NASA's estimates of the probability of shuttle failure. NASA's estimate was 1 in 100,000. The contractor used Bayes and estimated the odds of rocket booster failure at 1 in 35. In 1986, <i>Challenger</i> exploded.</p><p>&nbsp;</p><h1>Victory</h1><p>Adrian Raftery examined a set of statistics about coal-dust explosions in 19th-century British mines. Frequentist techniques had shown the coal mining accident rates changed over time gradually. Our of curiosity, Raftery experimented with Bayes' Theorem, and discovered that accident rates had plummeted suddenly in the early 1890s. A historian suggested why: in 1889, the miners had formed a safety coalition.</p><p>Frequentist statistics worked okay when one hypothesis was a special case of another, but when hypotheses were competing and abrupt changes were in the data, frequentism didn't work. Many sociologists were ready to give up on p-values already, and Raftery's short <a href=\"http://www.stat.washington.edu/raftery/Research/PDF/asr1986.pdf\">1986 paper</a> on his success with Bayes led many sociologists to jump ship to Bayesianism. Raftery's paper is now one of the most cited in sociology.</p><p>One challenge had always been that Bayesian statistical operations were harder to calculate, and computers were still quite slow. This changed in the 90s, when computers became much faster and cheaper than before, and especially with the invention of the <a href=\"http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\">Markov Chain Monte Carlo</a> method, which suddenly allowed Bayesians to do a <i>lot</i>&nbsp;more than frequentists can. The <a href=\"http://www.mrc-bsu.cam.ac.uk/bugs/\">BUGS</a> program also helped.</p><p>These advances launched the 'Bayesian revolution' in a long list of fields: medical diagnosis, ecology, geology, computer science, artificial intelligence, machine learning, genetics, astrophysics, archaeology, psychometrics, education performance, sports modeling, and more. This is only partly because Bayes' Theorem shows us the <a href=\"http://yudkowsky.net/rational/bayes\">mathematically correct</a> response to new evidence. It is also because Bayes' Theorem <i>works</i>.</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/2ff8dfc17daad9b0db4c4b63124ac6c4659e284453134b26.gif\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/2ff8dfc17daad9b0db4c4b63124ac6c4659e284453134b26.gif/w_144 144w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/2ff8dfc17daad9b0db4c4b63124ac6c4659e284453134b26.gif/w_224 224w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/2ff8dfc17daad9b0db4c4b63124ac6c4659e284453134b26.gif/w_304 304w\"></figure><figure><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Thomas_Bayes.gif\"></figure>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bY5MaF2EATwDkomvu": 2, "LhX3F2SvGDarZCuh6": 2, "Ng8Gice9KNkncxqcj": 2, "Xw6pxiicjuv6NJWjf": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RTt59BtFLqQbsSiqd", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 64, "baseScore": 74, "extendedScore": null, "score": 0.000152, "legacy": true, "legacyId": "9328", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 74, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3af01042c819c214e42312da1872de6acb082373e826a003.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3af01042c819c214e42312da1872de6acb082373e826a003.png/w_100 100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3af01042c819c214e42312da1872de6acb082373e826a003.png/w_180 180w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3af01042c819c214e42312da1872de6acb082373e826a003.png/w_260 260w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3af01042c819c214e42312da1872de6acb082373e826a003.png/w_340 340w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3af01042c819c214e42312da1872de6acb082373e826a003.png/w_420 420w\"></figure><blockquote><p>Sometime during the 1740s, the Reverend Thomas Bayes made the ingenious discovery that bears his name but then mysteriously abandoned it. It was rediscovered independently by a different and far more renowned man, Pierre Simon Laplace, who gave it its modern mathematical form and scientific application \u2014 and then moved on to other methods. Although Bayes\u2019 rule drew the attention of the greatest statisticians of the twentieth century, some of them vilified both the method and its adherents, crushed it, and declared it dead. Yet at the same time, it solved practical questions that were unanswerable by any other means: the defenders of Captain Dreyfus used it to demonstrate his innocence; insurance actuaries used it to set rates; Alan Turing used it to decode the German Enigma cipher and arguably save the Allies from losing the Second World War; the U.S. Navy used it to search for a missing H-bomb and to locate Soviet subs; RAND Corporation used it to assess the likelihood of a nuclear accident; and Harvard and Chicago researchers used it to verify the authorship of the Federalist Papers. In discovering its value for science, many supporters underwent a near-religious conversion yet had to conceal their use of Bayes\u2019 rule and pretend they employed something else. It&nbsp;was not until the twenty-first century that the method lost its stigma and was widely and enthusiastically embraced.</p></blockquote><p><br>So begins <a href=\"http://www.mcgrayne.com/\">Sharon McGrayne</a>'s fun new book,&nbsp;<a href=\"http://www.amazon.com/Theory-That-Would-Not-Die/dp/0300169698/\"><i>The Theory That Would Not Die</i></a>, a popular history of <a href=\"http://yudkowsky.net/rational/bayes\">Bayes' Theorem</a>. Instead of reviewing the book, I'll summarize some of its content below. I skip the details and many great stories from the book, for example the (Bayesian) search for a lost submarine that inspired <i>Hunt for Red October</i>. Also see McGrayne's Google Talk <a href=\"http://www.youtube.com/watch?v=8oD6eBkjF9o\">here</a>. She will be speaking at the upcoming Singularity Summit, too, which you can register for <a href=\"https://www.singularitysummit.com/registration/\">here</a> (price goes up after August 31st).</p><h1 id=\"Origins\">Origins</h1><p>In the 1700s, when probability theory was just a whiff in the air, the English Reverend <a href=\"http://en.wikipedia.org/wiki/Thomas_bayes\">Thomas Bayes</a> wanted to know how to infer causes from effects. He set up his working problem like this: How could he learn the probability of a future event occurring if he only knew how many times it had occurred or not occurred in the past?</p><p>He needed a number, and it was hard to decide which number to choose. In the end, his solution was to just <i>guess</i>&nbsp;and then improve his guess later as he gathered more information.</p><p>He used a thought experiment to illustrate the process. Imagine that Bayes has his back turned to a table, and he asks his assistant to drop a ball on the table. The table is such that the ball has just as much chance of landing at any&nbsp;one place on the table as anywhere else. Now Bayes has to figure out where the ball is, without looking.</p><p>He asks his assistant to throw another ball on the table and report whether it is to the left or the right of the first ball. If the new ball landed to the left of the first ball, then the first ball is more likely to be on the right side of the table than the left side. He asks his assistant to throw the second ball again. If it again lands to the left of the first ball, then the first ball is even <i>more</i> likely than before to be on the right side of the table. And so on.</p><p>Throw after throw, Bayes is able to narrow down the area in which the first ball probably sits. Each new piece of information constrains the area where the first ball probably is.</p><p>Bayes' system was: Initial Belief + New Data -&gt; Improved Belief.</p><p>Or, as the terms came to be called: Prior + Likelihood of your new observation given competing hypotheses -&gt; Posterior.</p><p>In each new round of belief updating, the most recent posterior becomes the prior for the new calculation.</p><p>There were two enduring criticisms to Bayes' system. First, mathematicians were horrified to see something as whimsical as a <i>guess</i>&nbsp;play a role in rigorous mathematics. Second, Bayes said that if he didn't know what guess to make, he'd just assign all possibilities <i>equal</i>&nbsp;probability to start. For most mathematicians, this <i>problem of priors</i>&nbsp;was insurmountable.</p><p>Bayes never published his discovery, but his friend <a href=\"http://en.wikipedia.org/wiki/Richard_Price\">Richard Price</a> found it among his notes after Bayes' death in 1761, re-edited it, and published it. Unfortunately, virtually no one seems to have read the paper, and Bayes' method lay cold until the arrival of Laplace.</p><p>&nbsp;</p><h1 id=\"Laplace\">Laplace</h1><p>By the late 18th century, Europe was awash in scientific data. Astronomers had observations made by the Chinese in 1100 BC, by the Greeks in 200 BC, by the Romans in AD 100, and by the Arabs in AD 1000. The data were not of equal reliability. How could scientists process all their observations and choose the best? Many astronomers simply <i>averaged</i>&nbsp;their three 'best' observations, but this was ad-hoc. The world needed a better way to handle all these data.</p><p><a href=\"http://en.wikipedia.org/wiki/Pierre-Simon_Laplace\">Pierre-Simon Laplace</a>, a brilliant young mathematician, came to believe that probability theory held the key, and he independently rediscovered Bayes' mechanism and published&nbsp;it in 1774. Laplace stated the principle not with an equation, but in words:&nbsp;the probability of a cause (given an event) is proportional to the probability of the event&nbsp;(given its cause). And for the next 40 years, Laplace used, extended, clarified, and proved his new principle.</p><p>In 1781, Richard Price visited Paris, and word of Bayes' earlier discovery eventually reached Laplace. Laplace was now all the more confident that he was on the right track.</p><p>He needed to test his principle, so he turned to the largest data set available: birth records. A few people had noticed that slightly more boys than girls were born, and Laplace wanted to know if this was an anomalous or constant phenomenon. He began by applying equal probability to his hunches, and then updated his belief as he examined data sets from Paris, from London, from Naples, from St. Petersburg, and from rural areas in France. Later he even asked friends for birth data from Egypt and Central America. Finally, by 1812, he was almost certain that the birth of more boys than girls was \"a general law for the human race.\"</p><p>Laplace's friend Bouvard used his method to calculate the masses of Jupiter and Saturn from a wide variety of observations. Laplace was so impressed that he offered his readers a famous bet: 11,000 to 1 odds that Bouvard's results for Saturn were within 1% of the correct answer, and a million to one odds for Jupiter. Nobody seems to have taken Laplace's bet, but today's technology confirms that Laplace should have won both bets.</p><p>Laplace used his principle on the issue of testimony, both in court and in the Bible, and made famous progress in astronomy. When asked by Napoleon who authored the heavens, Laplace replied that natural law could explain the behavior of the heavens. Napoleon asked why Laplace had failed to mention God in his book on the subject. Laplace replied: \"Sire, I have no need of that hypothesis.\"</p><p>The answer became a symbol of the new science: the search for natural laws that produced phenomena without the need to call upon magic in the explanation.&nbsp;</p><p>And then, Laplace invented the <a href=\"http://en.wikipedia.org/wiki/Central_limit_theorem\">central limit theorem</a>, which let him handle almost any kind of data. He soon realized that where large amounts of data were available, both the Bayesian and the frequentist approaches (judging an event's probability by how frequently it occurs among many observations) to probability tended to produce the same results. (Only much later did scientists discover how wildly the two approaches can diverge even given lots of data.)</p><p>And so at age 62, Laplace \u2014 the world's first Bayesian \u2014 converted to&nbsp;frequentism, which he used for the remaining 16 years of his life.</p><p>...though he did finally realize what the general theorem for Bayes' method had to be:</p><blockquote><p>P(C|E) = [ P(E|C)P<sub>prior</sub>(C) ] / [\u03a3P(E|C')P<sub>prior</sub>(C')</p></blockquote><p>Which says that the probability of a hypothesis C <i>given</i>&nbsp;some evidence E equals our initial estimate of the probability <i>times</i>&nbsp;the probability of the evidence given the hypothesis C divided by the sum of the probabilities of the data in all possible hypotheses.</p><p>Basically, Laplace did all the hard work, and he deserves most of the honor for what we call Bayes' Theorem. But historical accidents happen, and the method is named after Bayes.</p><p>&nbsp;</p><h1 id=\"The_Decline_of_Bayes__Theorem\">The Decline of Bayes' Theorem</h1><p>Empowered by Laplace's central limit theorem, government officials were expected to collect statistics on all sorts of things: cholera victims, the chest sizes of soldiers, the number of Prussian officers killed by kicking horses, and so on. But the idea that probability quantifies our ignorance was gone, replaced by the idea that the new science could not allow for anything 'subjective'. John Stuart Mill denounced probability as \"ignorance... coined into science.\"</p><p>By 1891, the Scottish mathematician George Chrystal urged: \"[Laplace's principle] being dead, [it] should be decently buried out of sight, and not embalmed in text-books and examination papers... The indiscretions of great men should be quietly allowed to be forgotten.\"</p><p>And thus, Bayes' Theorem fell yet again in disuse... at least among theoreticians. A smattering of practitioners continued to find it useful.</p><p><a href=\"http://en.wikipedia.org/wiki/Joseph_Louis_Fran%C3%A7ois_Bertrand\">Joseph Bertrand</a>&nbsp;was convinced that Bayes' Theorem was the only way for artillery officers to correctly deal with a host of uncertainties about the enemies' location, air density, wind direction, and more. From 1890-1935, French and Russian artillery officers used Bertrand's Bayesian textbook to fire their weapons.</p><p>When the French Jew Alfred Dreyfus was falsely accused of having sold a letter to German military expert, France's famous mathematician Henri Poincar\u00e9 was called to the stand.&nbsp;Poincar\u00e9 was a frequentist, but when asked whether Dreyfus had written the letter,&nbsp;Poincar\u00e9 invoked Bayes' Theorem as the only sensible way for a court of law to update a hypothesis with new evidence, and proclaimed that the prosecution's discussion of probability was nonsense. Dreyfus was still convicted, though his sentence was reduced, but the public was outraged and the president issued a pardon two weeks later.</p><p>Statisticians used Bayes' Theorem to set up a functioning Bell phone system, set of up the United States' first working social insurance system, and solve other problems.</p><p>Meanwhile, the biologist&nbsp;<a href=\"http://en.wikipedia.org/wiki/Ronald_Fisher\">R.A. Fisher</a> was pioneering new randomization methods, sampling theory, tests of significant, analyses of variance, and a variety of experimental designs. In 1925 he published his revolutionary manual, <i>Statistical Methods of Research Workers</i>. The success of the book enshrined frequentism and the standard statistical method.</p><p>&nbsp;</p><h1 id=\"Jeffreys\">Jeffreys</h1><p>Even during its decline, a few people made progress on Bayesian theory. At about the same time, three men in three countries \u2014 <a href=\"http://en.wikipedia.org/wiki/%C3%89mile_Borel\">\u00c9mile Borel</a>, <a href=\"http://en.wikipedia.org/wiki/Frank_P._Ramsey\">Frank Ramsey</a>, and <a href=\"http://en.wikipedia.org/wiki/Bruno_de_Finetti\">Bruno de Finetti</a> \u2014&nbsp;independently happened upon the same idea: knowledge <i>is</i>&nbsp;subjective, and we can quantify it with a bet. The amount we wager shows how strongly we believe something.</p><p>And then, the geologist <a href=\"http://en.wikipedia.org/wiki/Harold_Jeffreys\">Harold Jeffreys</a> made Bayes' Theorem useful for scientists, proposing it as an alternative to Fisher's 'p-values' and 'significance tests', which depended on \"imaginary repetitions.\" In contrast, Bayesianism considered data as fixed evidence. Moreover, the p-value is a statement about data, but Jeffreys wanted to know about his hypothesis <i>given</i>&nbsp;the data. He published the monumental <i>Theory of Probability</i>&nbsp;in 1939, which remained for many years the only explanation of how to use Bayes to do science.</p><p>For decades, Fisher and Jeffreys were the world's two greatest statisticians, though both were practicing scientists instead of theoreticians. They traded blows over probability theory in scientific journals and in public. Fisher was louder and bolder, and frequentism was easier to use than Bayesianism.</p><p>&nbsp;</p><h1 id=\"Bayes_at_War\">Bayes at War</h1><p>In 1941, German U-Boats were devastating allied naval forces. Britain was cut off from its sources of food, and couldn't grow enough on its own soil to feed its citizens. Winston Churchill said the U-boat problem was the scariest part of the war for him.</p><p>The German codes, produced by Enigma machines with customizable wheel positions that allowed the codes to be changed rapidly, were considered unbreakable, so nobody was working on them. This attracted <a href=\"http://en.wikipedia.org/wiki/Alan_Turing\">Alan Turing</a> to the problem, because he liked solitude. He built a machine that could test different code possibilities, but it was slow. The machine might need four days to test all 336 wheel positions on a particular Enigma code. Until more machines could be built, Turing had to find a way for reducing the burden on the machine.&nbsp;</p><p>He used a Bayesian system to guess the letters in an Enigma message, and add more clues as they arrived with new data. With this method he could reduce the number of wheel settings to be tested by his machine from 336 to as few as 18. But soon, Turing realized that he couldn't compare the probabilities of his hunches without a standard unit of measurement. So, he invented the 'ban', defined as \"about the smallest change in weight of evidence that is directly perceptible to human intuition.\" This unit turned out to be very similar to the bit, the measure of information discovered using Bayes' Theorem while working for Bell Telephone.</p><p>Now that he had a unit of measurement, he could target the amount of evidence he needed for a particular hunch and then stop the process when he had that much evidence.</p><p>While Turing was cracking the Enigma codes in Britain, <a href=\"http://en.wikipedia.org/wiki/Andrey_Kolmogorov\">Andrey Kolmogorov</a> was fleeing the German artillery bombardment of Moscow. In 1933 he had showed that probability theory can be derived from basic mathematical axioms, and now Russia's generals were asking him about how best to fire back at the Germans. Though a frequentist, Kolmogorov recommended they used Bertrand's Bayesian firing system in a crisis like this.</p><p>Shortly after this, the British learned that the Germans were now using stronger, faster encryption machines: Lorenz machines. The British team used Turing's Bayesian scoring system and tried a variety of priors to crack the codes.&nbsp;</p><p>Turing visited America and spent time with <a href=\"http://en.wikipedia.org/wiki/Claude_Shannon\">Claude Shannon</a>, whose brilliant insights about information theory came a bit later. He realized that the purpose of information is to reduce uncertainty and the purpose of encryption is to increase it. He was using Bayes for both. Basically, if the posterior in a Bayesian equation is very different from the prior, then much has been learned, but if the posterior is roughly the same as the prior, then the information content is low. Shannon's unit for information was the 'bit'.</p><p>Meanwhile, Allied patrol planes needed to narrow their search for German U-boats. If 7 different listening posts intercepted the same message from the same U-boat, it could be located to somewhere in a circle 236 miles across. That's a lot of uncertainty, and mathematician <a href=\"http://en.wikipedia.org/wiki/Bernard_Koopman\">Bernard Koopman</a> was assigned to solve the problem. He wasn't bashful about Bayes at all. He said: \"Every operation involved in search is beset with uncertainties; it can be understood quantitatively only in terms of... probability. This may now be regarded as a truism, but it seems to have taken the developments in operational research of the Second World War to drive home its practical implications.\"</p><p>Koopman started by assigning 50% probability that a U-boat was inside the 236-mile circle, and then update his probability as more data came in, apportioning plane flyover hours according to the probabilities of U-boat locations.</p><p>And then, a few day's after Germany's surrender, Churchill ordered the destruction of all evidence that decoding has helped win the war, apparently because the British didn't want the Soviets to know they could decrypt Lorenz codes. It wasn't until 1973 that the story of Turing and Bayes began to emerge.</p><p>&nbsp;</p><h1 id=\"Revival\">Revival</h1><p>Its wartime successes classified, Bayes' Theorem remained mostly in the dark after the Second World War. Textbooks self-righteously dismissed Bayes. During the McCarthyism of the 1950s, one government statistician half-jokingly called a colleague \"un-American because [he] was a Bayesian, ...undermining the United States Government.\"</p><p>In 1950, an economist preparing a report asked statistician <a href=\"http://en.wikipedia.org/wiki/David_blackwell\">David Blackwell</a> (not yet a Bayesian) to estimate the probability of another world war in the next five years. Blackwell answered: \"Oh, that question just doesn't make sense. Probability applies to a long sequence of repeatable events, and this is clearly a unique situation. The probability is either 0 or 1, but we won't know for five years.\" The economist replied, \"I was afraid you were going to say that. I've spoken to several other statisticians, and they all told me the same thing.\"</p><p>Still, there were flickers of life. For decades after the war, one of Turing's American colleagues taught Bayes to NSA cryptographers. <a href=\"http://en.wikipedia.org/wiki/I.J._Good\">I.J. Good</a>, one of Turing's statistics assistant, developed Bayesian methods and theory, writing about 900 articles about Bayes.</p><p>And then there was the Bible-quoting business executive Arthur Bailey.</p><p>Bailey was trained in statistics, and when he joined an insurance company he was horrified to see them using Bayesian techniques developed in 1918. They asked not \"What should the new rates be?\" but instead \"How much should the present rates be changed?\" But after a year of trying different things, he realized that the Bayesian actuarial methods worked better than frequentist methods. Bailey \"realized that the hard-shelled underwriters were recognizing certain facts of life neglected by the statistical theorists.\" For example, Fisher's method of maximum likelihood assigned a zero probability to nonevents. But since many businesses don't file insurance claims, Fisher's method produced premiums that were too low to cover future costs.</p><p>Bailey began writing a paper about his change in attitude about Bayes. By 1950 he was vice president of a large insurance company in Chicago. On May 22 he read his famous paper at a black-tie banquet for an actuarial society. The title: '<a href=\"http://www.casact.org/pubs/proceed/proceed50/50007.pdf\">Credibility Procedures: Laplace's Generalization of Bayes' Rule and the Combination of [Prior] Knowledge with Observed Data</a>.'</p><p>Bailey praised his colleagues for standing mostly alone against the statistics establishment. Then he announced that their beloved Credibility formula was actually Bayes Theorem, and in fact that the person who had published Bayes' work, Richard Price, would today be considered an actuary. He used Bayes' ball-and-table thought experiment to attack Fisher and his methods, and ended with a rousing call to put prior knowledge back into probability theory. His speech occupied theorists for years, and actuaries often see Bailey as taking their profession out of its dark ages.</p><p>That same year, I.J. Good published <i>Probability and the Weighing of Evidence</i>, which helped to found Bayes' Theorem into a logical, coherent methodology. Good was smart, quick, and by now perhaps the world's expert on codes. He introduced by holding out his hand and saying \"I am Good.\" When the British finally declassified his cryptanalysis work, allowing him to reveal Bayes' success during WWII, he bought a vanity licensed plate reading 007 IJG.</p><p>In the 1950s, <a href=\"http://en.wikipedia.org/wiki/Dennis_Lindley\">Dennis Lindley</a> and <a href=\"http://en.wikipedia.org/wiki/Leonard_Jimmie_Savage\">Jimmie Savage</a> worked to turn the statistician's hodgepodge of tools into a \"respectable branch of mathematics,\" as Kolmogorov had done for probability in in general in the 1930s. They found some success at putting statistics on a rigorous mathematical footing, and didn't realize at the time that they couldn't get from their theorems to the ad hoc methods of frequentism. Lindley said later, \"We were both fools because we failed completely to recognize the consequences of what we were doing.\"</p><p>In 1954, Savage published <i>Foundations of Statistics</i>, which built on Frank Ramsey's earlier attempts to use Bayes' Theorem not just for making inferences but for making decisions, too. His response to a classic objection to Bayesianism is worth remembering. He was asked, \"If prior opinions can differ from one researcher to the next, what happens to scientific objectivity in data analysis?\" Savage explained that as we gain data, subjectivists move into agreement, just as scientists come to consensus as evidence accumulates about, say, cigarettes causing lung cancer. When they have little data, scientists are subjectivists. When they have tons of data, they agree and become objectivists.</p><p>Savage became a Messianic advocate of Bayesianism, but died suddenly of a heart attack in 1971. I.J. Good was active but working at a small university and was poor at public speaking. David Lindley, however, moved to Britain and almost single-handedly created 10 Bayesian departments in the U.K. \u2014 professorship by professorship, battle by battle, he got Bayesians hired again and again. By 1977 he was exhausted and retired early.</p><p>&nbsp;</p><h1 id=\"Medicine\">Medicine</h1><p>In 1951, history major Jerome Cornfield used Bayes' Theorem to solve a puzzle about the chances of a person getting lung cancer. His paper helped epidemiologists to see how patients' histories could help measure the link between a disease and its possible cause. Moreover, he had begun to establish the link between smoking and lung cancer. Later efforts in England and the U.S. confirmed Cornfield's results.</p><p>Fisher and <a href=\"http://en.wikipedia.org/wiki/Neyman\">Neyman</a>, the world's two leading anti-Bayesians, didn't accept the research showing that cigarettes caused lung cancer. Fisher, especially, published many papers. He even developed the hypothesis that, somehow, lung cancer might cause smoking. But in 1959, Cornfield published <a href=\"http://ije.oxfordjournals.org/content/38/5/1175.full.pdf\">a paper</a> that systematically addressed every one of Fisher's arguments, and Fisher ended up looking ridiculous.</p><p>Cornfield went on to be involved in most of the major public health battles involving scientific data and statistics, and in 1974 was elected president of the American Statistical Association despite never having gotten any degree in statistics. He had developed a congenial spirit and&nbsp;infectious&nbsp;laugh, which came in handy when enduring long, bitter battles over health issues.</p><p>In 1979 he was diagnosed with pancreatic cancer, but his humor remained. A friend told him, \"I'm so glad to see you.\" Smiling, Cornfield replied, \"That's nothing compared to how happy <i>I</i>&nbsp;am to be able to see you.\" As he lay dying, he called to his two daughters and told them: \"You spend your whole life practicing humor for the times when you really need it.\"</p><p>&nbsp;</p><h1 id=\"Practical_Use\">Practical Use</h1><p>Frequentist methods worked for repetitive, standardized phenomena like crops, genetics, gambling, and insurance. But business executives needed to make decisions under conditions of uncertainty, without sample data. And frequentism didn't address that problem.</p><p>At Harvard Business School, <a href=\"http://en.wikipedia.org/wiki/Robert_Schlaifer\">Robert Schlaifer</a> thought about the problem. He realized that starting with prior information about demand for a product was better than nothing. From there, he realized that he could update his prior with new evidence, and independently arrived at Bayes' Theorem. Unaware of the literature, he reinvented Bayesian decision theory from scratch and began to teach it confidently. He did not think of it as 'an' approach. It was <i>the</i>&nbsp;approach, and everybody else was wrong, and he could <i>show</i>&nbsp;everybody else why they were wrong.</p><p>Later, he recruited <a href=\"http://en.wikipedia.org/wiki/Howard_Raiffa\">Howard Raiffa</a> to come work with him, because he needed another Bayesian to teach him more math. Together, the two invented the field of Decision-making Under Uncertainty (DUU). Schlaifer wrote the first practical textbook written entirely from a Bayesian perspective: <i>Probability and Statistics for Business Decisions</i> (1959). They introduced useful tools like decision trees, 'tree-flipping', and conjugate priors. They co-authored what would become the standard textbook of Bayesian statistics for two decades: <i>Applied Statistical Decision Theory</i>. Today, Bayesian methods dominate the business decision-making literature but frequentists still have some hold on statistics departments.</p><p>Meanwhile, <a href=\"http://en.wikipedia.org/wiki/Frederick_Mosteller\">Frederick Mosteller</a>&nbsp;spent a decade using early computers and hundreds of volunteers to painstakingly perform a Bayesian analysis of the disputed <i>Federalist Papers</i>, and concluded with high probability that they were all written by Madison, not Hamilton. The work impressed many statisticians, even frequentists.</p><p>Bayes had another chance at fame during the 1960 presidential race between Nixon and Kennedy. The race was too close to call, but the three major TV networks all wanted to be the first to make the correct call. NBC went looking for someone to help them predict the winner, and they found Princeton statistics professor <a href=\"http://en.wikipedia.org/wiki/John_Tukey\">John Tukey</a>. Tukey analyzed huge amounts of voting data, and by 2:30am during the election Tukey and his colleagues were ready to call Kennedy as the winner. The pressure was too much for NBC to make the call, though, so they locked Tukey and his team in a room until 8am when it was clear Kennedy was indeed the winner. NBC immediately asked him to come back for the 1962 election, and Tukey worked with NBC for 18 years.</p><p>But Tukey publicly denied Bayesianism. When working on the NBC projects, he said he wasn't using Bayes, instead he was \"borrowing strength.\" He didn't allow anybody on his team to talk about their methods, either, saying it was proprietary information.</p><p>In 1980 NBC soon switched to exit polling to predict elections. Exit polling was more visual, chatty, and fun than equations. It would be 28 years before someone used Bayes to predict presidential election results.&nbsp;When Nate Silver of <a href=\"http://fivethirtyeight.blogs.nytimes.com/\">FiveThirtyEight.com</a> used Bayes to predict results of the November 2008 race, he correctly predicted the winner in 49 states, an unmatched record among pollsters.</p><p>When the U.S. Atomic Energy Commission ordered a safety study of nuclear power plants, they hired Norman Rasmussen. At the time, there had never been a nuclear power plant accident. He couldn't use frequentist methods to estimate the probability of something that had never happened. So he looked to two sources: equipment failure rates, and expert opinion. But how could he combine those two types of evidence?</p><p>Bayes' Theorem, of course. But Rasmussen knew that Bayes was so out of favor that his results would be dismissed by the statistics community if he used the word 'Bayes'. So he used Raiffa's decision trees, instead. They were grounded in Bayes, but this way he didn't have to use the word 'Bayes.'</p><p>Alas, the report's subjectivist approach to statistics was roundly damned, and the U.S. Nuclear Regulatory Commission withdrew its support for the study five years later. And two months after they did so, the <a href=\"http://en.wikipedia.org/wiki/Three_Mile_Island_accident\">Three Mile Island accident</a>&nbsp;occurred.</p><p>Previous experts had said the odds of severe core damage were extremely low, but the effects would be catastrophic. Instead, the Rasmussen report had concluded that the probability of core damage was higher than anticipated, but the consequences wouldn't be catastrophic. The report also identified two important sources of the problem: human error and radioactivity outside the building. In the eyes of many, the report had been vindicated.</p><p>Finally, in 1983 the US Air Force sponsored a review of NASA's estimates of the probability of shuttle failure. NASA's estimate was 1 in 100,000. The contractor used Bayes and estimated the odds of rocket booster failure at 1 in 35. In 1986, <i>Challenger</i> exploded.</p><p>&nbsp;</p><h1 id=\"Victory\">Victory</h1><p>Adrian Raftery examined a set of statistics about coal-dust explosions in 19th-century British mines. Frequentist techniques had shown the coal mining accident rates changed over time gradually. Our of curiosity, Raftery experimented with Bayes' Theorem, and discovered that accident rates had plummeted suddenly in the early 1890s. A historian suggested why: in 1889, the miners had formed a safety coalition.</p><p>Frequentist statistics worked okay when one hypothesis was a special case of another, but when hypotheses were competing and abrupt changes were in the data, frequentism didn't work. Many sociologists were ready to give up on p-values already, and Raftery's short <a href=\"http://www.stat.washington.edu/raftery/Research/PDF/asr1986.pdf\">1986 paper</a> on his success with Bayes led many sociologists to jump ship to Bayesianism. Raftery's paper is now one of the most cited in sociology.</p><p>One challenge had always been that Bayesian statistical operations were harder to calculate, and computers were still quite slow. This changed in the 90s, when computers became much faster and cheaper than before, and especially with the invention of the <a href=\"http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\">Markov Chain Monte Carlo</a> method, which suddenly allowed Bayesians to do a <i>lot</i>&nbsp;more than frequentists can. The <a href=\"http://www.mrc-bsu.cam.ac.uk/bugs/\">BUGS</a> program also helped.</p><p>These advances launched the 'Bayesian revolution' in a long list of fields: medical diagnosis, ecology, geology, computer science, artificial intelligence, machine learning, genetics, astrophysics, archaeology, psychometrics, education performance, sports modeling, and more. This is only partly because Bayes' Theorem shows us the <a href=\"http://yudkowsky.net/rational/bayes\">mathematically correct</a> response to new evidence. It is also because Bayes' Theorem <i>works</i>.</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/2ff8dfc17daad9b0db4c4b63124ac6c4659e284453134b26.gif\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/2ff8dfc17daad9b0db4c4b63124ac6c4659e284453134b26.gif/w_144 144w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/2ff8dfc17daad9b0db4c4b63124ac6c4659e284453134b26.gif/w_224 224w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/2ff8dfc17daad9b0db4c4b63124ac6c4659e284453134b26.gif/w_304 304w\"></figure><figure><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Thomas_Bayes.gif\"></figure>", "sections": [{"title": "Origins", "anchor": "Origins", "level": 1}, {"title": "Laplace", "anchor": "Laplace", "level": 1}, {"title": "The Decline of Bayes' Theorem", "anchor": "The_Decline_of_Bayes__Theorem", "level": 1}, {"title": "Jeffreys", "anchor": "Jeffreys", "level": 1}, {"title": "Bayes at War", "anchor": "Bayes_at_War", "level": 1}, {"title": "Revival", "anchor": "Revival", "level": 1}, {"title": "Medicine", "anchor": "Medicine", "level": 1}, {"title": "Practical Use", "anchor": "Practical_Use", "level": 1}, {"title": "Victory", "anchor": "Victory", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "90 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 90, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-29T16:31:02.939Z", "modifiedAt": null, "url": null, "title": "Call for Personal Volunteers", "slug": "call-for-personal-volunteers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:57.973Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wYuNQyZ2kX4xB9kpG/call-for-personal-volunteers", "pageUrlRelative": "/posts/wYuNQyZ2kX4xB9kpG/call-for-personal-volunteers", "linkUrl": "https://www.lesswrong.com/posts/wYuNQyZ2kX4xB9kpG/call-for-personal-volunteers", "postedAtFormatted": "Monday, August 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Call%20for%20Personal%20Volunteers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACall%20for%20Personal%20Volunteers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwYuNQyZ2kX4xB9kpG%2Fcall-for-personal-volunteers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Call%20for%20Personal%20Volunteers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwYuNQyZ2kX4xB9kpG%2Fcall-for-personal-volunteers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwYuNQyZ2kX4xB9kpG%2Fcall-for-personal-volunteers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<p>Those who wish to volunteer some of their time toward reducing existential risk and increasing our chances of a positive singularity can follow the directions on <a href=\"http://www.singularityvolunteers.org/\">SingularityVolunteers.org</a>.</p>\n<p>And as a <a href=\"/lw/78s/help_fund_lukeprog_at_siai/\">freshly hired</a> Singularity Institute researcher, I also have my own list of tasks that, if completed by volunteers instead of myself, will speed along the delivery of the projects I'm working on: an 'FAI Open Problems' document, two papers bound for peer review, <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">metaethics research</a>, and more.</p>\n<p>So if you'd like to help me out with any of my volunteer-doable&nbsp;tasks, please contact me: luke [at] singinst [dot] org.</p>\n<p>Thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wYuNQyZ2kX4xB9kpG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 27, "extendedScore": null, "score": 7.618949979022031e-07, "legacy": true, "legacyId": "9491", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WqDGJNtxNMT8fHe37", "hN2aRnu798yas5b2k"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-29T19:06:27.639Z", "modifiedAt": null, "url": null, "title": "An introduction to Bayesianism [links]", "slug": "an-introduction-to-bayesianism-links", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s4A2ztSfsLozXfDE3/an-introduction-to-bayesianism-links", "pageUrlRelative": "/posts/s4A2ztSfsLozXfDE3/an-introduction-to-bayesianism-links", "linkUrl": "https://www.lesswrong.com/posts/s4A2ztSfsLozXfDE3/an-introduction-to-bayesianism-links", "postedAtFormatted": "Monday, August 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20introduction%20to%20Bayesianism%20%5Blinks%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20introduction%20to%20Bayesianism%20%5Blinks%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs4A2ztSfsLozXfDE3%2Fan-introduction-to-bayesianism-links%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20introduction%20to%20Bayesianism%20%5Blinks%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs4A2ztSfsLozXfDE3%2Fan-introduction-to-bayesianism-links", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs4A2ztSfsLozXfDE3%2Fan-introduction-to-bayesianism-links", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<p>Two recent papers in&nbsp;<em><a href=\"http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1747-9991\">Philosophy Compass</a></em>&nbsp;summarize the arguments for and against Bayesianism:</p>\n<p>Easwaran, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Easwaran-Bayesianism-I-Introduction-and-arguments-in-favor.pdf\">Bayesianism I: Introduction and Arguments in Favor</a></p>\n<p>Easwaran, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Easwaran-Bayesianism-II-Applications-and-criticisms.pdf\">Bayesianism II: Applications and Criticisms</a></p>\n<p><em>Philosophy Compass</em>&nbsp;is a journal of review articles, my <a href=\"/lw/5me/scholarship_how_to_do_it_efficiently/\">favorite reading material</a>.</p>\n<p><a href=\"http://www.kennyeaswaran.org/\">Kenny Easwaran</a> got his PhD from UC Berkeley under <a href=\"http://www.kennyeaswaran.org/\">formal epistemologist</a> Brandon Fitelson, did a post-doc at ANU, and is now an assistant professor at USC.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s4A2ztSfsLozXfDE3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 7.619454595783493e-07, "legacy": true, "legacyId": "9492", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["37sHjeisS9uJufi4u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-29T19:31:40.721Z", "modifiedAt": null, "url": null, "title": "'Preferences in AI: An Overview' [link]", "slug": "preferences-in-ai-an-overview-link", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dXZZKnL6pqGzPjb4M/preferences-in-ai-an-overview-link", "pageUrlRelative": "/posts/dXZZKnL6pqGzPjb4M/preferences-in-ai-an-overview-link", "linkUrl": "https://www.lesswrong.com/posts/dXZZKnL6pqGzPjb4M/preferences-in-ai-an-overview-link", "postedAtFormatted": "Monday, August 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20'Preferences%20in%20AI%3A%20An%20Overview'%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A'Preferences%20in%20AI%3A%20An%20Overview'%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdXZZKnL6pqGzPjb4M%2Fpreferences-in-ai-an-overview-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text='Preferences%20in%20AI%3A%20An%20Overview'%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdXZZKnL6pqGzPjb4M%2Fpreferences-in-ai-an-overview-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdXZZKnL6pqGzPjb4M%2Fpreferences-in-ai-an-overview-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>Those interested in AI preferences may appreciate this recent review:</p>\n<p>Domshlak et al., <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Domshlak-et-al-Preferences-in-AI-an-overview.pdf\">Preferences in AI: An overview</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dXZZKnL6pqGzPjb4M", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 7.619536483370481e-07, "legacy": true, "legacyId": "9493", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-29T19:33:41.569Z", "modifiedAt": null, "url": null, "title": "'Self-improving AI, an analysis' [link]", "slug": "self-improving-ai-an-analysis-link", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5B3JyS6DtKrHrqF98/self-improving-ai-an-analysis-link", "pageUrlRelative": "/posts/5B3JyS6DtKrHrqF98/self-improving-ai-an-analysis-link", "linkUrl": "https://www.lesswrong.com/posts/5B3JyS6DtKrHrqF98/self-improving-ai-an-analysis-link", "postedAtFormatted": "Monday, August 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20'Self-improving%20AI%2C%20an%20analysis'%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A'Self-improving%20AI%2C%20an%20analysis'%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5B3JyS6DtKrHrqF98%2Fself-improving-ai-an-analysis-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text='Self-improving%20AI%2C%20an%20analysis'%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5B3JyS6DtKrHrqF98%2Fself-improving-ai-an-analysis-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5B3JyS6DtKrHrqF98%2Fself-improving-ai-an-analysis-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>For those who haven't read it yet, here is Josh Hall's '<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Hall-Self-improving-AI-an-analysis.pdf\">Self-Improving AI: An Analysis</a>' (2007).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5B3JyS6DtKrHrqF98", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 7.619543023657421e-07, "legacy": true, "legacyId": "9494", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-30T00:28:27.767Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Planning Fallacy", "slug": "seq-rerun-planning-fallacy", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v4yKqq7vRmgc8gQPM/seq-rerun-planning-fallacy", "pageUrlRelative": "/posts/v4yKqq7vRmgc8gQPM/seq-rerun-planning-fallacy", "linkUrl": "https://www.lesswrong.com/posts/v4yKqq7vRmgc8gQPM/seq-rerun-planning-fallacy", "postedAtFormatted": "Tuesday, August 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Planning%20Fallacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Planning%20Fallacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv4yKqq7vRmgc8gQPM%2Fseq-rerun-planning-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Planning%20Fallacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv4yKqq7vRmgc8gQPM%2Fseq-rerun-planning-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv4yKqq7vRmgc8gQPM%2Fseq-rerun-planning-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 187, "htmlBody": "<p>Today's post, <a href=\"/lw/jg/planning_fallacy/\">Planning Fallacy</a> was originally published on 17 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>We tend to plan envisioning that everything will go as expected. Even assuming that such an estimate is accurate conditional on everything going as expected, things will not go as expected. As a result, we routinely see outcomes worse then the ex ante worst case scenario.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/7bc/seq_rerun_why_im_blooking/\">Why I'm Blooking</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v4yKqq7vRmgc8gQPM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 7.620500314560543e-07, "legacy": true, "legacyId": "9495", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CPm5LTwHrvBJCa9h5", "RzRSB62HN88b9Edma", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-30T01:12:08.851Z", "modifiedAt": null, "url": null, "title": "Michael Jordan dissolves Bayesian vs Frequentist inference debate [video lecture]", "slug": "michael-jordan-dissolves-bayesian-vs-frequentist-inference", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:22.019Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wnpvtzrdpqsFmKxKu/michael-jordan-dissolves-bayesian-vs-frequentist-inference", "pageUrlRelative": "/posts/wnpvtzrdpqsFmKxKu/michael-jordan-dissolves-bayesian-vs-frequentist-inference", "linkUrl": "https://www.lesswrong.com/posts/wnpvtzrdpqsFmKxKu/michael-jordan-dissolves-bayesian-vs-frequentist-inference", "postedAtFormatted": "Tuesday, August 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Michael%20Jordan%20dissolves%20Bayesian%20vs%20Frequentist%20inference%20debate%20%5Bvideo%20lecture%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMichael%20Jordan%20dissolves%20Bayesian%20vs%20Frequentist%20inference%20debate%20%5Bvideo%20lecture%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwnpvtzrdpqsFmKxKu%2Fmichael-jordan-dissolves-bayesian-vs-frequentist-inference%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Michael%20Jordan%20dissolves%20Bayesian%20vs%20Frequentist%20inference%20debate%20%5Bvideo%20lecture%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwnpvtzrdpqsFmKxKu%2Fmichael-jordan-dissolves-bayesian-vs-frequentist-inference", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwnpvtzrdpqsFmKxKu%2Fmichael-jordan-dissolves-bayesian-vs-frequentist-inference", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<p>UC Berkeley professor <a href=\"http://en.wikipedia.org/wiki/Michael_I._Jordan\">Michael Jordan</a>, a leading researcher in machine learning, has a great <a href=\"/lw/of/dissolving_the_question/\">reduction of the question</a> \"Are your inferences Bayesian or Frequentist?\". The reduction is basically \"Which term are you varying in the loss function?\". He calls this the \"decision theoretic perspective\" on the debate, and uses this terminology well in keeping with LessWrong interests.</p>\n<p>I don't have time to write a top-level post about this (maybe someone else does?), but I quite liked the lecture, and thought I should at least post the link!</p>\n<p><a href=\"http://videolectures.net/mlss09uk_jordan_bfway/\">http://videolectures.net/mlss09uk_jordan_bfway/</a></p>\n<p>The discussion gets much clearer starting at the 10:11 slide, which you can click on and skip to if you like, but I watched the first 10 minutes anyway to get a sense of his general attitude.</p>\n<p>Enjoy! I recommend watching while you eat, if it saves you time and the food's not too distracting :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wnpvtzrdpqsFmKxKu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 7.620642202413451e-07, "legacy": true, "legacyId": "9496", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Mc6QcrsbH5NRXbCRX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-30T02:38:33.482Z", "modifiedAt": null, "url": null, "title": "Do you expect a singularity within your life time?", "slug": "do-you-expect-a-singularity-within-your-life-time", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:26.340Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gBJwAjydby5rte79K", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WNF8BNbAovhphvkkG/do-you-expect-a-singularity-within-your-life-time", "pageUrlRelative": "/posts/WNF8BNbAovhphvkkG/do-you-expect-a-singularity-within-your-life-time", "linkUrl": "https://www.lesswrong.com/posts/WNF8BNbAovhphvkkG/do-you-expect-a-singularity-within-your-life-time", "postedAtFormatted": "Tuesday, August 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Do%20you%20expect%20a%20singularity%20within%20your%20life%20time%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADo%20you%20expect%20a%20singularity%20within%20your%20life%20time%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWNF8BNbAovhphvkkG%2Fdo-you-expect-a-singularity-within-your-life-time%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Do%20you%20expect%20a%20singularity%20within%20your%20life%20time%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWNF8BNbAovhphvkkG%2Fdo-you-expect-a-singularity-within-your-life-time", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWNF8BNbAovhphvkkG%2Fdo-you-expect-a-singularity-within-your-life-time", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<p>By singularity I mean a recursive self-improving intelligence created by man that will help us solve the worlds problems.</p>\n<p><em>please explain the downvotes.. sorry I didn't write an essay or link to lots of in-jokes about the sky being green. It's just a simple question so I didn't want to embellish it.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WNF8BNbAovhphvkkG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 8, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "9501", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-30T03:07:57.176Z", "modifiedAt": null, "url": null, "title": "Meetup : Shanghai Less Wrong Meetup", "slug": "meetup-shanghai-less-wrong-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:22.871Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Patrick", "createdAt": "2009-02-27T08:09:44.663Z", "isAdmin": false, "displayName": "Patrick"}, "userId": "KC7mjSorWj2XsdL3v", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hs5vJPJuq4bXeWczJ/meetup-shanghai-less-wrong-meetup", "pageUrlRelative": "/posts/hs5vJPJuq4bXeWczJ/meetup-shanghai-less-wrong-meetup", "linkUrl": "https://www.lesswrong.com/posts/hs5vJPJuq4bXeWczJ/meetup-shanghai-less-wrong-meetup", "postedAtFormatted": "Tuesday, August 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Shanghai%20Less%20Wrong%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Shanghai%20Less%20Wrong%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhs5vJPJuq4bXeWczJ%2Fmeetup-shanghai-less-wrong-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Shanghai%20Less%20Wrong%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhs5vJPJuq4bXeWczJ%2Fmeetup-shanghai-less-wrong-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhs5vJPJuq4bXeWczJ%2Fmeetup-shanghai-less-wrong-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2t'>Shanghai Less Wrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 September 2011 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Starbucks, Carrefour Mall, the Zhongshan Park Store, near the Zhongshan Park Station, Line 2, 3, or 4, Shanghai, China</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Where: Starbucks, Carrefour Mall, the Zhongshan Park Store, near the Zhongshan Park Station, Line 2, 3, or 4, Shanghai, China\nB1&amp;B2, No.1018, ChangNing Road, ChangNing District, Shanghai ( \u957f\u5b81\u8def1018\u53f7,   \u9f99\u4e4b\u68a6\u8d2d\u7269\u4e2d\u5fc3B1-2\u697c   \u8fd1\u51ef\u65cb\u8def )\nWhen: 2011, September 12th, Monday, 7pm-10pm\nContact John Teddy: 18721399070 (local phone no., just text me if you can't find us)</p>\n\n<p>I'm looking for any willing Shanghai Lesswrong readers to join this group if they haven't already. \nUp vote user Teddy in the comments below. This way I have karma to post to the meetup section without asking Patrick Robotham each time. This will help us boost members for the community here in Shanghai.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2t'>Shanghai Less Wrong Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hs5vJPJuq4bXeWczJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 7.621018359874297e-07, "legacy": true, "legacyId": "9504", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Shanghai_Less_Wrong_Meetup\">Discussion article for the meetup : <a href=\"/meetups/2t\">Shanghai Less Wrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 September 2011 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Starbucks, Carrefour Mall, the Zhongshan Park Store, near the Zhongshan Park Station, Line 2, 3, or 4, Shanghai, China</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Where: Starbucks, Carrefour Mall, the Zhongshan Park Store, near the Zhongshan Park Station, Line 2, 3, or 4, Shanghai, China\nB1&amp;B2, No.1018, ChangNing Road, ChangNing District, Shanghai ( \u957f\u5b81\u8def1018\u53f7,   \u9f99\u4e4b\u68a6\u8d2d\u7269\u4e2d\u5fc3B1-2\u697c   \u8fd1\u51ef\u65cb\u8def )\nWhen: 2011, September 12th, Monday, 7pm-10pm\nContact John Teddy: 18721399070 (local phone no., just text me if you can't find us)</p>\n\n<p>I'm looking for any willing Shanghai Lesswrong readers to join this group if they haven't already. \nUp vote user Teddy in the comments below. This way I have karma to post to the meetup section without asking Patrick Robotham each time. This will help us boost members for the community here in Shanghai.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Shanghai_Less_Wrong_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/2t\">Shanghai Less Wrong Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Shanghai Less Wrong Meetup", "anchor": "Discussion_article_for_the_meetup___Shanghai_Less_Wrong_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Shanghai Less Wrong Meetup", "anchor": "Discussion_article_for_the_meetup___Shanghai_Less_Wrong_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-30T03:12:05.683Z", "modifiedAt": null, "url": null, "title": "Making it even easier to find new comments in a thread", "slug": "making-it-even-easier-to-find-new-comments-in-a-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:19.704Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ieuNLE2JY6k7y53ox/making-it-even-easier-to-find-new-comments-in-a-thread", "pageUrlRelative": "/posts/ieuNLE2JY6k7y53ox/making-it-even-easier-to-find-new-comments-in-a-thread", "linkUrl": "https://www.lesswrong.com/posts/ieuNLE2JY6k7y53ox/making-it-even-easier-to-find-new-comments-in-a-thread", "postedAtFormatted": "Tuesday, August 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Making%20it%20even%20easier%20to%20find%20new%20comments%20in%20a%20thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMaking%20it%20even%20easier%20to%20find%20new%20comments%20in%20a%20thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FieuNLE2JY6k7y53ox%2Fmaking-it-even-easier-to-find-new-comments-in-a-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Making%20it%20even%20easier%20to%20find%20new%20comments%20in%20a%20thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FieuNLE2JY6k7y53ox%2Fmaking-it-even-easier-to-find-new-comments-in-a-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FieuNLE2JY6k7y53ox%2Fmaking-it-even-easier-to-find-new-comments-in-a-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<p>It occurs to me, scrolling down the <a href=\"/lw/79x/polyhacking/\">Polyhacking</a> discussion for the nth time, that for long discussions, it would be handy to have a text indicator for new comments as well as the bright green edge. That way, I could use search to find new comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ieuNLE2JY6k7y53ox", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 19, "extendedScore": null, "score": 7.621031813592706e-07, "legacy": true, "legacyId": "9505", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kLR5H4pbaBjzZxLv6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-30T03:31:22.033Z", "modifiedAt": null, "url": null, "title": "Philosophical apologetics book suggests replacing Bayes theorem with \"Inference to the Best Explanation\" (IBE)", "slug": "philosophical-apologetics-book-suggests-replacing-bayes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:21.480Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jwhendy", "createdAt": "2011-01-04T19:53:21.160Z", "isAdmin": false, "displayName": "jwhendy"}, "userId": "ZaJctSZkCvg7qvSEC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LrQDqHeNgzxuhx3GQ/philosophical-apologetics-book-suggests-replacing-bayes", "pageUrlRelative": "/posts/LrQDqHeNgzxuhx3GQ/philosophical-apologetics-book-suggests-replacing-bayes", "linkUrl": "https://www.lesswrong.com/posts/LrQDqHeNgzxuhx3GQ/philosophical-apologetics-book-suggests-replacing-bayes", "postedAtFormatted": "Tuesday, August 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Philosophical%20apologetics%20book%20suggests%20replacing%20Bayes%20theorem%20with%20%22Inference%20to%20the%20Best%20Explanation%22%20(IBE)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhilosophical%20apologetics%20book%20suggests%20replacing%20Bayes%20theorem%20with%20%22Inference%20to%20the%20Best%20Explanation%22%20(IBE)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrQDqHeNgzxuhx3GQ%2Fphilosophical-apologetics-book-suggests-replacing-bayes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Philosophical%20apologetics%20book%20suggests%20replacing%20Bayes%20theorem%20with%20%22Inference%20to%20the%20Best%20Explanation%22%20(IBE)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrQDqHeNgzxuhx3GQ%2Fphilosophical-apologetics-book-suggests-replacing-bayes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrQDqHeNgzxuhx3GQ%2Fphilosophical-apologetics-book-suggests-replacing-bayes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1033, "htmlBody": "<p>I'm about 2/3 through an apologetics book that was recommended to me, Menssen and Sullivan's, <a href=\"http://www.amazon.com/Agnostic-Inquirer-Revelation-Philosophical-Standpoint/dp/0802803946\">The Agnostic Inquirer</a>, and was quite surprised to run into a discussion of Bayes theorem and wanted some input from the LW community. The book is quite philosophical and I admit that I am probably not following all of it. I find heady philosophy to be one of these areas where something doesn't seem quite right (as in the conclusion that someone pushes), but I can't always identify what.</p>\n<p>In any case, the primary point of the book is to attempt to replace the traditional apologetics method with a new one. The status quo has been to appeal to \"natural theology,\" non-theological areas of discussion which attempt to bring one to the conclusion that some kind of theistic being exists, and from there establish that Christianity is the true formulation of what, exactly, this theistic being is/wants/does, etc by examining revealed theistic truths (aka the Bible). Menssen and Sullivan attempt to suggest that revelation need not be put off so long.</p>\n<p>I don't want to get too into it, but think this helps set the stage. Their argument is as follows:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0.5in; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; border-top-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-left-width: 0px; border-style: initial; border-color: initial; outline-width: 0px; outline-style: initial; outline-color: initial; font-size: 13px; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; font-family: 'Lucida Sans', 'Lucida Grande', 'Lucida Sans Unicode', 'Helvetica Neue', Helvetica, Arial, Verdana, sans-serif; \">(1) If it is not highly unlikely that a world-creator exists, then investigation of the contents of revelatory claims might well show that it is probable that a good God exists and has revealed.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0.5in; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; border-top-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-left-width: 0px; border-style: initial; border-color: initial; outline-width: 0px; outline-style: initial; outline-color: initial; font-size: 13px; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; font-family: 'Lucida Sans', 'Lucida Grande', 'Lucida Sans Unicode', 'Helvetica Neue', Helvetica, Arial, Verdana, sans-serif; \">(2) It is not highly unlikely that a world-creator exists.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0.5in; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; border-top-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-left-width: 0px; border-style: initial; border-color: initial; outline-width: 0px; outline-style: initial; outline-color: initial; font-size: 13px; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; font-family: 'Lucida Sans', 'Lucida Grande', 'Lucida Sans Unicode', 'Helvetica Neue', Helvetica, Arial, Verdana, sans-serif; \">(3) So, investigation of the content of a revelatory claim might well show it is probable that a good God exists and has revealed.</p>\n<p style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 1em; margin-left: 0.5in; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; border-top-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-left-width: 0px; border-style: initial; border-color: initial; outline-width: 0px; outline-style: initial; outline-color: initial; font-size: 13px; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; font-family: 'Lucida Sans', 'Lucida Grande', 'Lucida Sans Unicode', 'Helvetica Neue', Helvetica, Arial, Verdana, sans-serif; \">(4) So, a negative conclusion concerning the existence of a good God is not justified unless the content of a reasonable number of leading revelatory claims has been seriously considered. (p. 63)</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Issues Menssen and Sullivan have with Bayes applicability to this arena:</strong></p>\n<p>Then they begin trying to choose the best method for evaluating revelatory content. This is where Bayes comes in. The pages are almost all available via Google books <a href=\"http://books.google.com/books?id=Efdb6EJ9DGoC&amp;lpg=PR13&amp;ots=U439EcqK8N&amp;dq=agnostic%20inquirer%20rebuttal&amp;pg=PA173#v=onepage&amp;q=agnostic%20inquirer%20rebuttal&amp;f=false\">HERE</a>&nbsp;in Section 4.2.1, beginning on page 173. They suggest the following limitations:</p>\n<p>\n<ul>\n<li>Bayesian probability works well when the specific values are known (they use the example of predicting the color of a ball to be drawn out of a container). In theology, the values are not known.</li>\n<li>The philosophical community is divided about whether Bayesian probability is reliable, and thus everyone should be hesitant about it too if experts are hesitant.</li>\n<li>If one wants to evaluate the probability that this world exists and there are infinitely many possibilities, n, then no matter how small a probability one assigns to each one, the sum will be infinite. (My personal take on this is whether a literal infinity can exist in nature... 1/n * n is 1, but maybe I'm not understanding their exact gripe.)</li>\n<li>In some cases, they hold that prior probability is a useless term, as it would be \"inscrutable.\" For example, they use Elliott Sober's example of gravity. What is it's prior probability? If such a question is meaningless, they hold that \"Has a good god revealed?\" may be in the same category and thus Bayesian probability breaks down when one attempts to apply it.</li>\n<li>There are so many components to certain questions that it would be nearly impossible or impossible to actually name them all and assign probabilities so that the computation accounted for all the bits of information required.</li>\n<li>If Bayes' theorem produces an answer that conflicts with answers arrived at via other means, one might simply tweak his/her Bayes values until the answer aligned with what was desired.</li>\n</ul>\n<div><br /></div>\n<div><strong>Their suggested alternative, Inference to the Best Explanation (IBE)</strong></div>\n<div>They define IBE as follows:</div>\n<div><ol>\n<li>If a hypothesis sufficiently approximates an ideal explanation of an adequate range of data, then the hypothesis is probably or approximately true.</li>\n<li>h<sub>1 </sub>sufficiently approximates an ideal explanation of d, an adequate range of data.</li>\n<li>So h<sub>1 </sub>is probably or approximately true.</li>\n</ol>\n<div>Obviously the key lies in their definition of \"ideal explanation.\" They cover this in great detail, but it's not all that specific. Basically, they want the explanation to be deductive (vs. inductive), grounded in \"fundamental substances and properties\" (basic, established truths), and to be overwhelmingly more true than contending hypotheses. You can read a bit more by reading the Google books section following the above.</div>\n</div>\n<div>I'm interested in takes on the above limitations that I've summarized in comparison with their suggested replacement, IBE. I'd be especially interested in hearing from those more familiar with philosophy. Menssen &amp; Sullivan cover others' definitions of IBE prior to presenting theirs, which suggests that it's a common term in philosophy. My gut intuition is that Bayes<em>&nbsp;</em>should produce the same answer as the IBE, but should also be more reliable since it's not an inference by a defined method.</div>\n<div>It seems like their proposal for IBE is doing precisely what Bayes' theorem is doing... but they've just formalized a way to do it more sloppily since they claim one can't know the exact numbers to use in Bayes' theorem. I can't tell what, exactly, is different. You take your priors, factor in your \"adequate range of data\" (new evidence), and figure out of the hypothesis, given the prior probability and adjusted for additional evidence, comes out more probabile than not (making it probably or approximately true).</div>\n<div>Is this just funny games? Is there something to the proposed limitations of Bayes' theorem they present? I think Richard Carrier explained how he bypasses situations when exact numbers are not known (and how often <em>are</em>&nbsp;the numbers known <em>precisely?</em>) in his argument entitled <a href=\"http://www.infidels.org/library/modern/richard_carrier/resurrection/2.html\">Why I Don't Buy the Resurrection Story</a>. He specifies that even if you don't know the <em>exact</em>&nbsp;number, you can at least say that something wouldn't be <em>less likely</em>&nbsp;than X or <em>more likely</em>&nbsp;than Y. In this way you can use the limits of probability in your formula to still compute a useful answer, even if it's not as precise as you would like.</div>\n<div>If anyone is interested in this in more detail, I could perhaps scan the relevant 20 or so pages on this and make them available somewhere. There's only a few pages missing from Google books.</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LrQDqHeNgzxuhx3GQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 7.621094416979261e-07, "legacy": true, "legacyId": "9507", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-30T03:36:40.264Z", "modifiedAt": null, "url": null, "title": "Defrag conference scholarships", "slug": "defrag-conference-scholarships", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cXsEjYRP2W3Gwzuqt/defrag-conference-scholarships", "pageUrlRelative": "/posts/cXsEjYRP2W3Gwzuqt/defrag-conference-scholarships", "linkUrl": "https://www.lesswrong.com/posts/cXsEjYRP2W3Gwzuqt/defrag-conference-scholarships", "postedAtFormatted": "Tuesday, August 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Defrag%20conference%20scholarships&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADefrag%20conference%20scholarships%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcXsEjYRP2W3Gwzuqt%2Fdefrag-conference-scholarships%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Defrag%20conference%20scholarships%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcXsEjYRP2W3Gwzuqt%2Fdefrag-conference-scholarships", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcXsEjYRP2W3Gwzuqt%2Fdefrag-conference-scholarships", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 37, "htmlBody": "<p>http://www.defragcon.com/2011/general/defrag-announcements/</p>\n<p>Eric  Nolin of the Defrag conference is looking to organize a scholarship  fund for high school girls who want to study computer science in  university.</p>\n<p>Till that's in place, they're funding scholarships for people to attend the conference.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cXsEjYRP2W3Gwzuqt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 3, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "9509", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-30T03:56:54.547Z", "modifiedAt": null, "url": null, "title": ".", "slug": "", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:20.814Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "a7xJQpZ55R6SxFTik", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yiHrTJRkzNL8WDAtw/", "pageUrlRelative": "/posts/yiHrTJRkzNL8WDAtw/", "linkUrl": "https://www.lesswrong.com/posts/yiHrTJRkzNL8WDAtw/", "postedAtFormatted": "Tuesday, August 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyiHrTJRkzNL8WDAtw%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyiHrTJRkzNL8WDAtw%2F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyiHrTJRkzNL8WDAtw%2F", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NDjABjx6DRfSKcqED": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yiHrTJRkzNL8WDAtw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 40, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "9512", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-30T08:23:22.578Z", "modifiedAt": null, "url": null, "title": "That cat: not dead and alive", "slug": "that-cat-not-dead-and-alive", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:20.063Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DavidPlumpton", "createdAt": "2011-07-08T09:36:12.175Z", "isAdmin": false, "displayName": "DavidPlumpton"}, "userId": "JyR3HoGsDEckYxEGE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BuCA436XYDqNLqeZT/that-cat-not-dead-and-alive", "pageUrlRelative": "/posts/BuCA436XYDqNLqeZT/that-cat-not-dead-and-alive", "linkUrl": "https://www.lesswrong.com/posts/BuCA436XYDqNLqeZT/that-cat-not-dead-and-alive", "postedAtFormatted": "Tuesday, August 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20That%20cat%3A%20not%20dead%20and%20alive&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThat%20cat%3A%20not%20dead%20and%20alive%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBuCA436XYDqNLqeZT%2Fthat-cat-not-dead-and-alive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=That%20cat%3A%20not%20dead%20and%20alive%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBuCA436XYDqNLqeZT%2Fthat-cat-not-dead-and-alive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBuCA436XYDqNLqeZT%2Fthat-cat-not-dead-and-alive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 820, "htmlBody": "<p>I've read through the Quantum Physics sequence and feel that I managed to understand most of it. But now it seems to me that the Double Slit and Schrodinger's cat experiments are not described quite correctly. So I'd like to try to re-state them and see if anybody can correct any misunderstandings I likely have.</p>\n<p>With the Double Slit experiment we usually hear it said the particle travels through both slits and then we see interference bands. The more precise explanation is that there is an complex valued amplitude flow corresponding to the particle moving through the left slit and another for the right slit. But if we could manage to magically \"freeze time\" then we would find ourselves in one position in configuration space where the particle is unambiguously in one position (let's say the left slit). Now any observer will have no way of knowing this at the time, and if they did detect the particle's position in any way it would change the configuration and there would be no interference banding.</p>\n<p>But the particle really is going through the left slit right now (as far as we are concerned), simply because that is what it means to be at some point in configuration space. The particle is going through the right slit for other versions of ourselves nearby in configuration space.</p>\n<p>The amplitude flow then continues to the point in configuration space where it arrives at the back screen, and it is joined by the amplitude flow via the right slit to the same region of configuration space, causing an interference pattern. So this present moment in time now has more than one past, now we can genuinely say that it did go through both. Both pasts are equally valid. The branching tree of amplitude flow has turned into a graph.</p>\n<p>So far so good I hope (or perhaps I'm about to find out I'm completely wrong). Now for the cat.</p>\n<p>I read recently that experimenters have managed to keep two clouds of caesium atoms in a coherent state for a hour. So what would this look like if we could scale it up to a cat?</p>\n<p>The problem with this experiment is that a cat is a very complex system and the two particular types of states we are interested in (i.e. dead or alive) are very far apart in configuration space. It may help to imagine that we could rearrange configuration space a little to put all the points labelled \"alive\" on the left and all the dead points on the right of some line. If we want to make the gross simplification that we can treat the cat as a very simple system then this means that \"alive\" points are very close to the \"dead\" points in configuration space. In particular it means that there are significant amplitude flows between the two sets of points, that is significant flows across the line in both directions. Of course such flows happen all the time, but the key point is here the direction of the complex flow vectors would be aligned so as to cause a significant change in the magnitude of the final values in configuration space instead of tending to cancel out.</p>\n<p>This means that as time proceeds the cat can move from alive to dead to alive to dead again, in the sense that in any point of configuration space that we find ourselves will contain an amplitude contribution both from alive states and from dead states. In other words two different pasts are contributing to the present.</p>\n<p>So sometime after the experiment starts we magically stop the clock on the wall of the universe. Since we are at a particular point the cat is either alive or dead, let's say dead. So the cat is not alive and dead at the same time because we find ourselves at a single point in configuration space. There are also other points in the configuration space containing another instance of ourselves along with an alive cat. But since we have not entangled anything else in the universe with the cat/box system as time ticks along the cat would be buzzing around from dead to alive and back to dead again. When we open the box things entangle and we diverge far apart in configuration space, and now the cat remains completely dead or alive, at least for the point in configuration space we find ourselves in.</p>\n<p>How to sum up? Cats and photons are never dead or alive or going left or right at the same moment from the point of view of one observer somewhere in configuration space, but the present has an amplitude contribution from multiple pasts.</p>\n<p>If you're still reading this then thanks for hanging in there. I know there's some more detail about observations only being from a set of eigenvalues and so forth, but can I get some comments about whether I'm on the right track or way off base?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BuCA436XYDqNLqeZT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 3, "extendedScore": null, "score": 7.622043081530429e-07, "legacy": true, "legacyId": "9519", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-30T12:07:45.878Z", "modifiedAt": null, "url": null, "title": "Can You Build a Better Paper Clip?", "slug": "can-you-build-a-better-paper-clip", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:24.408Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pgDshSZsXo6JfFJGA/can-you-build-a-better-paper-clip", "pageUrlRelative": "/posts/pgDshSZsXo6JfFJGA/can-you-build-a-better-paper-clip", "linkUrl": "https://www.lesswrong.com/posts/pgDshSZsXo6JfFJGA/can-you-build-a-better-paper-clip", "postedAtFormatted": "Tuesday, August 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Can%20You%20Build%20a%20Better%20Paper%20Clip%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACan%20You%20Build%20a%20Better%20Paper%20Clip%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpgDshSZsXo6JfFJGA%2Fcan-you-build-a-better-paper-clip%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Can%20You%20Build%20a%20Better%20Paper%20Clip%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpgDshSZsXo6JfFJGA%2Fcan-you-build-a-better-paper-clip", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpgDshSZsXo6JfFJGA%2Fcan-you-build-a-better-paper-clip", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>Nice <a href=\"http://professional.wsj.com/article/SB10001424053111903327904576524671643378078.html?mod=ITP_AHED&amp;mg=reno-wsj\">article about paperclip industry</a>, I'm sure it will be of considerable interest to many LessWrong readers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pgDshSZsXo6JfFJGA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": -1, "extendedScore": null, "score": 7.622772205550798e-07, "legacy": true, "legacyId": "9521", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-30T12:27:19.528Z", "modifiedAt": null, "url": null, "title": "[LINK] \"Academic publishers make Murdoch look like a socialist\" says George Monbiot ", "slug": "link-academic-publishers-make-murdoch-look-like-a-socialist", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:23.848Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tSthucwX9oFAmzyaP/link-academic-publishers-make-murdoch-look-like-a-socialist", "pageUrlRelative": "/posts/tSthucwX9oFAmzyaP/link-academic-publishers-make-murdoch-look-like-a-socialist", "linkUrl": "https://www.lesswrong.com/posts/tSthucwX9oFAmzyaP/link-academic-publishers-make-murdoch-look-like-a-socialist", "postedAtFormatted": "Tuesday, August 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20%22Academic%20publishers%20make%20Murdoch%20look%20like%20a%20socialist%22%20says%20George%20Monbiot%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20%22Academic%20publishers%20make%20Murdoch%20look%20like%20a%20socialist%22%20says%20George%20Monbiot%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtSthucwX9oFAmzyaP%2Flink-academic-publishers-make-murdoch-look-like-a-socialist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20%22Academic%20publishers%20make%20Murdoch%20look%20like%20a%20socialist%22%20says%20George%20Monbiot%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtSthucwX9oFAmzyaP%2Flink-academic-publishers-make-murdoch-look-like-a-socialist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtSthucwX9oFAmzyaP%2Flink-academic-publishers-make-murdoch-look-like-a-socialist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 638, "htmlBody": "<p><a href=\"http://www.guardian.co.uk/commentisfree/2011/aug/29/academic-publishers-murdoch-socialist\">link</a></p>\n<blockquote>\n<p>Who are the most ruthless capitalists in the western world? Whose monopolistic practices make Walmart look like a corner shop and Rupert Murdoch a socialist? You won't guess the answer in a month of Sundays. While there are plenty of candidates, my vote goes not to the banks, the oil companies or the health insurers, but &ndash; wait for it &ndash; to academic publishers. Theirs might sound like a fusty and insignificant sector. It is anything but. Of all corporate scams, the racket they run is most urgently in need of referral to the competition authorities.</p>\n<p>Everyone claims to agree that people should be encouraged to understand science and other academic research. Without current knowledge, we cannot make coherent democratic decisions. But the publishers have slapped a padlock and a \"keep out\" sign on the gates.</p>\n<p>You might resent Murdoch's paywall policy, in which he charges &pound;1 for 24 hours of access to the Times and Sunday Times. But at least in that period you can read and download as many articles as you like. Reading a single article published by one of Elsevier's journals will cost you $31.50. Springer charges &euro;34.95, Wiley-Blackwell, $42. Read 10 and you pay 10 times. And the journals retain perpetual copyright. You want to read a letter printed in 1981? That'll be $31.50.</p>\n<p>Of course, you could go into the library (if it still exists). But they too have been hit by cosmic fees. The average cost of an annual subscription to a chemistry journal is $3,792. Some journals cost $10,000 a year or more to stock. The most expensive I've seen, <a href=\"http://www.elsevier.com/wps/find/journaldescription.cws_home/506062/bibliographic\">Elsevier's Biochimica et Biophysica Acta, is $20,930</a>. Though academic libraries have been frantically cutting subscriptions to make ends meet, <a href=\"http://www.economist.com/node/18744177\">journals now consume 65% of their budgets</a>, which means they have had to reduce the number of books they buy. Journal fees account for a significant component of universities' costs, which are being passed to their students.</p>\n<p>Murdoch pays his journalists and editors, and his companies generate much of the content they use. But the academic publishers get their articles, their peer reviewing (vetting by other researchers) and even much of their editing for free. The material they publish was commissioned and funded not by them but by us, through government research grants and academic stipends. But to see it, we must pay again, and through the nose.</p>\n<p>The returns are astronomical: in the past financial year, for example, Elsevier's operating profit margin was 36% (&pound;724m on revenues of &pound;2bn). They result from a stranglehold on the market. Elsevier, Springer and Wiley, who have bought up many of their competitors, <a href=\"http://southernlibrarianship.icaap.org/content/v09n03/mcguigan_g01.html\">now publish 42% of&nbsp;journal articles.</a></p>\n<p>...</p>\n</blockquote>\n<p><a href=\"http://blogs.discovermagazine.com/gnxp/2011/08/theyre-called-peer-reviewers/\">Razib Khan</a> found this paragraph rather striking (who is reminded of <a href=\"http://www.southparkstudios.com/clips/387407/stu-dent-ath-o-leets) scene in South Park\">this</a> episode of South Park) and I would tend to agree that its a rather convincing argument.</p>\n<blockquote>\n<p>Murdoch pays his journalists and editors, and his companies generate much of the content they use. But the academic publishers get their articles, <strong>their peer reviewing (vetting by other researchers) and even much of their editing for free. </strong>The material they publish was commissioned and funded not by them but by us, through government research grants and academic stipends. But to see it, we must pay again, and through the nose</p>\n</blockquote>\n<p>Are publishers really so successful as rent seekers or is there something the original article is missing here?<em> Also what useful strategies would LWers recommend to help minimize costs for someone trying to practice the virtue of scholarship?</em> The obvious suggestions (implied in the article) seem to be emailing authors (and perhaps those suscribed) asking for the papers and acquiring and paying for membership in some libraries.</p>\n<p>Another obvious option is using ... <a href=\"http://www.theatlanticwire.com/technology/2011/07/stolen-academic-papers-appear-pirate-bay-protest-arrest/40259/\">liberated</a> <a href=\"http://www.theatlanticwire.com/technology/2011/07/reddit-co-founder-charged-data-theft/40157/\">databases</a> of such academic papers.</p>\n<p>&nbsp;</p>\n<p><strong>Edit:</strong> Just wondering, has this been discussed before on Lesswrong?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PDJ6KqJBRzvKPfuS3": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tSthucwX9oFAmzyaP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 28, "extendedScore": null, "score": 7.622835771991156e-07, "legacy": true, "legacyId": "9522", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-30T13:59:16.876Z", "modifiedAt": null, "url": null, "title": "Ask LW: PredictionBook.com and logical uncertainty", "slug": "ask-lw-predictionbook-com-and-logical-uncertainty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:19.898Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hkf24h2aWgmzSoAqJ/ask-lw-predictionbook-com-and-logical-uncertainty", "pageUrlRelative": "/posts/hkf24h2aWgmzSoAqJ/ask-lw-predictionbook-com-and-logical-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/hkf24h2aWgmzSoAqJ/ask-lw-predictionbook-com-and-logical-uncertainty", "postedAtFormatted": "Tuesday, August 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ask%20LW%3A%20PredictionBook.com%20and%20logical%20uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAsk%20LW%3A%20PredictionBook.com%20and%20logical%20uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhkf24h2aWgmzSoAqJ%2Fask-lw-predictionbook-com-and-logical-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ask%20LW%3A%20PredictionBook.com%20and%20logical%20uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhkf24h2aWgmzSoAqJ%2Fask-lw-predictionbook-com-and-logical-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhkf24h2aWgmzSoAqJ%2Fask-lw-predictionbook-com-and-logical-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<p>I like to make predictions about things like quantum mechanics and decision theory and cosmology and fun stuff like that. Can PredictionBook be used for this? If so, what should the guidelines be? Figured I'd make this a Discussion post so as to encourage the habit of betting on logical uncertainty.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hkf24h2aWgmzSoAqJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 5, "extendedScore": null, "score": 7.623134611789762e-07, "legacy": true, "legacyId": "9523", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-30T15:58:03.442Z", "modifiedAt": null, "url": null, "title": "Frequentist vs Bayesian breakdown: interpretation vs inference", "slug": "frequentist-vs-bayesian-breakdown-interpretation-vs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:21.901Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mQfNymou9q5riEKrf/frequentist-vs-bayesian-breakdown-interpretation-vs", "pageUrlRelative": "/posts/mQfNymou9q5riEKrf/frequentist-vs-bayesian-breakdown-interpretation-vs", "linkUrl": "https://www.lesswrong.com/posts/mQfNymou9q5riEKrf/frequentist-vs-bayesian-breakdown-interpretation-vs", "postedAtFormatted": "Tuesday, August 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Frequentist%20vs%20Bayesian%20breakdown%3A%20interpretation%20vs%20inference&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFrequentist%20vs%20Bayesian%20breakdown%3A%20interpretation%20vs%20inference%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmQfNymou9q5riEKrf%2Ffrequentist-vs-bayesian-breakdown-interpretation-vs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Frequentist%20vs%20Bayesian%20breakdown%3A%20interpretation%20vs%20inference%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmQfNymou9q5riEKrf%2Ffrequentist-vs-bayesian-breakdown-interpretation-vs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmQfNymou9q5riEKrf%2Ffrequentist-vs-bayesian-breakdown-interpretation-vs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 710, "htmlBody": "<p>Suppose we have two different human beings, Connor and Diane, who agree to <strong>interpret</strong> their subjective anticipations as probabilities, thereby commonly earning them the title \"Bayesian\". &nbsp;On a particular project or venture, they might disagree on Trick A or Trick B to decide the next step in the project. &nbsp;It might be that Trick A is commonly labelled a&nbsp;\"Frequentist&nbsp;<strong>inference</strong> method\" and B is a \"Bayesian&nbsp;<strong>inference</strong>&nbsp;method\". &nbsp;Why might they disagree?</p>\n<p>As far as I can see, there are 3 disagreements that get labelled \"Bayesian vs Frequentist\" debates, and conflating them is a problem:</p>\n<p>(1) Whether to <strong>interpret</strong> all <em>subjective anticipations as probabilities</em>.</p>\n<p>(2) Whether to <strong>interpret</strong> all <em>probabilities as subjective anticipations</em>.</p>\n<p>(3) Whether, on a particular project, to <em>use Statistical Trick B instead of Statistical Trick A</em>&nbsp;to <strong>infer</strong> the best course of action, when B is commonly labelled a \"Bayesian method\" and A is a \"Frequentist method\".</p>\n<p>(Regarding 3, UC Berkeley professor Michael Jordan offers a good heuristic for how statistical tricks get labelled as Bayesisn or Frequentist, in terms of which terms in a loss function one treats as fixed or variable. &nbsp;I recommend <a href=\"/lw/2xe/interesting_talk_on_bayesians_and_frequentists/\">watching the first twenty minutes of his video lecture</a> on this if you're not familiar.)</p>\n<p>The question \"is Connor a Bayesian or a Frequentist?\" is commonly posed as though Connor's position on 1, 2, and 3 must be either \"yes, yes, yes\" or \"no, no, no\". &nbsp;I don't believe this is so often the case. For example, my position is:</p>\n<p style=\"padding-left: 30px;\">(1) - Yes. &nbsp;Insofar as we have subjective anticipations, I agree normatively that they should behave and update as probabilities.</p>\n<p style=\"padding-left: 30px;\">(2) - Don't care much. Expressions like P(X|Y) and P(X and Y) are useful for denoting both <em>subjective anticipations</em> and proportions of a whole, and in particular, <em>proportions of real future events</em>. &nbsp;Whether to use the word \"probability\" is a terminological question. &nbsp;Personally I try to reserve the word \"probability\" for when they mean subjective anticipations, and say \"proportion\" when they mean proportions of real future, but this is word choice. &nbsp;Unfortunately this word choice is strongly associated and confused with positions on (1) and (3).</p>\n<p style=\"padding-left: 30px;\">(3) - It depends. &nbsp;In statistical inference, we commonly consider data sets <strong>x</strong>, world models <strong>M</strong>, and parameters&nbsp;<strong>&theta;</strong>&nbsp;that specify the model <strong>M</strong> more precisely. &nbsp;I consider the separation of belief into&nbsp;<strong>M</strong>&nbsp;and&nbsp;<strong>&theta;</strong>&nbsp;to be purely formal. When guessing the next data set <strong>y</strong>, one considers expressions of the form <strong>P</strong>(<strong>x|M,</strong><strong>&theta;</strong>) in some way. &nbsp;If I'm already very confident in a specific world model <strong>M</strong>, and expect&nbsp;<strong>&theta;</strong>&nbsp;to actually&nbsp;vary from situation to situation, I'll probably try to estimate the parameters&nbsp;<strong>&theta;</strong>&nbsp;from <strong>x</strong>&nbsp;in a way that has the best expected success rate across all possible data sets <strong>M</strong> would generate. You might say here that I \"trust the model more than the data\" (though what I really don't trust are the changing model parameters), and this is a trick commonly referred to as \"Frequentist\". &nbsp;If I'm not confident in the model <strong>M</strong>, or expect the parameters&nbsp;<strong>&theta;</strong>&nbsp;to the be the same in many future situations, I'll probably try to estimate <strong>M,</strong><strong>&theta;</strong>&nbsp;from <strong>x</strong>&nbsp;in a way that has the best expected success rate<strong>&nbsp;assuming x.</strong>&nbsp; You might say here that I \"trust the data more than the model\", and label this a \"Bayesian\" trick. &nbsp;</p>\n<p>Throughout (3), since my position in (1) is not changing, a member of the Bayes Tribe will say I'm \"really a Bayesian all along\", but I don't want to continue with this conflation of position names. &nbsp;It's true that if I use the \"Frequentist trick\", it will be because I've updated in favor of it, i.e. my subjective confidence levels in the various theory elements are appropriate for it.&nbsp;</p>\n<p>... But from now on, when term \"Bayesian\" or \"Frequentist\" arises in a debate, <em>my plan is to taboo the terms immediately</em>, and proceed to either dissolve the issue into (1), (2), and (3) above, or change the conversation if people don't have the energy or interest for that length of conversation.</p>\n<p>Do people agree with this breakdown? &nbsp;I think I could be persuaded otherwise and would of course appreciate it if I were :)</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>ETA: I think the wisdom to&nbsp;<a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">treat beliefs as anticipation controllers</a>&nbsp;and&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Belief_update\">update our confidences based on evidence</a>&nbsp;might be too precious to alienate people from it with the label \"Bayesian\", especially if the label is as ambiguous as my breakdown has found it to be.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mQfNymou9q5riEKrf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 36, "extendedScore": null, "score": 7.623520643570076e-07, "legacy": true, "legacyId": "9524", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BW9A6hiwByJH3uEcx", "a7n8GdKiAZRX86T5A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-30T16:52:56.122Z", "modifiedAt": null, "url": null, "title": "My intentions for my metaethics sequence", "slug": "my-intentions-for-my-metaethics-sequence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:22.376Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bxPK3BF5ECsi7rnaA/my-intentions-for-my-metaethics-sequence", "pageUrlRelative": "/posts/bxPK3BF5ECsi7rnaA/my-intentions-for-my-metaethics-sequence", "linkUrl": "https://www.lesswrong.com/posts/bxPK3BF5ECsi7rnaA/my-intentions-for-my-metaethics-sequence", "postedAtFormatted": "Tuesday, August 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20intentions%20for%20my%20metaethics%20sequence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20intentions%20for%20my%20metaethics%20sequence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbxPK3BF5ECsi7rnaA%2Fmy-intentions-for-my-metaethics-sequence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20intentions%20for%20my%20metaethics%20sequence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbxPK3BF5ECsi7rnaA%2Fmy-intentions-for-my-metaethics-sequence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbxPK3BF5ECsi7rnaA%2Fmy-intentions-for-my-metaethics-sequence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 486, "htmlBody": "<p>Recently a friend of mine told me that he and a few others were debating how likely it is that I've 'solved metaethics.' Others on this site have gotten the impression that I'm claiming to have made a fundamental breakthrough that I'm currently keeping a secret, and that's what my metaethics sequence is leading up to. Alas, it isn't the case. The <a href=\"/lw/54p/heading_toward_nononsense_metaethics/\">first post</a> in my sequence began:</p>\n<blockquote>\n<p>A few months ago, I predicted that we could solve metaethics in 15 years. To most people, that was outrageously optimistic. But I've updated since then. I think much of metaethics can be solved now (depending on where you draw the boundary around the term 'metaethics'.) My upcoming sequence 'No-Nonsense Metaethics' will solve the part that can be solved, and make headway on the parts of metaethics that aren't yet solved. Solving the easier problems of metaethics will give us a clear and stable platform from which to solve the hard questions of morality.</p>\n</blockquote>\n<p>The part I consider 'solved' is the part discussed in <a href=\"/lw/5kn/conceptual_analysis_and_moral_theory/\">Conceptual Analysis and Moral Theory</a> and <a href=\"/lw/5u2/pluralistic_moral_reductionism/\">Pluralistic Moral Reductionism</a>. These posts represent an application of the lessons learned from Eliezer's <a href=\"http://wiki.lesswrong.com/wiki/Free_will_(solution)\">free will sequence</a>&nbsp;and his <a href=\"http://wiki.lesswrong.com/wiki/A_Human%27s_Guide_to_Words\">words sequence</a> to the subject of metaethics.</p>\n<p>I did this because Eliezer mostly skipped this step in his metaethics sequence, perhaps assuming that readers had already applied these lessons to metaethics to solve the easy problems of metaethics, so he could skip right to discussing the harder problems of metaethics. But I think this move was a source of confusion for many LWers, so I wanted to go back and work through the details of what it looks like to solve the easy parts of metaethics with lessons learned from Eliezer's sequences.</p>\n<p>The <em>next</em>&nbsp;part of my metaethics sequence will be devoted to \"bringing us all up to speed\" on several lines of research that seem relevant to solving open problems in metaethics: the literature on how human values work (in brain and behavior), the literature on extracting preferences from what human brains <em>actually</em>&nbsp;do, and the literature on value extrapolation algorithms. For the most part, these literature sets haven't been discussed on Less Wrong despite their apparent relevance to metaethics, so I'm trying to share them with LW myself (e.g. <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">A Crash Course in the Neuroscience of Human Motivation</a>).</p>\n<p>Technically, most of these posts will not be listed as being part of my metaethics sequence, but I will refer to them from posts that are technically part of my metaethics sequence, drawing lessons for metaethics from them.</p>\n<p>After \"bringing us all up to speed\" on these topics and perhaps a couple others, I'll use my metaethics sequence to clarify the open problems in metaethics and suggest some places we can hack away at and perhaps make progress. Thus, my metaethics sequence aims to end with something like a <a href=\"http://en.wikipedia.org/wiki/Timothy_Gowers#Polymath_Project\">Polymath Project</a> set up for collaboratively solving metaethics problems.</p>\n<p>I hope this clarifies my intentions for my metaethics sequence.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bxPK3BF5ECsi7rnaA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 22, "extendedScore": null, "score": 7.623699013091074e-07, "legacy": true, "legacyId": "9525", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SFnfhJkGsBQk8jakK", "2YPbdHgcjt7g5ZaFN", "3zDX3f3QTepNeZHGc", "hN2aRnu798yas5b2k"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-30T16:54:36.285Z", "modifiedAt": null, "url": null, "title": "How likely is Peter Thiel's investment into seasteading to pay off? ", "slug": "how-likely-is-peter-thiel-s-investment-into-seasteading-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:45.664Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ntpZR8Tx9gMPsDmgS/how-likely-is-peter-thiel-s-investment-into-seasteading-to", "pageUrlRelative": "/posts/ntpZR8Tx9gMPsDmgS/how-likely-is-peter-thiel-s-investment-into-seasteading-to", "linkUrl": "https://www.lesswrong.com/posts/ntpZR8Tx9gMPsDmgS/how-likely-is-peter-thiel-s-investment-into-seasteading-to", "postedAtFormatted": "Tuesday, August 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20likely%20is%20Peter%20Thiel's%20investment%20into%20seasteading%20to%20pay%20off%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20likely%20is%20Peter%20Thiel's%20investment%20into%20seasteading%20to%20pay%20off%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FntpZR8Tx9gMPsDmgS%2Fhow-likely-is-peter-thiel-s-investment-into-seasteading-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20likely%20is%20Peter%20Thiel's%20investment%20into%20seasteading%20to%20pay%20off%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FntpZR8Tx9gMPsDmgS%2Fhow-likely-is-peter-thiel-s-investment-into-seasteading-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FntpZR8Tx9gMPsDmgS%2Fhow-likely-is-peter-thiel-s-investment-into-seasteading-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 119, "htmlBody": "<p>Recently the relatively awesome entrepreneur invested <a href=\"http://inhabitat.com/paypal-founder-invests-1-25-million-to-create-floating-micro-countries/\">1.25 million USD</a> into this (seasteading institute website <a href=\"http://seasteading.org/\">here</a>).</p>\n<p>It seems such a wonderful concept, finally <em>somewhere</em> where new forms of government could be tried out. But I'm just wondering how in the world they hope to deal with existing governments since their reaction to any kind of serious alternatives, especially one that either economically or ideologically presented a significant challenge, is bound to not be positive.</p>\n<p>I was just wondering what LWer thoughts are on this matter? Also has there been any discussion of seasteading in the past that I've missed? Also I'm wondering if anyone would hazard to perhaps offer a prediction or judge how likley this is to succeed (maybe on <a href=\"http://predictionbook.com/\">predictionbook</a>)?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ntpZR8Tx9gMPsDmgS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 18, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "9526", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 143, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-31T03:33:35.749Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Kahneman's Planning Anecdote", "slug": "seq-rerun-kahneman-s-planning-anecdote", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:20.475Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xvadt4Mt2JggMr8Ec/seq-rerun-kahneman-s-planning-anecdote", "pageUrlRelative": "/posts/xvadt4Mt2JggMr8Ec/seq-rerun-kahneman-s-planning-anecdote", "linkUrl": "https://www.lesswrong.com/posts/xvadt4Mt2JggMr8Ec/seq-rerun-kahneman-s-planning-anecdote", "postedAtFormatted": "Wednesday, August 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Kahneman's%20Planning%20Anecdote&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Kahneman's%20Planning%20Anecdote%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxvadt4Mt2JggMr8Ec%2Fseq-rerun-kahneman-s-planning-anecdote%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Kahneman's%20Planning%20Anecdote%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxvadt4Mt2JggMr8Ec%2Fseq-rerun-kahneman-s-planning-anecdote", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxvadt4Mt2JggMr8Ec%2Fseq-rerun-kahneman-s-planning-anecdote", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 171, "htmlBody": "<p>Today's post, <a href=\"/lw/jh/kahnemans_planning_anecdote/\">Kahneman's Planning Anecdote</a> was originally published on 17 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Nobel Laureate Daniel Kahneman recounts an incident where the inside view and the outside view of the time it would take to complete a project of his were widely different.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/7br/seq_rerun_planning_fallacy/\">Planning Fallacy</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xvadt4Mt2JggMr8Ec", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 10, "extendedScore": null, "score": 7.625781892675235e-07, "legacy": true, "legacyId": "9535", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["B9SF3v5vNzhJZFveH", "v4yKqq7vRmgc8gQPM", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-31T05:28:43.473Z", "modifiedAt": null, "url": null, "title": "Scientifically optimizing education: Hard problem, or solved problem? Introducing the Theory of Direct Instruction", "slug": "scientifically-optimizing-education-hard-problem-or-solved", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:22.997Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Owen_Richardson", "createdAt": "2011-04-08T22:03:10.482Z", "isAdmin": false, "displayName": "Owen_Richardson"}, "userId": "uR7QxXK65gYZQ4PrL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GR9YT9fd6vYfRQZB8/scientifically-optimizing-education-hard-problem-or-solved", "pageUrlRelative": "/posts/GR9YT9fd6vYfRQZB8/scientifically-optimizing-education-hard-problem-or-solved", "linkUrl": "https://www.lesswrong.com/posts/GR9YT9fd6vYfRQZB8/scientifically-optimizing-education-hard-problem-or-solved", "postedAtFormatted": "Wednesday, August 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Scientifically%20optimizing%20education%3A%20Hard%20problem%2C%20or%20solved%20problem%3F%20Introducing%20the%20Theory%20of%20Direct%20Instruction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScientifically%20optimizing%20education%3A%20Hard%20problem%2C%20or%20solved%20problem%3F%20Introducing%20the%20Theory%20of%20Direct%20Instruction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGR9YT9fd6vYfRQZB8%2Fscientifically-optimizing-education-hard-problem-or-solved%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Scientifically%20optimizing%20education%3A%20Hard%20problem%2C%20or%20solved%20problem%3F%20Introducing%20the%20Theory%20of%20Direct%20Instruction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGR9YT9fd6vYfRQZB8%2Fscientifically-optimizing-education-hard-problem-or-solved", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGR9YT9fd6vYfRQZB8%2Fscientifically-optimizing-education-hard-problem-or-solved", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5489, "htmlBody": "<p><!-- @page { margin: 2cm } P { margin-bottom: 0.21cm } A:link { so-language: zxx } --></p>\n<p style=\"padding-left: 30px;\"><strong><span style=\"color: #000000;\"><span style=\"font-family: Arial,sans-serif;\"><span style=\"font-size: small;\"><em>Re</em>-edited to remove/integrate much of the added notes - Sep 5th:</span></span></span></strong></p>\n<p style=\"font-style: normal; font-weight: normal; padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: Arial,sans-serif;\"><span style=\"font-size: small;\">This is a long post, and it was a first attempt to simply <em>start</em> trying to explain the whole topic, and see what kind of mistakes I made in the communication.</span></span></span></p>\n<p style=\"font-style: normal; font-weight: normal; padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: Arial,sans-serif;\"><span style=\"font-size: small;\">I did indeed make many mistakes, and started to feel that I should ask people <em>not</em> to read this original attempt at first, and so posted added notes to the beginning to say so and try to clear up the worst confusions.</span></span></span></p>\n<p style=\"font-style: normal; font-weight: normal; padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: Arial,sans-serif;\"><span style=\"font-size: small;\">But now that my audience is starting to close the inferential gap themselves thanks to amazingly wonderful people like <a href=\"/r/discussion/lw/7g3/what_direct_instruction_is/\">Misha with &ldquo;What Direct Instruction Is&rdquo;</a>, I think important points that I tried to express in this original foray might start to become more transparent to that audience.</span></span></span></p>\n<p style=\"font-style: normal; font-weight: normal; padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: Arial,sans-serif;\"><span style=\"font-size: small;\">It's still a very long post, with lots of new terminology, and, as Alicorn said, &ldquo;sales-y enthusiasm'.</span></span></span></p>\n<p style=\"font-style: normal; font-weight: normal; padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: Arial,sans-serif;\"><span style=\"font-size: small;\">If you do read it, I must ask that you please don't skim, giving me the benefit of a doubt that anything confusing or nonsensical seeming might actually be something that's important and meaningful in some non-obvious way that you do not yet understand, and that some of the 'sales-y enthusiasm' and &ldquo;applause lights&rdquo; may be have been intended to serve some useful purpose.</span></span></span></p>\n<p style=\"font-style: normal; font-weight: normal; padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: Arial,sans-serif;\"><span style=\"font-size: small;\">Again, please don't skim (although it is completely my fault if you feel like skimming!), because I just don't know how to do any better until I get more feedback on how <em>the complete whole of what I</em><em> wrote</em><em> </em>is understood.</span></span></span></p>\n<p style=\"font-style: normal; font-weight: normal; padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: Arial,sans-serif;\"><span style=\"font-size: small;\">If you do start skimming, and give up, just tell me <em>where</em> you did so.</span></span></span></p>\n<p style=\"font-weight: normal; padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: Arial,sans-serif;\"><span style=\"font-size: small;\">[The &ldquo;added notes&rdquo; from the first edit I've removed, and will go through at a later time to extract anything that was original and useful and integrate it into the post itself or whatever.]</span></span></span></p>\n<p style=\"font-weight: normal; padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: Arial,sans-serif;\"><span style=\"font-size: small;\">Thank you.</span></span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-size: small;\"><span style=\"font-family: Arial,sans-serif;\">Begin original:</span></span></span></p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">In this post, I'm going to introduce Direct Instruction, or DI (pronounced Dee-Eye, capital D, capital I, accept no imitations). DI is essentially the theory of how to find the best way to teach anything to anyone. And I mean a theory in the true scientific sense: parsimonious, rigorously pinned down by experiment, and with an impressive history of predictive successes.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Furthermore, it bestows upon the skillful wielder astonishing powers of engineering, allowing them accomplish educational feats that are nothing short of spectacular compared to what's traditionally accepted as, well, acceptable. If DI were universally implemented in the school system, it would easily raise the average intelligence to a level that would be considered genius by the standards of today's average.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">Or say someone wanted to set up a community that could consistently raise all its citizens (starting from children </span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\">or</span></em></span><span style=\"text-decoration: none;\"> </span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">adults) to formidable heights of intelligence, abilities, and rationality. DI would be one of the foundational tools they'd need to make it happen.</span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">It's obvious how this should interest anyone with the LessWrongian mission of changing the world from the crazy-stupid mess it is now to a sane, smart, good place to live.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">And that's my main purpose in writing this post: to interest. I'm not going to write a tutorial to teach or scholarly report to convince, because that would be redundant with resources already out there (and a lot more work!). Instead, I'm going to do my best to compress a very broad, very deep subject into a (relatively) short &ldquo;Hey, check </span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\">this</span></em></span><span style=\"text-decoration: none;\"> </span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">out!&rdquo;-style piece of writing, quickly hitting the highlights of the science and history well enough to explain why you should go follow the links I'll post and get your hands on the books I'll list. </span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">[If you find my compression to be more confusing than intriguing in any place, please help me fix that with feedback.]</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">Once we're all on the same page with respect to this background, I'll be able to write another post on the details how we could </span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\">use</span></em></span><span style=\"text-decoration: none;\"> </span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">this powerful tool to win. I'll talk about the highly unusual round-about way I originally came to find out about DI myself, and how that allowed me to notice some creative strategies that (I think) should allow DI to win against strongly established anti-rational forces in the field of education (&ldquo;How we can help DI win\"), as well as ways in which we could apply DI towards our goals ourselves, within our community (&ldquo;How DI can help us win&rdquo;).</span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">Again, &ldquo;what we can do for DI&rdquo;/&ldquo;what DI can do for us&rdquo; in a later post. This post will just be a super-abridged intro to DI itself, pretty much saying &ldquo;Hey, check </span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\">this</span></em></span><span style=\"text-decoration: none;\"> </span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">out!&rdquo;.</span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">And at this point, I feel I should move on to a *slightly* more concrete description of what DI </span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\">looks</span></em></span><span style=\"text-decoration: none;\"> </span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">like (but nowhere near as concrete as showing a particular program [like \"Reading Mastery Signatures level K\"] at this point). <br /></span></span></span></span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"text-decoration: underline;\"><strong><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">So, what does DI look like in practice?</span></span></strong></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">One thing that it's important to understand about DI is that, while it's certainly possible for someone who's very proficient with the theory to teach amazingly well, it is not <em>necessary</em> for the teacher to even know the difference between induction and deduction.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Because of the very logical nature of the sequences implied for teaching any particular learning outcome, following algorithms that are throughly understood by their designers, it is possible for a small number of expert 'educational engineers' to create <em>scripted</em> <span style=\"font-style: normal;\">courses that whole schools of non-experts can easily be trained to use for great success. (In somewhat the same way we can have a sufficient number of pilots in the world without having to train each and every one of them to build their own planes, and certainly not expecting them to cobble them together from bits.)</span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">These courses control what the teacher does and says, provide carefully matched expansion activities and independent student work, and even specify the correction procedures that may be necessary. They also provide tests to place new students at the appropriate level in the program, and frequent tests of mastery throughout the course.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">These courses are designed using logical rules derived from the basic axioms of the theory (which have been empirically pinned down as correct!), and then, like any high-quality complex machine, field-tested to find any design-errors and correct them before full-scale production (or rather, printing).</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-style: normal;\">[And yes, this logical, algorithmic aspect of the instruction means that DI would be extremely well-suited to creating computer-delivered lessons. If you remember the big fuss about 'Computer Assisted Learning' ages ago, yup, DI finally makes it possible for CAL to actually </span><em>deliver</em> <span style=\"font-style: normal;\">on all those gushing promises.]</span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-weight: normal;\">Unfortunately, this blessing of scalability-thanks-to-algorithmic-scriptability is also DI's curse. The very idea of using such tightly scripted lessons </span><em><span style=\"font-weight: normal;\">immediately</span></em> <span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">sticks in the craw of the vast majority of teachers, and showing them graphs of the overwhelming data showing how much better it is for the students is not very effective, and nor is explaining the theory or attempting to straighten out their philosophical confusions.</span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">For instance, an often articulated concern is that these scripted lessons will restrict the creative freedom of the teacher. A common counter is that this is analogous to claiming the creative freedom of a driver is restricted by not having them design and build their vehicle as they drive it, knowing nothing about the science and engineering necessary. </span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">But of course, this rhetoric, while pretty damn well aligned with evidence, has not been amazingly successful as a strategic tool. It seems that the only significant way in which normal teachers are converted over to DI is by actually </span></span><em><span style=\"font-weight: normal;\">using</span></em> <span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">a program correctly themselves, and seeing the amazing difference in their own kids. Unfortunately this does not lead to a multiplication factor greater than one in the spread of DI. </span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">But the subject of DI's historical and continuing struggle to overthrow the anti-scientific establishment of the field of education is covered in some of the resources I'll list at the end of this post, so I won't go into any more detail here. And again, I will discuss creative strategies for turning the tide of this struggle in a later post. At this point, knowing that this is an audience that </span></span><em><span style=\"font-weight: normal;\">does</span></em> <span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">properly appreciate experimental evidence, I will move to a discussion of something called Project Follow-Through.</span></span></span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-style: normal;\"><span style=\"text-decoration: underline;\"><strong>E</strong></span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"text-decoration: underline;\"><strong>vidence from Project Follow-Through:</strong></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Project Follow-Through was originally conceived as a social program in \"The War Against Poverty\", but, due to lack of funding, ended up morphing instead into the largest educational experiment in history. It ran nine years from 1968 to 1977, cost like a billion bucks, and involved over 200,000 students in over 170 communities across the US, from kindergarten though to grade three.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">It had a 'planned variation' design, otherwise described as a 'horse race' between all the different models popular in the field of education at the time, comparing them as composite wholes to find which worked best (rather than prematurely trying to isolate the effects of different variables within the models). And despite some name changes, the range of ideas in these models is pretty much representative of the common ideas in the field today.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Each school site was 'sponsored' by one of the competing models, or was self-sponsored. Sponsors got funding and some support to make sure their models actually got implemented in all the schools they were responsible for.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">The majority of involved communities had disadvantaged populations, and the average level of performance in the controls was at the 20th percentile.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Data was handled by two third-parties, with Stanford Research Institute using a variety of standardized achievement tests to collect it all, and Abt Associates doing the analysis.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Here's a graph showing performance in different basic skills for the nine models with sufficient data to evaluate, relative to that 20th percentile baseline:</span></span></p>\n<p><span style=\"color: #000000;\"><img src=\"http://psych.athabascau.ca/html/387/OpenModules/Engelmann/sv16016017.gif\" border=\"0\" alt=\"\" width=\"502\" height=\"323\" align=\"BOTTOM\" /></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Pretty striking, eh? </span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Here's another graph showing children's gains/losses for the nine models in the area of basic skills (the above-graphed skills lumped together), cognitive skills (things like problem solving, creative thinking, etc), and affective skills (things like self-esteem, sense of responsibility for own learning, attitude towards school, etc). Baseline zero represents children who did not participate in Follow-Through.</span></span></p>\n<p><span style=\"color: #000000;\"><img src=\"http://psych.athabascau.ca/html/387/OpenModules/Engelmann/sv16016018.gif\" border=\"0\" alt=\"\" width=\"502\" height=\"434\" align=\"BOTTOM\" /></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">You can see along the bottom that although models had been pre-classified as focused on primarily addressing one of these three areas, none of the 'affective' models had a positive effect on affective skills, and none of models had a positive effect on cognitive skills except for 'basic skills-oriented' DI, which raised everything, and quite a lot. </span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Also, while I've never looked very deeply into the details of the other models (for rather the same reason you don't look very deeply into the details of various tribal witch-doctor systems when you want to know about physics), I'd bet that DI was the sponsor with the most problems getting sites to implement the model properly. These results by no means show the limits of what DI can do.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Follow Through was by far the largest and best-funded experiment, doing the most comprehensive comparison of DI and the other competing 'theories' in the field. It is also easy to tell as a dramatic story, hence my selection of it for an introduction.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">However, there have been many other interesting experiments since, demonstrating impressive things that DI makes possible (and confirming that both low-performers </span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\">and</span></em></span><span style=\"text-decoration: none;\"> </span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">high-performers are best served by DI!). One researcher who conducted a meta-analysis of 34 studies making 173 direct experimental comparisons of DI and non-DI educational interventions said this:</span></span></span></span></span></p>\n<blockquote>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">The mean effect size average per study is more that .75, which confirms that the overall effect is substantial. &hellip; effects of .75 and above are rare in educational research. DI's consistent achievement of such scores is </span></span></span><span style=\"text-decoration: none;\"><em><strong>unique</strong></em></span><span style=\"text-decoration: none;\"> </span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">in educational research. [My emphasis]</span></span></span></span></span></p>\n</blockquote>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Again, I'll list resources at the end. At this point, I'm going to move on from evidence demonstrating DI's outstanding superiority in the field of education, to the theory of DI itself.</span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: underline;\"><strong>A</strong></span><span style=\"text-decoration: none;\"><span style=\"text-decoration: underline;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"text-decoration: underline;\"><strong>quick sketch of the basic theory:</strong></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">T<span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">he LessWrong audience should be uniquely prepared to 'click' on DI theory, already understanding things like extensional/intensional definitions, 'looking into the dark', thingspace, and being more likely to respond with a \"hm, that sounds like it might be interesting\" than a blank look if someone says 'guided induction'. </span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Still, because of the depth of the subject, I had particular trouble in compressing this section, because I had to choose between:</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">a) Writing this section as a detailed-and-easy-to-follow intro to the very beginning, but leaving you with no clear idea of how far it goes from there</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">or</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\">b) Writing it as a</span></span></span> super-abridged whirlwind tour in order to better capture the full breadth, but with some risk of ending up burying you in an avalanche of new terminology and lightning jumps from concept to concept</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">I ended up opting for (b) here, since a tutorial for the basics already exists as an online open module at Athabasca University (as usual, link later), and my purpose in this post is, as I said, more to pique interest than to teach.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">[But again, if you find the whirlwind below more confusing than intriguing, please help me fix that with some feedback.]</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">So, I'm gonna jump right in and kick things off the same way as the book <em>Theory of Instruction: Principles and Applications</em>, and tell you that 'the analysis of cognitive learning is at the intersection of three other analyses'. One is the behavioral analysis of the learner, also called the 'response-locus analysis' in DI. It's covered in DI theory, but all I'm gonna do here is note that and move on to the other two analyses: that of the communication used to teach, and that of the knowledge systems being taught.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">These, the analysis of communications and the analysis of knowledge systems, form the 'stimulus-locus analysis', and are the utterly fascinating first focus of DI.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Imagine you want a student to learn something, so you present an instructional sequence, and it fails. You wonder, why? If you had a hundred copies of that that student, and you presented the exact same sequence to all of them, would it fail 100% of the time? Or would it succeed some portion of the time because the student has some random chance of correctly 'guessing what it's supposed to mean' occasionally?</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Of course you can't <em>do</em> an experiment like that, because there's no way you could control the variable of the learner finely enough. So what about controlling the variable of the stimulus used to communicate with the learner?</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">What you could do is create a 'logically faultless communication', with a structure that you know from logical analysis of the <em>communication itself</em> <span style=\"font-style: normal;\">will be</span> successful with a learner with certain characteristics. (Then, even if the instruction fails, you end up with some highly specific information about the learner, which you can <em>then</em> use to figure out how to create success, by applying it to a behavioral analysis of the <em>learner</em><span style=\"font-style: normal;\">)</span>.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">[The term \"logically faultless communication\" does <em>not</em> suggest that if a learner fails to learn, then the learner is the problem and not the theory. In fact, the most common aphorism in the DI community is, \"If the learner hasn't learned, the teacher hasn't taught\". Until this seems perfectly consistent to you, you will know for sure you are not understanding the technical meaning of \"logically faultless communication\".] <br /></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">The basic axioms of the stimulus-locus analysis, therefore, are:</span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">1) The learning mechanism of the learner can learn any concept/quality from examples</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">2) The learning mechanism generalizes based on the samenesses of examples (it 'makes up a rule')</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><br /></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">(Note that how exactly the 'learning mechanism' does these things is unimportant here; This isn't a theory of <em>learning</em>, but of <em>instruction</em>.)</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Given these axioms, and a minimal amount of information about the learner's prior knowledge, it's now possible to design the logically faultless communication as a sequence of positive and negative examples of the concept to be taught. The major principles for doing this, which logically follow from the axioms, are:</span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">- Signals of positive and negative must be clear and consistent</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">- Only the features the learner is supposed to generalize should be shared by the whole set of positive examples</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">- Greatly different positive examples must be juxtaposed to show the range of variation of the concept</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">- Minimally different positive and negative examples must be juxtaposed to show the borders of the concept</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">- The instruction must integrate a test of generalization</span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">This is why I say that a huge part of the basics of DI is 'guided-induction' (my term, not used in the field). </span></span></p>\n<p style=\"padding-left: 30px;\"><em><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Aside: If you're familiar with the logical induction game 'zendo', it's like you're playing some sort of backwards version where you as the Master are trying to communicate the koan to the Students as well as possible by first showing a sequence of koans with and without Buddha-nature, and marked so, and then presenting a sequence of unmarked koans for the Students to respond to.</span></span></em></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">That's a quick sketch of the basis of the analysis of communications (skipping over <em>lots</em> of details of how this plays out in controlling extrapolation, stipulation, and interpolation and stuff). </span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Now, as I said, the analysis of communications is the first part of the stimulus-locus analysis, and it leads directly to the second part: the analysis of knowledge systems.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">The aim of the knowledge-systems analysis is to create a classification scheme that groups concepts by their samenesses in <em>logical structure</em>, so that samenesses in the logical structure of concepts are systematically related to samenesses in the logical structure of the communications used to teach the concepts. Thus classification of anything you want to teach in this scheme will tell you the basic template forms you must use and the steps you must go through to design effective instruction for it. </span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">I won't go into any details of this hierarchy here, since that would involve explaining a lot of terminology and the concepts behind it, but that's all in <em>Theory of Instruction</em>, along with details on the response-locus analysis, and details of designing programs, field-testing them, and using the data from the field-tests to correct design-errors and optimize the whole thing.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">I also want share a quote from Siegfried Engelmann, 'the father of DI', about when he was writing the text <em>Theory of Instruction</em> with colleague Douglas Carnine:</span></span></p>\n<blockquote>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">If we drew a unique logical conclusion about behavior, Doug would indicate that he knew of no experimental data on this issue and would ask if I knew of any empirical data. The answer was usually \"no,\" so Doug would conduct a study.</span></span></p>\n</blockquote>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Ten studies alone were done on all the details of the template for teaching a basic non-comparative concept like \"red\". For instance, the presence of negative examples, the number of features they differed from positives by, the way examples were juxtaposed, variations in presentation wording, etc. In every case, DI theory's unique and detailed predictions were validated.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">However, at this point I feel that I have probably focused enough on the very basic principles of the theory, the application of which is relatively obvious to the teaching of basic discriminations, but at a greater inferential distance to more advanced concepts. Therefore I'm going to hop over to one of my favorite short examples of the original kind of thinking that comes out of DI about how to teach things.</span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"text-decoration: underline;\"><strong><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">A quick example of a more advanced application of the stimulus-locus analysis:</span></span></strong></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">This section is a quick adaptation of something I wrote elsewhere. I'm including it here as an example of how DI can produce unexpectedly original conclusions about how to teach various things, which differ greatly from what is intuitively obvious, but which, once understood, are obviously logically overwhelmingly superior.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">T</span></span></span></span>his particular story starts with a list I once saw presented in a book as an example of possible long-term goals<span style=\"background: none repeat scroll 0% 0% transparent;\"> for a kindergarten class. In context, it was just used as an example of what an explicit list of goals might look like, but it was the very last bullet that caught my eye:</span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">&ldquo;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">Develop basic math concepts (for examples, numbers 1-20 and shapes)&rdquo;</span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Numbers 1-20.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">How can I best put this.... EPIC FAIL!</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">It&rsquo;s not at all obvious at first </span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">why</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">this is so wrong, so I&rsquo;ll explain by outlining the correct way to teach the transformation relationship between numerals and their English names, and the rational for this method.</span></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">You don&rsquo;t teach 1-20 first, you teach 1-99. And you do it in a very special order:</span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">- First, teach 1-10</span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">- Then, do the 40s, 60s, 70s, 80s, and 90s</span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Why? Because this is the simplest, most regular, largest subset of this numeral-name transformation relationship.</span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">The rule is simply, &ldquo;First you say the number of tens, add a &lsquo;ty&rsquo; (which is just a distorted &lsquo;ten&rsquo;), and then follow it with the number of ones if it&rsquo;s not a zero&rdquo;. So you see &ldquo;41&rdquo;, and you think &ldquo;okay, that&rsquo;s four-ty-one&rdquo;.</span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">[Note that this verbal explanation I just presented is </span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">not</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">how this is presented to the kids. This a description of </span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">what</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">you&rsquo;re teaching, not how.]</span></span></span></span></span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">- Then you move on to teaching the 20s, 30s, and 50s.</span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Why? Because these form another large subset, which involves one more addition to the rules that governed the last subset: You simply distort &ldquo;two&rdquo; to &ldquo;twen&rdquo;, &ldquo;three&rdquo; to &ldquo;thir&rdquo;, and &ldquo;five&rdquo; to &ldquo;fif&rdquo;, so that you get &ldquo;twen-ty-one&rdquo; rather than &ldquo;two-ty-one&rdquo;.</span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">-</span><em><span style=\"background: none repeat scroll 0% 0% transparent;\">Then</span></em><span style=\"background: none repeat scroll 0% 0% transparent;\"> you can move on to 14, 16, 17, 18, and 19.</span></span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">This subset is far smaller, and involves more complicated behaviors.</span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">The part of the number&rsquo;s name that tells the ones digit comes first, followed by the part that tells the tens digit, &ldquo;-teen&rdquo;, which is another, </span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">different</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">distortion of ten. </span></span></span></span></span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">You think: &ldquo;14 -&gt; ten-four -&gt; teen-four (distort) -&gt; four-teen (invert).&rdquo;</span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">-</span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">Then</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">you can do 13 and 15 (the tiniest subset), which are the same as above, but </span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">also</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">involve </span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">another</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">distortion of &ldquo;three&rdquo;-&gt;&rdquo;thir&rdquo; and &ldquo;five&rdquo;-&gt;&rdquo;fif&rdquo;. (Luckily this distortion is already familiar to the kids from working the 30s and 50s! Clever, eh?)</span></span></span></span></span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">-And finally, the wacky irregulars 12 and 11 can be thrown in.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">This</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">order is optimal for making clear to the learner that there is an orderly relationship here. They get the simple rules that cover the largest single group of cases, then they get the slightly more complicated rules that cover the next largest subtype, etc. </span></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">This makes clear what the basic pattern is, that</span></span></span></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">the exceptions </span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">are</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">exceptions, and exactly </span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">how</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">they are exceptions. </span></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">Thus </span></span><span style=\"text-decoration: none;\"><strong><span style=\"background: none repeat scroll 0% 0% transparent;\">you can teach 1-99 far faster and easier than you can 1-20</span></strong></span><span style=\"text-decoration: none;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">.</span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">[Note that the student must </span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">not</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">be worked to mastery on each subtype before the introduction of the next, because this would induce stipulation that the subtype was universal, but given proper pacing for any sequence of introduction, this order is optimal.]</span></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">Can you </span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">imagine</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">being a very young kid, truly na&iuml;ve to this concept (not having had ridiculous amount of informal exposure at home as a kid from a non-disadvantage background), and having someone try to teach you 11 to 20 after just getting up to 1 to 10? </span></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">For 11 and 12 you&rsquo;re thinking somewhere in your brain, &ldquo;Okay, does every number have its own unique name as you keep counting up?&rdquo; (You </span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">might</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">also wonder:</span></span></span></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> &ldquo;</span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">How many numbers </span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">are</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">there, anyway?&rdquo;)</span></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">For 13 you really haven't seen anything that contradicts that &lsquo;every number gets it&rsquo;s own unique name&rsquo; hypothesis (how likely do you think it is to occur to you that the 'thir' is related to the '3' in '13' and the 'teen' to the '1'? Nah, aint gonna happen). </span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">At 14 it might</span></span></span></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">occur to you to wonder if the &lsquo;four&rsquo; in &lsquo;fourteen&rsquo; has something to do with the &lsquo;4&rsquo; in &lsquo;14, but since &lsquo;FIFteen&rsquo; doesn&rsquo;t seem to have a &lsquo;five&rsquo; in it, you&rsquo;ll move that hypothesis to the backburner.</span></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">At the introduction of 16 you'll go, &ldquo;Hm, I wonder if the 'six' in 'sixteen' is related to the '6' in... Naaaah, I'm not gonna fall for that one again!&rdquo;</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">At 17 you start to reconsider it. 18 and 19 bring it back up to full level of serious consideration, by which time you&rsquo;re </span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">pretty</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">sure there&rsquo;s at least some </span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">bits</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">with some sort of pattern in here...</span></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">And then they throw 'twenty' at you. </span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">Huh? I mean, </span></span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">huh?</span></span></em></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Now hopefully you can see how the obvious intuitive way of teaching something can be not merely, \"Oh, maybe it could be better if you did, like, this or that\", but actually downright horrifyingly logically broken and wrong and bad.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">Whosoever adopts this crazy &lsquo;teach my kindergarteners 1-20&rsquo; goal is going to horribly slow down and confuse their kids. Not just &lsquo;they might be able to teach it better&rsquo;. They&rsquo;re doing it</span></span></span></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">wrong</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">.</span></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">In DI, a relationship like this one between numerals and their names is classified as a 'transformation concept', and the treatment I described above is called 'subtype analysis'. Hopefully it should now seem quite reasonable to think that this abstract concept could be applied to the teaching of many other not obviously related things (like grammatical conjugation rules in a foreign language, for instance), and that similarly suprising-yet-logical conclusions would be drawn by the stimulus-locus analysis for other concepts in the classification schemes as well as these 'transformations'.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Moving on, I feel at this point that I am unlikely to improve the quality of my super-abridged compression significantly per unit of additional agonizing over it, and I will now present the promised list of resources.</span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"text-decoration: underline;\"><strong><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Resources on DI online and in print:</span></span></strong></span></p>\n<p><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">- <a href=\"http://psych.athabascau.ca/html/387/OpenModules/Engelmann/\">The introductory open module at Athabasca University</a> </span></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">This provides a very short biography of Siegfried Engelmann (as I mentioned, the 'father of DI'), an overview of Project Follow-Through and associated history, and a much easier to follow introduction to the basics of the theory and the application of of the stimulus-locus analysis to the first of the 'basic forms' in the classification hierarchy.</span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">- The book, </span></span></span></span></span></span><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"><a href=\"http://adihome.org/index.php?page=shop.product_details&amp;flypage=flypage-ask.tpl&amp;product_id=13&amp;category_id=1&amp;option=com_virtuemart&amp;Itemid=107\">Theory of Instruction: Principles and Application</a> </span></span></em></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">This is pretty much the equivalent of Newton's </span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">Principia</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">for the field of education, except luckily it's not written in Latin.</span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">Ironically, in reading this text you will often find yourself wishing that the techniques in this book had been applied to the book itself (and seeing quite clearly how they </span></span></span><span style=\"text-decoration: none;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">could</span></span></em></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">be). Understandably though, the authors had to first articulate it all rigorously for themselves, and having done so, and given the low interest in the field in a true scientific theory, they decided to focus their engineering efforts on creating more programs for school children instead.</span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Nevertheless, the LessWrong audience shouldn't find it too difficult. The AthabascaU module largely covers the basics presented in the first few modules, and having read that and thus already having the concepts in mind, it's quite easy to adapt to the language, after which the extremely logical nature of the ideas presented makes it quite easy to follow.</span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">- The book, </span></span></span></span></span></span><a href=\"http://adihome.org/index.php?page=shop.product_details&amp;flypage=flypage-ask.tpl&amp;product_id=5&amp;category_id=1&amp;option=com_virtuemart&amp;Itemid=107\"><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><em><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">Research on Direct Instruction: 25 Years Beyond DISTAR</span></span></em></span></span></span></span></a><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span></span></span></span><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">[DISTAR was an early set of DI programs focused on arithmetic and reading]</span></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">This is the source of the quote from the meta-analysis I mentioned. It also covers studies on other things such as a program for teaching deaf and non-deaf people to interpret spoken words transformed into tactile vibrations, and some experiments that falsified Piaget's developmental theory (!)</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">Theory of Instruction</span></span></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">also has a section on research.</span></span></span></span></span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">- Engelmann has also written two books intended for a popular audience with titles that may be overly provocative from a strategic standpoint, but are definitely spot-on in terms of accuracy: &ldquo;</span></span></span></span></span></span></span><a href=\"http://adihome.org/index.php?page=shop.product_details&amp;flypage=flypage-ask.tpl&amp;product_id=12&amp;category_id=1&amp;option=com_virtuemart&amp;Itemid=107\"><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">War Against the School's Academic Child Abuse</span></span></span></span></span></span></a><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">&rdquo; </span></span></span></span></span><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">and &ldquo;</span></span></span></span></span></span></span><a href=\"http://adihome.org/index.php?page=shop.product_details&amp;flypage=flypage-ask.tpl&amp;product_id=11&amp;category_id=1&amp;option=com_virtuemart&amp;Itemid=107\"><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">Teaching Needy Kids in Our Backwards System</span></span></span></span></span></span></a><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">&rdquo;</span></span></span></span></span><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">.</span></span></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">These books deal with many educational issues which are more often than not both historical and current. Much of the material is presented in a partially autobiographical context.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">Although I believe we are going to be able to largely step </span></span></span></span><span style=\"text-decoration: none;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">around</span></span></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">most of the frustrating quagmires of institutionalized irrationality detailed in these books, I believe it's still good to have a good understanding of exactly what it is we're side-stepping, and many interesting bits of science and things are tied into a common narrative framework too. And finally, since they're written for a popular audience and quite easy reads, I would definitely recommend these books as worthwhile.</span></span></span></span></span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">- Engelmann's personal website zigsite.com has many interesting short (and not-so-short) documents. I would recommend \"<a href=\"http://www.zigsite.com/PDFs/CurriculumCauseFailure.pdf\">Curriculum as the cause of failure</a>\", (a couple pages are duplicated in that pdf) and its <a href=\"http://www.zigsite.com/curriculum_as_the_cause_of_failure_prologue.html\">contextual prologue</a>, for instance, and the video interviews.</span></span></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">How much of the material you're interested in depends on how much you just want to know only about the science itself, and how much you want to know about the horrible </span></span></span></span><span style=\"text-decoration: none;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">lack</span></span></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">of science in the field outside of DI.</span></span></span></span></span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">- I also found the interview by </span></span></span></span></span></span></span><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">Children of the Code</span></span></span></span></span></span><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span></span></span></span><span style=\"color: #000000;\"><span style=\"text-decoration: none;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"font-size: small;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">(a dyslexia organization) to be worthwhile. It's in two parts, <a href=\"http://www.childrenofthecode.org/interviews/engelmann.htm\">here</a>, and <a href=\"http://www.childrenofthecode.org/interviews/engelmann2.htm\">here</a>.</span></span></span></span></span></span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"text-decoration: underline;\"><strong><span style=\"background: none repeat scroll 0% 0% transparent;\">Conclusion:</span></strong></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">That is probably a sufficient amount of material for now. I'm hoping that your first taste will draw you in to voraciously devouring everything you can get your hands on, as happened for me.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">However, there is one very significant way in which my experience will differ from yours. As I mentioned in passing, I originally found out about DI in a very unusual round-about way. In fact, I became interested in it long before I first heard of it.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">To make a long, complicated story as short and streamlined as possible:</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Some years back, I decided I wanted to teach myself French, and after much failure, eventually stumbled upon a set of audio lessons that used something called the &ldquo;Michel Thomas Method&rdquo;.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">The difference between these lessons and all the other 'teach yourself' and formal instruction I'd messed about with was simply incredible. In about a month of using these audio lessons on my mp3 player while walking or riding the bus, I had a strong grasp of the entire structure of the language, and could use it to express my own ideas in a conversational context.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">Needless to say, I was a) very excited, and b) very angry that none of the supposed experts in language learning had </span></span></span></span><span style=\"text-decoration: none;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">told</span></span></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">me about this sooner. </span></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">I wanted to know what this &ldquo;Michel Thomas Method&rdquo; actually was. Would it work for everyone, or just learners like me? Would it work for subjects other than languages?</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">I eventually tracked down a book called &ldquo;The Learning Revolution&rdquo; by Jonathan Solity (which I had to order from the UK), and it was here that I first found references to &ldquo;Direct Instruction&rdquo; and &ldquo;Ziggy Engelmann&rdquo;. I googled it and, like I said, was soon hooked.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">But what I got from Solity's book in the end (although not exactly what he said), was that everything in the Michel Thomas lessons that made them so unusually effective was an approximation of DI.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">To explain the way I usually think about it now, I'll make a short digression to summarize one of Engelmann's articles criticizing &ldquo;research-based&rdquo; educational reforms in reading (\"The Dalmation and Its Spots\" on zigsite.com if you want to read it yourself).</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">In it he basically says this:</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">- These reforms were targeted at mandating that reading instruction have certain features (eg. paying some attention to phonemic awareness), because research had shown that instruction that was effective had these features, and therefore if instruction had these features it would be effective</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">-However, this is like saying that all dalmatians have spots, and therefore if something has spots, it's a dalmatian.</span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">So I would now say that the Michel Thomas programs were dalmatians rather than merely spotted. A bit mangy, with some mutt in them, but dalmatian enough to suffice for many practical purposes.</span></span></p>\n<p style=\"padding-left: 30px;\"><em><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Aside: Nobody knows whether Michel Thomas, now deceased, was ever directly aware of Engelmann's work, but he must have at least started developing some of the principles independently, given details of his rather dramatic biography in Europe during the WWII which I won't go into. And independent recreations of the same things are common in science and technology, after all.</span></span></em></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">At any rate, my personal emotional experience - failing very hard at learning something I wanted to do, and then finally succeeding quickly and easily thanks to, surprise, an instructor that actually had a clue how to teach - is unquestionably responsible for a </span></span></span></span><span style=\"text-decoration: none;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">lot</span></span></span><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">of the enthusiasm I have for this subject. And I just felt I should mention that.<br /></span></span></span></span></span></span></p>\n<p style=\"padding-left: 30px;\"><em><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">A final aside: If you're interested in learning a language yourself, I can personally recommend both the French and Spanish courses. (I haven't used the German and Italian, and don't know about the courses for other languages made by other people after Michel Thomas's death.)</span></span></em></p>\n<p style=\"padding-left: 30px;\"><em><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">I can't recommend that you simply download these from the internet, since that may be illegal in some jurisdictions, but there's a good chance you can find a copy at a local library, as I originally did.</span></span></em></p>\n<p style=\"padding-left: 30px;\"><em><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Having used these courses does provide an enlightening additional perspective on DI, as well as being, as I mentioned, the context in which I originally thought of some strategies that could allow DI to finally win against irrational forces in the educational establishment, which I will talk about in a later post.</span></span></em></p>\n<p style=\"padding-left: 30px;\"><em><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">Of course it's not necessary to have the same experience yourself in order to understand what I'll talk about, if you are not particularly interested in learning (or already know) French or Spanish, but if you are, then it would definitely be worthwhile.</span></span></em></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\"><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">And that, I believe, wraps up this super-sized &ldquo;Hey, check </span></span></span></span><em><span style=\"text-decoration: none;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">this</span></span></span></em><span style=\"text-decoration: none;\"><span style=\"background: none repeat scroll 0% 0% transparent;\"> </span></span><span style=\"text-decoration: none;\"><span style=\"font-style: normal;\"><span style=\"font-weight: normal;\"><span style=\"background: none repeat scroll 0% 0% transparent;\">out!&rdquo;</span></span></span></span></span></span></p>\n<p><span style=\"color: #000000;\"><span style=\"font-family: arial, sans-serif; font-size: 13px;\">I look forward to your feedback.</span></span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GR9YT9fd6vYfRQZB8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 22, "extendedScore": null, "score": 7.62615629735986e-07, "legacy": true, "legacyId": "9541", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MNSgW5j8bRvynSZHR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-31T15:39:47.981Z", "modifiedAt": null, "url": null, "title": "Meetup : Ottawa Weekly Monday LessWrong Meetup", "slug": "meetup-ottawa-weekly-monday-lesswrong-meetup-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XFrequentist", "createdAt": "2009-03-22T17:06:22.991Z", "isAdmin": false, "displayName": "XFrequentist"}, "userId": "zfW5w3TbDWjRW3YaD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/upSfcnRropMrsbbNh/meetup-ottawa-weekly-monday-lesswrong-meetup-2", "pageUrlRelative": "/posts/upSfcnRropMrsbbNh/meetup-ottawa-weekly-monday-lesswrong-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/upSfcnRropMrsbbNh/meetup-ottawa-weekly-monday-lesswrong-meetup-2", "postedAtFormatted": "Wednesday, August 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Ottawa%20Weekly%20Monday%20LessWrong%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Ottawa%20Weekly%20Monday%20LessWrong%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FupSfcnRropMrsbbNh%2Fmeetup-ottawa-weekly-monday-lesswrong-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Ottawa%20Weekly%20Monday%20LessWrong%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FupSfcnRropMrsbbNh%2Fmeetup-ottawa-weekly-monday-lesswrong-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FupSfcnRropMrsbbNh%2Fmeetup-ottawa-weekly-monday-lesswrong-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2u'>Ottawa Weekly Monday LessWrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 September 2011 07:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Pub Italia: 434 Preston St, Ottawa, ON</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>Type</strong>: Social</p>\n\n<p><strong>Date</strong>: Monday September 5, 7:30pm until at least 9:00pm.</p>\n\n<p><strong>Venue</strong>: Pub Italia (likely in a booth in the abbey - back of the pub)</p>\n\n<p>(We've settled on Monday evenings as the best time for most people, so we'll try this as the standard time and place for a while. Proposal for a more cost-effective or convenient location are eagerly solicited.)</p>\n\n<p><strong>Discussion post</strong>: <a href=\"http://lesswrong.com/lw/58m/build_small_skills_in_the_right_order/\">Build small skills in the right order</a></p>\n\n<p><strong>Planning</strong>: I'd like to discuss plans for a short boot camp on Linear Algebra in preparation for the <a href=\"http://www.ai-class.com/\" rel=\"nofollow\">Stanford AI course</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2u'>Ottawa Weekly Monday LessWrong Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "upSfcnRropMrsbbNh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.628144101662449e-07, "legacy": true, "legacyId": "9551", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Ottawa_Weekly_Monday_LessWrong_Meetup\">Discussion article for the meetup : <a href=\"/meetups/2u\">Ottawa Weekly Monday LessWrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 September 2011 07:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Pub Italia: 434 Preston St, Ottawa, ON</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>Type</strong>: Social</p>\n\n<p><strong>Date</strong>: Monday September 5, 7:30pm until at least 9:00pm.</p>\n\n<p><strong>Venue</strong>: Pub Italia (likely in a booth in the abbey - back of the pub)</p>\n\n<p>(We've settled on Monday evenings as the best time for most people, so we'll try this as the standard time and place for a while. Proposal for a more cost-effective or convenient location are eagerly solicited.)</p>\n\n<p><strong>Discussion post</strong>: <a href=\"http://lesswrong.com/lw/58m/build_small_skills_in_the_right_order/\">Build small skills in the right order</a></p>\n\n<p><strong>Planning</strong>: I'd like to discuss plans for a short boot camp on Linear Algebra in preparation for the <a href=\"http://www.ai-class.com/\" rel=\"nofollow\">Stanford AI course</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Ottawa_Weekly_Monday_LessWrong_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/2u\">Ottawa Weekly Monday LessWrong Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Ottawa Weekly Monday LessWrong Meetup", "anchor": "Discussion_article_for_the_meetup___Ottawa_Weekly_Monday_LessWrong_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Ottawa Weekly Monday LessWrong Meetup", "anchor": "Discussion_article_for_the_meetup___Ottawa_Weekly_Monday_LessWrong_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qwdupkFd6kmeZHYXy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-31T18:09:55.069Z", "modifiedAt": null, "url": null, "title": "[LINK] The Illusion of Asymmetric Insight", "slug": "link-the-illusion-of-asymmetric-insight", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B9JWqSgSTnSB6mkrx/link-the-illusion-of-asymmetric-insight", "pageUrlRelative": "/posts/B9JWqSgSTnSB6mkrx/link-the-illusion-of-asymmetric-insight", "linkUrl": "https://www.lesswrong.com/posts/B9JWqSgSTnSB6mkrx/link-the-illusion-of-asymmetric-insight", "postedAtFormatted": "Wednesday, August 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20The%20Illusion%20of%20Asymmetric%20Insight&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20The%20Illusion%20of%20Asymmetric%20Insight%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB9JWqSgSTnSB6mkrx%2Flink-the-illusion-of-asymmetric-insight%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20The%20Illusion%20of%20Asymmetric%20Insight%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB9JWqSgSTnSB6mkrx%2Flink-the-illusion-of-asymmetric-insight", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB9JWqSgSTnSB6mkrx%2Flink-the-illusion-of-asymmetric-insight", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 26, "htmlBody": "<p>I'm sure a number of LWers read You are Not So Smart, but his most recent <a href=\"http://youarenotsosmart.com/2011/08/21/the-illusion-of-asymmetric-insight/\">post</a> there relates to a large number of the sequences.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B9JWqSgSTnSB6mkrx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 7.628632570683684e-07, "legacy": true, "legacyId": "9552", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-31T19:42:24.907Z", "modifiedAt": null, "url": null, "title": "Are there better ways of identifying the most creative scientists?", "slug": "are-there-better-ways-of-identifying-the-most-creative", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:21.041Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MZ5PSWjonqnSC9Cj2/are-there-better-ways-of-identifying-the-most-creative", "pageUrlRelative": "/posts/MZ5PSWjonqnSC9Cj2/are-there-better-ways-of-identifying-the-most-creative", "linkUrl": "https://www.lesswrong.com/posts/MZ5PSWjonqnSC9Cj2/are-there-better-ways-of-identifying-the-most-creative", "postedAtFormatted": "Wednesday, August 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20there%20better%20ways%20of%20identifying%20the%20most%20creative%20scientists%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20there%20better%20ways%20of%20identifying%20the%20most%20creative%20scientists%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMZ5PSWjonqnSC9Cj2%2Fare-there-better-ways-of-identifying-the-most-creative%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20there%20better%20ways%20of%20identifying%20the%20most%20creative%20scientists%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMZ5PSWjonqnSC9Cj2%2Fare-there-better-ways-of-identifying-the-most-creative", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMZ5PSWjonqnSC9Cj2%2Fare-there-better-ways-of-identifying-the-most-creative", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 165, "htmlBody": "<p><em>Marginal Revolution</em> linked today an old 1963 essay by Isaac Asimov, who argues that a very cheap test for scientific capability in children &amp; adolescents is to see whether they like science fiction and in particular, harder science fiction, \"The Sword of Achilles\".</p>\n<p>I copied it out and made an HTML version of the essay: <a href=\"http://www.gwern.net/docs/1963-asimov-sword-of-achilles\">http://www.gwern.net/docs/1963-asimov-sword-of-achilles</a></p>\n<p>I'd be interested if anyone knows of better tests for such scientific aptitude.</p>\n<p>I think it'd also be interesting to see how well the SF test's predictive power has held up. Asimov's numbers seem reasonable for 1963, but may be very different these days: perhaps SF readers back then were &lt;1% of the population and &gt;50% of scientists, so it <em>was</em> a very informative, but these days? SF seems more popular, even discounting the comic books and Hollywood material as Asimov explicitly does, but the SF magazines are mostly dead and my understanding is that scientists are a vastly larger group in 2011 than 1963, both in absolute numbers and per capita.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MZ5PSWjonqnSC9Cj2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 7.628933574931973e-07, "legacy": true, "legacyId": "9553", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-08-31T19:56:02.948Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley weekly meetup", "slug": "meetup-berkeley-weekly-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uzoKuS5et2A9qzEgs/meetup-berkeley-weekly-meetup", "pageUrlRelative": "/posts/uzoKuS5et2A9qzEgs/meetup-berkeley-weekly-meetup", "linkUrl": "https://www.lesswrong.com/posts/uzoKuS5et2A9qzEgs/meetup-berkeley-weekly-meetup", "postedAtFormatted": "Wednesday, August 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%20weekly%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%20weekly%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuzoKuS5et2A9qzEgs%2Fmeetup-berkeley-weekly-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%20weekly%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuzoKuS5et2A9qzEgs%2Fmeetup-berkeley-weekly-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuzoKuS5et2A9qzEgs%2Fmeetup-berkeley-weekly-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2v'>Berkeley weekly meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 August 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2128 Oxford Street, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll meet in the Oxford Street Starbucks at 7pm and then get dinner. Over dinner I'll solicit people for help brainstorming communication exercises.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2v'>Berkeley weekly meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uzoKuS5et2A9qzEgs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.628977944364827e-07, "legacy": true, "legacyId": "9554", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley_weekly_meetup\">Discussion article for the meetup : <a href=\"/meetups/2v\">Berkeley weekly meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 August 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2128 Oxford Street, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll meet in the Oxford Street Starbucks at 7pm and then get dinner. Over dinner I'll solicit people for help brainstorming communication exercises.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley_weekly_meetup1\">Discussion article for the meetup : <a href=\"/meetups/2v\">Berkeley weekly meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley weekly meetup", "anchor": "Discussion_article_for_the_meetup___Berkeley_weekly_meetup", "level": 1}, {"title": "Discussion article for the meetup : Berkeley weekly meetup", "anchor": "Discussion_article_for_the_meetup___Berkeley_weekly_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-01T01:32:17.376Z", "modifiedAt": null, "url": null, "title": "How come everyone that disagrees with me is wrong?", "slug": "how-come-everyone-that-disagrees-with-me-is-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:21.812Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gBJwAjydby5rte79K", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vtDxbaPWgiyjnwei7/how-come-everyone-that-disagrees-with-me-is-wrong", "pageUrlRelative": "/posts/vtDxbaPWgiyjnwei7/how-come-everyone-that-disagrees-with-me-is-wrong", "linkUrl": "https://www.lesswrong.com/posts/vtDxbaPWgiyjnwei7/how-come-everyone-that-disagrees-with-me-is-wrong", "postedAtFormatted": "Thursday, September 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20come%20everyone%20that%20disagrees%20with%20me%20is%20wrong%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20come%20everyone%20that%20disagrees%20with%20me%20is%20wrong%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvtDxbaPWgiyjnwei7%2Fhow-come-everyone-that-disagrees-with-me-is-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20come%20everyone%20that%20disagrees%20with%20me%20is%20wrong%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvtDxbaPWgiyjnwei7%2Fhow-come-everyone-that-disagrees-with-me-is-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvtDxbaPWgiyjnwei7%2Fhow-come-everyone-that-disagrees-with-me-is-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>deleted</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vtDxbaPWgiyjnwei7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 3, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "9557", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-01T02:19:44.837Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Conjunction Fallacy", "slug": "seq-rerun-conjunction-fallacy", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dpN7tCqdwigjQKt6B/seq-rerun-conjunction-fallacy", "pageUrlRelative": "/posts/dpN7tCqdwigjQKt6B/seq-rerun-conjunction-fallacy", "linkUrl": "https://www.lesswrong.com/posts/dpN7tCqdwigjQKt6B/seq-rerun-conjunction-fallacy", "postedAtFormatted": "Thursday, September 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Conjunction%20Fallacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Conjunction%20Fallacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdpN7tCqdwigjQKt6B%2Fseq-rerun-conjunction-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Conjunction%20Fallacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdpN7tCqdwigjQKt6B%2Fseq-rerun-conjunction-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdpN7tCqdwigjQKt6B%2Fseq-rerun-conjunction-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p>Today's post, <a href=\"/lw/ji/conjunction_fallacy/\">Conjunction Fallacy</a> was originally published on 19 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Elementary probability theory tells us that the probability of one thing (we write P(A)) is necessarily greater than or equal to the conjunction of that thing and another thing (write P(A&amp;B)). However, in the psychology lab, subjects' judgments do not conform to this rule. This is not an isolated artifact of a particular study design. Debiasing won't be as simple as practicing specific questions, it requires certain general habits of thought.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/7cv/seq_rerun_kahnemans_planning_anecdote/#comments\">Kahneman's Planning Anecdote</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1a0": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dpN7tCqdwigjQKt6B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 7.630226810061667e-07, "legacy": true, "legacyId": "9561", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QAK43nNCTQQycAcYe", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-01T02:33:24.919Z", "modifiedAt": null, "url": null, "title": "Meetup : DC Meetup: NoVa", "slug": "meetup-dc-meetup-nova", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:23.353Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9SZS8n6fb5Acpr2Fg/meetup-dc-meetup-nova", "pageUrlRelative": "/posts/9SZS8n6fb5Acpr2Fg/meetup-dc-meetup-nova", "linkUrl": "https://www.lesswrong.com/posts/9SZS8n6fb5Acpr2Fg/meetup-dc-meetup-nova", "postedAtFormatted": "Thursday, September 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20DC%20Meetup%3A%20NoVa&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20DC%20Meetup%3A%20NoVa%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9SZS8n6fb5Acpr2Fg%2Fmeetup-dc-meetup-nova%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20DC%20Meetup%3A%20NoVa%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9SZS8n6fb5Acpr2Fg%2Fmeetup-dc-meetup-nova", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9SZS8n6fb5Acpr2Fg%2Fmeetup-dc-meetup-nova", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2w'>DC Meetup: NoVa</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 September 2011 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Ballston Commons Mall</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to talk about Non-Violent Communication, come prepared with questions, and some willingness to practice.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2w'>DC Meetup: NoVa</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9SZS8n6fb5Acpr2Fg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "9562", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___DC_Meetup__NoVa\">Discussion article for the meetup : <a href=\"/meetups/2w\">DC Meetup: NoVa</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 September 2011 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Ballston Commons Mall</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to talk about Non-Violent Communication, come prepared with questions, and some willingness to practice.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___DC_Meetup__NoVa1\">Discussion article for the meetup : <a href=\"/meetups/2w\">DC Meetup: NoVa</a></h2>", "sections": [{"title": "Discussion article for the meetup : DC Meetup: NoVa", "anchor": "Discussion_article_for_the_meetup___DC_Meetup__NoVa", "level": 1}, {"title": "Discussion article for the meetup : DC Meetup: NoVa", "anchor": "Discussion_article_for_the_meetup___DC_Meetup__NoVa1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-01T04:00:36.853Z", "modifiedAt": null, "url": null, "title": "Partial rewrite of the \"Direct Instruction\" thing", "slug": "partial-rewrite-of-the-direct-instruction-thing", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Owen_Richardson", "createdAt": "2011-04-08T22:03:10.482Z", "isAdmin": false, "displayName": "Owen_Richardson"}, "userId": "uR7QxXK65gYZQ4PrL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BDTkro8B2xtLF4v2v/partial-rewrite-of-the-direct-instruction-thing", "pageUrlRelative": "/posts/BDTkro8B2xtLF4v2v/partial-rewrite-of-the-direct-instruction-thing", "linkUrl": "https://www.lesswrong.com/posts/BDTkro8B2xtLF4v2v/partial-rewrite-of-the-direct-instruction-thing", "postedAtFormatted": "Thursday, September 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Partial%20rewrite%20of%20the%20%22Direct%20Instruction%22%20thing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APartial%20rewrite%20of%20the%20%22Direct%20Instruction%22%20thing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBDTkro8B2xtLF4v2v%2Fpartial-rewrite-of-the-direct-instruction-thing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Partial%20rewrite%20of%20the%20%22Direct%20Instruction%22%20thing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBDTkro8B2xtLF4v2v%2Fpartial-rewrite-of-the-direct-instruction-thing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBDTkro8B2xtLF4v2v%2Fpartial-rewrite-of-the-direct-instruction-thing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<p>So yeah, \"<a href=\"/r/discussion/lw/7d1/scientifically_optimizing_education_hard_problem/\">Scientifically optimizing education: Hard problem, or solved problem? Introducing the Theory of Direct Instruction</a>\". <em>Probably</em> not gonna go down in history as my best piece of writing ever, to say the least.</p>\n<p>Clearly I need to fix that. As an initial measure, I added some notes addressing key points and problems to the beginning as a much shorter replacement to the original post, and recommended the new reader not attempt to slog through the original below that unless they're strangely compelled.</p>\n<p>I felt that the most sensible context for this would be at the beginning of the original post, so I put it there and put up this post as a notification of that. (If this somehow breaks some sort of rule of etiquette or style, please just tell me and I'll rectify it most snappily.)</p>\n<p>Thank you for your patience.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BDTkro8B2xtLF4v2v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 7.630555172335692e-07, "legacy": true, "legacyId": "9569", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GR9YT9fd6vYfRQZB8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-01T04:41:28.173Z", "modifiedAt": null, "url": null, "title": ".", "slug": "", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:21.402Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "a7xJQpZ55R6SxFTik", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2AiDrwPs89Z9hwZKJ/", "pageUrlRelative": "/posts/2AiDrwPs89Z9hwZKJ/", "linkUrl": "https://www.lesswrong.com/posts/2AiDrwPs89Z9hwZKJ/", "postedAtFormatted": "Thursday, September 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2AiDrwPs89Z9hwZKJ%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2AiDrwPs89Z9hwZKJ%2F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2AiDrwPs89Z9hwZKJ%2F", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2AiDrwPs89Z9hwZKJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 7.630688179919687e-07, "legacy": true, "legacyId": "9570", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-01T08:46:33.562Z", "modifiedAt": null, "url": null, "title": "College course on the Singularity", "slug": "college-course-on-the-singularity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:22.355Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XmTp9gXKjHNwu5RfM/college-course-on-the-singularity", "pageUrlRelative": "/posts/XmTp9gXKjHNwu5RfM/college-course-on-the-singularity", "linkUrl": "https://www.lesswrong.com/posts/XmTp9gXKjHNwu5RfM/college-course-on-the-singularity", "postedAtFormatted": "Thursday, September 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20College%20course%20on%20the%20Singularity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACollege%20course%20on%20the%20Singularity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXmTp9gXKjHNwu5RfM%2Fcollege-course-on-the-singularity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=College%20course%20on%20the%20Singularity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXmTp9gXKjHNwu5RfM%2Fcollege-course-on-the-singularity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXmTp9gXKjHNwu5RfM%2Fcollege-course-on-the-singularity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 30, "htmlBody": "<p>...taught by James Marshall of Sarah Lawrence College in New York: '<a href=\"http://science.slc.edu/~jmarshall/courses/2007/fall/singularity/\">Is the Singularity Near?</a>' His motivations for the course are outlined <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.2124&amp;rep=rep1&amp;type=pdf\">here</a>. He got his Ph.D. studying under Hofstadter.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XmTp9gXKjHNwu5RfM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 7.63148617442846e-07, "legacy": true, "legacyId": "9579", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-01T11:59:55.783Z", "modifiedAt": null, "url": null, "title": "Is Rationality Teachable?", "slug": "is-rationality-teachable", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:05.362Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H2zKAfiSJR6WJQ8pn/is-rationality-teachable", "pageUrlRelative": "/posts/H2zKAfiSJR6WJQ8pn/is-rationality-teachable", "linkUrl": "https://www.lesswrong.com/posts/H2zKAfiSJR6WJQ8pn/is-rationality-teachable", "postedAtFormatted": "Thursday, September 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20Rationality%20Teachable%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20Rationality%20Teachable%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH2zKAfiSJR6WJQ8pn%2Fis-rationality-teachable%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20Rationality%20Teachable%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH2zKAfiSJR6WJQ8pn%2Fis-rationality-teachable", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH2zKAfiSJR6WJQ8pn%2Fis-rationality-teachable", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1490, "htmlBody": "<p>It is generally <a href=\"/lw/5x8/teachable_rationality_skills/\">assumed</a> around here that people can <em>learn</em> to be more rational.&nbsp;That's the purpose of <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences</a>, after all. And veteran Less Wrongers do seem (to me) vastly more rational than the average person.</p>\n<p>But maybe it's a selection effect: maybe Less Wrong doesn't <em>make</em>&nbsp;people more rational, it's just that&nbsp;the people who are already relatively rational&nbsp;are the ones most likely to be attracted Less Wrong.</p>\n<p>Daniel Willingham (2008)&nbsp;thinks it's pretty hard to teach rationality / critical thinking,<sup>1</sup> but&nbsp;what evidence do we have on the matter? Is rationality teachable?</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h4>Statistics and logic training</h4>\n<p>Statistics training appears to help.&nbsp;Schoemaker (1979) found that students who had taken&nbsp;a statistics course gave more consistent answers to questions about gambles than those who hadn't taken a statistics course. Students who had not taken the course were also more likely to bid more&nbsp;money than they could possibly win.</p>\n<p>Fong et al. (1986) found that statistical training sometimes transfers to real-world decision making. Half the men in a statistics course were interviewed at the beginning of the course, and the other half were interviewed at its end. The interview was ostensibly about sports, but was intended to test for skills in applying statistics to everyday life. The men interviewed after the course did significantly better in giving statistics-informed answers than those answered at the beginning of the course.</p>\n<p>For example, when asked why a Rookie of the Year in baseball usually does less well in his second year than in his first year, those interviewed at the beginning of the course tended to give answers like \"he's resting on his laurels; he's not trying so hard his second year,\" while those interviewed at the end of the course tended to give answers which appreciated <a href=\"http://en.wikipedia.org/wiki/Regression_toward_the_mean\">regression toward the mean</a>.</p>\n<p>What about logic? Nisbett et al. (1987) conducted several tests on the effects of different kinds of training on logical skills. Perhaps surprisingly, they found that logical skills (as measured by their tests) did not improve during courses on formal logic, informal fallacies, or two years of graduate courses in chemistry, but <em>did</em>&nbsp;improve as a result of two years of graduate courses in law, medicine, or psychology.&nbsp;Additionally, Lipman (1983) found that a <a href=\"http://en.wikipedia.org/wiki/Philosophy_for_Children\">Philosophy for Children</a> course increased the logical thinking skills of children in grades four to eight.</p>\n<p>&nbsp;</p>\n<h4>Debiasing</h4>\n<p>To examine the conflict that can arise between fast, intuitive reasoning and slow, deliberate reasoning, consider the following problems:</p>\n<blockquote>\n<p>Imagine that in order to win a prize you&nbsp;have to pick a red marble from one of two urns (Urn A and B). Urn A&nbsp;contains 20 red and 80 blue marbles, and Urn B contains 1 red and 9 blue&nbsp;marbles. When you respond to the task you can compare the ratio of&nbsp;winning marbles in each urn (20% vs 10%), which requires some time,&nbsp;mental effort, and computations, or you can simply rely on the feeling/intuition that it is preferable to pick from the urn with more 'favourable&nbsp;events'. In this example both processes cue the normatively correct answer&nbsp;(that is, Urn A). On the other hand, it is possible to set up a task where&nbsp;[intuitive] and [deliberative] reasoning cue different responses. For example, if you&nbsp;can choose between picking a marble from an urn containing 10 red and&nbsp;90 blue marbles, or from an urn containing 2 red and 8 blue marbles, the&nbsp;feeling/intuition that it is preferable to pick from the urn with more&nbsp;'favourable events' results in a normatively incorrect choice.<sup>2</sup></p>\n</blockquote>\n<p>When faced with such conflicts, most people tend to give intuitive answers rather than normatively correct answers.<sup>3</sup> However, some studies have found that those with greater cognitive capacity (more working memory, etc.) are less likely to use intuitions inappropriately.<sup>4</sup> Moreover, biases increase when working memory is burdened with cognitive load.<sup>5</sup> On the other hand, many biases are just as prevalent among people of high intelligence and greater cognitive capacity as among others.<sup>6</sup></p>\n<p>So: does debiasing work? Can resistance to cognitive biases be <em>taught</em>?</p>\n<p>Preliminary evidence suggests that debiasing <em>can</em> be done:</p>\n<ul>\n<li>A simple instruction to \"think about alternatives\" can promote resistance to overconfidence and confirmation bias. In one study, subjects asked to generate their own hypotheses are more responsive to their accuracy than subjects asked to choose from among pre-picked hypotheses.<sup>7</sup> Another study required subjects&nbsp;to list reasons for and against&nbsp;each of the possible answers to each question on a quiz prior to choosing an answer and assessing the probability of its being correct. This process resulted in more accurate confidence judgments relative to a control group.<sup>8</sup>&nbsp;</li>\n<li>Training in microeconomics can help subjects avoid the sunk cost fallacy.<sup>9</sup>&nbsp;</li>\n<li>Because people avoid the base rate fallacy more often when they encounter problems phrased in terms of frequencies instead of probabilities,<sup>10</sup> teaching people to translate probabilistic reasoning tasks into frequency formats improves their performance.<sup>11</sup>&nbsp;</li>\n<li>Warning people about biases can decrease their prevalence. So far, this has been demonstrated to work with regard to framing effects,<sup>12</sup> hindsight bias,<sup>13</sup> and the outcome effect,<sup>14</sup>&nbsp;though attempts to mitigate anchoring effects by warning people about them have produced weak results so far.<sup>15</sup></li>\n</ul>\n<p>&nbsp;</p>\n<h4>Conclusion</h4>\n<p>To be sure, many attempts to teach rationality have failed. But with results like those cited above, when I consider the prospects for teaching rationality I am hopeful, and left with <a href=\"/lw/2c/a_sense_that_more_is_possible/\">a sense that more is possible</a>. I believe we can indeed <a href=\"/lw/1e/raising_the_sanity_waterline/\">raise the sanity waterline</a>.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h4>Notes</h4>\n<p><small><sup>1</sup> 'Critical thinking' usually refers to a subset of some of the most basic skills sometimes called '<a href=\"/lw/5x8/teachable_rationality_skills/\">rationality skills</a>' by Less Wrongers, and usually the more qualitative forms of those skills, and is thus more aligned with <a href=\"/lw/55w/what_is_wrong_with_traditional_rationality/3vk2\">Traditional Rationality</a> than with <a href=\"http://commonsenseatheism.com/?p=12147\">Technical Rationality</a>. As Willingham puts it:</small></p>\n<blockquote>\n<p><small>In layperson&rsquo;s terms, critical thinking consists of seeing both sides of an issue, being open to new&nbsp;evidence that disconfirms your ideas, reasoning dispassionately, demanding that claims be backed by evidence, deducing and inferring conclusions from available facts, solving problems, and so forth.</small></p>\n</blockquote>\n<p><small><sup>2</sup>&nbsp;Chiesi et al. (2011).</small></p>\n<p><small><sup>3</sup> See, e.g.&nbsp;Klaczynski (2001).</small></p>\n<p><small><sup>4</sup> Stanovich &amp; West (2000, 2008a).</small></p>\n<p><small><sup>5</sup>&nbsp;De Neys (2006a, 2006b).</small></p>\n<p><small><sup>6</sup> Stanovich &amp; West (2008b, 2008c).</small></p>\n<p><small><sup>7</sup> Koehler (1994).</small></p>\n<p><small><sup>8</sup> Koriat et al. (1980). Also see Soll &amp; Klayman (2004); Mussweiler et al. (2000).</small></p>\n<p><small><sup>9</sup> Larrick et al. (1990).</small></p>\n<p><small><sup>10</sup>&nbsp;Gigerenzer &amp;&nbsp;Hoffrage (1995).</small></p>\n<p><small><sup>11</sup>&nbsp;Sedlmeier (1999).</small></p>\n<p><small><sup>12</sup> Cheng &amp; Wu (2010).</small></p>\n<p><small><sup>13</sup> Hasher et al. (1981);&nbsp;Reimers &amp; Butler (1992).</small></p>\n<p><small><sup>14</sup>&nbsp;Clarkson et al. (2002).</small></p>\n<p><small><sup>15</sup> Block &amp; Harper (1991); George et al. (2000).</small></p>\n<p><small>&nbsp;</small></p>\n<h4>References</h4>\n<p><small>Block &amp; Harper (1991). Overconfidence in estimation: testing the anchoring-and-adjustment hypothesis. <em>Organizational Behavior and Human Decision Processes, 49</em>: 188&ndash;207.</small></p>\n<p><small>Cheng &amp; Wu (2010). Debiasing the framing effect: The effect of warning and involvement. <em>Decision Support Systems, 49</em>: 328-334.</small></p>\n<p><small>Chiesi, Primi, &amp; Morsanyi (2011).&nbsp;Developmental changes in probabilistic reasoning: The&nbsp;role of cognitive capacity, instructions, thinking styles,&nbsp;and relevant knowledge. <em>Thinking and Reasoning, 17</em>:&nbsp;315&ndash;350.</small></p>\n<p><small>Clarkson, Emby, &amp; Watt (2002). Debiasing the effect of outcome knowledge: the role of instructions in an audit litigation setting. <em>Auditing: A Journal of Practice and Theory, 21</em>: 1&ndash;14.</small></p>\n<p><small>De Neys (2006a).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/04/De-Neys-Dual-processing-in-reasoning.pdf\">Dual processing in reasoning: Two systems but one reasoner</a>.&nbsp;<em>Psychological Science, 17</em>: 428&ndash;433.</small></p>\n<p><small>De Neys (2006b).&nbsp;<a href=\"/Automatic-heuristic and executive-analytic processing in reasoning\">Automatic-heuristic and executive-analytic processing in reasoning:&nbsp;Chronometric and dual task considerations</a>. <em>Quarterly Journal of Experimental Psychology,&nbsp;59</em>: 1070&ndash;1100.</small></p>\n<p><small>Fong, Krantz, and Nisbett (1986). <a href=\"http://deepblue.lib.umich.edu/bitstream/2027.42/26118/1/0000194.pdf\">The effects of statistical training on thinking about everyday problems</a>.&nbsp;<em>Cognitive Psychology, 18</em>: 253-292.</small></p>\n<p><small>George, Duffy, &amp; Ahuja (2000). Countering the anchoring and adjustment bias with decision support systems. <em>Decision Support Systems, 29</em>: 195&ndash;206.</small></p>\n<p><small>Gigerenzer &amp; Hoffrage (1995).&nbsp;How to improve Bayesian reasoning without instruction:&nbsp;Frequency formats. <em>Psychological Review, 102</em>: 684&ndash;704.</small></p>\n<p><small>Hasher, Attig, &amp; Alba (1981). I knew it all along: or did I? <em>Journal of Verbal and Learning Behavior, 20</em>: 86-96.</small></p>\n<p><small>Klaczynski (2001). Framing effects on adolescent task representations, analytic and heuristic processing, and decision making: Implications for the normative/descriptive gap. <em>Journal of Applied Developmental Psychology, 22</em>: 289-309.</small></p>\n<p><small>Koehler (1994). Hypothesis generation and confidence in judgment. <em>Journal of Experimental Psychology: Learning, Memory, and Cognition, 20</em>:&nbsp;461-469.</small></p>\n<p><small>Koriat, Lichtenstein, &amp; Fischhoff (1980). <a href=\"http://step.psy.cmu.edu/articles/Koriat.pdf\">Reasons for confidence</a>. <em>Journal of Experimental Psychology: Human Learning and Memory, 6</em>: 107-118.</small></p>\n<p><small>Larrick, Morgan, &amp; Nisbett (1990). Teaching the use of cost-benefit reasoning in everyday life. <em>Psychological Science, 1</em>: 362-370.</small></p>\n<p><small>Lipman (1983).&nbsp;<em>Thinking Skills Fostered by&nbsp;Philosophy for Children</em>. Institute for the Advancement of Philosophy for&nbsp;Children.</small></p>\n<p><small>Mussweiler,&nbsp;Strack, &amp; Pfeiffer (2000). Overcoming the inevitable anchoring effect:&nbsp;Considering the opposite compensates for selective accessibility. <em>Personality and Social Psychology&nbsp;Bulletin, 26</em>: 1142&ndash;50.</small></p>\n<p><small>Nisbett, Fong, Lehman, and Cheng (1987). Teaching reasoning.&nbsp;<em>Science, 238</em>: 625-631.</small></p>\n<p><small>Reimers &amp; Butler (1992). The effect of outcome knowledge on auditor's judgmental evaluations. <em>Accounting, Organizations and Society, 17</em>: 185&ndash;194.</small></p>\n<p><small>Sedlmeier (1999).&nbsp;<em><a href=\"http://www.amazon.com/Improving-Statistical-Reasoning-Theoretical-Implications/dp/0805832823/\">Improving Statistical Reasoning: Theoretical Models and Practical Implications</a></em>.&nbsp;Erlbaum.</small></p>\n<p><small>Shoemaker (1979). The role of statistical knowledge in gambling decisions: Moment vs. risk dimension approaches.&nbsp;<em>Organizational Behavior and Human Performance, 24</em>: 1-17.</small></p>\n<p><small>Soll &amp; Klayman (2004).&nbsp;Overconfidence in interval estimates. <em>Journal of Experimental&nbsp;Psychology: Learning, Memory, and Cognition, 30</em>: 299&ndash;314.</small></p>\n<p><small>Stanovich &amp; West (2000).&nbsp;<a href=\"http://psy2.ucsd.edu/~mckenzie/StanovichBBS.pdf\">Individual differences in reasoning: Implications for the&nbsp;rationality debate</a>. <em>Behavior Brain Science, 23</em>: 645&ndash;726.</small></p>\n<p><small>Stanovich &amp; West (2008a).&nbsp;Evolutionary versus instrumental goals: How&nbsp;evolutionary psychology misconceives human rationality. In Over (ed.), <em>Evolution&nbsp;and the psychology of thinking</em> (pp. 171&ndash;230). Psychology Press.</small></p>\n<p><small>Stanovich &amp; West (2008b).&nbsp;<a href=\"http://web.mac.com/kstanovich/Site/Research_on_Reasoning_files/SWTandR08.pdf\">On the failure of cognitive ability to predict myside and one-sided thinking biases</a>. <em>Thinking and Reasoning, 14</em>: 129-167.</small></p>\n<p><small>Stanovich &amp; West (2008c).&nbsp;<a href=\"http://web.mac.com/kstanovich/Site/Research_on_Reasoning_files/JPSP08.pdf\">On the relative independence of thinking biases and cognitive ability</a>. <em>Journal of Personality and Social Psychology, 94</em>: 672-695.</small></p>\n<p><small>Willingham (2008). <a href=\"http://www.aft.org/pdfs/americaneducator/summer2007/Crit_Thinking.pdf\">Critical thinking: Why is it so hard to teach?</a>&nbsp;<em>Arts Education Policy Review, 109</em>: 21-32.</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "fH8jPjHF2R27sRTTG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H2zKAfiSJR6WJQ8pn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 65, "extendedScore": null, "score": 0.000134, "legacy": true, "legacyId": "9321", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 65, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>It is generally <a href=\"/lw/5x8/teachable_rationality_skills/\">assumed</a> around here that people can <em>learn</em> to be more rational.&nbsp;That's the purpose of <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences</a>, after all. And veteran Less Wrongers do seem (to me) vastly more rational than the average person.</p>\n<p>But maybe it's a selection effect: maybe Less Wrong doesn't <em>make</em>&nbsp;people more rational, it's just that&nbsp;the people who are already relatively rational&nbsp;are the ones most likely to be attracted Less Wrong.</p>\n<p>Daniel Willingham (2008)&nbsp;thinks it's pretty hard to teach rationality / critical thinking,<sup>1</sup> but&nbsp;what evidence do we have on the matter? Is rationality teachable?</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h4 id=\"Statistics_and_logic_training\">Statistics and logic training</h4>\n<p>Statistics training appears to help.&nbsp;Schoemaker (1979) found that students who had taken&nbsp;a statistics course gave more consistent answers to questions about gambles than those who hadn't taken a statistics course. Students who had not taken the course were also more likely to bid more&nbsp;money than they could possibly win.</p>\n<p>Fong et al. (1986) found that statistical training sometimes transfers to real-world decision making. Half the men in a statistics course were interviewed at the beginning of the course, and the other half were interviewed at its end. The interview was ostensibly about sports, but was intended to test for skills in applying statistics to everyday life. The men interviewed after the course did significantly better in giving statistics-informed answers than those answered at the beginning of the course.</p>\n<p>For example, when asked why a Rookie of the Year in baseball usually does less well in his second year than in his first year, those interviewed at the beginning of the course tended to give answers like \"he's resting on his laurels; he's not trying so hard his second year,\" while those interviewed at the end of the course tended to give answers which appreciated <a href=\"http://en.wikipedia.org/wiki/Regression_toward_the_mean\">regression toward the mean</a>.</p>\n<p>What about logic? Nisbett et al. (1987) conducted several tests on the effects of different kinds of training on logical skills. Perhaps surprisingly, they found that logical skills (as measured by their tests) did not improve during courses on formal logic, informal fallacies, or two years of graduate courses in chemistry, but <em>did</em>&nbsp;improve as a result of two years of graduate courses in law, medicine, or psychology.&nbsp;Additionally, Lipman (1983) found that a <a href=\"http://en.wikipedia.org/wiki/Philosophy_for_Children\">Philosophy for Children</a> course increased the logical thinking skills of children in grades four to eight.</p>\n<p>&nbsp;</p>\n<h4 id=\"Debiasing\">Debiasing</h4>\n<p>To examine the conflict that can arise between fast, intuitive reasoning and slow, deliberate reasoning, consider the following problems:</p>\n<blockquote>\n<p>Imagine that in order to win a prize you&nbsp;have to pick a red marble from one of two urns (Urn A and B). Urn A&nbsp;contains 20 red and 80 blue marbles, and Urn B contains 1 red and 9 blue&nbsp;marbles. When you respond to the task you can compare the ratio of&nbsp;winning marbles in each urn (20% vs 10%), which requires some time,&nbsp;mental effort, and computations, or you can simply rely on the feeling/intuition that it is preferable to pick from the urn with more 'favourable&nbsp;events'. In this example both processes cue the normatively correct answer&nbsp;(that is, Urn A). On the other hand, it is possible to set up a task where&nbsp;[intuitive] and [deliberative] reasoning cue different responses. For example, if you&nbsp;can choose between picking a marble from an urn containing 10 red and&nbsp;90 blue marbles, or from an urn containing 2 red and 8 blue marbles, the&nbsp;feeling/intuition that it is preferable to pick from the urn with more&nbsp;'favourable events' results in a normatively incorrect choice.<sup>2</sup></p>\n</blockquote>\n<p>When faced with such conflicts, most people tend to give intuitive answers rather than normatively correct answers.<sup>3</sup> However, some studies have found that those with greater cognitive capacity (more working memory, etc.) are less likely to use intuitions inappropriately.<sup>4</sup> Moreover, biases increase when working memory is burdened with cognitive load.<sup>5</sup> On the other hand, many biases are just as prevalent among people of high intelligence and greater cognitive capacity as among others.<sup>6</sup></p>\n<p>So: does debiasing work? Can resistance to cognitive biases be <em>taught</em>?</p>\n<p>Preliminary evidence suggests that debiasing <em>can</em> be done:</p>\n<ul>\n<li>A simple instruction to \"think about alternatives\" can promote resistance to overconfidence and confirmation bias. In one study, subjects asked to generate their own hypotheses are more responsive to their accuracy than subjects asked to choose from among pre-picked hypotheses.<sup>7</sup> Another study required subjects&nbsp;to list reasons for and against&nbsp;each of the possible answers to each question on a quiz prior to choosing an answer and assessing the probability of its being correct. This process resulted in more accurate confidence judgments relative to a control group.<sup>8</sup>&nbsp;</li>\n<li>Training in microeconomics can help subjects avoid the sunk cost fallacy.<sup>9</sup>&nbsp;</li>\n<li>Because people avoid the base rate fallacy more often when they encounter problems phrased in terms of frequencies instead of probabilities,<sup>10</sup> teaching people to translate probabilistic reasoning tasks into frequency formats improves their performance.<sup>11</sup>&nbsp;</li>\n<li>Warning people about biases can decrease their prevalence. So far, this has been demonstrated to work with regard to framing effects,<sup>12</sup> hindsight bias,<sup>13</sup> and the outcome effect,<sup>14</sup>&nbsp;though attempts to mitigate anchoring effects by warning people about them have produced weak results so far.<sup>15</sup></li>\n</ul>\n<p>&nbsp;</p>\n<h4 id=\"Conclusion\">Conclusion</h4>\n<p>To be sure, many attempts to teach rationality have failed. But with results like those cited above, when I consider the prospects for teaching rationality I am hopeful, and left with <a href=\"/lw/2c/a_sense_that_more_is_possible/\">a sense that more is possible</a>. I believe we can indeed <a href=\"/lw/1e/raising_the_sanity_waterline/\">raise the sanity waterline</a>.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h4 id=\"Notes\">Notes</h4>\n<p><small><sup>1</sup> 'Critical thinking' usually refers to a subset of some of the most basic skills sometimes called '<a href=\"/lw/5x8/teachable_rationality_skills/\">rationality skills</a>' by Less Wrongers, and usually the more qualitative forms of those skills, and is thus more aligned with <a href=\"/lw/55w/what_is_wrong_with_traditional_rationality/3vk2\">Traditional Rationality</a> than with <a href=\"http://commonsenseatheism.com/?p=12147\">Technical Rationality</a>. As Willingham puts it:</small></p>\n<blockquote>\n<p><small>In layperson\u2019s terms, critical thinking consists of seeing both sides of an issue, being open to new&nbsp;evidence that disconfirms your ideas, reasoning dispassionately, demanding that claims be backed by evidence, deducing and inferring conclusions from available facts, solving problems, and so forth.</small></p>\n</blockquote>\n<p><small><sup>2</sup>&nbsp;Chiesi et al. (2011).</small></p>\n<p><small><sup>3</sup> See, e.g.&nbsp;Klaczynski (2001).</small></p>\n<p><small><sup>4</sup> Stanovich &amp; West (2000, 2008a).</small></p>\n<p><small><sup>5</sup>&nbsp;De Neys (2006a, 2006b).</small></p>\n<p><small><sup>6</sup> Stanovich &amp; West (2008b, 2008c).</small></p>\n<p><small><sup>7</sup> Koehler (1994).</small></p>\n<p><small><sup>8</sup> Koriat et al. (1980). Also see Soll &amp; Klayman (2004); Mussweiler et al. (2000).</small></p>\n<p><small><sup>9</sup> Larrick et al. (1990).</small></p>\n<p><small><sup>10</sup>&nbsp;Gigerenzer &amp;&nbsp;Hoffrage (1995).</small></p>\n<p><small><sup>11</sup>&nbsp;Sedlmeier (1999).</small></p>\n<p><small><sup>12</sup> Cheng &amp; Wu (2010).</small></p>\n<p><small><sup>13</sup> Hasher et al. (1981);&nbsp;Reimers &amp; Butler (1992).</small></p>\n<p><small><sup>14</sup>&nbsp;Clarkson et al. (2002).</small></p>\n<p><small><sup>15</sup> Block &amp; Harper (1991); George et al. (2000).</small></p>\n<p><small>&nbsp;</small></p>\n<h4 id=\"References\">References</h4>\n<p><small>Block &amp; Harper (1991). Overconfidence in estimation: testing the anchoring-and-adjustment hypothesis. <em>Organizational Behavior and Human Decision Processes, 49</em>: 188\u2013207.</small></p>\n<p><small>Cheng &amp; Wu (2010). Debiasing the framing effect: The effect of warning and involvement. <em>Decision Support Systems, 49</em>: 328-334.</small></p>\n<p><small>Chiesi, Primi, &amp; Morsanyi (2011).&nbsp;Developmental changes in probabilistic reasoning: The&nbsp;role of cognitive capacity, instructions, thinking styles,&nbsp;and relevant knowledge. <em>Thinking and Reasoning, 17</em>:&nbsp;315\u2013350.</small></p>\n<p><small>Clarkson, Emby, &amp; Watt (2002). Debiasing the effect of outcome knowledge: the role of instructions in an audit litigation setting. <em>Auditing: A Journal of Practice and Theory, 21</em>: 1\u201314.</small></p>\n<p><small>De Neys (2006a).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/04/De-Neys-Dual-processing-in-reasoning.pdf\">Dual processing in reasoning: Two systems but one reasoner</a>.&nbsp;<em>Psychological Science, 17</em>: 428\u2013433.</small></p>\n<p><small>De Neys (2006b).&nbsp;<a href=\"/Automatic-heuristic and executive-analytic processing in reasoning\">Automatic-heuristic and executive-analytic processing in reasoning:&nbsp;Chronometric and dual task considerations</a>. <em>Quarterly Journal of Experimental Psychology,&nbsp;59</em>: 1070\u20131100.</small></p>\n<p><small>Fong, Krantz, and Nisbett (1986). <a href=\"http://deepblue.lib.umich.edu/bitstream/2027.42/26118/1/0000194.pdf\">The effects of statistical training on thinking about everyday problems</a>.&nbsp;<em>Cognitive Psychology, 18</em>: 253-292.</small></p>\n<p><small>George, Duffy, &amp; Ahuja (2000). Countering the anchoring and adjustment bias with decision support systems. <em>Decision Support Systems, 29</em>: 195\u2013206.</small></p>\n<p><small>Gigerenzer &amp; Hoffrage (1995).&nbsp;How to improve Bayesian reasoning without instruction:&nbsp;Frequency formats. <em>Psychological Review, 102</em>: 684\u2013704.</small></p>\n<p><small>Hasher, Attig, &amp; Alba (1981). I knew it all along: or did I? <em>Journal of Verbal and Learning Behavior, 20</em>: 86-96.</small></p>\n<p><small>Klaczynski (2001). Framing effects on adolescent task representations, analytic and heuristic processing, and decision making: Implications for the normative/descriptive gap. <em>Journal of Applied Developmental Psychology, 22</em>: 289-309.</small></p>\n<p><small>Koehler (1994). Hypothesis generation and confidence in judgment. <em>Journal of Experimental Psychology: Learning, Memory, and Cognition, 20</em>:&nbsp;461-469.</small></p>\n<p><small>Koriat, Lichtenstein, &amp; Fischhoff (1980). <a href=\"http://step.psy.cmu.edu/articles/Koriat.pdf\">Reasons for confidence</a>. <em>Journal of Experimental Psychology: Human Learning and Memory, 6</em>: 107-118.</small></p>\n<p><small>Larrick, Morgan, &amp; Nisbett (1990). Teaching the use of cost-benefit reasoning in everyday life. <em>Psychological Science, 1</em>: 362-370.</small></p>\n<p><small>Lipman (1983).&nbsp;<em>Thinking Skills Fostered by&nbsp;Philosophy for Children</em>. Institute for the Advancement of Philosophy for&nbsp;Children.</small></p>\n<p><small>Mussweiler,&nbsp;Strack, &amp; Pfeiffer (2000). Overcoming the inevitable anchoring effect:&nbsp;Considering the opposite compensates for selective accessibility. <em>Personality and Social Psychology&nbsp;Bulletin, 26</em>: 1142\u201350.</small></p>\n<p><small>Nisbett, Fong, Lehman, and Cheng (1987). Teaching reasoning.&nbsp;<em>Science, 238</em>: 625-631.</small></p>\n<p><small>Reimers &amp; Butler (1992). The effect of outcome knowledge on auditor's judgmental evaluations. <em>Accounting, Organizations and Society, 17</em>: 185\u2013194.</small></p>\n<p><small>Sedlmeier (1999).&nbsp;<em><a href=\"http://www.amazon.com/Improving-Statistical-Reasoning-Theoretical-Implications/dp/0805832823/\">Improving Statistical Reasoning: Theoretical Models and Practical Implications</a></em>.&nbsp;Erlbaum.</small></p>\n<p><small>Shoemaker (1979). The role of statistical knowledge in gambling decisions: Moment vs. risk dimension approaches.&nbsp;<em>Organizational Behavior and Human Performance, 24</em>: 1-17.</small></p>\n<p><small>Soll &amp; Klayman (2004).&nbsp;Overconfidence in interval estimates. <em>Journal of Experimental&nbsp;Psychology: Learning, Memory, and Cognition, 30</em>: 299\u2013314.</small></p>\n<p><small>Stanovich &amp; West (2000).&nbsp;<a href=\"http://psy2.ucsd.edu/~mckenzie/StanovichBBS.pdf\">Individual differences in reasoning: Implications for the&nbsp;rationality debate</a>. <em>Behavior Brain Science, 23</em>: 645\u2013726.</small></p>\n<p><small>Stanovich &amp; West (2008a).&nbsp;Evolutionary versus instrumental goals: How&nbsp;evolutionary psychology misconceives human rationality. In Over (ed.), <em>Evolution&nbsp;and the psychology of thinking</em> (pp. 171\u2013230). Psychology Press.</small></p>\n<p><small>Stanovich &amp; West (2008b).&nbsp;<a href=\"http://web.mac.com/kstanovich/Site/Research_on_Reasoning_files/SWTandR08.pdf\">On the failure of cognitive ability to predict myside and one-sided thinking biases</a>. <em>Thinking and Reasoning, 14</em>: 129-167.</small></p>\n<p><small>Stanovich &amp; West (2008c).&nbsp;<a href=\"http://web.mac.com/kstanovich/Site/Research_on_Reasoning_files/JPSP08.pdf\">On the relative independence of thinking biases and cognitive ability</a>. <em>Journal of Personality and Social Psychology, 94</em>: 672-695.</small></p>\n<p><small>Willingham (2008). <a href=\"http://www.aft.org/pdfs/americaneducator/summer2007/Crit_Thinking.pdf\">Critical thinking: Why is it so hard to teach?</a>&nbsp;<em>Arts Education Policy Review, 109</em>: 21-32.</small></p>", "sections": [{"title": "Statistics and logic training", "anchor": "Statistics_and_logic_training", "level": 1}, {"title": "Debiasing", "anchor": "Debiasing", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "27 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["f4CZNEHirweN3XEjs", "Nu3wa6npK4Ry66vFp", "XqmjdBKa4ZaXJtNmf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-01T15:05:55.393Z", "modifiedAt": null, "url": null, "title": ".", "slug": "", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:34.965Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "a7xJQpZ55R6SxFTik", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/49ZAmTomDdaCZJkEe/", "pageUrlRelative": "/posts/49ZAmTomDdaCZJkEe/", "linkUrl": "https://www.lesswrong.com/posts/49ZAmTomDdaCZJkEe/", "postedAtFormatted": "Thursday, September 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F49ZAmTomDdaCZJkEe%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F49ZAmTomDdaCZJkEe%2F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F49ZAmTomDdaCZJkEe%2F", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "49ZAmTomDdaCZJkEe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 1, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "9582", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 101, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-01T19:18:03.634Z", "modifiedAt": null, "url": null, "title": "2011 Summer Matching Challenge Success!", "slug": "2011-summer-matching-challenge-success", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:22.364Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hkfLPhYtHpQoAzZM9/2011-summer-matching-challenge-success", "pageUrlRelative": "/posts/hkfLPhYtHpQoAzZM9/2011-summer-matching-challenge-success", "linkUrl": "https://www.lesswrong.com/posts/hkfLPhYtHpQoAzZM9/2011-summer-matching-challenge-success", "postedAtFormatted": "Thursday, September 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202011%20Summer%20Matching%20Challenge%20Success!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2011%20Summer%20Matching%20Challenge%20Success!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhkfLPhYtHpQoAzZM9%2F2011-summer-matching-challenge-success%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2011%20Summer%20Matching%20Challenge%20Success!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhkfLPhYtHpQoAzZM9%2F2011-summer-matching-challenge-success", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhkfLPhYtHpQoAzZM9%2F2011-summer-matching-challenge-success", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p>The $125,000, 2011 Summer Matching Challenge was a success! We met our goal 2 days early, raising $250,000 total for Singularity Institute operations. Here is the <a href=\"http://intelligence.org/blog/2011/09/01/2011-summer-matching-challenge-success/\">blog post announcement</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hkfLPhYtHpQoAzZM9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 7.633542981582968e-07, "legacy": true, "legacyId": "9583", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-01T21:48:53.792Z", "modifiedAt": null, "url": null, "title": "Correspondence Bias Reversal?", "slug": "correspondence-bias-reversal", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.885Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4fAP42H2bNnkzs76Y/correspondence-bias-reversal", "pageUrlRelative": "/posts/4fAP42H2bNnkzs76Y/correspondence-bias-reversal", "linkUrl": "https://www.lesswrong.com/posts/4fAP42H2bNnkzs76Y/correspondence-bias-reversal", "postedAtFormatted": "Thursday, September 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Correspondence%20Bias%20Reversal%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACorrespondence%20Bias%20Reversal%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4fAP42H2bNnkzs76Y%2Fcorrespondence-bias-reversal%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Correspondence%20Bias%20Reversal%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4fAP42H2bNnkzs76Y%2Fcorrespondence-bias-reversal", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4fAP42H2bNnkzs76Y%2Fcorrespondence-bias-reversal", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 274, "htmlBody": "<p>I'm currently taking an introductory Russian class. I have been using Anki to memorize the vocabulary, and I do appear to know more vocabulary than anyone else in the class<span style=\"font-size: 11px;\"><sup>1 </sup></span> except for one other individual. This individual has far surpassed everyone else in the class, in every area (grammar, vocabulary, etc). Several other students have made comments along the lines of \"Geez, do you spend all your time studying?\", and it had occurred to me that I should ask him what sort of study techniques he's using, and possibly try them out myself.&nbsp;</p>\n<p>At this point, it occurred to me that this may be a reversal of <a href=\"/lw/hz/correspondence_bias/\">Correspondence Bias</a>. The other students and I assumed that his superior abilities were due to his own particular methods of studying, and not to any sort of innate language ability. And yet, I think it is at least likely that there are more people in the world with a natural talent for languages, than there are people who have found some kind of spectacular studying technique.</p>\n<p>This is just a brief anecdote of a single life experience. Are there any systematic effects that we know of that work counter to Correspondence Bias?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>&nbsp;Data supporting this claim: in activities that we conduct inside the classroom, I have consistently remembered words that other members of the class do not, and it is very rare that a fellow student remembers a word from previous classes that I did not recall independently. Exceptions seem to have occurred mostly when a) it was not a word I put into Anki after class, or b) the other student is the individual I mentioned in the post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4fAP42H2bNnkzs76Y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "9584", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DB6wbyrMugYMK5o6a"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-02T00:30:44.006Z", "modifiedAt": null, "url": null, "title": "Meetup : Austin, TX", "slug": "meetup-austin-tx-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:26.780Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AEWae9MiNtnJFPZxo/meetup-austin-tx-1", "pageUrlRelative": "/posts/AEWae9MiNtnJFPZxo/meetup-austin-tx-1", "linkUrl": "https://www.lesswrong.com/posts/AEWae9MiNtnJFPZxo/meetup-austin-tx-1", "postedAtFormatted": "Friday, September 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Austin%2C%20TX&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Austin%2C%20TX%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAEWae9MiNtnJFPZxo%2Fmeetup-austin-tx-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Austin%2C%20TX%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAEWae9MiNtnJFPZxo%2Fmeetup-austin-tx-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAEWae9MiNtnJFPZxo%2Fmeetup-austin-tx-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 89, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2x'>Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 September 2011 01:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2222B Guadalupe St  Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Once again, we're meeting at Caffe Medici at 1:30 PM. I've looked into making a UT student organization and we only need <em>3</em> current UT students, and I believe we have at least four. I'll bring the paperwork for that and we should be able to get the ball rolling on that, and hopefully end up with a significantly swankier meeting space.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2x'>Austin, TX</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AEWae9MiNtnJFPZxo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.634561723939115e-07, "legacy": true, "legacyId": "9585", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Austin__TX\">Discussion article for the meetup : <a href=\"/meetups/2x\">Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 September 2011 01:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2222B Guadalupe St  Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Once again, we're meeting at Caffe Medici at 1:30 PM. I've looked into making a UT student organization and we only need <em>3</em> current UT students, and I believe we have at least four. I'll bring the paperwork for that and we should be able to get the ball rolling on that, and hopefully end up with a significantly swankier meeting space.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Austin__TX1\">Discussion article for the meetup : <a href=\"/meetups/2x\">Austin, TX</a></h2>", "sections": [{"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX", "level": 1}, {"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-02T02:00:45.191Z", "modifiedAt": null, "url": null, "title": "LW September WebDiplomacy Games: Starting soon!", "slug": "lw-september-webdiplomacy-games-starting-soon", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:20.162Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MW2q5BnfzB85FwcFY/lw-september-webdiplomacy-games-starting-soon", "pageUrlRelative": "/posts/MW2q5BnfzB85FwcFY/lw-september-webdiplomacy-games-starting-soon", "linkUrl": "https://www.lesswrong.com/posts/MW2q5BnfzB85FwcFY/lw-september-webdiplomacy-games-starting-soon", "postedAtFormatted": "Friday, September 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%20September%20WebDiplomacy%20Games%3A%20Starting%20soon!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%20September%20WebDiplomacy%20Games%3A%20Starting%20soon!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMW2q5BnfzB85FwcFY%2Flw-september-webdiplomacy-games-starting-soon%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%20September%20WebDiplomacy%20Games%3A%20Starting%20soon!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMW2q5BnfzB85FwcFY%2Flw-september-webdiplomacy-games-starting-soon", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMW2q5BnfzB85FwcFY%2Flw-september-webdiplomacy-games-starting-soon", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 131, "htmlBody": "<p>Check out <a href=\"/r/discussion/lw/79v/lw_september_webdiplomacy_games/\">this post</a> for background details.<br /><br />The <strong>final</strong> list for the 48 hour game is:<br />Prismattic<br />ahartell<br />Kutta<br />Randaly<br />GuySrinivasan<br />prase<br />RobertLumley<br /><br />The <strong>final</strong> list for the 24 hour game is:<br />folkTheory<br />printing-spoon<br />handoflixue<br />Vaniver<br />ahartell<br /> shokwave<br />Randaly</p>\n<p>In order to accommodate handoflixue, the short game will not start until <strong>Wednesday, September 7th</strong>. This has the positive side effects that the long game should have gone through around 2 turns by then, easing the load on people playing in both games.&nbsp; The long game will start <strong>as soon as possible</strong>.</p>\n<p>You can follow the long game <a href=\"http://www.webdiplomacy.net/board.php?gameID=66933\">here</a>. Various things have derailed the short game; not quite sure what's going to happen there.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MW2q5BnfzB85FwcFY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.63485506831799e-07, "legacy": true, "legacyId": "9588", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TyNtGwW7p6dNeESHY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-02T03:01:35.670Z", "modifiedAt": null, "url": null, "title": "Job Postings?", "slug": "job-postings", "viewCount": null, "lastCommentedAt": "2011-09-03T00:16:05.400Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Daniel_Burfoot", "createdAt": "2009-03-12T02:28:50.970Z", "isAdmin": false, "displayName": "Daniel_Burfoot"}, "userId": "XhcXE3Qk5adX6v2Cg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tR7Sc2YxmcnpTZo2J/job-postings", "pageUrlRelative": "/posts/tR7Sc2YxmcnpTZo2J/job-postings", "linkUrl": "https://www.lesswrong.com/posts/tR7Sc2YxmcnpTZo2J/job-postings", "postedAtFormatted": "Friday, September 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Job%20Postings%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJob%20Postings%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtR7Sc2YxmcnpTZo2J%2Fjob-postings%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Job%20Postings%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtR7Sc2YxmcnpTZo2J%2Fjob-postings", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtR7Sc2YxmcnpTZo2J%2Fjob-postings", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<p>I often come across job postings that require a skill set - math, statistics, programming, etc - that might make them ideal for some LW readers. Since finding fun jobs for LWers seems like a good thing to do, I often think I should post these job profiles to the discussion section.</p>\n<p>But nobody else has done this, and it seems like the kind of thing that people might get annoyed about. Certainly we don't want LW to be overrun with recruiters.</p>\n<p>Any thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tR7Sc2YxmcnpTZo2J", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 7.635053341234085e-07, "legacy": true, "legacyId": "9590", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2011-09-02T03:01:35.670Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-02T03:07:29.640Z", "modifiedAt": null, "url": null, "title": "Meetup : Orange County Atheist Meetup Thursday September 8", "slug": "meetup-orange-county-atheist-meetup-thursday-september-8", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JGWeissman", "createdAt": "2009-04-01T04:43:56.740Z", "isAdmin": false, "displayName": "JGWeissman"}, "userId": "Mw8rsM7m7E8nnEFEp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PRsgWQtYsWfXaznaj/meetup-orange-county-atheist-meetup-thursday-september-8", "pageUrlRelative": "/posts/PRsgWQtYsWfXaznaj/meetup-orange-county-atheist-meetup-thursday-september-8", "linkUrl": "https://www.lesswrong.com/posts/PRsgWQtYsWfXaznaj/meetup-orange-county-atheist-meetup-thursday-september-8", "postedAtFormatted": "Friday, September 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Orange%20County%20Atheist%20Meetup%20Thursday%20September%208&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Orange%20County%20Atheist%20Meetup%20Thursday%20September%208%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPRsgWQtYsWfXaznaj%2Fmeetup-orange-county-atheist-meetup-thursday-september-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Orange%20County%20Atheist%20Meetup%20Thursday%20September%208%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPRsgWQtYsWfXaznaj%2Fmeetup-orange-county-atheist-meetup-thursday-september-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPRsgWQtYsWfXaznaj%2Fmeetup-orange-county-atheist-meetup-thursday-september-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/2y'>Orange County Atheist Meetup Thursday September 8</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 September 2011 07:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">18542 MacArthur Blvd Irvine, CA 92612</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Once again we are going adjust the weekly Irvine meetup to join the Orange County Atheists for their <a href=\"http://www.ocatheists.com/archives/000113.shtml\" rel=\"nofollow\">September Meetup</a>. The main event starts at 7:30 at the upstairs table at the IHOP. Some members also show up for drinks and appetizers at the El Torito across the street at 6:30. They would appreciate if you RSVP in the comments of the linked announcement.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/2y'>Orange County Atheist Meetup Thursday September 8</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PRsgWQtYsWfXaznaj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.635072567224605e-07, "legacy": true, "legacyId": "9592", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Orange_County_Atheist_Meetup_Thursday_September_8\">Discussion article for the meetup : <a href=\"/meetups/2y\">Orange County Atheist Meetup Thursday September 8</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 September 2011 07:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">18542 MacArthur Blvd Irvine, CA 92612</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Once again we are going adjust the weekly Irvine meetup to join the Orange County Atheists for their <a href=\"http://www.ocatheists.com/archives/000113.shtml\" rel=\"nofollow\">September Meetup</a>. The main event starts at 7:30 at the upstairs table at the IHOP. Some members also show up for drinks and appetizers at the El Torito across the street at 6:30. They would appreciate if you RSVP in the comments of the linked announcement.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Orange_County_Atheist_Meetup_Thursday_September_81\">Discussion article for the meetup : <a href=\"/meetups/2y\">Orange County Atheist Meetup Thursday September 8</a></h2>", "sections": [{"title": "Discussion article for the meetup : Orange County Atheist Meetup Thursday September 8", "anchor": "Discussion_article_for_the_meetup___Orange_County_Atheist_Meetup_Thursday_September_8", "level": 1}, {"title": "Discussion article for the meetup : Orange County Atheist Meetup Thursday September 8", "anchor": "Discussion_article_for_the_meetup___Orange_County_Atheist_Meetup_Thursday_September_81", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-02T06:06:35.017Z", "modifiedAt": null, "url": null, "title": "Another treatment of Direct Instruction getting more into the technical details of the theory", "slug": "another-treatment-of-direct-instruction-getting-more-into", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.402Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Owen_Richardson", "createdAt": "2011-04-08T22:03:10.482Z", "isAdmin": false, "displayName": "Owen_Richardson"}, "userId": "uR7QxXK65gYZQ4PrL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Frp55RdAbFTLa2Ae7/another-treatment-of-direct-instruction-getting-more-into", "pageUrlRelative": "/posts/Frp55RdAbFTLa2Ae7/another-treatment-of-direct-instruction-getting-more-into", "linkUrl": "https://www.lesswrong.com/posts/Frp55RdAbFTLa2Ae7/another-treatment-of-direct-instruction-getting-more-into", "postedAtFormatted": "Friday, September 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20treatment%20of%20Direct%20Instruction%20getting%20more%20into%20the%20technical%20details%20of%20the%20theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20treatment%20of%20Direct%20Instruction%20getting%20more%20into%20the%20technical%20details%20of%20the%20theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFrp55RdAbFTLa2Ae7%2Fanother-treatment-of-direct-instruction-getting-more-into%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20treatment%20of%20Direct%20Instruction%20getting%20more%20into%20the%20technical%20details%20of%20the%20theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFrp55RdAbFTLa2Ae7%2Fanother-treatment-of-direct-instruction-getting-more-into", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFrp55RdAbFTLa2Ae7%2Fanother-treatment-of-direct-instruction-getting-more-into", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 476, "htmlBody": "<p>[NOTE: This was a discussion post <em>asking if anyone would mind giving feedback on a very rough draft in progress</em>.</p>\n<p>If you are downvoting it because you do not want to see discussion posts asking for feedback like this, then by all means, that's a valid use of a downvote.</p>\n<p>But if you are downvoting it in order to express your opinion of the quality of the draft, I urge you to reconsider]</p>\n<p>&nbsp;</p>\n<p>This is another work in progress coming at the DI issue from a somewhat different direction. It's <a href=\"/r/discussion/lw/7d1/scientifically_optimizing_education_hard_problem/4qx3\">contained in the comments of the original</a>, and I'm posting this to ask for more wonderful beta-reader critics to tell me if it's a step in the right direction. (It's still very informal writing, but it's the ideas I'm dealing with now.)</p>\n<p>And about what I'm looking for in the LW audience, someone asked me in a private message:</p>\n<blockquote>\n<p>... who is the audience, here? Are you hoping that LW readers are school administrators, who will introduce DI into their schools? Are you hoping that they are teachers, who will introduce DI into their classes? Are you hoping that they are students, who will be able to seek out instructors using DI? ...</p>\n<p>I'm personally interested because I have an interest in alternate education methods; I think the method sounds promising and what I know matches up with what I know about solid epistemology. The people who are interested in epistemic rationality (another group you could tailor posts towards) would probably be interested in learning about epistemic methods that are quantitatively superior to others.</p>\n</blockquote>\n<p>And I said:</p>\n<blockquote>\n<p>Oh, I dunno about DI involving \"epistemic methods that are quantitatively superior to others\". The founder recently wrote <a href=\"http://adihome.org/index.php?page=shop.product_details&amp;flypage=flypage-ask.tpl&amp;product_id=48&amp;category_id=1&amp;option=com_virtuemart&amp;Itemid=107\">a book about what John Stuart Mill could have done for education</a>, so that's the epistemology that DI is applying.</p>\n<p>So actually, another reason I keep using Newton's laws analogies is that I suspect there's an analogious 'general relativity' to be found.</p>\n<p>So what I really want is for the people from the LW audience to learn DI theory themselves, because I think they could improve the theory.</p>\n</blockquote>\n<p>Well, that's the major part of what I want that's important here. I also had to add:</p>\n<blockquote>\n<p>you remember how I mentioned 'creative strategic twists' [for how we could help DI win, and how DI could help us win], and indicated that the inspiration for that came from comparing the Michel Thomas lessons with DI proper (the internal details of each and their separate histories)?</p>\n<p>That's another long-inferential distance topic...</p>\n</blockquote>\n<p>But that's not important here (except to disclose that <em>is</em> where I'm coming from). LWers would first have to understand DI to fully grasp that. And I am significantly less certain of my current beliefs about those 'strategic twists' (although still pretty certain), and LWers proficient in DI would be the best to evaluate the ideas.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Frp55RdAbFTLa2Ae7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -7, "extendedScore": null, "score": 7.635656247229021e-07, "legacy": true, "legacyId": "9600", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-02T06:41:12.500Z", "modifiedAt": null, "url": null, "title": "Contrarians judged mad after being proven right (John Hempton)", "slug": "contrarians-judged-mad-after-being-proven-right-john-hempton", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:21.759Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jonathan_Graehl", "createdAt": "2009-02-27T23:21:15.671Z", "isAdmin": false, "displayName": "Jonathan_Graehl"}, "userId": "eKsWtKKceoRYwcc7s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WWrLzzxgNf9MjmLX4/contrarians-judged-mad-after-being-proven-right-john-hempton", "pageUrlRelative": "/posts/WWrLzzxgNf9MjmLX4/contrarians-judged-mad-after-being-proven-right-john-hempton", "linkUrl": "https://www.lesswrong.com/posts/WWrLzzxgNf9MjmLX4/contrarians-judged-mad-after-being-proven-right-john-hempton", "postedAtFormatted": "Friday, September 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Contrarians%20judged%20mad%20after%20being%20proven%20right%20(John%20Hempton)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AContrarians%20judged%20mad%20after%20being%20proven%20right%20(John%20Hempton)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWWrLzzxgNf9MjmLX4%2Fcontrarians-judged-mad-after-being-proven-right-john-hempton%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Contrarians%20judged%20mad%20after%20being%20proven%20right%20(John%20Hempton)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWWrLzzxgNf9MjmLX4%2Fcontrarians-judged-mad-after-being-proven-right-john-hempton", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWWrLzzxgNf9MjmLX4%2Fcontrarians-judged-mad-after-being-proven-right-john-hempton", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 338, "htmlBody": "<p>I recommend&nbsp;<a href=\"http://brontecapital.blogspot.com/2011/09/risk-management-and-sounding-crazy.html\">John Hempton's blog post</a>&nbsp;on how badly people judge seeming madmen in the case the conventional view has only conventional-wisdom support. I also like how he explains his research and conclusions in general.</p>\n<p>the gist:</p>\n<blockquote>\n<p>In early June Carson Block and his firm Muddy Waters research published a report which made outrageous sounding allegations against Sino Forest - then a highly respected Canadian listed Chinese forestry company that had borrowed well over $2 billion to develop and expand forestry operations in China.<br style=\"color: #333333; font-family: arial; font-size: 14px; line-height: 22px; text-align: left; background-color: #ccdee8;\" /><br style=\"color: #333333; font-family: arial; font-size: 14px; line-height: 22px; text-align: left; background-color: #ccdee8;\" />The base allegation in the report was that most the forests did not exist and by implication the (more than) $2 billion borrowed was stolen. Presumably many more shares have been sold too taking the total theft well above $2 billion.</p>\n</blockquote>\n<blockquote>\n<p>I am obsessed about discovering the ways my positions can be wrong.</p>\n</blockquote>\n<blockquote>\n<p>Dundee Securities was the most prominent Sino-supporter&nbsp;labeling&nbsp;Muddy Water's research a \"<a href=\"http://ftalphaville.ft.com/blog/2011/06/20/599606/pile-of-crap-suspending-coverage/\">pile of crap</a>\". Somewhat more considered sounding (but also flat wrong just more reasonable sounding) was&nbsp;<a href=\"http://www.metalaugmentor.com/analysis/charlatan-exposed-seeing-the-sino-forest-for-the-trees-part-1.html\">Metal Augmentor&nbsp;</a>who found Carson \"loose with the facts and somewhat breathless\". On the naive-sounding side was Susan Mallin whose&nbsp;<a href=\"http://susan-mallin.blogspot.com/2011/06/sino-forest-muddy-waters-saga-report.html\">complaint</a>&nbsp;was that she had \"never seen a research report written in this manner\". More prominent people were fooled too.</p>\n<p>The analysis of these people was staggeringly weak and self-referential. They judged Sino Forest against data provided by Sino Forest or people associated with Sino Forest. This is an elementary mistake in assessing fraud. To find fraud you need to be able to judge against things you are fairly sure are not fraudulent.<br style=\"color: #333333; font-family: arial; font-size: 14px; line-height: 22px; text-align: left; background-color: #ccdee8;\" /><br style=\"color: #333333; font-family: arial; font-size: 14px; line-height: 22px; text-align: left; background-color: #ccdee8;\" />Everything the Carson Block doubters said&nbsp;sounded&nbsp;reasonable. Certainly more reasonable than Carson Block sounded because Carson Block held the radical position. Sounding reasonable however was wrong.<br style=\"color: #333333; font-family: arial; font-size: 14px; line-height: 22px; text-align: left; background-color: #ccdee8;\" /><br style=\"color: #333333; font-family: arial; font-size: 14px; line-height: 22px; text-align: left; background-color: #ccdee8;\" />I think what is going on here is a general problem. When someone says something - anything - that is so far from the consensus as to sound outrageous then they will be considered mad, and sometimes they will be considered mad even after they are proven right.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WWrLzzxgNf9MjmLX4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "9604", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-02T07:37:08.154Z", "modifiedAt": null, "url": null, "title": "Consequentialism Need Not Be Nearsighted", "slug": "consequentialism-need-not-be-nearsighted", "viewCount": null, "lastCommentedAt": "2021-10-27T09:39:55.097Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/prb8raC4XGJiRWs5n/consequentialism-need-not-be-nearsighted", "pageUrlRelative": "/posts/prb8raC4XGJiRWs5n/consequentialism-need-not-be-nearsighted", "linkUrl": "https://www.lesswrong.com/posts/prb8raC4XGJiRWs5n/consequentialism-need-not-be-nearsighted", "postedAtFormatted": "Friday, September 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Consequentialism%20Need%20Not%20Be%20Nearsighted&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConsequentialism%20Need%20Not%20Be%20Nearsighted%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fprb8raC4XGJiRWs5n%2Fconsequentialism-need-not-be-nearsighted%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Consequentialism%20Need%20Not%20Be%20Nearsighted%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fprb8raC4XGJiRWs5n%2Fconsequentialism-need-not-be-nearsighted", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fprb8raC4XGJiRWs5n%2Fconsequentialism-need-not-be-nearsighted", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1486, "htmlBody": "<p><em><strong>Summary:</strong> If you object to consequentialist ethical theories because you think they endorse horrible or catastrophic decisions, then you may instead be objecting to short-sighted utility functions or poor decision theories.</em></p><p><strong>Recommended:</strong> <a href=\"https://www.lesserwrong.com/lw/767/decision_theory_paradox_pd_with_three_implies/\">Decision Theory Paradox: PD with Three Implies Chaos?</a></p><p><strong>Related:</strong> <a href=\"https://www.lesserwrong.com/lw/n9/the_intuitions_behind_utilitarianism/\">The &quot;Intuitions&quot; Behind &quot;Utilitarianism&quot;</a></p><p>The simple idea that we ought to choose actions according to their probable consequences, ever since it was formulated, has garnered a rather shocking amount of dissent. Part of this may be due to <a href=\"https://www.lesserwrong.com/lw/74f/are_deontological_moral_judgments_rationalizations/\">causes other than philosophical objections</a>, and some of the objections get into the metaphysics of metaethics. But there&#x27;s a fair amount of opposition on rather simple grounds: that consequentialist reasoning appears to endorse bad decisions, either in the long run or as an effect of collective action.</p><p>Every so often, you&#x27;ll hear someone offer a <em>reductio ad absurdum</em> of the following form: &quot;Consider dilemma X. If we were consequentialists, then we would be forced to choose Y. But in the long run (or if widely adopted) the strategy of choosing Y leads to horrible consequence Z, and so consequentialism fails on its own terms.&quot;</p><p>There&#x27;s something fishy about the argument when you lay it out like that: if it can be known that the strategy of choosing Y has horrible consequence Z, then why do we agree that consequentialists choose Y? In fact, there are two further unstated assumptions in every such argument I&#x27;ve heard, and it is those assumptions rather than consequentialism on which the absurdity really falls. But to discuss the assumptions, we need to delve into a bit of <a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">decision theory</a>.</p><p>In <a href=\"https://www.lesserwrong.com/lw/767/decision_theory_paradox_pd_with_three_implies/\">my last post</a>, I posed an apparent paradox: a case where it looked as if a simple rule could trump the most rational of decision theories in a fair fight. But there was a sleight of hand involved (which, to your credit, many of you spotted immediately). I judged <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">Timeless Decision Theory</a> on the basis of its long-term success, but each agent was stipulated to only care about its <em>immediate children</em>, not any further descendants! And indeed, the strategy of allowing free-riding defectors maximizes the number of an agent&#x27;s immediate children, albeit at the price of hampering future generations by cluttering the field with defectors.<a href=\"https://www.lesserwrong.com/lw/778/consequentialism/#foot1\">1</a></p><p>If instead we let the TDT agents care about their distant descendants, then they&#x27;ll crowd out the defectors by only cooperating when both other agents are TDT,<a href=\"https://www.lesserwrong.com/lw/778/consequentialism/#foot2\">2</a> and profit with a higher sustained growth rate once they form a supermajority. Not only do the TDTs with properly long-term decision theories beat out what I called DefectBots, but they get at least a fair fight against the carefully chosen simple algorithm I called CliqueBots. The paradox vanishes once you allow the agents to care about the long-term consequences of their choice.</p><p>Similarly, the purported <em>reductio</em>s of consequentialism rely on the following two tricks: they implicitly assume that consequentialists must care only about the <em>immediate</em> consequences of an action, or they implicitly assume that consequentialists must be causal decision theorists.<a href=\"https://www.lesserwrong.com/lw/778/consequentialism/#foot3\">3</a></p><p>Let&#x27;s consider one of the more famous examples, a dilemma posed by Judith Jarvis Thomson:</p><blockquote>A brilliant transplant surgeon has five patients, each in need of a different organ, each of whom will die without that organ. Unfortunately, there are no organs available to perform any of these five transplant operations. A healthy young traveler, just passing through the city the doctor works in, comes in for a routine checkup. In the course of doing the checkup, the doctor discovers that his organs are compatible with all five of his dying patients. Suppose further that if the young man were to disappear, no one would suspect the doctor.</blockquote><p></p><p>First, we can presume that the doctor cares about the welfare, not just of the five patients and the traveler, but of people more generally. If we drop the last supposition for a moment, it&#x27;s clear that a consequentialist utilitarian doctor shouldn&#x27;t kill the traveler for his organs; if word gets out that doctors do that sort of thing, then people will stay away from hospitals unless they&#x27;re either exceptional altruists or at the edge of death, and this will result in people being less healthy overall.<a href=\"https://www.lesserwrong.com/lw/778/consequentialism/#foot4\">4</a></p><p>But what if the doctor is confident of keeping it a secret? Well, then causal decision theory would indeed tell her to harvest his organs, but TDT (and also <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">UDT</a>) would strongly advise her against it. Because if TDT endorsed the action, then other people would be able to <em>deduce</em> that TDT endorsed the action, and that (whether or not it had happened in any particular case) their lives would be in danger in any hospital run by a timeless decision theorist, and then we&#x27;d be in much the same boat. Therefore TDT calculates that the correct thing for TDT to output in order to maximize utility is &quot;Don&#x27;t kill the traveler,&quot;<a href=\"https://www.lesserwrong.com/lw/778/consequentialism/#foot5\">5</a> and thus the doctor doesn&#x27;t kill the traveler.</p><p>The question that a good consequentialist ought to be asking themselves is not &quot;What happens in situation Y if I do X?&quot;, nor even &quot;What happens in general if I do X whenever I&#x27;m in situation Y&quot;, but &quot;What happens in general if everyone at least as smart as me deduces that I would do X whenever I&#x27;m in situation Y&quot;? That, rather than the others, is the full exploration of the effects of choosing X in situation Y, and not coincidentally it&#x27;s a colloquial version of Timeless Decision Theory. And as with <a href=\"http://en.wikipedia.org/wiki/Superrationality\">Hofstadter&#x27;s superrationality</a>, TDT and UDT will avoid contributing to tragedies of the commons so long as enough people subscribe to them (or base their own decisions on the extrapolations of TDT and UDT).</p><p>In general, I&#x27;d like to offer (without proof) the following rationalist ethical inequality:</p><p><strong>Your <em>true</em> valuation of <em>all</em> consequences + a good decision theory \u2265 any particular deontology.</strong></p><p>Now, a deontological rule might be <em>easier to calculate</em>, and work practically as well in the vast majority of circumstances (like approximating real physics with Newtonian mechanics). But if you have to deal with an edge case or something unfamiliar, you can get in trouble by persisting with the approximation; if you&#x27;re programming a GPS, you need relativity. And as rule utilitarians can point out, you need to get your deontological rules from somewhere; if it&#x27;s not from a careful consequentialist reckoning, then it might not be as trustworthy as it feels.<a href=\"https://www.lesserwrong.com/lw/778/consequentialism/#foot6\">6</a></p><p>Or it could be that <a href=\"http://wiki.lesswrong.com/wiki/Ethical_injunction\">particular deontological rules</a> are much more reliable for <a href=\"https://www.lesserwrong.com/lw/uv/ends_dont_justify_means_among_humans/\">running on corrupted hardware</a>, and that no amount of caution will prevent people from shooting themselves in the foot if they&#x27;re allowed to. That is a real concern, and it&#x27;s beyond the scope of this post. But what&#x27;s <em>actually right</em> probably doesn&#x27;t include a component of making oneself stupid with regard to the actual circumstances in order to prevent other parts of one&#x27;s mind from hijacking the decision. If we ever outgrow this hardware, we ought to leave the deontologies behind with it.</p><p><strong>Footnotes:</strong></p><p><strong>1.</strong> Note that the evolutionary setup is necessary to the &quot;paradox&quot;: if Omega dished out utils instead of children, then the short-term strategy is optimal in the long run too.</p><p><strong>2.</strong> This is only right in a heuristic sense. If the agents suspect Omega will be ending the game soon, or they have too high a temporal discount rate, this won&#x27;t work quite that way. Also, there&#x27;s an entire gamut of other decision theories that TDT could include in its circle of cooperators. That&#x27;s a good feature to have- the CliqueBots from the last post, by contrast, declare war on every other decision theory, and <a href=\"https://www.lesserwrong.com/lw/767/decision_theory_paradox_pd_with_three_implies/4pnn\">this costs them relative to TDT in a more mixed population</a> (thanks to Jack for the example).</p><p><strong>3.</strong> One more implicit assumption about consequentialism is the false dichotomy that consequentialists must choose either to be perfectly altruistic utilitarians or perfectly selfish hedonists, with no middle ground for caring about oneself and others to different positive degrees. Oddly enough, few people object to the <a href=\"http://en.wikipedia.org/wiki/Demandingness_objection\">deontological rules we&#x27;ve developed</a> to avoid helping distant others without incurring guilt.</p><p><strong>4.</strong> I&#x27;m assuming that in the world of the thought experiment, it&#x27;s good for your health to see a doctor for check-ups and when you&#x27;re ill. It&#x27;s a different question <a href=\"http://www.overcomingbias.com/2011/02/how-med-harms.html\">whether that hypothetical holds in the real world</a>. Also, while my reply is vulnerable to a <a href=\"http://wiki.lesswrong.com/wiki/Least_convenient_possible_world\">least convenient possible world</a> objection, I honestly have no idea how my moral intuitions should translate to a world where (say) people genuinely <em>didn&#x27;t mind</em> knowing that doctors might do this as long as it maximized the lives saved.</p><p><strong>5.</strong> The sort of epistemic advantage that would be necessary for TDT to conclude otherwise is implausible for a human being, and even in that case, there are decision theories like UDT that would <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">refuse nonetheless</a> (for the sake of other worlds where people suspected doctors of having such an epistemic advantage).</p><p><strong>6.</strong> The reason that morality feels like deontology to us is an evolutionary one: if you haven&#x27;t yet <a href=\"https://www.lesserwrong.com/lw/l2/protein_reinforcement_and_dna_consequentialism/\">built an excellent consequentialist</a> with a proper decision theory, then hard-coded rules are much more reliable than explicit reasoning.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZTRNmvQGgoYiymYnq": 3, "dPPATLhRmhdJtJM2t": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "prb8raC4XGJiRWs5n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": 81, "extendedScore": null, "score": 0.000165, "legacy": true, "legacyId": "9332", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 81, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em><strong>Summary:</strong> If you object to consequentialist ethical theories because you think they endorse horrible or catastrophic decisions, then you may instead be objecting to short-sighted utility functions or poor decision theories.</em></p><p><strong>Recommended:</strong> <a href=\"https://www.lesserwrong.com/lw/767/decision_theory_paradox_pd_with_three_implies/\">Decision Theory Paradox: PD with Three Implies Chaos?</a></p><p><strong>Related:</strong> <a href=\"https://www.lesserwrong.com/lw/n9/the_intuitions_behind_utilitarianism/\">The \"Intuitions\" Behind \"Utilitarianism\"</a></p><p>The simple idea that we ought to choose actions according to their probable consequences, ever since it was formulated, has garnered a rather shocking amount of dissent. Part of this may be due to <a href=\"https://www.lesserwrong.com/lw/74f/are_deontological_moral_judgments_rationalizations/\">causes other than philosophical objections</a>, and some of the objections get into the metaphysics of metaethics. But there's a fair amount of opposition on rather simple grounds: that consequentialist reasoning appears to endorse bad decisions, either in the long run or as an effect of collective action.</p><p>Every so often, you'll hear someone offer a <em>reductio ad absurdum</em> of the following form: \"Consider dilemma X. If we were consequentialists, then we would be forced to choose Y. But in the long run (or if widely adopted) the strategy of choosing Y leads to horrible consequence Z, and so consequentialism fails on its own terms.\"</p><p>There's something fishy about the argument when you lay it out like that: if it can be known that the strategy of choosing Y has horrible consequence Z, then why do we agree that consequentialists choose Y? In fact, there are two further unstated assumptions in every such argument I've heard, and it is those assumptions rather than consequentialism on which the absurdity really falls. But to discuss the assumptions, we need to delve into a bit of <a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">decision theory</a>.</p><p>In <a href=\"https://www.lesserwrong.com/lw/767/decision_theory_paradox_pd_with_three_implies/\">my last post</a>, I posed an apparent paradox: a case where it looked as if a simple rule could trump the most rational of decision theories in a fair fight. But there was a sleight of hand involved (which, to your credit, many of you spotted immediately). I judged <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">Timeless Decision Theory</a> on the basis of its long-term success, but each agent was stipulated to only care about its <em>immediate children</em>, not any further descendants! And indeed, the strategy of allowing free-riding defectors maximizes the number of an agent's immediate children, albeit at the price of hampering future generations by cluttering the field with defectors.<a href=\"https://www.lesserwrong.com/lw/778/consequentialism/#foot1\">1</a></p><p>If instead we let the TDT agents care about their distant descendants, then they'll crowd out the defectors by only cooperating when both other agents are TDT,<a href=\"https://www.lesserwrong.com/lw/778/consequentialism/#foot2\">2</a> and profit with a higher sustained growth rate once they form a supermajority. Not only do the TDTs with properly long-term decision theories beat out what I called DefectBots, but they get at least a fair fight against the carefully chosen simple algorithm I called CliqueBots. The paradox vanishes once you allow the agents to care about the long-term consequences of their choice.</p><p>Similarly, the purported <em>reductio</em>s of consequentialism rely on the following two tricks: they implicitly assume that consequentialists must care only about the <em>immediate</em> consequences of an action, or they implicitly assume that consequentialists must be causal decision theorists.<a href=\"https://www.lesserwrong.com/lw/778/consequentialism/#foot3\">3</a></p><p>Let's consider one of the more famous examples, a dilemma posed by Judith Jarvis Thomson:</p><blockquote>A brilliant transplant surgeon has five patients, each in need of a different organ, each of whom will die without that organ. Unfortunately, there are no organs available to perform any of these five transplant operations. A healthy young traveler, just passing through the city the doctor works in, comes in for a routine checkup. In the course of doing the checkup, the doctor discovers that his organs are compatible with all five of his dying patients. Suppose further that if the young man were to disappear, no one would suspect the doctor.</blockquote><p></p><p>First, we can presume that the doctor cares about the welfare, not just of the five patients and the traveler, but of people more generally. If we drop the last supposition for a moment, it's clear that a consequentialist utilitarian doctor shouldn't kill the traveler for his organs; if word gets out that doctors do that sort of thing, then people will stay away from hospitals unless they're either exceptional altruists or at the edge of death, and this will result in people being less healthy overall.<a href=\"https://www.lesserwrong.com/lw/778/consequentialism/#foot4\">4</a></p><p>But what if the doctor is confident of keeping it a secret? Well, then causal decision theory would indeed tell her to harvest his organs, but TDT (and also <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">UDT</a>) would strongly advise her against it. Because if TDT endorsed the action, then other people would be able to <em>deduce</em> that TDT endorsed the action, and that (whether or not it had happened in any particular case) their lives would be in danger in any hospital run by a timeless decision theorist, and then we'd be in much the same boat. Therefore TDT calculates that the correct thing for TDT to output in order to maximize utility is \"Don't kill the traveler,\"<a href=\"https://www.lesserwrong.com/lw/778/consequentialism/#foot5\">5</a> and thus the doctor doesn't kill the traveler.</p><p>The question that a good consequentialist ought to be asking themselves is not \"What happens in situation Y if I do X?\", nor even \"What happens in general if I do X whenever I'm in situation Y\", but \"What happens in general if everyone at least as smart as me deduces that I would do X whenever I'm in situation Y\"? That, rather than the others, is the full exploration of the effects of choosing X in situation Y, and not coincidentally it's a colloquial version of Timeless Decision Theory. And as with <a href=\"http://en.wikipedia.org/wiki/Superrationality\">Hofstadter's superrationality</a>, TDT and UDT will avoid contributing to tragedies of the commons so long as enough people subscribe to them (or base their own decisions on the extrapolations of TDT and UDT).</p><p>In general, I'd like to offer (without proof) the following rationalist ethical inequality:</p><p><strong id=\"Your_true_valuation_of_all_consequences___a_good_decision_theory___any_particular_deontology_\">Your <em>true</em> valuation of <em>all</em> consequences + a good decision theory \u2265 any particular deontology.</strong></p><p>Now, a deontological rule might be <em>easier to calculate</em>, and work practically as well in the vast majority of circumstances (like approximating real physics with Newtonian mechanics). But if you have to deal with an edge case or something unfamiliar, you can get in trouble by persisting with the approximation; if you're programming a GPS, you need relativity. And as rule utilitarians can point out, you need to get your deontological rules from somewhere; if it's not from a careful consequentialist reckoning, then it might not be as trustworthy as it feels.<a href=\"https://www.lesserwrong.com/lw/778/consequentialism/#foot6\">6</a></p><p>Or it could be that <a href=\"http://wiki.lesswrong.com/wiki/Ethical_injunction\">particular deontological rules</a> are much more reliable for <a href=\"https://www.lesserwrong.com/lw/uv/ends_dont_justify_means_among_humans/\">running on corrupted hardware</a>, and that no amount of caution will prevent people from shooting themselves in the foot if they're allowed to. That is a real concern, and it's beyond the scope of this post. But what's <em>actually right</em> probably doesn't include a component of making oneself stupid with regard to the actual circumstances in order to prevent other parts of one's mind from hijacking the decision. If we ever outgrow this hardware, we ought to leave the deontologies behind with it.</p><p><strong id=\"Footnotes_\">Footnotes:</strong></p><p><strong>1.</strong> Note that the evolutionary setup is necessary to the \"paradox\": if Omega dished out utils instead of children, then the short-term strategy is optimal in the long run too.</p><p><strong>2.</strong> This is only right in a heuristic sense. If the agents suspect Omega will be ending the game soon, or they have too high a temporal discount rate, this won't work quite that way. Also, there's an entire gamut of other decision theories that TDT could include in its circle of cooperators. That's a good feature to have- the CliqueBots from the last post, by contrast, declare war on every other decision theory, and <a href=\"https://www.lesserwrong.com/lw/767/decision_theory_paradox_pd_with_three_implies/4pnn\">this costs them relative to TDT in a more mixed population</a> (thanks to Jack for the example).</p><p><strong>3.</strong> One more implicit assumption about consequentialism is the false dichotomy that consequentialists must choose either to be perfectly altruistic utilitarians or perfectly selfish hedonists, with no middle ground for caring about oneself and others to different positive degrees. Oddly enough, few people object to the <a href=\"http://en.wikipedia.org/wiki/Demandingness_objection\">deontological rules we've developed</a> to avoid helping distant others without incurring guilt.</p><p><strong>4.</strong> I'm assuming that in the world of the thought experiment, it's good for your health to see a doctor for check-ups and when you're ill. It's a different question <a href=\"http://www.overcomingbias.com/2011/02/how-med-harms.html\">whether that hypothetical holds in the real world</a>. Also, while my reply is vulnerable to a <a href=\"http://wiki.lesswrong.com/wiki/Least_convenient_possible_world\">least convenient possible world</a> objection, I honestly have no idea how my moral intuitions should translate to a world where (say) people genuinely <em>didn't mind</em> knowing that doctors might do this as long as it maximized the lives saved.</p><p><strong>5.</strong> The sort of epistemic advantage that would be necessary for TDT to conclude otherwise is implausible for a human being, and even in that case, there are decision theories like UDT that would <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">refuse nonetheless</a> (for the sake of other worlds where people suspected doctors of having such an epistemic advantage).</p><p><strong>6.</strong> The reason that morality feels like deontology to us is an evolutionary one: if you haven't yet <a href=\"https://www.lesserwrong.com/lw/l2/protein_reinforcement_and_dna_consequentialism/\">built an excellent consequentialist</a> with a proper decision theory, then hard-coded rules are much more reliable than explicit reasoning.</p>", "sections": [{"title": "Your true valuation of all consequences + a good decision theory \u2265 any particular deontology.", "anchor": "Your_true_valuation_of_all_consequences___a_good_decision_theory___any_particular_deontology_", "level": 1}, {"title": "Footnotes:", "anchor": "Footnotes_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "120 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 120, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HT8jwNJ6vH7p9gaTT", "r5MSQ83gtbjWRBDWJ", "62p74DvwNHgQXCXcH", "K9ZaZXDnL3SEmYZqB", "gTNB9CQd5hnbkMxAG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-02T07:38:10.556Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes September 2011", "slug": "rationality-quotes-september-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:09.651Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dvasya", "createdAt": "2011-03-08T00:30:12.369Z", "isAdmin": false, "displayName": "dvasya"}, "userId": "2484AHxytrNyQXajh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RpQam9yPw3SwByMEv/rationality-quotes-september-2011", "pageUrlRelative": "/posts/RpQam9yPw3SwByMEv/rationality-quotes-september-2011", "linkUrl": "https://www.lesswrong.com/posts/RpQam9yPw3SwByMEv/rationality-quotes-september-2011", "postedAtFormatted": "Friday, September 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20September%202011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20September%202011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRpQam9yPw3SwByMEv%2Frationality-quotes-september-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20September%202011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRpQam9yPw3SwByMEv%2Frationality-quotes-september-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRpQam9yPw3SwByMEv%2Frationality-quotes-september-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Here's the new thread for posting quotes, with the usual rules:</p>\n<ul style=\"padding: 0px;\">\n<li>Please post all quotes separately, so that they can be voted up/down separately.&nbsp;&nbsp;(If they are strongly related, reply to your own comments.&nbsp;&nbsp;If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments/posts on LW/OB.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>\n</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RpQam9yPw3SwByMEv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 7.63595321415596e-07, "legacy": true, "legacyId": "9577", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 492, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-02T08:55:04.487Z", "modifiedAt": null, "url": null, "title": "Mini-camp was indeed awesome, and so was Luke (just add Bayes)", "slug": "mini-camp-was-indeed-awesome-and-so-was-luke-just-add-bayes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.984Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ysm6iXycJaAgw62bN/mini-camp-was-indeed-awesome-and-so-was-luke-just-add-bayes", "pageUrlRelative": "/posts/Ysm6iXycJaAgw62bN/mini-camp-was-indeed-awesome-and-so-was-luke-just-add-bayes", "linkUrl": "https://www.lesswrong.com/posts/Ysm6iXycJaAgw62bN/mini-camp-was-indeed-awesome-and-so-was-luke-just-add-bayes", "postedAtFormatted": "Friday, September 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mini-camp%20was%20indeed%20awesome%2C%20and%20so%20was%20Luke%20(just%20add%20Bayes)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMini-camp%20was%20indeed%20awesome%2C%20and%20so%20was%20Luke%20(just%20add%20Bayes)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYsm6iXycJaAgw62bN%2Fmini-camp-was-indeed-awesome-and-so-was-luke-just-add-bayes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mini-camp%20was%20indeed%20awesome%2C%20and%20so%20was%20Luke%20(just%20add%20Bayes)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYsm6iXycJaAgw62bN%2Fmini-camp-was-indeed-awesome-and-so-was-luke-just-add-bayes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYsm6iXycJaAgw62bN%2Fmini-camp-was-indeed-awesome-and-so-was-luke-just-add-bayes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 984, "htmlBody": "<p>Yep, I'm saying that without hard data. &nbsp;But I was there. &nbsp;So let me say it again, in response to numerous comments I've seen complaining that no judgement should be passed until a quantitative analysis confirms it:</p>\n<p><strong>Mini-camp was awesome.</strong>&nbsp; Note that mini-camp was far from the first time I've travelled to an event to surround myself with like-minded peers working toward common goals... &nbsp;I find such events events extremely motivating and enjoyable, which is why I've been to&nbsp;many such workshops, inside and outside academia (~3 per year for the past 10 years).</p>\n<p>Yet mini-camp is still topping my charts. &nbsp;Specifically, the camp is tied for the title of the most life-altering workshop-like event of my life, and the tie is with the workshop that got me onto my PhD topic (graphical causal modelling), so that's saying something.</p>\n<p>In particular, I've been <strong>visibly-to-myself-and-others</strong> <strong>more motivated</strong> and <strong>hard-working</strong> since the camp. &nbsp;I've had more <strong>energy</strong> for <strong>learning and adaptation</strong>, and I find <strong>Luke</strong> to have been a highly inspiring input to that result.</p>\n<p>(I'm talking about <strong>Luke</strong> because his position is the one being discussed right now, but I got a lot of really inspiring ideas and motivation from <strong>Anna</strong> before, during, and after the camp as well.)</p>\n<p>Hard data will be great to have, but it's hard to get,&nbsp;especially certifiably causal data (though the prospect is not hopeless, with enough conditional independence tests),&nbsp;<em>especially</em> since the camp was planned and executed on short notice. &nbsp;</p>\n<p>In the meantime, let's do a little Bayes. &nbsp;First, assign priors to how well you expect a week-long sustained interaction between growth-oriented rationalists to go. &nbsp;(If your prior is something like 80%[failure], I'd like to know where you're getting your growth-oriented rationalists). &nbsp;Now <strong>which of the following theories, \"failure\" or \"success\", assigns a higher likelihood to the following observations?</strong></p>\n<p><strong>-----</strong></p>\n<p><strong>1.</strong>&nbsp;People wrote these:&nbsp;</p>\n<p><a href=\"https://docs.google.com/spreadsheet/pub?key=0AnoM_ZsIBBwEdGNicUMzRkNJNzRKLVpEb2RxZzU3V0E\">https://docs.google.com/spreadsheet/pub?key=0AnoM_ZsIBBwEdGNicUMzRkNJNzRKLVpEb2RxZzU3V0E</a></p>\n<p>In particular,&nbsp;</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 16px; background-color: #f7f7f8;\"> </span></p>\n<blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-left: 30px; \">&ldquo;The week I spent in minicamp had by far the highest density of fun and learning I have ever experienced. It's like taking two years of college and condensing it to a week: you learn just as much and you have just as much fun. The skills I've learned will help me set and achieve my own life goal, and the friends I've made will help me get there.&rdquo; --<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"/user/alexei\">Alexei</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-left: 30px; \">&ldquo;This was an intensely positive experience. This was easily the most powerful change self-modification I've ever made, in all of the social, intellectual, and emotional spheres. I'm now a more powerful person than I was a week ago -- and I can explain exactly how and why this is true.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-left: 30px; \">At mini-camp, I've learned techniques for effective self-modification -- that is, I have a much deeper understanding of how to change my desires, gather my willpower, channel my time and cognitive resources, and model and handle previously confusing situations. What's more, I have a fairly clear map of how to build these skills henceforth, and how to inculcate them in others. And all this was presented in such a way that any sufficiently analytical folk -- anyone who has understood a few of the LW sequences, say -- can gain in extreme measures.&rdquo; --Matt Elder /&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"/user/Fiddlemath\">Fiddlemath</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-left: 30px; \">&ldquo;I expected a week of interesting things and some useful tools to take away. What I got was 8 days of constant, deep learning, challenges to my limits that helped me grow. I finally grokked that I can and should optimize myself on every dimension I care about, that practice and reinforcement can make me a better thinker, and that I can change very quickly when I'm not constrained by artificial barriers or stress.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-left: 30px; \">I would not recommend doing something like this right before another super-busy week, because I was learning at 100% of capacity and will need a lot of time to unpack all the things I learned and apply them to my life, but I came away with a clear plan for becoming better. It is now a normal and easy thing for me to try things out, test my beliefs, and self-improve. And I'm likely to be much more effective at making the world a better place as well, by prioritizing without fear.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-left: 30px; \">The material was all soundly-researched and effectively taught, with extremely helpful supplemental exercises and activities. The instructors were very helpful in and out of session. The other participants were excited, engaged, challenging, and supportive.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-left: 30px; \">I look forward to sharing what I've learned with my local Lesswrong meetup and others in the area. If that's even 1/4 as awesome as my time at the Mini-Camp, it will make our lives&nbsp;<em>much</em>&nbsp;better.&rdquo; --Ben Hoffman /&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"/user/Benquo\">Benquo</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-left: 30px; \">&ldquo;I really can't recommend this camp enough! This workshop broke down a complex and intertwined set of skills labelled in my brain as \"common sense\" and distinguished each part so that I could work on them separately. Sessions on motivation, cognition, and what habits to build to not fool yourself were particularly helpful. This camp was also the first example that I've seen of people taking current cognitive science and other research, decoding it, and showing people what's been documented to work so that they can use it too. It feels to me now as though the coolest parts of the sequences have been given specific exercises and habits to build off of. This camp, and the people in it, have changed my path for the better.&rdquo; --David Jones /&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"/user/TheDave\">TheDave</a></p>\n</blockquote>\n<p style=\"padding-left: 30px; \">&nbsp;</p>\n<p><strong>2.</strong> I wrote this post.</p>\n<p><strong>3.</strong> Eliezer wants to keep Luke as a permanent hire.</p>\n<p><strong>4.</strong> Whatever other comments you've seen/heard about the camp from people who attended.</p>\n<p>-----</p>\n<p>Is this a biased sample? &nbsp;Probably. &nbsp;Is it hard data? &nbsp;Easy to quantify? &nbsp;Not so much. &nbsp;Might this be a big conspiracy by Luke-originating ninja bloggers? &nbsp;Perhaps. &nbsp;But really... which theory assigns the higher likelihood here? &nbsp;Success, or failure?</p>\n<p>Lets allow the arguments that <em>can</em>&nbsp;be made about the minicamp <em>be </em>made, rather than ritualistically abstaining from decision-making until numbers show up.</p>\n<p>That, and I really hope Luke stays with SingInst :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ysm6iXycJaAgw62bN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 4, "extendedScore": null, "score": 7.63620545868486e-07, "legacy": true, "legacyId": "9611", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-02T13:38:44.725Z", "modifiedAt": null, "url": null, "title": "Proposal: Rationality Quotes Thread With Attributions in rot13", "slug": "proposal-rationality-quotes-thread-with-attributions-in", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:23.028Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lessdazed", "createdAt": "2011-02-02T05:06:52.010Z", "isAdmin": false, "displayName": "lessdazed"}, "userId": "ehZzKt5ByYBeyCLkz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZzLp6cqYxSetBed5J/proposal-rationality-quotes-thread-with-attributions-in", "pageUrlRelative": "/posts/ZzLp6cqYxSetBed5J/proposal-rationality-quotes-thread-with-attributions-in", "linkUrl": "https://www.lesswrong.com/posts/ZzLp6cqYxSetBed5J/proposal-rationality-quotes-thread-with-attributions-in", "postedAtFormatted": "Friday, September 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proposal%3A%20Rationality%20Quotes%20Thread%20With%20Attributions%20in%20rot13&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProposal%3A%20Rationality%20Quotes%20Thread%20With%20Attributions%20in%20rot13%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZzLp6cqYxSetBed5J%2Fproposal-rationality-quotes-thread-with-attributions-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proposal%3A%20Rationality%20Quotes%20Thread%20With%20Attributions%20in%20rot13%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZzLp6cqYxSetBed5J%2Fproposal-rationality-quotes-thread-with-attributions-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZzLp6cqYxSetBed5J%2Fproposal-rationality-quotes-thread-with-attributions-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 26, "htmlBody": "<p>To judge quotes on their own merits, if without some context, I propose an experimental thread in which the original authors of quotes are somehow hidden.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1, "MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZzLp6cqYxSetBed5J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 30, "extendedScore": null, "score": 7e-05, "legacy": true, "legacyId": "9613", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-02T17:22:44.526Z", "modifiedAt": null, "url": null, "title": "[Link] The Typical Mind Fallacy, Illustrated", "slug": "link-the-typical-mind-fallacy-illustrated", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:21.856Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pjeby", "createdAt": "2009-02-27T23:51:22.854Z", "isAdmin": false, "displayName": "pjeby"}, "userId": "Zzxr5JZpkitaNxL4Q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ggDT45Rzq67W2SuS4/link-the-typical-mind-fallacy-illustrated", "pageUrlRelative": "/posts/ggDT45Rzq67W2SuS4/link-the-typical-mind-fallacy-illustrated", "linkUrl": "https://www.lesswrong.com/posts/ggDT45Rzq67W2SuS4/link-the-typical-mind-fallacy-illustrated", "postedAtFormatted": "Friday, September 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20The%20Typical%20Mind%20Fallacy%2C%20Illustrated&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20The%20Typical%20Mind%20Fallacy%2C%20Illustrated%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FggDT45Rzq67W2SuS4%2Flink-the-typical-mind-fallacy-illustrated%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20The%20Typical%20Mind%20Fallacy%2C%20Illustrated%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FggDT45Rzq67W2SuS4%2Flink-the-typical-mind-fallacy-illustrated", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FggDT45Rzq67W2SuS4%2Flink-the-typical-mind-fallacy-illustrated", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<p>If you've ever wanted a quick way to explain the <a title=\"LW Wiki: Typical Mind Fallacy\" href=\"http://wiki.lesswrong.com/wiki/Typical_mind_fallacy\">Typical Mind Fallacy</a>, this <a title=\"Subnormality #186 - We Assume Of Others What We Know Of Ourselves\" href=\"http://www.viruscomix.com/page553.html\">cartoon by Winston Rowntree</a> will do the trick nicely.</p>\n<p>(Panel 3 seems of particular relevance to LessWrongians, and the last two panels also remind me of Hufflepuff/Slytherin themes in HP:MOR.)</p>\n<p>Enjoy!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ggDT45Rzq67W2SuS4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 25, "extendedScore": null, "score": 5.4e-05, "legacy": true, "legacyId": "9615", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-02T19:29:31.883Z", "modifiedAt": null, "url": null, "title": "[Link] The mismeasure of morals: Antisocial personality traits predict utilitarian responses to moral dilemmas", "slug": "link-the-mismeasure-of-morals-antisocial-personality-traits", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CRunuBasu3ugeqftq/link-the-mismeasure-of-morals-antisocial-personality-traits", "pageUrlRelative": "/posts/CRunuBasu3ugeqftq/link-the-mismeasure-of-morals-antisocial-personality-traits", "linkUrl": "https://www.lesswrong.com/posts/CRunuBasu3ugeqftq/link-the-mismeasure-of-morals-antisocial-personality-traits", "postedAtFormatted": "Friday, September 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20The%20mismeasure%20of%20morals%3A%20Antisocial%20personality%20traits%20predict%20utilitarian%20responses%20to%20moral%20dilemmas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20The%20mismeasure%20of%20morals%3A%20Antisocial%20personality%20traits%20predict%20utilitarian%20responses%20to%20moral%20dilemmas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCRunuBasu3ugeqftq%2Flink-the-mismeasure-of-morals-antisocial-personality-traits%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20The%20mismeasure%20of%20morals%3A%20Antisocial%20personality%20traits%20predict%20utilitarian%20responses%20to%20moral%20dilemmas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCRunuBasu3ugeqftq%2Flink-the-mismeasure-of-morals-antisocial-personality-traits", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCRunuBasu3ugeqftq%2Flink-the-mismeasure-of-morals-antisocial-personality-traits", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<blockquote>\n<p>Researchers have recently argued that utilitarianism is the appropriate framework by which to evaluate moral judgment, and that individuals who endorse non-utilitarian solutions to moral dilemmas (involving active vs. passive harm) are committing an error. We report a study in which participants responded to a battery of personality assessments and a set of dilemmas that pit utilitarian and non-utilitarian options against each other. Participants who indicated greater endorsement of utilitarian solutions had higher scores on measures of Psychopathy, machiavellianism, and life meaninglessness. These results question the widely-used methods by which lay moral judgments are evaluated, as these approaches lead to the counterintuitive conclusion that those individuals who are least prone to moral errors also possess a set of psychological characteristics that many would consider prototypically immoral.</p>\n</blockquote>\n<p>Bartels,  D., &amp; Pizarro, D. (2011). The mismeasure of morals: Antisocial  personality traits predict utilitarian responses to moral dilemmas <em>Cognition, 121</em> (1), 154-161 DOI: <a rev=\"review\" href=\"http://dx.doi.org/10.1016/j.cognition.2011.05.010\">10.1016/j.cognition.2011.05.010</a></p>\n<p>via <a href=\"http://charbonniers.org/2011/09/01/is-and-ought/\">charbonniers.org/2011/09/01/is-and-ought/<br /></a></p>\n<p><a href=\"http://charbonniers.org/2011/09/01/is-and-ought/\"> </a></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CRunuBasu3ugeqftq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "9616", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-02T23:48:09.248Z", "modifiedAt": null, "url": null, "title": "What are good topics for literature review prizes?", "slug": "what-are-good-topics-for-literature-review-prizes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.689Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8LfkhgWkqp2gTqug4/what-are-good-topics-for-literature-review-prizes", "pageUrlRelative": "/posts/8LfkhgWkqp2gTqug4/what-are-good-topics-for-literature-review-prizes", "linkUrl": "https://www.lesswrong.com/posts/8LfkhgWkqp2gTqug4/what-are-good-topics-for-literature-review-prizes", "postedAtFormatted": "Friday, September 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20good%20topics%20for%20literature%20review%20prizes%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20good%20topics%20for%20literature%20review%20prizes%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8LfkhgWkqp2gTqug4%2Fwhat-are-good-topics-for-literature-review-prizes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20good%20topics%20for%20literature%20review%20prizes%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8LfkhgWkqp2gTqug4%2Fwhat-are-good-topics-for-literature-review-prizes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8LfkhgWkqp2gTqug4%2Fwhat-are-good-topics-for-literature-review-prizes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<p>The last prize, for the best literature review of spaced repetition, was <a href=\"/lw/76h/spaced_repetition_literature_review_prize_and_the/\">moderately successful</a>, inspiring a pretty good review of the academic literature on spaced repetition.&nbsp;I am interested in experimenting more with prizes, but I would like to get input other people's input: <em>what are other good topics for future prizes?</em></p>\n<p>The topic should be:</p>\n<ol>\n<li>Well defined.</li>\n<li>Not too big. Something someone could understand pretty well in two weeks.&nbsp;</li>\n<li>Academically researched.</li>\n<li>Relevant to being effective.</li>\n</ol>\n<div>I am also open to projects besides literature reviews but literature reviews seem the most attractive to me right now.&nbsp;</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8LfkhgWkqp2gTqug4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 7.639117700319641e-07, "legacy": true, "legacyId": "9617", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eD6TZm2r25HzYzZzY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-03T00:39:25.717Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Conjunction Controversy (Or, How They Nail It Down)", "slug": "seq-rerun-conjunction-controversy-or-how-they-nail-it-down", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:22.417Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xC8LepcDidosrvgjp/seq-rerun-conjunction-controversy-or-how-they-nail-it-down", "pageUrlRelative": "/posts/xC8LepcDidosrvgjp/seq-rerun-conjunction-controversy-or-how-they-nail-it-down", "linkUrl": "https://www.lesswrong.com/posts/xC8LepcDidosrvgjp/seq-rerun-conjunction-controversy-or-how-they-nail-it-down", "postedAtFormatted": "Saturday, September 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Conjunction%20Controversy%20(Or%2C%20How%20They%20Nail%20It%20Down)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Conjunction%20Controversy%20(Or%2C%20How%20They%20Nail%20It%20Down)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxC8LepcDidosrvgjp%2Fseq-rerun-conjunction-controversy-or-how-they-nail-it-down%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Conjunction%20Controversy%20(Or%2C%20How%20They%20Nail%20It%20Down)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxC8LepcDidosrvgjp%2Fseq-rerun-conjunction-controversy-or-how-they-nail-it-down", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxC8LepcDidosrvgjp%2Fseq-rerun-conjunction-controversy-or-how-they-nail-it-down", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 227, "htmlBody": "<p>Today's post, <a href=\"/lw/jj/conjunction_controversy_or_how_they_nail_it_down/\">Conjunction Controversy (Or, How They Nail It Down)</a> was originally published on 20 September 2007. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>When it seems like an experiment that's been cited does not provide enough support for the interpretation given, remember that Scientists are generally pretty smart. Especially if the experiment was done a long time ago, or it is described as \"classic\" or \"famous\". In that case, you should consider the possibility that there is more evidence that you haven't seen. Instead of saying \"This experiment could also be interpreted in this way\", ask \"How did they distinguish this interpretation from ________________?\"</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/r/discussion/lw/7dl/seq_rerun_conjunction_fallacy/\">Conjunction Fallacy</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xC8LepcDidosrvgjp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 7.639284960855348e-07, "legacy": true, "legacyId": "9618", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cXzTpSiCrNGzeoRAz", "dpN7tCqdwigjQKt6B", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-03T02:56:14.354Z", "modifiedAt": null, "url": null, "title": "A prize to become artist in residence at CERN", "slug": "a-prize-to-become-artist-in-residence-at-cern", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:22.362Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TD3gm2PbzjFrtapp3/a-prize-to-become-artist-in-residence-at-cern", "pageUrlRelative": "/posts/TD3gm2PbzjFrtapp3/a-prize-to-become-artist-in-residence-at-cern", "linkUrl": "https://www.lesswrong.com/posts/TD3gm2PbzjFrtapp3/a-prize-to-become-artist-in-residence-at-cern", "postedAtFormatted": "Saturday, September 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20prize%20to%20become%20artist%20in%20residence%20at%20CERN&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20prize%20to%20become%20artist%20in%20residence%20at%20CERN%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTD3gm2PbzjFrtapp3%2Fa-prize-to-become-artist-in-residence-at-cern%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20prize%20to%20become%20artist%20in%20residence%20at%20CERN%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTD3gm2PbzjFrtapp3%2Fa-prize-to-become-artist-in-residence-at-cern", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTD3gm2PbzjFrtapp3%2Fa-prize-to-become-artist-in-residence-at-cern", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 119, "htmlBody": "<p><a title=\"Artist in Residence CERN\" href=\"http://www.aec.at/collide/\" target=\"_blank\">http://www.aec.at/collide/</a></p>\n<p>Prix Ars Electronica Collide@CERN is the new international competition for digital artists to win a residency at CERN the world's largest particle physics laboratory in Geneva. It is the first prize to be announced as part of the new Collide@CERN artists residency programme initiated by the laboratory.</p>\n<p>The residency is in two parts - with an initial two months at CERN, where the winning artist will have a specially dedicated science mentor from the world famous science lab to inspire him/her and his/her work. The second part will be a month with the Futurelab team and mentor at Ars Electronica Linz with whom the winner will develop and make new work inspired by the CERN residency.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TD3gm2PbzjFrtapp3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 2, "extendedScore": null, "score": 7.63973127675015e-07, "legacy": true, "legacyId": "9624", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-03T05:14:27.111Z", "modifiedAt": null, "url": null, "title": "Impact of India-Pakistan nuclear war on x-risk?", "slug": "impact-of-india-pakistan-nuclear-war-on-x-risk", "viewCount": null, "lastCommentedAt": "2019-06-01T11:14:50.174Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "multifoliaterose", "createdAt": "2010-06-13T08:56:10.885Z", "isAdmin": false, "displayName": "multifoliaterose"}, "userId": "747HfTZFyfTqGyoPM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K6QwfaGAMJPvuX9Cg/impact-of-india-pakistan-nuclear-war-on-x-risk", "pageUrlRelative": "/posts/K6QwfaGAMJPvuX9Cg/impact-of-india-pakistan-nuclear-war-on-x-risk", "linkUrl": "https://www.lesswrong.com/posts/K6QwfaGAMJPvuX9Cg/impact-of-india-pakistan-nuclear-war-on-x-risk", "postedAtFormatted": "Saturday, September 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Impact%20of%20India-Pakistan%20nuclear%20war%20on%20x-risk%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AImpact%20of%20India-Pakistan%20nuclear%20war%20on%20x-risk%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK6QwfaGAMJPvuX9Cg%2Fimpact-of-india-pakistan-nuclear-war-on-x-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Impact%20of%20India-Pakistan%20nuclear%20war%20on%20x-risk%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK6QwfaGAMJPvuX9Cg%2Fimpact-of-india-pakistan-nuclear-war-on-x-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK6QwfaGAMJPvuX9Cg%2Fimpact-of-india-pakistan-nuclear-war-on-x-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<p>Last month I was involved in a <a href=\"/lw/6w3/the_125000_summer_singularity_challenge/4kuz\">conversation thread</a> about what the impact of a hypothetical nuclear war would be on existential risk.</p>\n<p>There are many potential nuclear war scenarios which would have varying impacts on existential risk. It's difficult to know where to start to gain an understanding of the long-term of nuclear proliferation.</p>\n<p>For concreteness, consider the case of an India-Pakistan nuclear war.</p>\n<p>According to <a href=\"http://climate.envsci.rutgers.edu/pdf/RobockToonSciAmJan2010.pdf\">Local Nuclear War, Global Suffering</a> by Robock and Toon,</p>\n<blockquote>\n<p>India and Pakistan, long at odds, have more than 50 nuclear warheads apiece; if each country dropped that many bombs on cities and industrial areas, the smoke from fires would stunt agriculture worldwide for 10 years.&nbsp;</p>\n</blockquote>\n<p>[...]</p>\n<blockquote>\n<p>1 billion people worldwide with marginal food supplies today could die of starvation because of ensuing agricultural collapse</p>\n</blockquote>\n<p>Note that this would presumably cause some degree of chaos in the developed world.</p>\n<p>I have not yet investigated the credibility of the papers' claims. However,</p>\n<p><strong>Suppose that an all-out nuclear war between India and Pakistan were to occur and were to result in climate change killing 1 billion people. Then would the probability of a positive singularity increase or decrease and if so why?</strong></p>\n<p>This question seems very difficult to answer; maybe altogether too difficult for humans to answer. I welcome responses raising relevant considerations even in absence of a good way to compare the relevant considerations. Please read the linked conversation thread before commentating.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K6QwfaGAMJPvuX9Cg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 7.640180634685024e-07, "legacy": true, "legacyId": "9628", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-03T11:56:05.166Z", "modifiedAt": null, "url": null, "title": "Gender differences in spatial reasoning appear to be nurture", "slug": "gender-differences-in-spatial-reasoning-appear-to-be-nurture", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:03.043Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/k8piGeE26FrtqAj9z/gender-differences-in-spatial-reasoning-appear-to-be-nurture", "pageUrlRelative": "/posts/k8piGeE26FrtqAj9z/gender-differences-in-spatial-reasoning-appear-to-be-nurture", "linkUrl": "https://www.lesswrong.com/posts/k8piGeE26FrtqAj9z/gender-differences-in-spatial-reasoning-appear-to-be-nurture", "postedAtFormatted": "Saturday, September 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Gender%20differences%20in%20spatial%20reasoning%20appear%20to%20be%20nurture&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGender%20differences%20in%20spatial%20reasoning%20appear%20to%20be%20nurture%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk8piGeE26FrtqAj9z%2Fgender-differences-in-spatial-reasoning-appear-to-be-nurture%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Gender%20differences%20in%20spatial%20reasoning%20appear%20to%20be%20nurture%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk8piGeE26FrtqAj9z%2Fgender-differences-in-spatial-reasoning-appear-to-be-nurture", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk8piGeE26FrtqAj9z%2Fgender-differences-in-spatial-reasoning-appear-to-be-nurture", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 214, "htmlBody": "<p><em>\"In this study, we use a large-scale incentivized experiment with nearly  1,300 participants to show that the gender                      gap in spatial abilities, measured by time to solve  a puzzle, disappears when we move from a patrilineal society to an  adjoining                      matrilineal society.\"</em></p>\n<p>It is presently a commonplace of Western culture that women are worse at spatial reasoning than men, and this is commonly attributed to intrinsic biological differences.</p>\n<p>It turns out this may be highly questionable. A <a href=\"http://www.pnas.org/content/early/2011/08/19/1015182108\">study in PNAS</a> studied two nearby tribes in northeast India, one with a strongly patriarchal organisation, one with a strongly matriarchal organisation. Both share the same agrarian diet and lifestyle and DNA tests indicate they are closely related.</p>\n<p>In the patriarchal society, women did noticeably worse on spatial reasoning. In the matriarchal society, women and men did about the same.</p>\n<p>The authors carefully do not overstate their results, claiming only that they demonstrated that culture  influences spatial performance \"in the task that we study.\" However, this promisingly suggests quite a bit of room for improvement of measurable aspects of intelligence may be feasible with proper attention to culture and nurture.</p>\n<p>What measurable aspects of intelligence do you attribute to genetic causes? Can you test it this well? How would you fix it and help people be all they can be?</p>\n<p>News coverage: <a href=\"http://arstechnica.com/science/news/2011/08/gender-gap-in-spatial-reasoning-mia-in-matrilineal-society.ars\">ArsTechnica</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "k8piGeE26FrtqAj9z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 20, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "9632", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-03T16:42:59.793Z", "modifiedAt": null, "url": null, "title": "[LINK] Report on the Fourth Conference on Artificial General Intelligence ", "slug": "link-report-on-the-fourth-conference-on-artificial-general", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:22.974Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8Xy4Kvs8HzKDz5m8s/link-report-on-the-fourth-conference-on-artificial-general", "pageUrlRelative": "/posts/8Xy4Kvs8HzKDz5m8s/link-report-on-the-fourth-conference-on-artificial-general", "linkUrl": "https://www.lesswrong.com/posts/8Xy4Kvs8HzKDz5m8s/link-report-on-the-fourth-conference-on-artificial-general", "postedAtFormatted": "Saturday, September 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Report%20on%20the%20Fourth%20Conference%20on%20Artificial%20General%20Intelligence%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Report%20on%20the%20Fourth%20Conference%20on%20Artificial%20General%20Intelligence%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Xy4Kvs8HzKDz5m8s%2Flink-report-on-the-fourth-conference-on-artificial-general%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Report%20on%20the%20Fourth%20Conference%20on%20Artificial%20General%20Intelligence%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Xy4Kvs8HzKDz5m8s%2Flink-report-on-the-fourth-conference-on-artificial-general", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Xy4Kvs8HzKDz5m8s%2Flink-report-on-the-fourth-conference-on-artificial-general", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2279, "htmlBody": "<p><a href=\"http://hplusmagazine.com/2011/09/01/report-on-the-fourth-conference-on-artificial-general-intelligence/\">http://hplusmagazine.com/2011/09/01/report-on-the-fourth-conference-on-artificial-general-intelligence/</a></p>\n<blockquote>\n<div class=\"views-field-field-author-value\"><label class=\"views-label-field-author-value\"> By: </label> <span id=\"authors\" class=\"field-content author vcard\"> Ben Goertzel </span></div>\n<div class=\"views-field-created\"><label class=\"views-label-created\"> Published: </label> <span class=\"field-content\"> <abbr class=\"published\" title=\"2011-09-01T00:01:58+0000\">September 1, 2011</abbr></span></div>\n<div class=\"views-field-created\">The Fourth Conference on Artificial General Intelligence (<a href=\"http://agi-conf.org/2011\">AGI-11</a>) was held on Google&rsquo;s campus in Mountain View (Silicon Valley), California, in the first week of August 2011.&nbsp;&nbsp; This was the largest AGI conference yet, with more than 200 people attending, and it had a markedly different tone from the prior conferences in the series.&nbsp; A number of participants noted that there was less of an out-of-the-mainstream, wild-eyed maverick feel to the proceedings, and more of a sense of &ldquo;business as usual&rdquo; or &ldquo;normal science&rdquo; &ndash; a sense in the air that AGI is obviously an important, feasible R&amp;D area to be working on, albeit a bit &ldquo;cutting-edge&rdquo; compared to the majority of (more narrowly specialized) AI R&amp;D.</div>\n<div class=\"views-field-created\"><br /></div>\n<div class=\"views-field-created\">\n<p>I think this difference in tone was due partly to the Google and Bay Area location, and partly to the fact that the conference was held in close spatiotemporal proximity to two larger and older AI-related conferences, AAAI-11 and IJCNN-11.&nbsp; IJCNN was just before AGI in San Jose, and AAAI was just after AGI in San Francisco &mdash; so a number of academic AI researchers who usually go to the larger conferences but not AGI, decided to try out AGI as well this year.&nbsp;&nbsp; Complementing this academic group, there was also a strong turnout from the Silicon Valley software industry, and the Bay Area futurist and transhumanist community.</p>\n<p>The first day of the conference was occupied by tutorials on the LIDA and OpenCog systems, and the Church probabilistic logic programming language.&nbsp; The second day comprised two workshops: one on self-programming in AGI systems, and the next the traditional &ldquo;Future of AGI&rdquo; workshop, which was particularly lively due to the prominence of future-of-technology issues in Bay Area culture (the conference site was not so far off from the headquarters of a variety of futurist organizations like Singularity University, the Singularity Institute for AI, the Foresight Institute, etc.). &nbsp;Most of the talks from the Future of AGI workshop have corresponding papers or presentations on the <a href=\"http://agi-conf.org/2011/conference-schedule/\">conference&rsquo;s schedule page</a> &mdash; with themes such as</p>\n<ul>\n<li>Steve Omohundro,<em> <a href=\"http://agi-conf.org/2011/abstract-stephen-omohundro\">Design Principles for a Safe and Beneficial AGI&nbsp; Infrastructure</a></em></li>\n<li>Anna Salamon,&nbsp;<a href=\"http://agi-conf.org/2011/anna-salamon-abstract\"><em>Can Whole Brain Emulation help us build safe AGI?</em></a></li>\n<li>Carl Shulman,&nbsp;<a href=\"http://agi-conf.org/2011/carl-shulman-abstract\"><em>Risk-averse preferences as AGI safety technique</em></a></li>\n<li>Mark Waser,&nbsp;<em><a href=\"http://becominggaia.wordpress.com/papers/rational-universal-benevolence-simpler-safer-and-wiser-than-%E2%80%9Cfriendly-ai%E2%80%9D-agi-11/\">Rational Universal Benevolence: Simpler, Safer, and Wiser than &ldquo;Friendly AI&rdquo;</a></em></li>\n<li>Itamar Arel,&nbsp;<a href=\"http://agi-conf.org/2011/abstract-itamar-arel/\">Reward Driven Learning and the Risk of an Adversarial Artificial General Intelligence</a></li>\n<li>Ahmed Abdel-Fattah &amp; Kai-Uwe Kuehnberger,&nbsp;<em><a href=\"http://agi-conf.org/2011/abstract-ahmed-abdel-fattah-and-kai-uwe-kuehnberger/\">Remarks on the Feasibility and the Ethical Challenges of a Next Milestone in AGI</a></em></li>\n<li><em></em>Matt Chapman,&nbsp;<em><a href=\"http://agi-conf.org/2011/matt-chapman-abstract/\">Maximizing The Power of Open-Source for AGI</a></em></li>\n<li><em></em>Ben Goertzel and Joel Pitt,&nbsp;<em><a href=\"http://agi-conf.org/2011/wp-content/uploads/2009/06/NineWaysToBias.pdf\">Nine Ways to Bias Open-Source AGI Toward Friendliness</a></em></li>\n</ul>\n<p>These may be of particular interest to H+ Magazine readers!</p>\n<p>The final two days constituted the conference proper, with technical talks corresponding to papers in the conference proceedings, which were published in Springer&rsquo;s <em>Lecture Notes in AI book </em>series. &nbsp;Videos of the conference talks, including the workshops and tutorials, will be posted by Google during the next months, and linked from the conference website.</p>\n<p>Peter Norvig, Google&rsquo;s head of research and the co-author of the best-selling AI textbook (whose latest edition does mention AGI, albeit quite briefly), gave brief opening remarks.&nbsp; He didn&rsquo;t announce any grand Google AGI initiatives, making clear that his own current research focus is elsewhere than the direct pursuit of powerful artificial general intelligence.&nbsp; Yet, he also made clear that he sees a lot of the research going on at Google as part of an overall body of work that is ultimately building toward advanced AGI.</p>\n<p>The four keynote speeches highlighted different aspects of the AGI field, as well as the strongly international nature of the AGI community.</p>\n<p>Ernst Dickmanns, from Germany, reviewed his pioneering work on self-driving cars from the 1980s, which in some ways was more advanced that current world on self-driving cars being conducted by Google and others.&nbsp;&nbsp; He wrapped up with a discussion of general lessons for AGI implied by his experience with self-driving cars, including the importance of adaptive learning and of &ldquo;dynamic vision&rdquo; that performs vision in a manner closely coordinated with action.</p>\n<p>Aaron Sloman, from Britain, discussed &ldquo;toddler theorems&rdquo; &ndash; the symbolic understandings of the world that young children learn and create based on their sensorimotor and cognitive experiences.&nbsp; He challenged the researchers in the audience to understand and model the kind of learning and world-modeling that crows or human babies do, and sketched some concepts that he felt would be useful for this sort of modeling.</p>\n<p>MIT&rsquo;s Ed Boyden reviewed his recent work on optogenetics, one of the most exciting and rapidly developing technologies for imaging the brain &ndash; a very important area, given the point raised in the conference&rsquo;s Special Track on Neuroscience and AGI, that the main factor holding back the design of AGI systems based on human brain emulation is currently the lack of appropriate tools for measuring what&rsquo;s happening in the brain.&nbsp; We can&rsquo;t yet measure the brain well enough to construct detailed dynamic brain simulations.&nbsp; Boyden&rsquo;s work is one of the approaches that, step by step, is seeking to overcome this barrier.</p>\n<p>Zhongzhi Shi, from the Chinese Academy of Sciences in Beijing, described his integrative AGI architecture, which incorporates aspects from multiple Western AGI designs into a novel overall framework.&nbsp; He also stressed the importance of cloud computing for enabling practical experimentation with complex AGI architectures like the one he described.</p>\n<p>As well as the regular technical AGI talks, there was a Special Session on Neuroscience and AGI, led by neuroscientist Randal Koene, who is probably the world&rsquo;s most successful advocate of mind uploading, or what he now calls &ldquo;substrate independent minds.&rdquo;&nbsp;&nbsp; Most of the AGI field today is only loosely connected to neuroscience; and yet, in principle, nearly every AGI researcher would agree that careful emulation of the brain is one potential path to AGI, with a high probability of succeeding eventually.&nbsp; The Special Session served to bring neuroscientists and AGI researchers together, to see what they could learn from each other.&nbsp; Neuroscience is not yet at the point where one can architect an AGI based solely on neuroscience knowledge, yet there are many areas where AGI can draw inspiration from neuroscience.&nbsp; Demis Hassabis emphasized the fact that AGI currently lacks any strong theories of how sensorimotor processing interfaces with abstract conceptual processing, and suggested some ways that neuroscience may provide inspiration here, e.g. analysis of cortical-hippocampal interactions.&nbsp;&nbsp; Another point raised in discussions was that reinforcement learning could potentially gain inspiration from study of the various ways in which the brain treats internal intrinsic rewards (alerting or surprisingness) comparably to explicit external rewards.</p>\n<p>Three prizes were awarded at the conference: two Kurzweil Prizes and one Solomonoff Prize.</p>\n<p>The Kurzweil Prize for Best AGI Paper was awarded to Linus Gisslen, Matt Luciw, Vincent Graziano and Juergen Schmidhuber for their paper entitled <em>Sequential Constant Size Compressors and Reinforcement Learning</em>.&nbsp;&nbsp; This paper represents an effort to bridge the gap between the general mathematical theory of AGI (which in its purest form applies only to AI programs achieving massive general intelligence via using unrealistically much processing power) and the practical business of building useful AGI programs.&nbsp; Specifically, one of the key ideas in the general theory of AGI is &ldquo;reinforcement learning&rdquo; &ndash; learning via reward signals from the environment &ndash; but the bulk of the mathematical theory of reinforcement learning makes the assumption that the AI system has complete visibility into the environment.&nbsp;&nbsp; Obviously this is unrealistic &mdash; no real-world intelligence has full knowledge of its environment.&nbsp;&nbsp; The award-winning paper describes a novel, creative method of using recurrent neural networks to apply reinforcement learning methods to partially-observable environments &mdash; indicating a promising research direction to follow, for those who wish to make reinforcement learning algorithms that scale up to real world problems such as human level AGIs will have to deal with.</p>\n<p>The 2011 Kurzweil Award for Best AGI Idea was awarded to Paul Rosenbloom for his paper entitled <em>From Memory to Problem Solving: Mechanism Reuse in a Graphical Cognitive Architecture</em>. Rosenbloom has a long history in the AI field, including a role co-creating the classic SOAR AI architecture in the 1980s.&nbsp; While still supporting the general concepts underlying his older AI work, his current research focuses more heavily on scalable probabilistic methods &ndash; but more flexible and powerful ones than Bayes nets, Markov Logic Networks and other current popular techniques.&nbsp;&nbsp; Extending his previous work on factor graphs as a core construct for scalable uncertainty management in AGI systems, his award-winning paper shows how factor graph mechanisms described for memory, can also be used for problem-solving tasks.&nbsp; In the human brain there is no crisp distinction between memory and problem-solving, so it is conceptually satisfying to see AGI approaches that also avoid this sort of crisp distinction.&nbsp; It is yet unclear to what extent any single mechanism can be used to achieve all the capabilities needed for human-level AGI; but, it is a very interesting and valuable research direction to take a single powerful and flexible mechanism like factor graphs and see how far one can push it, and Dr. Rosenbloom&rsquo;s paper comprises a wonderful example of this sort of work.</p>\n<p>The 2011 Solomonoff AGI Theory Prize &ndash; named in honor of AGI pioneer Ray Solomonoff, who passed away in 2010 &mdash; was awarded to Laurent Orseau and Mark Ring, for a pair of papers titled <em>Self-Modification and Mortality in Artificial Agents</em> and <em>Delusion, Survival, and Intelligent Agents</em>.&nbsp;&nbsp; These papers explore aspects of theoretical generally intelligent agents inspired by Marcus Hutter&rsquo;s AIXI model (a theoretical AGI system that would achieve massive general intelligence using infeasibly much computational resources, but that may potentially be approximated by more feasible AGI approaches).&nbsp;&nbsp; The former paper considers some consequences of endowing an intelligent agent of this nature with the ability to modify its own code; and the latter analyzes aspects of what happens when this sort of theoretical intelligent agent is interfaced with the real world.&nbsp;&nbsp; These papers constitute important steps in bridging the gap between the abstract mathematical theory of AGI, and the real-world business of creating AGI systems and embedding them in the world.</p>\n<p>While there was a lot of strong and interesting research presented at the AGI-11 conference, I think it&rsquo;s fair to say that there were no dramatic breakthroughs presented.&nbsp; Rather, there was more of a feeling of steady incremental progress.&nbsp;&nbsp; Also, compared to previous years, there was less of a feeling of separate, individual research projects working in a vacuum &ndash; the connections between different AI approaches seem to be getting clearer each year, in spite of the absence of a clearly defined common vocabulary or conceptual framework among various AGI researchers.&nbsp;&nbsp; Links were built between abstract AGI theory and practical work, and between neuroscience and AGI engineering.&nbsp; Hybridization of previously wholly different AGI architectures was reported (e.g. the paper I presented, describing the incorporation of aspects of Joscha Bach&rsquo;s MicroPsi system in my OpenCog system).&nbsp; All signs of a field that&rsquo;s gradually maturing.</p>\n<p>These observations lead me inexorably to some more personal musings on AGI.&nbsp; I can&rsquo;t help wondering: Can we get to human-level AGI and beyond via step-by-step, incremental progress, year after year?</p>\n<p>It&rsquo;s a subtle question, actually.&nbsp;&nbsp; It&rsquo;s clear that we are far from having a rigorous scientific understanding of how general intelligence works.&nbsp;&nbsp; At some point, there&rsquo;s going to be a breakthrough in the science of general intelligence &ndash; and I&rsquo;m really looking forward to it! &nbsp;I even hope to play a large part in it.&nbsp; But the question is: will this scientific breakthrough come before or after the engineering of an AGI system with powerful, evidently near-human-level capability?</p>\n<p>It may be that we need a scientific breakthrough in the rigorous theory of general intelligence before we can engineer an advanced AGI system.&nbsp; But &hellip; I presently suspect that we don&rsquo;t.&nbsp; My current opinion is that it should be possible to create a powerful AGI system via proceeding step-by-step from the current state of knowledge &ndash; doing engineering inspired by an integrative <em>conceptual</em>, not quite fully rigorous understanding of general intelligence.&nbsp; If this is right, then we can build a system that will have the impact of a &ldquo;Sputnik of AGI,&rdquo; via combining variants of existing algorithms in a reasonable cognitive architecture in a manner guided by a solid conceptual understanding of mind.&nbsp;&nbsp; And then, by studying this Sputnik AGI system and its successors and variants, we will be able to arrive at the foreseen scientific breakthrough in the science of general intelligence.&nbsp; This of course is what my colleagues and I are trying to do with the OpenCog project &ndash; but the general point I&rsquo;m making here is independent of our specific OpenCog AGI design.</p>\n<p>Anyway, that&rsquo;s my personal view of the near to mid term future of AGI, which I advocated in asides during my OpenCog tutorial, and various discussions at the Future of AGI Workshop.&nbsp;&nbsp; But my view on these matters is far from universal among AGI researchers &ndash; even as the AGI field matures and becomes less marginal, it is still characterized by an extremely healthy diversity of views and attitudes!&nbsp;&nbsp; I look forward to ongoing discussions of these matters with my colleagues in the AGI community as the AGI conference series proceeds and develops.&nbsp; Mostly, it&rsquo;s awesome to even <strong><em>have</em></strong> a serious AGI community.&nbsp; It&rsquo;s hard sometimes to remember that 10 years ago this was far from the case!</p>\n</div>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8Xy4Kvs8HzKDz5m8s", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 7.642429383987919e-07, "legacy": true, "legacyId": "9633", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-03T17:06:56.249Z", "modifiedAt": null, "url": null, "title": "How to hack one's self to want to want to ... hack one's self.", "slug": "how-to-hack-one-s-self-to-want-to-want-to-hack-one-s-self", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:25.740Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "N6W7sAzCo3fGauM7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4j8yytsPs5WruMbBx/how-to-hack-one-s-self-to-want-to-want-to-hack-one-s-self", "pageUrlRelative": "/posts/4j8yytsPs5WruMbBx/how-to-hack-one-s-self-to-want-to-want-to-hack-one-s-self", "linkUrl": "https://www.lesswrong.com/posts/4j8yytsPs5WruMbBx/how-to-hack-one-s-self-to-want-to-want-to-hack-one-s-self", "postedAtFormatted": "Saturday, September 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20hack%20one's%20self%20to%20want%20to%20want%20to%20...%20hack%20one's%20self.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20hack%20one's%20self%20to%20want%20to%20want%20to%20...%20hack%20one's%20self.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4j8yytsPs5WruMbBx%2Fhow-to-hack-one-s-self-to-want-to-want-to-hack-one-s-self%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20hack%20one's%20self%20to%20want%20to%20want%20to%20...%20hack%20one's%20self.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4j8yytsPs5WruMbBx%2Fhow-to-hack-one-s-self-to-want-to-want-to-hack-one-s-self", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4j8yytsPs5WruMbBx%2Fhow-to-hack-one-s-self-to-want-to-want-to-hack-one-s-self", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 787, "htmlBody": "<p>I was inspired by the <a href=\"http://www.lesswrong.com/lw/79x/polyhacking/\">recent post</a> discussing self-hacking for the purpose of changing a relationship perspective to achieve a goal. Despite my feeling inspired, though, I also felt like life hacking was not something I could ever want to do even if I perceived benefits to doing it. It seems to me that the place where I would need to begin is hacking myself in order to cause myself to want to be hacked. But then I started contemplating whether this is a plausible thing to do.<br /><br />In my own case, there are two concrete examples in mind. I am a graduate student working on applied math and probability theory in the field of machine vision. I was one of those bright-eyes, bushy-tailed dolts as an undergrad who just sort of floated to grad school believing that as long as I worked sufficiently hard, it was a logical conclusion that I would get a tenure-track faculty position at a desirable university. Even though I am a fellowship award winner and I am working with a well-known researcher at an Ivy League school, my experience in grad school (along with some <a href=\"http://www.economist.com/node/17723223\">noted</a> <a href=\"http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/pubs/archive/35179.pdf\">articles</a>) has forced me to re-examine a lot of my priorities. Tenure-track positions are just too difficult to achieve and achieving them is based on networking, politics, and whether the popularity of your research happens to have a peak at the same time that your productivity in that area also has a peak.<br /><br />But the alternatives that I see are: join the consulting/business/startup world, become a programmer/analyst for a large software/IT/computer company, work for a government research lab. I worked for two years at MIT's Lincoln Laboratory as a radar analyst and signal processing algorithm developer prior to grad school. The main reason I left that job was because I (foolishly) thought that graduate school was where someone goes to specifically learn the higher-level knowledge and skills to do theoretical work that transcends the software development / data processing work that is so common. I'm more interested in creating tools that go into the toolbox of an engineer than with actually using those tools to create something that people want to pay for.<br /><br />I have been deeply thinking about these issues for more than two years now, almost every day. I read everything that I can and I try to be as blunt and to-the-point about it as I can be. Future career prospects seem bleak to me. Everyone is getting crushed by data right now. I was just talking with my adviser recently about how so much of the mathematical framework for studying vision over the last 30 years is just being flushed down the tubes because of the massive amount of data processing and large scale machine learning we can now tractably perform. If you want to build a cup-detector for example, you can do lots of fancy modeling, stochastic texture mapping, active contour models, fancy differential geometry, occlusion modeling, etc. Or.. you can just train an SVM on 50,000,000 weakly labeled images of cups you find on the internet. And that SVM will utterly crush the performance of the expert system based on 30 years of research from amazing mathematicians. And this crushing effect only stands to get much much worse and at an increasing pace.<br /><br />In light of this, it seems to me that I should be learning as much as I can about large-scale data processing, GPU computing, advanced parallel architectures, and the gross details of implementing bleeding edge machine learning. But, currently, this is exactly the sort of thing I hate and went to graduate school to avoid. I wanted to study Total Variation minimization, or PDE-driven diffusion models in image processing, etc. And these are things that are completely crushed by large data processing.<br /><br />So anyway, long story short: suppose that I really like \"math theory and teaching at a respected research university\" but I see the coming data steamroller and believe that this preference will cause me to feel unhappy in the future when many other preferences I have (and some I don't yet know about) are effected negatively by pursuit of a phantom tenure-track position. But suppose also that another preference I have is that I really hate \"writing computer code to build widgets for customers\" which can include large scale data analyses, and thus I feel an aversion to even trying to *want* to hack myself and orient myself to a more practical career goal.<br /><br />How does one hack one's self to change one's preferences when the preference in question is \"I don't want to hack myself?\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4j8yytsPs5WruMbBx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 15, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "9634", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kLR5H4pbaBjzZxLv6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-03T19:50:59.689Z", "modifiedAt": null, "url": null, "title": "Open Thread: September 2011", "slug": "open-thread-september-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:33.757Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pavitra", "createdAt": "2009-09-22T08:32:44.250Z", "isAdmin": false, "displayName": "Pavitra"}, "userId": "yC2JgX3ENu7mionKh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/b5FFjJRmjo53JLes3/open-thread-september-2011", "pageUrlRelative": "/posts/b5FFjJRmjo53JLes3/open-thread-september-2011", "linkUrl": "https://www.lesswrong.com/posts/b5FFjJRmjo53JLes3/open-thread-september-2011", "postedAtFormatted": "Saturday, September 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20September%202011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20September%202011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb5FFjJRmjo53JLes3%2Fopen-thread-september-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20September%202011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb5FFjJRmjo53JLes3%2Fopen-thread-september-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb5FFjJRmjo53JLes3%2Fopen-thread-september-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 38, "htmlBody": "<p>If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</p>\n<p>If continuing the discussion becomes impractical, that means you win at open threads; a celebratory top-level post on the topic is traditional.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "b5FFjJRmjo53JLes3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 7.643043148209584e-07, "legacy": true, "legacyId": "9636", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 447, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-03T20:03:33.034Z", "modifiedAt": null, "url": null, "title": "Meetup : Munich Meetup, Saturday September 10th, 2PM", "slug": "meetup-munich-meetup-saturday-september-10th-2pm", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:51.991Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wallowinmaya", "createdAt": "2011-03-21T00:39:18.855Z", "isAdmin": false, "displayName": "David Althaus"}, "userId": "xY8DDzk6TyvRroJEo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qXgrJEaDMv89L96d9/meetup-munich-meetup-saturday-september-10th-2pm", "pageUrlRelative": "/posts/qXgrJEaDMv89L96d9/meetup-munich-meetup-saturday-september-10th-2pm", "linkUrl": "https://www.lesswrong.com/posts/qXgrJEaDMv89L96d9/meetup-munich-meetup-saturday-september-10th-2pm", "postedAtFormatted": "Saturday, September 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Munich%20Meetup%2C%20Saturday%20September%2010th%2C%202PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Munich%20Meetup%2C%20Saturday%20September%2010th%2C%202PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqXgrJEaDMv89L96d9%2Fmeetup-munich-meetup-saturday-september-10th-2pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Munich%20Meetup%2C%20Saturday%20September%2010th%2C%202PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqXgrJEaDMv89L96d9%2Fmeetup-munich-meetup-saturday-september-10th-2pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqXgrJEaDMv89L96d9%2Fmeetup-munich-meetup-saturday-september-10th-2pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/2z\">Munich Meetup, Saturday September 10th, 2PM</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">10 September 2011 02:00:00PM (+0200)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Munich Central Station, Coffee Fellows cafe, Bahnhofsplatz 2, First floor.</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>I will be there with a Lesswrong sign. If you can't find it, you can call me: 0160-93132663 .</p>\n<p>According to a doodle-survey 5 people (including me) could attend. And of course you are welcome too!</p>\n<p>If the cafe sucks, we could easily go elsewhere. I've merely chosen the place, because it's relatively nice, near the central station and easy to find.</p>\n<p>Lurkers and newbies are very welcome!</p>\n<p>&nbsp;</p>\n<p>P.S. :&nbsp; Could you please drop a short comment if you're planning to attend?</p>\n</div>\n</div>\n<!-- .content -->", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qXgrJEaDMv89L96d9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 7.643084142064641e-07, "legacy": true, "legacyId": "9637", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-03T21:25:08.952Z", "modifiedAt": null, "url": null, "title": "Rationality Attractors in Personspace", "slug": "rationality-attractors-in-personspace", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.222Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X8G6XYADC2xrfSxvZ/rationality-attractors-in-personspace", "pageUrlRelative": "/posts/X8G6XYADC2xrfSxvZ/rationality-attractors-in-personspace", "linkUrl": "https://www.lesswrong.com/posts/X8G6XYADC2xrfSxvZ/rationality-attractors-in-personspace", "postedAtFormatted": "Saturday, September 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Attractors%20in%20Personspace&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Attractors%20in%20Personspace%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX8G6XYADC2xrfSxvZ%2Frationality-attractors-in-personspace%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Attractors%20in%20Personspace%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX8G6XYADC2xrfSxvZ%2Frationality-attractors-in-personspace", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX8G6XYADC2xrfSxvZ%2Frationality-attractors-in-personspace", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<p>These two questions seem to be somewhat interesting, especially for those interested in rationality outreach.</p>\n<p>What makes someone more likely to study rationality?</p>\n<p>More likely to become a higher level rationalist?</p>\n<p>A few thoughts:</p>\n<p>Empirically, rationalists seem to be more into technical fields than average, and more interested in an explicit understanding of social things than most technical people.</p>\n<p>People who can more clearly see deficiencies in themselves, and who try to solve problems seem more likely to become rationalists, when exposed to rationality.</p>\n<p>People who are motivated to pursue rationality for instrumental goals, rather than for funsies, seem to become better rationalists.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X8G6XYADC2xrfSxvZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "9638", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-04T01:51:04.248Z", "modifiedAt": null, "url": null, "title": "What is the most rational view of Peak Oil and its near term consequences?", "slug": "what-is-the-most-rational-view-of-peak-oil-and-its-near-term", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:33.041Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DavidPlumpton", "createdAt": "2011-07-08T09:36:12.175Z", "isAdmin": false, "displayName": "DavidPlumpton"}, "userId": "JyR3HoGsDEckYxEGE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uyvrNcRBamZzK6WfY/what-is-the-most-rational-view-of-peak-oil-and-its-near-term", "pageUrlRelative": "/posts/uyvrNcRBamZzK6WfY/what-is-the-most-rational-view-of-peak-oil-and-its-near-term", "linkUrl": "https://www.lesswrong.com/posts/uyvrNcRBamZzK6WfY/what-is-the-most-rational-view-of-peak-oil-and-its-near-term", "postedAtFormatted": "Sunday, September 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20the%20most%20rational%20view%20of%20Peak%20Oil%20and%20its%20near%20term%20consequences%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20the%20most%20rational%20view%20of%20Peak%20Oil%20and%20its%20near%20term%20consequences%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuyvrNcRBamZzK6WfY%2Fwhat-is-the-most-rational-view-of-peak-oil-and-its-near-term%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20the%20most%20rational%20view%20of%20Peak%20Oil%20and%20its%20near%20term%20consequences%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuyvrNcRBamZzK6WfY%2Fwhat-is-the-most-rational-view-of-peak-oil-and-its-near-term", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuyvrNcRBamZzK6WfY%2Fwhat-is-the-most-rational-view-of-peak-oil-and-its-near-term", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 556, "htmlBody": "<p>To me the following points seem hard to argue against:</p>\n<p>&nbsp;</p>\n<ol>\n<li>Oil is harder and harder to find every year (we already took the easy stuff, nobody finds super-giant fields anymore)</li>\n<li>The peak production year was 2005 with 73.7 million barrels produced</li>\n<li>The amount of oil produced each year is declining</li>\n<li>The price of oil (and therefore energy) rises</li>\n<li>All the alternatives that were supposed to fill the gap are failing to deliver</li>\n<li>Even oil that's harder to get (e.g. in deep water) doesn't help much as it is generally produced at a slow rate</li>\n<li>Available energy production rate (i.e. power) drops</li>\n<li>Since nearly everything needs power to create/mine/produce prices rise</li>\n<li>Food for example becomes more expensive as fertilizer prices rise</li>\n<li>The average person is mystified as the price of everything seems to rise at once</li>\n<li>Business and whole national economies are squeezed by rising prices</li>\n<li>As businesses fail unemployment increases</li>\n<li>Politicians are powerless, so promise general feel-good nonsense like \"energy independence\". Nobody even tries to tackle the problem.</li>\n<li>Everything continues to get worse, and at an increasing rate</li>\n<li>Within the near future the lights start to go out.</li>\n</ol>\n<div>Sure there's a possibility that a form of nuclear fusion/thorium/cold fusion/zero point energy that is safe and cheap to build and operate might be invented tomorrow, but given that such things usually take a decade or so from inception to delivery it looks like there's no practical alternative on the horizon. Thermodynamics is a harsh mistress. Work out the energy in 73 million barrels of oil, and figure out how many wind farms are needed to offset a 5% decline. And then another decline the next year. Even uranium prices are rising as demand outstrips supply for just the current set of reactors.</div>\n<div>The more we examing the situation the worse it seems to be. Some early wells had a enormous energy return on investment, e.g. for the energy of burning one barrel of oil we could pump 100 barrels from the ground. Now we are pumping wells that produce only about 5 barrels. This is known as EROEI (energy return on energy investment). EROEI is falling everywhere as all the low hanging fruit was plucking decades before, and only the difficult stuff is left. The net result is that it is ever harder to increase production rates.</div>\n<div>Civilization runs on the constant supply of power. If that power declines 5% every year we are back to the middle ages before very long, and it's hard to develop a Friendly AI on an abacus.</div>\n<div>One thing I've noticed a lot is reports about \"Oil Sands\", \"Oil Shale\", \"Vast new possibilities of X barrels from biodiesel/microbes/algae/thermal depolymerization\" etc. and none of these reports *ever* mention the *rate* of production that is expected (and years later they seem to have delivered nothing). The production rate is far more important than anything else; if the entire Earth was made of oil but we could only pump a million barrels a year for technical reasons then the total amount is pointless.</div>\n<div>Please read at least the &nbsp;<a href=\"http://en.wikipedia.org/wiki/Peak_oil\">Peak Oil Wikipedia article</a>&nbsp; before commenting. I'd rather not see a bunch of comments about \"they don't even look for oil when there's 30 years of supply waiting in the ground\".</div>\n<div>So... what are my cognitive biases?</div>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uyvrNcRBamZzK6WfY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -5, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "9639", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-04T02:51:38.312Z", "modifiedAt": null, "url": null, "title": "July 2011 review of experimental philosophy", "slug": "july-2011-review-of-experimental-philosophy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:22.429Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gDQZXwScbCdz5pKQJ/july-2011-review-of-experimental-philosophy", "pageUrlRelative": "/posts/gDQZXwScbCdz5pKQJ/july-2011-review-of-experimental-philosophy", "linkUrl": "https://www.lesswrong.com/posts/gDQZXwScbCdz5pKQJ/july-2011-review-of-experimental-philosophy", "postedAtFormatted": "Sunday, September 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20July%202011%20review%20of%20experimental%20philosophy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJuly%202011%20review%20of%20experimental%20philosophy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgDQZXwScbCdz5pKQJ%2Fjuly-2011-review-of-experimental-philosophy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=July%202011%20review%20of%20experimental%20philosophy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgDQZXwScbCdz5pKQJ%2Fjuly-2011-review-of-experimental-philosophy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgDQZXwScbCdz5pKQJ%2Fjuly-2011-review-of-experimental-philosophy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 62, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/Experimental_philosophy\">Experimental philosophy</a> is the mainstream academic field that is most directly attempting to <a href=\"/lw/of/dissolving_the_question/\">dissolve</a>&nbsp;(where possible) persistent philosophical problems by revealing the cognitive algorithms that generate old philosophical debates. Those who want to catch up with some of what these researchers have discovered already may want to read <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Knobe-et-al-Experimental-philosophy.pdf\">this July 2011 review of the field</a>.</p>\n<p>Also see my post <a href=\"/lw/74f/are_deontological_moral_judgments_rationalizations/\">Are Deontological Moral Judgments Rationalizations?</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gDQZXwScbCdz5pKQJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 18, "extendedScore": null, "score": 7.644416753137176e-07, "legacy": true, "legacyId": "9641", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Mc6QcrsbH5NRXbCRX", "62p74DvwNHgQXCXcH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-04T04:39:46.564Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Burdensome Details", "slug": "seq-rerun-burdensome-details", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:22.663Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4icPqPYqCygzQSHvR/seq-rerun-burdensome-details", "pageUrlRelative": "/posts/4icPqPYqCygzQSHvR/seq-rerun-burdensome-details", "linkUrl": "https://www.lesswrong.com/posts/4icPqPYqCygzQSHvR/seq-rerun-burdensome-details", "postedAtFormatted": "Sunday, September 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Burdensome%20Details&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Burdensome%20Details%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4icPqPYqCygzQSHvR%2Fseq-rerun-burdensome-details%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Burdensome%20Details%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4icPqPYqCygzQSHvR%2Fseq-rerun-burdensome-details", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4icPqPYqCygzQSHvR%2Fseq-rerun-burdensome-details", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p>Today's post, <a href=\"/lw/jk/burdensome_details/\">Burdensome Details</a> was originally published on 20 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If you want to avoid the conjunction fallacy, you must try to feel a stronger emotional impact from Occam's Razor. Each additional detail added to a claim must feel as though it is driving the probability of the claim down towards zero.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/7f6/seq_rerun_conjunction_controversy_or_how_they/#comments\">Conjunction Controversy (Or, How They Nail It Down)</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4icPqPYqCygzQSHvR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 7.644769945040431e-07, "legacy": true, "legacyId": "9642", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Yq6aA4M3JKWaQepPJ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-04T05:02:03.186Z", "modifiedAt": null, "url": null, "title": "Table of cognitive tasks that do and do not show correlations with cognitive ability", "slug": "table-of-cognitive-tasks-that-do-and-do-not-show", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:23.144Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mGskFYkKqFx9hMM9W/table-of-cognitive-tasks-that-do-and-do-not-show", "pageUrlRelative": "/posts/mGskFYkKqFx9hMM9W/table-of-cognitive-tasks-that-do-and-do-not-show", "linkUrl": "https://www.lesswrong.com/posts/mGskFYkKqFx9hMM9W/table-of-cognitive-tasks-that-do-and-do-not-show", "postedAtFormatted": "Sunday, September 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Table%20of%20cognitive%20tasks%20that%20do%20and%20do%20not%20show%20correlations%20with%20cognitive%20ability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATable%20of%20cognitive%20tasks%20that%20do%20and%20do%20not%20show%20correlations%20with%20cognitive%20ability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmGskFYkKqFx9hMM9W%2Ftable-of-cognitive-tasks-that-do-and-do-not-show%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Table%20of%20cognitive%20tasks%20that%20do%20and%20do%20not%20show%20correlations%20with%20cognitive%20ability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmGskFYkKqFx9hMM9W%2Ftable-of-cognitive-tasks-that-do-and-do-not-show", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmGskFYkKqFx9hMM9W%2Ftable-of-cognitive-tasks-that-do-and-do-not-show", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Tasks-that-do-and-do-not-correlate-with-cognitive-ability.pdf\">Here</a>. From <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Stanovich-et-al-Individual-differences-as-essential-components-of-heuristics-and-biases-research.pdf\">this 2010 book chapter</a> by Stanovich, Toplak, and West. (<a href=\"http://www.amazon.com/Science-Reason-Festschrift-Jonathan-Psychology/dp/1848720157/\">Here</a> is the book.)</p>\n<p>See also Baron's <a href=\"/lw/76l/table_of_biases_the_normative_models_they_violate/\">table</a> of cognitive biases, the normative models they violate, and their explanations.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mGskFYkKqFx9hMM9W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 7.644842708157042e-07, "legacy": true, "legacyId": "9645", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mPbrjPKv28s7y6ZzS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-04T05:02:08.915Z", "modifiedAt": null, "url": null, "title": "On Eliezer's post \"The Cluster Structure of Thingspace\"", "slug": "on-eliezer-s-post-the-cluster-structure-of-thingspace", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:22.876Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Owen_Richardson", "createdAt": "2011-04-08T22:03:10.482Z", "isAdmin": false, "displayName": "Owen_Richardson"}, "userId": "uR7QxXK65gYZQ4PrL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Jt9K5fpvAkbdSFHCo/on-eliezer-s-post-the-cluster-structure-of-thingspace", "pageUrlRelative": "/posts/Jt9K5fpvAkbdSFHCo/on-eliezer-s-post-the-cluster-structure-of-thingspace", "linkUrl": "https://www.lesswrong.com/posts/Jt9K5fpvAkbdSFHCo/on-eliezer-s-post-the-cluster-structure-of-thingspace", "postedAtFormatted": "Sunday, September 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20Eliezer's%20post%20%22The%20Cluster%20Structure%20of%20Thingspace%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20Eliezer's%20post%20%22The%20Cluster%20Structure%20of%20Thingspace%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJt9K5fpvAkbdSFHCo%2Fon-eliezer-s-post-the-cluster-structure-of-thingspace%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20Eliezer's%20post%20%22The%20Cluster%20Structure%20of%20Thingspace%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJt9K5fpvAkbdSFHCo%2Fon-eliezer-s-post-the-cluster-structure-of-thingspace", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJt9K5fpvAkbdSFHCo%2Fon-eliezer-s-post-the-cluster-structure-of-thingspace", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 28, "htmlBody": "<p>I would like to request feedback on <a href=\"/lw/nl/the_cluster_structure_of_thingspace/4rc3\">this comment</a>, please.</p>\n<p>I would also like to point out the cross-reference with jsalvatier's \"<a href=\"/r/discussion/lw/7f5/what_are_good_topics_for_literature_review_prizes/\">What are good topics for literature review prizes?</a>\"</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Jt9K5fpvAkbdSFHCo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": -3, "extendedScore": null, "score": 7.644843019982322e-07, "legacy": true, "legacyId": "9646", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8LfkhgWkqp2gTqug4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-04T05:57:08.205Z", "modifiedAt": null, "url": null, "title": "Elqayam & Evans (2011) argue against a certain kind of normativism about rationality", "slug": "elqayam-and-evans-2011-argue-against-a-certain-kind-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:33.579Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K2xe6t8QmdSboSvS5/elqayam-and-evans-2011-argue-against-a-certain-kind-of", "pageUrlRelative": "/posts/K2xe6t8QmdSboSvS5/elqayam-and-evans-2011-argue-against-a-certain-kind-of", "linkUrl": "https://www.lesswrong.com/posts/K2xe6t8QmdSboSvS5/elqayam-and-evans-2011-argue-against-a-certain-kind-of", "postedAtFormatted": "Sunday, September 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Elqayam%20%26%20Evans%20(2011)%20argue%20against%20a%20certain%20kind%20of%20normativism%20about%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AElqayam%20%26%20Evans%20(2011)%20argue%20against%20a%20certain%20kind%20of%20normativism%20about%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK2xe6t8QmdSboSvS5%2Felqayam-and-evans-2011-argue-against-a-certain-kind-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Elqayam%20%26%20Evans%20(2011)%20argue%20against%20a%20certain%20kind%20of%20normativism%20about%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK2xe6t8QmdSboSvS5%2Felqayam-and-evans-2011-argue-against-a-certain-kind-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK2xe6t8QmdSboSvS5%2Felqayam-and-evans-2011-argue-against-a-certain-kind-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 277, "htmlBody": "<p>A forthcoming edition of <em>Behavioral and Brain Sciences</em>&nbsp;will be devoted to Elqayam &amp; Evans' (2011) <a href=\"http://www.psy.dmu.ac.uk/elqayam/Elqayam_BBS-D-10-00343_preprint.pdf\">critique of normativism about rationality</a> and brief responses to it.</p>\n<p>Abstract:</p>\n<blockquote>\n<p>We propose a critique of normativism, defined as the idea that human thinking reflects a&nbsp;normative system against which it should be measured and judged. We analyze the methodological&nbsp;problems associated with normativism, proposing that it invites the controversial is-ought inference,&nbsp;much contested in the philosophical literature. This problem is triggered when there are competing&nbsp;normative accounts (the arbitration problem), as empirical evidence can help arbitrate between&nbsp;descriptive theories, but not between normative systems. Drawing on linguistics as a model, we&nbsp;propose that clear distinction between normative systems and competence theories is essential, arguing&nbsp;that equating them invites an &lsquo;is-ought&rsquo; inference; to wit, supporting normative &lsquo;ought&rsquo; theories with&nbsp;empirical &lsquo;is&rsquo; evidence. We analyze in detail two research programs with normativist features,&nbsp;Oaksford and Chater&rsquo;s rational analysis, and Stanovich and West&rsquo;s individual differences approach,&nbsp;demonstrating how in each case equating norm and competence leads to an is-ought inference.&nbsp;Normativism triggers a host of research biases in psychology of reasoning and decision making:focusing on untrained participants and novel problems, analyzing psychological processes in terms of&nbsp;their normative correlates, and neglecting philosophically significant paradigms when they do not&nbsp;supply clear standards for normative judgment. For example, in a dual-process framework,&nbsp;normativism can lead to a fallacious &lsquo;ought-is&rsquo; inference, in which normative responses are taken as&nbsp;diagnostic of analytic reasoning. We propose that little can be gained from normativism that cannot be&nbsp;achieved by descriptivist computational-level analysis, illustrating our position with Hypothetical Thinking Theory and the theory of the suppositional conditional.&nbsp;We conclude that descriptivism is a&nbsp;viable option, and that theories of higher mental processing would be better off freed from normative considerations.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K2xe6t8QmdSboSvS5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 5, "extendedScore": null, "score": 7.645022632904528e-07, "legacy": true, "legacyId": "9640", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-04T06:59:39.443Z", "modifiedAt": null, "url": null, "title": "Hacking on LessWrong Just Got Easier", "slug": "hacking-on-lesswrong-just-got-easier", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:57.230Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SRdvYJrDNvWkpcm8F/hacking-on-lesswrong-just-got-easier", "pageUrlRelative": "/posts/SRdvYJrDNvWkpcm8F/hacking-on-lesswrong-just-got-easier", "linkUrl": "https://www.lesswrong.com/posts/SRdvYJrDNvWkpcm8F/hacking-on-lesswrong-just-got-easier", "postedAtFormatted": "Sunday, September 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hacking%20on%20LessWrong%20Just%20Got%20Easier&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHacking%20on%20LessWrong%20Just%20Got%20Easier%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSRdvYJrDNvWkpcm8F%2Fhacking-on-lesswrong-just-got-easier%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hacking%20on%20LessWrong%20Just%20Got%20Easier%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSRdvYJrDNvWkpcm8F%2Fhacking-on-lesswrong-just-got-easier", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSRdvYJrDNvWkpcm8F%2Fhacking-on-lesswrong-just-got-easier", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<p>TrikeApps has done a great job running LessWrong and adding new features, but they could use a little help. Have you thought about improving the LessWrong website but haven't done it because you weren't sure how? Or had installation issues? Well, now is a great time to start, because hacking on LessWrong just got much easier!</p>\n<p>On behalf of the <a href=\"http://groups.google.com/group/lw-public-goods-team\">LessWrong Public Goods Team</a>, I have built a <a href=\"http://en.wikipedia.org/wiki/Virtual_machine\">Virtual Machine</a> Image which hosts its own version of the LessWrong website. This eliminates the need to figure out how to host LessWrong yourself. To hack on LessWrong you simply:</p>\n<ol>\n<li style=\"margin-left: 15px;\">Install VirtualBox</li>\n<li style=\"margin-left: 15px;\">Download and use the VM image</li>\n<li style=\"margin-left: 15px;\">Edit LessWrong's code&nbsp;</li>\n<li style=\"margin-left: 15px;\">Test</li>\n<li style=\"margin-left: 15px;\">Submit pull request</li>\n</ol>\n<p>Detailed instructions and download link <a href=\"https://github.com/tricycle/lesswrong/wiki/Development-VM-Image\">here</a>.<br /><br /> Interested, but not sure what to work on? The LessWrong issue tracker is <a href=\"http://code.google.com/p/lesswrong/issues/list\">here</a>. Run into trouble with the code? Ask questions on the <a href=\"http://groups.google.com/group/lesswrong-dev\">dev list</a>.</p>\n<div>Many thanks to Matt, Jon and David at&nbsp;TrikeApps for helping me do this, and John Salvatier for initiating this project.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HFou6RHqFagkyrKkW": 1, "MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SRdvYJrDNvWkpcm8F", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 61, "baseScore": 87, "extendedScore": null, "score": 0.000218, "legacy": true, "legacyId": "9643", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": "2019-05-06T23:00:12.417Z", "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": true, "hideFrontpageComments": false, "maxBaseScore": 59, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-04T13:26:57.261Z", "modifiedAt": null, "url": null, "title": "Make evidence charts, not review papers? [Link]", "slug": "make-evidence-charts-not-review-papers-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:25.926Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tWSg3Ektv9bJ2FChs/make-evidence-charts-not-review-papers-link", "pageUrlRelative": "/posts/tWSg3Ektv9bJ2FChs/make-evidence-charts-not-review-papers-link", "linkUrl": "https://www.lesswrong.com/posts/tWSg3Ektv9bJ2FChs/make-evidence-charts-not-review-papers-link", "postedAtFormatted": "Sunday, September 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Make%20evidence%20charts%2C%20not%20review%20papers%3F%20%5BLink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMake%20evidence%20charts%2C%20not%20review%20papers%3F%20%5BLink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtWSg3Ektv9bJ2FChs%2Fmake-evidence-charts-not-review-papers-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Make%20evidence%20charts%2C%20not%20review%20papers%3F%20%5BLink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtWSg3Ektv9bJ2FChs%2Fmake-evidence-charts-not-review-papers-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtWSg3Ektv9bJ2FChs%2Fmake-evidence-charts-not-review-papers-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 321, "htmlBody": "<blockquote>\n<p>How do you get on top of the literature associated with a controversial scientific topic? For many empirical issues, the science gives a conflicted picture. Like the role of sleep in memory consolidation, the effect of caffeine on cognitive function, or the best theory of a particular visual illusion. To form your own opinion, you&rsquo;ll need to become familiar with many studies in the area.</p>\n<p>You might start by reading the latest review article on the topic. Review articles provide descriptions of many relevant studies. Also, they usually provide a nice tidy story that seems to bring the literature all together into a common thread- that the author&rsquo;s theory is correct! Because of this bias, a review article may not help you much to make an independent evaluation of the evidence. And the evidence usually isn&rsquo;t all there. Review articles very rarely describe, or even cite, all the relevant studies. Unfortunately, if you&rsquo;re just getting started, you can&rsquo;t recognize which relevant studies the author didn&rsquo;t cite.</p>\n<p>[...]</p>\n<p>Hal Pashler and I have created, together with Chris Simon of the Scotney Group who did the actual programming, <a href=\"http://www.evidencechart.com/\">a tool</a> that addresses these problems. It allows one to create systematic reviews of a topic, without having to write many thousands of words, and without having to weave all the studies together with a narrative unified by a single theory. You do it all in a tabular form called an &lsquo;evidence chart&rsquo;. Evidence charts are an old idea, closely related to the &ldquo;<a href=\"http://en.wikipedia.org/wiki/Analysis_of_Competing_Hypotheses\">analysis of competing hypotheses</a>&rdquo; technique. <a href=\"http://www.evidencechart.com/\">Our <strong>evidencechart.com website</strong></a> is fully functioning and free to all, but it&rsquo;s in beta and we&rsquo;d love any feedback.</p>\n</blockquote>\n<p><strong>More:</strong> <a href=\"http://alexholcombe.wordpress.com/2010/09/02/make-evidence-charts-not-review-papers/\">alexholcombe.wordpress.com/2010/09/02/make-evidence-charts-not-review-papers/</a></p>\n<p><strong>Example:</strong> <a href=\"http://www.evidencechart.com/charts/514\"><span id=\"EvidenceChart.name-514\" class=\"name\">What is the role of sleep on hippocampus-dependent memory consolidation?</span></a></p>\n<p>I thought this was an interesting idea. Do you think it would be possible and useful to create an evidence chart for risks from AI, existential risks in general and other topics on lesswrong?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tWSg3Ektv9bJ2FChs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 21, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "9648", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-04T17:37:20.280Z", "modifiedAt": null, "url": null, "title": "[Link] \u201cHow to seem good at everything: Stop doing stupid shit\u201d", "slug": "link-how-to-seem-good-at-everything-stop-doing-stupid-shit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:58.325Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kpreid", "createdAt": "2009-04-28T03:07:40.133Z", "isAdmin": false, "displayName": "kpreid"}, "userId": "rKiev4kfnTWRmCJit", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xsiX4XZGHWTjW9uwa/link-how-to-seem-good-at-everything-stop-doing-stupid-shit", "pageUrlRelative": "/posts/xsiX4XZGHWTjW9uwa/link-how-to-seem-good-at-everything-stop-doing-stupid-shit", "linkUrl": "https://www.lesswrong.com/posts/xsiX4XZGHWTjW9uwa/link-how-to-seem-good-at-everything-stop-doing-stupid-shit", "postedAtFormatted": "Sunday, September 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20%E2%80%9CHow%20to%20seem%20good%20at%20everything%3A%20Stop%20doing%20stupid%20shit%E2%80%9D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20%E2%80%9CHow%20to%20seem%20good%20at%20everything%3A%20Stop%20doing%20stupid%20shit%E2%80%9D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxsiX4XZGHWTjW9uwa%2Flink-how-to-seem-good-at-everything-stop-doing-stupid-shit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20%E2%80%9CHow%20to%20seem%20good%20at%20everything%3A%20Stop%20doing%20stupid%20shit%E2%80%9D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxsiX4XZGHWTjW9uwa%2Flink-how-to-seem-good-at-everything-stop-doing-stupid-shit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxsiX4XZGHWTjW9uwa%2Flink-how-to-seem-good-at-everything-stop-doing-stupid-shit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<p>Possibly interesting article on <em>winning</em>: <a href=\"http://jinfiesto.posterous.com/how-to-seem-good-at-everything-stop-doing-stu\">How to seem good at everything: Stop doing stupid shit</a></p>\n<p><strong>Summary</strong>, as I interpreted it: In practicing a skill, focus on increasing the <em>minimum</em> of the quality of the individual actions comprising performing the skill (because that is the greatest marginal benefit).</p>\n<p>[This article previously posted <a href=\"/lw/6y9/open_thread_august_2011/4m8a\">as an open thread comment</a>.]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xsiX4XZGHWTjW9uwa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 25, "extendedScore": null, "score": 7.647310418798177e-07, "legacy": true, "legacyId": "9649", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-04T22:36:35.194Z", "modifiedAt": null, "url": null, "title": "Podcast on Cryonics by 'Stuff you should know'", "slug": "podcast-on-cryonics-by-stuff-you-should-know", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.875Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FiftyTwo", "createdAt": "2011-03-21T03:38:33.142Z", "isAdmin": false, "displayName": "FiftyTwo"}, "userId": "BZL3TWZXx2wyptxeJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y5CgkzmYrYAzNq27c/podcast-on-cryonics-by-stuff-you-should-know", "pageUrlRelative": "/posts/Y5CgkzmYrYAzNq27c/podcast-on-cryonics-by-stuff-you-should-know", "linkUrl": "https://www.lesswrong.com/posts/Y5CgkzmYrYAzNq27c/podcast-on-cryonics-by-stuff-you-should-know", "postedAtFormatted": "Sunday, September 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Podcast%20on%20Cryonics%20by%20'Stuff%20you%20should%20know'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APodcast%20on%20Cryonics%20by%20'Stuff%20you%20should%20know'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY5CgkzmYrYAzNq27c%2Fpodcast-on-cryonics-by-stuff-you-should-know%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Podcast%20on%20Cryonics%20by%20'Stuff%20you%20should%20know'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY5CgkzmYrYAzNq27c%2Fpodcast-on-cryonics-by-stuff-you-should-know", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY5CgkzmYrYAzNq27c%2Fpodcast-on-cryonics-by-stuff-you-should-know", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 33, "htmlBody": "<p>The podcast <a href=\"http://entertainment.howstuffworks.com/hsw-shows/stuff-you-should-know-podcast.htm\">'Stuff you should know'</a> has done an episode on cryonics.</p>\n<p>&nbsp;</p>\n<p>Available&nbsp;here:&nbsp;</p>\n<p><a href=\"http://podcasts.howstuffworks.com/hsw/podcasts/sysk/2011-08-30-sysk-cryonics.mp3?_kip_ipx=862998473-1315175433\">http://podcasts.howstuffworks.com/hsw/podcasts/sysk/2011-08-30-sysk-cryonics.mp3?_kip_ipx=862998473-1315175433</a>&nbsp;</p>\n<p>&nbsp;</p>\n<p>I don't know much about the subject, but what do people think of it as a depiction of cryonics in popular culture?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y5CgkzmYrYAzNq27c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 7.648288531039353e-07, "legacy": true, "legacyId": "9650", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-04T23:03:01.531Z", "modifiedAt": null, "url": null, "title": "What Direct Instruction is", "slug": "what-direct-instruction-is", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:33.308Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "7unuEpvX4De7PHQ8W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MNSgW5j8bRvynSZHR/what-direct-instruction-is", "pageUrlRelative": "/posts/MNSgW5j8bRvynSZHR/what-direct-instruction-is", "linkUrl": "https://www.lesswrong.com/posts/MNSgW5j8bRvynSZHR/what-direct-instruction-is", "postedAtFormatted": "Sunday, September 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Direct%20Instruction%20is&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Direct%20Instruction%20is%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMNSgW5j8bRvynSZHR%2Fwhat-direct-instruction-is%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Direct%20Instruction%20is%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMNSgW5j8bRvynSZHR%2Fwhat-direct-instruction-is", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMNSgW5j8bRvynSZHR%2Fwhat-direct-instruction-is", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1106, "htmlBody": "<p>A couple of days ago, prompted by <a href=\"/r/discussion/lw/7d1/scientifically_optimizing_education_hard_problem/\">several</a> <a href=\"/r/discussion/lw/7dt/partial_rewrite_of_the_direct_instruction_thing/\">recent</a> <a href=\"/r/discussion/lw/7eo/another_treatment_of_direct_instruction_getting/\">posts</a> by <a href=\"/user/Owen_Richardson/\">Owen_Richardson</a>, I checked out the book \"Theory of Instruction\" (Engelmann and Carnine, 1982) from my university library and <a href=\"/r/discussion/lw/7eo/another_treatment_of_direct_instruction_getting/4r2l\">promised</a> to read it this weekend and write a post about Direct Instruction. This is that post.<a id=\"more\"></a></p>\r\n<h2>Learning through examples</h2>\r\n<p>Direct Instruction is based on a theory of learning that assumes the learner capable of extracting a concept inductively through examples of that concept. I may not know what a <a href=\"/lw/nm/disguised_queries/\">blegg</a> is, but after you show me several examples of bleggs and rubes, I will be able to figure it out. The principle of DI is to use the same basic procedure of giving examples to teach every concept imaginable. Naturally, in some cases, the process might be sped up by giving an explanation first; furthermore, there are some things in every subject you just have to memorize, and DI doesn't magically change that. However, it is assumed that the examples are where the real learning occurs.</p>\r\n<p>The meat of the theory is using experimental data and cognitive science to establish rules for how examples ought to be given. Here are a few of the more basic ones:</p>\r\n<ul>\r\n<li>It is impossible to demonstrate a concept using positive examples alone. Here I am reminded of the <a href=\"/lw/iw/positive_bias_look_into_the_dark/\">2-4-6 game</a>, in which subjects fail to test triplets that disconfirm their hypothesis. A teacher has control over the examples presented, so it is important to disconfirm the hypotheses that the learners (consciously or unconsciously) generate.</li>\r\n<li>To successfully teach a quality, it is important that all positive examples only share that one quality. Imagine that you are being taught what a blegg is by a sequence of examples that include blue eggs and red cubes. By the end, you will not be certain whether the defining feature of a blegg is that it's blue, or that it's an egg, or both at once, or if the critical factor is the vanadium ore content of an object.</li>\r\n<li>The way the example is presented is also a quality that must be controlled in this fashion. This is because inductive learning is not entirely a deliberate process on the part of the learner. For instance, if positive and negative examples alternate, the learner may extract the rule that \"every other object is a blegg\". There are multiple ways this can become a real problem: I've encountered calculus students who were confused by a problem that asked them to integrate with respect to a variable called \"t\", rather than \"x\".</li>\r\n<li>The examples must be followed by tests, which fall in the range of given examples but are not identical. This is the way to diagnose the learning process, and is the reason that you get ideas such as \"DI is about asking the students 10 questions a minute.\" This is not a defining feature of DI, but you can see now that it can easily happen when the concept being taught is a simple one.</li>\r\n</ul>\r\n<p>I don't mean to imply that DI is restricted to dealing with yes-or-no identification questions. The examples and concepts can get more complicated, and there is a classification of concepts as comparative, multi-dimensional, joining, etc. This determines how the examples should be presented, but I won't get into the classification here. In practice, a lot of concepts are taught through several sequences of examples. For instance, teaching integration by substitution might first involve a simple sequence of examples about identifying when the method is appropriate, then a sequence about choosing the correct substitution, before actually teaching students to solve an integration problem using the method.</p>\r\n<h2>Faultless communication</h2>\r\n<p>\"Faultless communication\" isn't a misnomer exactly, but I think it lends itself to some easy misconceptions. The basic idea is that a sequence of examples is a faultless communication when there is only one possible rule describing all the examples; there is then the often-repeated statement that if a faultless communication fails, the problem is with the learner, not with the method.</p>\r\n<p>When the book gets into details, however, the actual theory is much less dismissive. In fact, it is emphasized that in general, when a method fails, there's something wrong with the method. A well-designed sequence of examples is not (usually) a faultless communication. Rather, it is a sequence of examples calibrated in such a way that, if the learner arrives at an incorrect rule, the test examples will identify the incorrect rule, which can then be traced back to an ambiguity in the examples given. Alternatively, it can make it clear that the learner lacks sufficient background to identify the correct rule.</p>\r\n<p>The actual issue that the concept of faultless communication is meant to address is the following. When you don't have a clear way to diagnose failure while teaching a concept, it leads to blind experimentation: you ask \"Did everyone understand that?\" and, upon a negative answer, say \"Okay, let me try explaining it in some different way...\" You might never stumble upon the reason that you are misunderstood, except by chance.</p>\r\n<h2>My own thoughts</h2>\r\n<p>A disclaimer: I have very little experience with teaching in general, and this is my first encounter with a complete theory of teaching. Parts of Direct Instruction feel overly restrictive to me; it seems that it doesn't have much of a place for things like lecturing, for instance. Then again, a theory must be somewhat restrictive to be effective; unless the intuitive way I would teach something is already magically the optimal way, the theory is no good unless it prevents me from doing something I would otherwise do.</p>\r\n<p>An interesting aspect of Direct Instruction that I don't think has been pointed out yet (well, the book, written in 1982, might not be a likely place to find such a thought): this method of teaching seems ideally suited for teaching an Artificial Intelligence. Part of the gimmick of Direct Instruction is that it tries, as much as possible, not to make assumptions about what sort of things will be obvious to the learner. Granted, a lot of the internal structure still relies on experimental data gathered from human learners, but if we're creating an AI, it's a lot easier to program in a set of fundamental responses describing the way it should learn inductively, than to program in the concept of \"red\" or \"faster than\" by hand.</p>\r\n<p>I still have the book and plan to hold on to it for a week or so; if there are any questions about what Direct Instruction is or is not, ask them in the comments and I will do my best to figure out what the theory says one way or the other.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MNSgW5j8bRvynSZHR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 69, "extendedScore": null, "score": 0.000116, "legacy": true, "legacyId": "9651", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>A couple of days ago, prompted by <a href=\"/r/discussion/lw/7d1/scientifically_optimizing_education_hard_problem/\">several</a> <a href=\"/r/discussion/lw/7dt/partial_rewrite_of_the_direct_instruction_thing/\">recent</a> <a href=\"/r/discussion/lw/7eo/another_treatment_of_direct_instruction_getting/\">posts</a> by <a href=\"/user/Owen_Richardson/\">Owen_Richardson</a>, I checked out the book \"Theory of Instruction\" (Engelmann and Carnine, 1982) from my university library and <a href=\"/r/discussion/lw/7eo/another_treatment_of_direct_instruction_getting/4r2l\">promised</a> to read it this weekend and write a post about Direct Instruction. This is that post.<a id=\"more\"></a></p>\n<h2 id=\"Learning_through_examples\">Learning through examples</h2>\n<p>Direct Instruction is based on a theory of learning that assumes the learner capable of extracting a concept inductively through examples of that concept. I may not know what a <a href=\"/lw/nm/disguised_queries/\">blegg</a> is, but after you show me several examples of bleggs and rubes, I will be able to figure it out. The principle of DI is to use the same basic procedure of giving examples to teach every concept imaginable. Naturally, in some cases, the process might be sped up by giving an explanation first; furthermore, there are some things in every subject you just have to memorize, and DI doesn't magically change that. However, it is assumed that the examples are where the real learning occurs.</p>\n<p>The meat of the theory is using experimental data and cognitive science to establish rules for how examples ought to be given. Here are a few of the more basic ones:</p>\n<ul>\n<li>It is impossible to demonstrate a concept using positive examples alone. Here I am reminded of the <a href=\"/lw/iw/positive_bias_look_into_the_dark/\">2-4-6 game</a>, in which subjects fail to test triplets that disconfirm their hypothesis. A teacher has control over the examples presented, so it is important to disconfirm the hypotheses that the learners (consciously or unconsciously) generate.</li>\n<li>To successfully teach a quality, it is important that all positive examples only share that one quality. Imagine that you are being taught what a blegg is by a sequence of examples that include blue eggs and red cubes. By the end, you will not be certain whether the defining feature of a blegg is that it's blue, or that it's an egg, or both at once, or if the critical factor is the vanadium ore content of an object.</li>\n<li>The way the example is presented is also a quality that must be controlled in this fashion. This is because inductive learning is not entirely a deliberate process on the part of the learner. For instance, if positive and negative examples alternate, the learner may extract the rule that \"every other object is a blegg\". There are multiple ways this can become a real problem: I've encountered calculus students who were confused by a problem that asked them to integrate with respect to a variable called \"t\", rather than \"x\".</li>\n<li>The examples must be followed by tests, which fall in the range of given examples but are not identical. This is the way to diagnose the learning process, and is the reason that you get ideas such as \"DI is about asking the students 10 questions a minute.\" This is not a defining feature of DI, but you can see now that it can easily happen when the concept being taught is a simple one.</li>\n</ul>\n<p>I don't mean to imply that DI is restricted to dealing with yes-or-no identification questions. The examples and concepts can get more complicated, and there is a classification of concepts as comparative, multi-dimensional, joining, etc. This determines how the examples should be presented, but I won't get into the classification here. In practice, a lot of concepts are taught through several sequences of examples. For instance, teaching integration by substitution might first involve a simple sequence of examples about identifying when the method is appropriate, then a sequence about choosing the correct substitution, before actually teaching students to solve an integration problem using the method.</p>\n<h2 id=\"Faultless_communication\">Faultless communication</h2>\n<p>\"Faultless communication\" isn't a misnomer exactly, but I think it lends itself to some easy misconceptions. The basic idea is that a sequence of examples is a faultless communication when there is only one possible rule describing all the examples; there is then the often-repeated statement that if a faultless communication fails, the problem is with the learner, not with the method.</p>\n<p>When the book gets into details, however, the actual theory is much less dismissive. In fact, it is emphasized that in general, when a method fails, there's something wrong with the method. A well-designed sequence of examples is not (usually) a faultless communication. Rather, it is a sequence of examples calibrated in such a way that, if the learner arrives at an incorrect rule, the test examples will identify the incorrect rule, which can then be traced back to an ambiguity in the examples given. Alternatively, it can make it clear that the learner lacks sufficient background to identify the correct rule.</p>\n<p>The actual issue that the concept of faultless communication is meant to address is the following. When you don't have a clear way to diagnose failure while teaching a concept, it leads to blind experimentation: you ask \"Did everyone understand that?\" and, upon a negative answer, say \"Okay, let me try explaining it in some different way...\" You might never stumble upon the reason that you are misunderstood, except by chance.</p>\n<h2 id=\"My_own_thoughts\">My own thoughts</h2>\n<p>A disclaimer: I have very little experience with teaching in general, and this is my first encounter with a complete theory of teaching. Parts of Direct Instruction feel overly restrictive to me; it seems that it doesn't have much of a place for things like lecturing, for instance. Then again, a theory must be somewhat restrictive to be effective; unless the intuitive way I would teach something is already magically the optimal way, the theory is no good unless it prevents me from doing something I would otherwise do.</p>\n<p>An interesting aspect of Direct Instruction that I don't think has been pointed out yet (well, the book, written in 1982, might not be a likely place to find such a thought): this method of teaching seems ideally suited for teaching an Artificial Intelligence. Part of the gimmick of Direct Instruction is that it tries, as much as possible, not to make assumptions about what sort of things will be obvious to the learner. Granted, a lot of the internal structure still relies on experimental data gathered from human learners, but if we're creating an AI, it's a lot easier to program in a set of fundamental responses describing the way it should learn inductively, than to program in the concept of \"red\" or \"faster than\" by hand.</p>\n<p>I still have the book and plan to hold on to it for a week or so; if there are any questions about what Direct Instruction is or is not, ask them in the comments and I will do my best to figure out what the theory says one way or the other.</p>", "sections": [{"title": "Learning through examples", "anchor": "Learning_through_examples", "level": 1}, {"title": "Faultless communication", "anchor": "Faultless_communication", "level": 1}, {"title": "My own thoughts", "anchor": "My_own_thoughts", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "81 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GR9YT9fd6vYfRQZB8", "BDTkro8B2xtLF4v2v", "Frp55RdAbFTLa2Ae7", "4FcxgdvdQP45D6Skg", "rmAbiEKQDpDnZzcRf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-05T02:39:08.647Z", "modifiedAt": null, "url": null, "title": "The Carbohydrate Hypothesis of Obesity: A Critical Examination", "slug": "the-carbohydrate-hypothesis-of-obesity-a-critical", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.761Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t9hGWhkBGefGRLze8/the-carbohydrate-hypothesis-of-obesity-a-critical", "pageUrlRelative": "/posts/t9hGWhkBGefGRLze8/the-carbohydrate-hypothesis-of-obesity-a-critical", "linkUrl": "https://www.lesswrong.com/posts/t9hGWhkBGefGRLze8/the-carbohydrate-hypothesis-of-obesity-a-critical", "postedAtFormatted": "Monday, September 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Carbohydrate%20Hypothesis%20of%20Obesity%3A%20A%20Critical%20Examination&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Carbohydrate%20Hypothesis%20of%20Obesity%3A%20A%20Critical%20Examination%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9hGWhkBGefGRLze8%2Fthe-carbohydrate-hypothesis-of-obesity-a-critical%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Carbohydrate%20Hypothesis%20of%20Obesity%3A%20A%20Critical%20Examination%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9hGWhkBGefGRLze8%2Fthe-carbohydrate-hypothesis-of-obesity-a-critical", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9hGWhkBGefGRLze8%2Fthe-carbohydrate-hypothesis-of-obesity-a-critical", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 27, "htmlBody": "<p><a href=\"http://wholehealthsource.blogspot.com/2011/08/carbohydrate-hypothesis-of-obesity.html\">Here</a>. A number of people here are fans of Taubes' work, and so I thought they would be interested in a well-referenced criticism. Hat tip to Landsknecht.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t9hGWhkBGefGRLze8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 26, "extendedScore": null, "score": 7.649081506648514e-07, "legacy": true, "legacyId": "9658", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-05T05:57:27.291Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] What is Evidence?", "slug": "seq-rerun-what-is-evidence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:30.898Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q9b7iahJrLheMBXas/seq-rerun-what-is-evidence", "pageUrlRelative": "/posts/Q9b7iahJrLheMBXas/seq-rerun-what-is-evidence", "linkUrl": "https://www.lesswrong.com/posts/Q9b7iahJrLheMBXas/seq-rerun-what-is-evidence", "postedAtFormatted": "Monday, September 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20What%20is%20Evidence%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20What%20is%20Evidence%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ9b7iahJrLheMBXas%2Fseq-rerun-what-is-evidence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20What%20is%20Evidence%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ9b7iahJrLheMBXas%2Fseq-rerun-what-is-evidence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ9b7iahJrLheMBXas%2Fseq-rerun-what-is-evidence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 195, "htmlBody": "<p>Today's post, <a href=\"/lw/jl/what_is_evidence/\">What is Evidence?</a> was originally published on 22 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Evidence is an event connected by a chain of causes and effects to whatever it is you want to learn about. It also has to be an event that is more likely if reality is one way, than if reality is another. If a belief is not formed this way, it cannot be trusted.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/7fu/seq_rerun_burdensome_details/\">Burdensome Details</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q9b7iahJrLheMBXas", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 7.649729937815563e-07, "legacy": true, "legacyId": "9664", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6s3xABaXKPdFwA3FS", "4icPqPYqCygzQSHvR", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-05T17:07:16.928Z", "modifiedAt": null, "url": null, "title": "The Fatal Gift of Beauty: The Trials of Amanda Knox", "slug": "the-fatal-gift-of-beauty-the-trials-of-amanda-knox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:30.637Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrF", "createdAt": "2009-03-15T13:53:56.105Z", "isAdmin": false, "displayName": "FrF"}, "userId": "YAx653P4YqE5ouFjn", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pZk69rSBfKaPGo6jq/the-fatal-gift-of-beauty-the-trials-of-amanda-knox", "pageUrlRelative": "/posts/pZk69rSBfKaPGo6jq/the-fatal-gift-of-beauty-the-trials-of-amanda-knox", "linkUrl": "https://www.lesswrong.com/posts/pZk69rSBfKaPGo6jq/the-fatal-gift-of-beauty-the-trials-of-amanda-knox", "postedAtFormatted": "Monday, September 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Fatal%20Gift%20of%20Beauty%3A%20The%20Trials%20of%20Amanda%20Knox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Fatal%20Gift%20of%20Beauty%3A%20The%20Trials%20of%20Amanda%20Knox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpZk69rSBfKaPGo6jq%2Fthe-fatal-gift-of-beauty-the-trials-of-amanda-knox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Fatal%20Gift%20of%20Beauty%3A%20The%20Trials%20of%20Amanda%20Knox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpZk69rSBfKaPGo6jq%2Fthe-fatal-gift-of-beauty-the-trials-of-amanda-knox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpZk69rSBfKaPGo6jq%2Fthe-fatal-gift-of-beauty-the-trials-of-amanda-knox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>Charles Petzold <a href=\"http://www.charlespetzold.com/blog/2011/09/Reading-The-Fatal-Gift-of-Beauty.html\">writes</a> about <a href=\"http://www.ninaburleigh.com/\">Nina Burleigh's</a> \"The Fatal Gift of Beauty: The Trials of Amanda Knox\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pZk69rSBfKaPGo6jq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "9673", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-05T19:47:02.012Z", "modifiedAt": null, "url": null, "title": "Meetup : UMD, Social Effectiveness and Calibration Exercises", "slug": "meetup-umd-social-effectiveness-and-calibration-exercises", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:25.802Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7y8c5P6uTBAA3oETb/meetup-umd-social-effectiveness-and-calibration-exercises", "pageUrlRelative": "/posts/7y8c5P6uTBAA3oETb/meetup-umd-social-effectiveness-and-calibration-exercises", "linkUrl": "https://www.lesswrong.com/posts/7y8c5P6uTBAA3oETb/meetup-umd-social-effectiveness-and-calibration-exercises", "postedAtFormatted": "Monday, September 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20UMD%2C%20Social%20Effectiveness%20and%20Calibration%20Exercises&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20UMD%2C%20Social%20Effectiveness%20and%20Calibration%20Exercises%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7y8c5P6uTBAA3oETb%2Fmeetup-umd-social-effectiveness-and-calibration-exercises%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20UMD%2C%20Social%20Effectiveness%20and%20Calibration%20Exercises%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7y8c5P6uTBAA3oETb%2Fmeetup-umd-social-effectiveness-and-calibration-exercises", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7y8c5P6uTBAA3oETb%2Fmeetup-umd-social-effectiveness-and-calibration-exercises", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/30'>UMD, Social Effectiveness and Calibration Exercises</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 September 2011 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">University of Maryland, Anne Arundel Building</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The DC Meetup group is starting to run meetups at the University of Maryland!</p>\n\n<p>We'll be meeting in the basement of the Anne Arundel building at 6:00, then will probably move to a dorm room, outside, or to a food place.</p>\n\n<p>At the first one we plan to do some quick calibration exercises, then move on to practicing social skills, like eye contact. If we're feeling brave, rejection therapy might ensue.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/30'>UMD, Social Effectiveness and Calibration Exercises</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7y8c5P6uTBAA3oETb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "9674", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___UMD__Social_Effectiveness_and_Calibration_Exercises\">Discussion article for the meetup : <a href=\"/meetups/30\">UMD, Social Effectiveness and Calibration Exercises</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 September 2011 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">University of Maryland, Anne Arundel Building</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The DC Meetup group is starting to run meetups at the University of Maryland!</p>\n\n<p>We'll be meeting in the basement of the Anne Arundel building at 6:00, then will probably move to a dorm room, outside, or to a food place.</p>\n\n<p>At the first one we plan to do some quick calibration exercises, then move on to practicing social skills, like eye contact. If we're feeling brave, rejection therapy might ensue.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___UMD__Social_Effectiveness_and_Calibration_Exercises1\">Discussion article for the meetup : <a href=\"/meetups/30\">UMD, Social Effectiveness and Calibration Exercises</a></h2>", "sections": [{"title": "Discussion article for the meetup : UMD, Social Effectiveness and Calibration Exercises", "anchor": "Discussion_article_for_the_meetup___UMD__Social_Effectiveness_and_Calibration_Exercises", "level": 1}, {"title": "Discussion article for the meetup : UMD, Social Effectiveness and Calibration Exercises", "anchor": "Discussion_article_for_the_meetup___UMD__Social_Effectiveness_and_Calibration_Exercises1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-05T23:13:33.256Z", "modifiedAt": null, "url": null, "title": "Decision Theory Paradox: Answer Key", "slug": "decision-theory-paradox-answer-key", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.140Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HxYneuv9XRit4dMRm/decision-theory-paradox-answer-key", "pageUrlRelative": "/posts/HxYneuv9XRit4dMRm/decision-theory-paradox-answer-key", "linkUrl": "https://www.lesswrong.com/posts/HxYneuv9XRit4dMRm/decision-theory-paradox-answer-key", "postedAtFormatted": "Monday, September 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Decision%20Theory%20Paradox%3A%20Answer%20Key&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecision%20Theory%20Paradox%3A%20Answer%20Key%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHxYneuv9XRit4dMRm%2Fdecision-theory-paradox-answer-key%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Decision%20Theory%20Paradox%3A%20Answer%20Key%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHxYneuv9XRit4dMRm%2Fdecision-theory-paradox-answer-key", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHxYneuv9XRit4dMRm%2Fdecision-theory-paradox-answer-key", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 998, "htmlBody": "<p><a href=\"/lw/767/decision_theory_paradox_pd_with_three_implies/4qgl\">As promised</a>, I'm posting the answers to the exercises I wrote in the <a href=\"/lw/767/decision_theory_paradox_pd_with_three_implies/\">decision theory post</a>.</p>\n<blockquote>\n<p><strong>Exercise 1:</strong> Prove that if the population consists of TDT agents and DefectBots, then a TDT agent will cooperate precisely when at least one of the other agents is also TDT. <em>(Difficulty: 1 star.)</em></p>\n</blockquote>\n<p>With the utility function specified (i.e. the shortsighted one that only cares about immediate children), the TDT agent's decision can be deduced from simple <a href=\"http://en.wikipedia.org/wiki/Superrationality\">superrationality</a> concerns; that is, if any other TDT agents are present with analogous utility functions, then their decisions will be identical. Thus if a TDT faces off against 2 DefectBots, it chooses between 0 children (for C) and 2 children (for D), and thus it defects. If it faces off against a TDT and a DefectBot, it chooses between 3 children (for C) and 2 children (for D), so it cooperates. And if it faces off against two other TDTs, it chooses between 6 children (for C) and 2 children (for D), so it cooperates. <em>(<strong>EDIT:</strong> It's actually not that simple after all- see <a href=\"/r/discussion/lw/7gs/decision_theory_paradox_answer_key/4ruh\">Douglas Knight's comment</a> and the ensuing discussion.)</em></p>\n<blockquote>\n<p><strong>Exercise 2:</strong> Prove that if a very large population starts with equal numbers of TDTs and DefectBots, then the expected population growth in TDTs and DefectBots is practically equal. (If Omega samples with replacement&ndash; assuming that the agents don't care about their exact copy's children&ndash; then the expected population growth is precisely equal.) <em>(Difficulty: 2 stars.)</em></p>\n</blockquote>\n<p>For the sake of simplicity, we'll consider only the parenthetical case. (The interested reader can see that if sampled without replacement, the figures will differ by a factor on the order of one divided by the population.) There are four cases to consider when Omega picks a trio: it includes 0, 1, 2 or 3 TDT agents, with probability 1/8, 3/8, 3/8 and 1/8 respectively. The first case results in 6 DefectBots being returned; the second results in 4 DefectBots and 2 TDTs; the third results in 8 DefectBots and 6 TDTs; the last results in 18 TDTs. Weighting and adding the cases, each \"side\" has expected population growth of 5.25 agents in that round.</p>\n<blockquote>\n<p><strong>Exercise 3:&nbsp; </strong>Prove that if the initial population consists of TDTs and DefectBots, then the ratio of the two will (with probability 1) tend to 1 over time. <em>(Difficulty: 3 stars.)</em></p>\n</blockquote>\n<p>This is a bit tricky; note that the expected population growth is higher for the more populous side! However, the <em>expected fertility</em> of each agent is higher on the less populous side, and thus its share grows proportionally. (Think of the demographics of a small minority with high fertility- while they won't have as many total children as the rest of the population, their proportion of the population will increase.)</p>\n<p>For very large populations, we can model the fraction of DefectBots as a differential equation, and we will show that this differential equation has a single stable attractive equilibrium at 1/2. Let <em>N</em> be the total population at a given moment, and <em>x</em> (in (0,1)) the fraction of that population consisting of DefectBots. Then we let P(x)=6x<sup>3</sup>+12x<sup>2</sup>(1-x)+24x(1-x)<sup>2</sup> and Q(x)=6x<sup>2</sup>(1-x)+18x(1-x)<sup>2</sup>+18(1-x)<sup>3</sup> denote the expected population growth for DefectBots and TDTs, respectively (these numbers are arrived at in the same way we calculated Exercise 2 in the special case x=1/2), and note that the \"difference quotient\" between the old and new fractions of the population comes out to ((1-x)P(x)-xQ(x))/(N+P(x)+Q(x)). If we consider this to be x' and study the differential equation (with an extra parameter N for the current population), we see that indeed, x has a stable equilibrium when the expected fertilities are equal (that is, P(x)/x = Q(x)/(1-x)) at x=1/2, and that x steadily increases for x&lt;1/2 and steadily decreases for x&gt;1/2.</p>\n<p>I'll admit that this isn't a rigorous proof, but it's the correct heuristic calculation; increased formalism only makes it more difficult to communicate.</p>\n<blockquote>\n<p><strong>Exercise 4: </strong>If the initial population consists of CliqueBots, DefectBots and TDTs in any proportion, then the ratio of both others to CliqueBots approaches 0 (with probability 1). <em>(Difficulty: 4 stars.)</em></p>\n</blockquote>\n<p><strong></strong></p>\n<p>We can model this as a differential equation in the two-dimensional region {x+y+z=1: 0&lt;x,y,z&lt;1}, and as in Exercise 3 a stable equilibrium is a point at which the three expected fertilities are equal. At this point, it's worthwhile to note that you can calculate expected fertilities more simply by, for a given agent, counting only its individual fertility given the possible other two partners in the PD. If we let x, y and z denote the proportions of DefectBots, TDTs and CliqueBots, respectively, and let P, Q, and R denote their respective fertilities as functions of x,y and z, then we get</p>\n<ul>\n<li>P(x,y,z)=2x<sup>2</sup>+4xy+8y<sup>2</sup>+4xz+4yz+2z<sup>2</sup></li>\n<li>Q(x,y,z)=2x<sup>2</sup>+6xy+6y<sup>2</sup>+4xz+6yz+2z<sup>2</sup></li>\n<li>R(x,y,z)=2x<sup>2</sup>+4xy+8y<sup>2</sup>+4xz+4yz+6z<sup>2</sup></li>\n</ul>\n<p>It is simple to show that if we set x=0, then R&gt;Q for all {(y,z):y+z=1,0&lt;y,z&lt;1}; that is, CliqueBots beat TDTs completely when DefectBots are not present. It is even simpler to show that they beat DefectBots entirely when TDTs are not present. It is a bit more complicated when all three are together: there is a tiny unstable region near (3/4,1/4,0) where Q&gt;R, but the proportions drift out of this region as y gains against x, and they do not return; the stable equilibrium is at (0,0,1) as claimed.</p>\n<p>Finally,</p>\n<blockquote>\n<p><strong>Problem: </strong>The setup looks perfectly fair for TDT agents. So why do they lose? <em>(Difficulty: 2+3i stars.)</em></p>\n</blockquote>\n<p>As explained in the <a href=\"/lw/778/consequentialism_need_not_be_nearsighted/\">consequentialism post</a>, we've handicapped TDT by giving our agents shortsighted utility functions. If they instead care about distant descendants (let's say that Omega is only running the experiment finitely many times, either for a fixed number of tournaments or until the population reaches a certain number), then (unless it's known that the experiment is soon to end) the gains in population made by dominating the demographics will overwhelm the immediate gains of letting a DefectBot or CliqueBot take advantage of the agent. Growing almost 6-fold each time one's selected (or occupying a larger fraction of the fixed final population) will justify the correct decision, essentially via the more complicated calculations we've done above.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HxYneuv9XRit4dMRm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "9676", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HT8jwNJ6vH7p9gaTT", "prb8raC4XGJiRWs5n"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-06T00:46:35.112Z", "modifiedAt": "2020-07-21T18:32:55.435Z", "url": null, "title": "Prisoner's Dilemma Tournament Results", "slug": "prisoner-s-dilemma-tournament-results", "viewCount": null, "lastCommentedAt": "2013-11-27T04:22:43.597Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "prase", "createdAt": "2009-02-28T10:00:10.260Z", "isAdmin": false, "displayName": "prase"}, "userId": "WAP32wvmNt9QdutSu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hamma4XgeNrsvAJv5/prisoner-s-dilemma-tournament-results", "pageUrlRelative": "/posts/hamma4XgeNrsvAJv5/prisoner-s-dilemma-tournament-results", "linkUrl": "https://www.lesswrong.com/posts/hamma4XgeNrsvAJv5/prisoner-s-dilemma-tournament-results", "postedAtFormatted": "Tuesday, September 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Prisoner's%20Dilemma%20Tournament%20Results&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrisoner's%20Dilemma%20Tournament%20Results%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhamma4XgeNrsvAJv5%2Fprisoner-s-dilemma-tournament-results%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Prisoner's%20Dilemma%20Tournament%20Results%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhamma4XgeNrsvAJv5%2Fprisoner-s-dilemma-tournament-results", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhamma4XgeNrsvAJv5%2Fprisoner-s-dilemma-tournament-results", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3434, "htmlBody": "<p>About two weeks ago I <a href=\"/lw/79r/prisoners_dilemma_as_a_game_theory_laboratory/\">announced</a> an open competition for LessWrong readers inspired by Robert Axelrod's famous <a href=\"http://en.wikipedia.org/wiki/Evolution_of_cooperation#Axelrod.27s_Tournaments\">tournaments</a>. The competitors had to submit a strategy which would play an iterated prisoner's dilemma of fixed length: first in the round-robin tournament where the strategy plays a hundred-turn match against each of its competitors exactly once, and second in the evolutionary tournament where the strategies are randomly paired against each other and their gain is translated in number of their copies present in next generation; the strategy with the highest number of copies after generation 100 wins. More details about the rules were described in the announcement. This post summarises the results.</p>\n<p><a id=\"more\"></a></p>\n<p><strong>The Zoo of Strategies</strong></p>\n<p>I have received 25 contest entries containing 21 distinct strategies. Those I have divided into six classes based on superficial similarities (except the last class, which is a catch-all category for everything which doesn't belong anywhere else, something like <em>adverbs</em> within the classification of parts of speech or now defunct <em>vermes</em> in the animal kingdom). The first class is formed by <strong><em>Tit-for-tat variants</em></strong>, probably the most obvious choice for a potentially successful strategy. Apparently so obvious that at least one commenter <a href=\"/lw/79r/prisoners_dilemma_as_a_game_theory_laboratory/4p3f\">declared</a> high confidence that tit-for-tat will make more than half of the strategy pool. That was actually a good example of misplaced confidence, since the number of received tit-for-tat variants (where I put anything which behaves like tit-for-tat except for isolated deviations) was only six, two of them being identical and thus counted as one. Moreover there wasn't a single true tit-for-tatter among the contestants; the closest we got was</p>\n<p><strong>A</strong> (-, -): <em>On the first turn of each match, cooperate. On every other turn, with probability 0.0000004839, cooperate; otherwise play the move that the opponent played on the immediately preceding turn.</em></p>\n<p>(In the presentation of strategies, the letter in bold serves as a unique identificator. The following parentheses include the name of the strategy &mdash; if the author has provided one &mdash; and the name of the author. I use the author's original description of the strategy when possible. If that's too long, an abbreviated paraphrase is given. If I found the original description ambiguous, I may give a slightly reformulated version based on subsequent clarifications with the author.) The author of <strong>A</strong> was the only one who requested his/her name should be withheld and the strategy is nameless, so both arguments in the bracket are empty. The reason for the obscure probability was to make the strategy unique. The author says:</p>\n<blockquote>\n<p>I wish to enter a trivial variation on the tit-for-tat strategy. (The trivial variation is to force the strategy to be unique; I wish to punish defectorish strategies by having lots of tit-for-tat-style strategies in the pool.)</p>\n</blockquote>\n<p>This was perhaps a slight abuse of rules, but since I am responsible for failing to make the rules immune to abuse, I had to accept the strategy as it is. Anyway, it turned out that the trivial variation was needless for the stated purpose.</p>\n<p>The remaining strategies from this class were more or less standard with <strong>B</strong> being the most obvious choice.</p>\n<p><strong>B</strong> (-, <a href=\"/user/Alexei\">Alexei</a>): <em>Tit-for-Tat, but always defect on last turn.</em></p>\n<p><strong>C</strong> (-, <a href=\"/user/Caerbannog\">Caerbannog</a>): <em>Tit-for-tat with 20% chance of forgiving after opponent's defection. Defect on the last turn.</em></p>\n<p><strong>D</strong> (-, <a href=\"/user/fubarobfusco\">fubarobfusco</a> and <a href=\"/user/DuncanS\">DuncanS</a>): <em>Tit-for-tat with 10% chance of forgiving.</em></p>\n<p><strong>E</strong> (-, <a href=\"/user/Jem\">Jem</a>): <em>First two turns cooperate. Later tit-for-tat with chance of forgiving equal to 1/2<sup>x</sup> where x is equal to number of opponent's defections after own cooperations. Last turn defect.</em></p>\n<p>The next category of strategies I call <strong><em>Avengers</em></strong>. The Avengers play a nice strategy until the opponent's defective behaviour reaches a specified threshold. After that they switch to irrevocable defection.</p>\n<p><strong>F</strong> (-, <a href=\"/user/Eugine_Nier/\">Eugine_Nier</a>): <em>Standard Tit-for-Tat with the following modifications: 1) Always defect on the last move. 2) Once the other player defects 5 times, switch to all defect.</em></p>\n<p><strong>G</strong> (-, <a href=\"/user/rwallace\">rwallace</a>): <em>1. Start with vanilla tit-for-tat. 2. When the opponent has defected a total of three times this match, switch to always defect for the rest of the match. 3. If the number of rounds is known and fixed, defect on the last round.</em></p>\n<p><strong>H</strong> (-, <a href=\"/user/Michaelos\">Michaelos</a>): <em>Tit for Tat, with two exceptions: 1: Ignore all other considerations and always defect on the final iteration. 2: After own defection and opponent's cooperation, 50 percent of the time, cooperate. The other 50 percent of the time, always defect for the rest of the game.<br /></em><br /><strong>I</strong> (-, <a href=\"/user/malthrin/\">malthrin</a>): <em>if OpponentDefectedBefore(7) then MyMove=D else if n&gt;98 then MyMove=D else MyMove=OpponentLastMove</em></p>\n<p><strong>J</strong> (<em>Vengeful Cheater</em>, <a href=\"/user/Vaniver/\">Vaniver</a>): <em>Set Rage = 0. Turn 1: cooperate. Turn 2-99: If the opponent defected last round, set Rage to 1. (If they cooperate, no change.) If Rage is 1, defect. Else, cooperate. Turn 100: defect.</em><br /><br /><strong>K</strong> (<em>Grim Trigger</em>, <a href=\"/user/shinoteki/\">shinoteki</a>): <em>Cooperate if and only if the opponent has not defected in the current match.</em></p>\n<p>Then we have a special \"class\" of <em><strong>DefectBots</strong></em> (a name stolen from <a href=\"/lw/767/decision_theory_paradox_pd_with_three_implies/\">Orthonormal's excellent article</a>) consisting of only one single strategy. But this is by far the most popular strategy&nbsp;&mdash; I have received it in four copies. Unfortunately for DefectBots, according to the rules they have to be included in the pool only once. The peculiar name for the single DefectBot comes from wedrifid:</p>\n<p><strong>L</strong> (<em>Fuck them all!</em>, <a href=\"/user/MileyCyrus/\">MileyCyrus</a> and <a href=\"/user/wedrifid/\">wedrifid</a> and <a href=\"/user/vespiacic/\">vespiacic</a> and <a href=\"/user/peter_hurford/\">peter_hurford</a>): <em>Defect.</em></p>\n<p>Then, we have a rather heterogenous class of <em><strong>Lists</strong></em>, whose only common thing is that their play is specified by a rather long list of instructions.</p>\n<p><strong>M</strong> (-, <a href=\"/user/JesusPetry/\">JesusPetry</a>): <em>Turn 1: cooperate. Turns 2 to 21: tit-for-tat. Turn 22: defect. Turn 23: cooperate. Turn 24: if opponent cooperated in turns 21 and 22, cooperate; otherwise, do whatever the opponent did on previous turn. Then tit-for-tat, the scenario from turns 22-24 repeats itself in turns 35-37, 57-59, 73-75. Turns 99 and 100: defect.</em></p>\n<p><strong>N</strong> (-, <a href=\"/user/FAWS\">FAWS</a>): <em>1. Cooperate on the first turn. 2. If the opponent has defected at least 3 times: Defect for the rest of the match. 3. Play tit for tat unless specified otherwise. 4. If the opponent has cooperated on the first 20 turns: Randomly choose a turn between 21 and 30. Otherwise continue with 11. 5. If the opponent defects after turn 20, but before the chosen turn comes up: Play CDC the next three turns, then continue with 12. 6. Defect on the chosen turn if the opponent hasn't defected before. 7. If the opponent defects on the same turn: Cooperate, then continue with 12. 8. If the opponent defects on the next turn: Cooperate, then continue with 11. 9. If the opponent defects on the turn after that (i. e. the second turn after we defected): Cooperate, then continue with 12. 10. If the opponent cooperates on every turn up to and including two turns after the defection: Defect until the opponent defects, then cooperate twice and continue with 11. 11. Defect on the last two turns (99 and 100). 12. Defect on the last two turns unless the opponent defected exactly once.</em><br /><br /><strong>O</strong> (<em>Second Chance</em>, <a href=\"/user/benelliott\">benelliott</a>): <em>Second Chance is defined by 5 rules. When more than one rule applies to the current situation, always defer to the one with the lower number. Rules: 1. Co-operate on move 1, defect on moves 98, 99 and 100. 2. If Second Chance has co-operated at least 4 times (excluding the most recent move) and in every case co-operation has been followed by defection from the other strategy, then always defect from now on. 3. If Second Chance has co-operated at least 8 times, and defected at least 10 times (excluding the previous move), then calculate x, the proportion of the time that co-operation has been followed by co-operation and y, the proportion of the time that defection has been followed by co-operation. If 4x &lt; 6y + 1 then defect until that changes. 4. If the number of times the opposing strategy has defected (including the most recent move) is a multiple of 4, co-operate. 5. Do whatever the opposing strategy did on the previous move.</em></p>\n<p>The next class are <em><strong>CliqueBots</strong></em> (once more a name from the Orthonormal's post). CliqueBots try to identify whether their opponent is a copy of themselves and then cooperate. If they identify a foreign opponent, they usually become pretty nasty.</p>\n<p><strong>P</strong> (<em>Simple Identity ChecK</em>, <a href=\"/user/red75/\">red75</a>): <em>Move 1: cooperate. Moves 2 to 57: tit-for-tat. Move 58 - defect. Move 59 - if moves 0-57 were coop-coop and 58 was defect-defect then coop else defect. Moves 60-100: if my move 59 was coop then tit-for-tat else defect.<br /></em></p>\n<p><strong>Q</strong> (<em>EvilAlliance</em>, <a href=\"/user/ArisKatsaris/\">ArisKatsaris</a>): <em>Start with 5 defections. On later turns, if the opponent had cooperated at least once in the first 5 turns or defected ever since, defect; else cooperate.</em></p>\n<p>The remaining strategies are <strong><em>Unclassified</em></strong>, since they have little in common. Here they come:</p>\n<p><strong>R</strong> (<em>Probe &amp; Punish</em>, <a href=\"/user/Eneasz\">Eneasz</a>): <em>1. Cooperate for a turn. 2. Cooperate for a second turn. If opponent cooperated on the second turn, continue to cooperate in every subsequent turn until the opponent defects. 3. If/when the opponent defects, defect for 12 consecutive rounds. 4. Return to 1, repeat.</em></p>\n<p><strong>S</strong> (<em>Win-stay lose-shift</em>, <a href=\"/user/nerzhin/\">Nerzhin</a>): <em>It cooperates if and only if it and the opponent both played the same strategy on the last round.</em></p>\n<p>The author of <strong>S</strong> says he's adopted the strategy from Nowak and Sigmund, Nature 364 pp. 56-58.</p>\n<p><strong>T</strong> (<em>Tit for Two Tats</em>, <a href=\"/user/DataPacRat/\">DataPacRat</a>): <a href=\"http://en.wikipedia.org/wiki/Tit_for_two_tats#Tit_for_two_tats\"><em>As the name suggests.</em></a></p>\n<p><strong>U</strong> (<em>RackBlockShooter</em>, <a href=\"/user/Nic_Smith\">Nic_Smith</a>): <em>The code is available <a href=\"http://ariarule.com/files/rbs.html\">here</a>.</em></p>\n<p>This one is a real beast, with code more that twice as long as that of its competitors. It actually models the opponent's behaviour as a random process specified by two parameters: probability of cooperation when the second strategy (which is the RackBlockShooter) also cooperates, and probability of cooperation when RBS defects. This was acausal: the parameters depend on RBS's behaviour in the same turn, not in the preceding one. Moreover, RBS stores whole probability distributions of those parameters and correctly updates them in an orthodox Bayesian manner after each turn. To further complicate things, if RBS thinks that the opponent is itself, it cooperates in the CliqueBottish style.</p>\n<p>And finally, there's a strategy included by default:</p>\n<p><strong>Z</strong> (<em>Fully Random</em>, default): <em>Cooperate with 50% probability.</em></p>\n<p><strong>The Round-Robin Tournament</strong></p>\n<p>Each strategy had played 21 matches where a total maximum of 14,700 points could theoretically be won. A more reasonable threshold was 8,400 points: an award for any member of a pool consisting solely of nice strategies (i.e. those which never defect first). No strategy reached this optimum. The standings are given in the following table (<em>columns from left: strategy identifier, matches won, matches drawn, matches lost, total points</em>).</p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_1.png\" alt=\"\" width=\"112\" height=\"349\" /></p>\n<p>The winner is malthrin's unnamed strategy. Second and third place are occupied by Eugine_Nier's modified tit-for-tat and benelliott's <em>Second Chance</em>, respectively. Congratulations!</p>\n<p>Here are results of all matches (<em>view the picture in full size by right-clicking</em>).</p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_2.png?v=9fff80f7f2b67544a2f94de8ed92df21\" alt=\"\" width=\"733\" height=\"185\" /></p>\n<p>Is there something interesting to say about the results? Well, there were three strategies which performed worse than random: the DefectBot <strong>L</strong>, the CliqueBot <strong>Q</strong> (which effectively acted like a DefectBot) and the complex strategy <strong>U</strong>. As expected, very nasty strategies were unsuccessful. On the other hand, totally nice strategies weren't much successful either. No member of the set {<strong>A</strong>, <strong>D</strong>, <strong>K</strong>, <strong>R</strong>, <strong>S</strong>, <strong>T</strong>} got into the first five. Not defecting on the last turn was a mistake, defecting on the last two turns seemed even a better choice. As for the somewhat arbitrary classes, the average gains were as such:</p>\n<ol>\n<li>Tit-for-tat variants 7197.4</li>\n<li>Avengers 7144.3</li>\n<li>Lists 6551.3</li>\n<li>Unclassified 5963.8</li>\n<li>CliqueBots 4371.5</li>\n<li>DefectBots 3174.0</li>\n</ol>\n<p>Another probably obvious point is that in a non-zero sum game it doesn't matter too much how many opponents you beat in direct confrontation. The only undefeated strategies that won all their matches except one draw, <strong>L</strong> and <strong>Q</strong>, finished last.</p>\n<p><strong>The Evolutionary<a href=\"#pz1\"><sup>1</sup></a> Tournament</strong></p>\n<p>The initial population consisted of 1,980 strategies, 90 of each kind. But that didn't last for long. The very nasty strategies started to die out rapidly and by generation 6 there were no representants of <strong>L</strong>, <strong>Q</strong>, <strong>U</strong> and <strong>Z</strong>. (I have to say I was very glad to see <strong>U</strong> extinct so early because with non-negligible number of <strong>U</strong>s in the pool the simulation run very, very slowly.) By generation 40, <strong>M</strong>, <strong>N</strong> and <strong>P</strong> were also gone. Unfortunately, 100 was too low as a number of generations to see any interesting effects associated with environmental change as different strategies become extinct (but see the next section for a longer simulation); the strategies successful in the round-robin tournament were generally successful here as well.</p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_0.png?v=c0ac98e434b3b7380a538814d5702d33\" alt=\"\" width=\"360\" height=\"226\" /></p>\n<p>The graph shows the history of subpopulations (<em>TfT variants are blue, Avengers red, Lists green, CliqueBots purple, Unclassified strategies yellow, the DefectBot gray and FullyRandom is black</em>). The final standings (orderd by number of copies in generation 100) are summarised in the following table.</p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_3.png\" alt=\"\" width=\"552\" height=\"29\" /></p>\n<p>The gold and silver medals are once again awarded to malthrin and Eugine_Nier. There is a change at the third place which is now occupied by Caerbannog's 20%-forgiving tit-for-tat.</p>\n<p>One remarkable fact is the absolute failure of CliqueBots which were without doubt designed to succeed in the evolutionary tournament. The idea was, perhaps, that CliqueBots win by cooperating among themselves and punish the others by defecting. That may be a working meta-strategy once you are dominant in the population, but for players starting as small minority it is suicidal. It would be a pretty bad idea even if most contestants were CliqueBots: while TfT variants generally cooperate with each other regardless of slight differences between their source codes, CliqueBots are losing points in defectionful matches even against other CliqueBots except those of their own species.</p>\n<p><strong>The Control Group<br /></strong></p>\n<p>I run the same experiment with readers of my blog and thus I have a control group to match against the LW competitors. The comparison is done simply by putting all strategies together in an even larger tournament. Before showing the results, let me introduce the control group strategies.</p>\n<p><strong>C1</strong>: <em>Let p be the relative frequency of opponent's cooperations (after my cooperation if I have cooperated last turn / after my defection if I have defected). If the relative frequency can't be calculated, let p = 1/2. Let E<sub>c</sub> = 5p - 7 and E<sub>d</sub> = 6p - 6. Cooperate with probability exp E<sub>c</sub> / (exp E<sub>c</sub> + exp E<sub>d</sub>).</em></p>\n<p>This strategy was in many respects similar to strategy <strong>U</strong>: mathematically elegant, but very unsuccessful.</p>\n<p><strong>C2</strong>: <em>Defect if and only if the opponent had defected more than twice or if it defected on the first turn.</em></p>\n<p><strong>C3</strong>: <em>If the opponent played the same move on the last and last but one turn, play that. Else, play the opposite what I have played last turn. On turns 1 and 2 play defect, cooperate.<br /></em></p>\n<p><strong>C4</strong>: <em>Cooperate on the first three turns, defect on the last two. Else, cooperate if the opponent has cooperated at least 85% of the time.</em></p>\n<p><strong>C5</strong>: <em>Cooperate on the first three turns, defect on the last turn. Else, cooperate with probability equal to the proportion of opponent's cooperations.</em></p>\n<p><strong>C6</strong>: <em>Tit-for-tat, but cooperate after the opponent's first defection.</em></p>\n<p><strong>C7</strong>: <em>On the first turn play randomly, on turns 2n and 2n+1 play what the opponent played on turn n.</em></p>\n<p><strong>C8</strong>: <em>Defect, if the opponent defected on two out of three preceding turns, or if I have defected on the last turn and cooperated on the last but one turn, or if I haven't defected in preceding 10 turns. On turns 20, 40, 60 and 80 cooperate always.</em></p>\n<p><strong>C9</strong>: <em>Have two substrategies: 1. tit for two tats and 2. defect after opponent's first defection. Begin with 1. After each tenth turn change the substrategy if the gain in the last ten turns was between 16 and 34 points. If the substrategy is changed, reset all defection counters.</em></p>\n<p><strong>C10</strong>: <em>Turns 1 and 2: cooperate. Turns 3-29: tit for tat. Turns 30-84: defect if the opponent defected more than seven times, else tit for tat. Turn 85: defect. Turns 86 and 87: defect if the opponent defected more than seven times, else cooperate. Turns 88-97: If the opponent defected on turn 87 or more than four times during the match, defect. Else cooperate. Turns 98-100: defect.</em></p>\n<p><strong>C11</strong>: <em>Turn 1: cooperate. Turn 2: tit for tat. Turn 100: defect. Else, if the opponent defected on the last and (I have cooperated or the opponent has defected on the last but one turn) defect, else cooperate.</em></p>\n<p>Two round-robin tournament standings are given in the following tables (two tournaments were played to illustrate the r&ocirc;le of randomness).</p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_4.png?v=b971bcdc16d1c82cdf1d72345e40aff3\" alt=\"\" width=\"146\" height=\"525\" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src=\"http://images.lesswrong.com/t3_7f2_5.png?v=a8dfa43ee91e4a7272b8a5dbcfc4f81e\" alt=\"\" width=\"146\" height=\"525\" /></p>\n<p>You can see how the new strategies had changed the standings. The original winner, <strong>I</strong>, now hovers around rather unremarkable 15th position, 10th among LW strategies. The average score for the LW strategies (calculated from the second tournament) is 9702.9, for the control group only 9296.9. How much this means that reading LW improves game-theoretical skills I leave to the readers' judgement.</p>\n<p>In the evolutionary setting the strategy <strong>I</strong> recovered back its leading position. Here are the 100th generation populations and the graph:</p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_6.png?v=fee9d7b9d9451a6da9d30b03bac65348\" alt=\"\" width=\"832\" height=\"29\" /></p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_7.png?v=21f07517ad3710d49bd543c199c17d2a\" alt=\"\" width=\"360\" height=\"223\" /></p>\n<p>(In the graph, LW strategies are green and the control group strategies are red.)</p>\n<p>It is relatively clear that the situation after hudred generations is not equilibrium and further changes would ensue if the simulation continued. Therefore I have run one more simulation, this time lasting for thousand of generations. This was more interesting, since more strategies died out. The first was <strong>C10</strong> which was gone after generation 125. Around generation 300 there were only four surviving strategies from the control group (two of them moribund) and twelve from the original LW pool (two of them seriously endangered, too.) The leaders were <strong>I</strong> (with population 539), <strong>C4</strong> (432) and <strong>C11</strong> (183). The last one went through a steady slow decline and was soon overtaken by <strong>O</strong>, which assumed the third position around generation 420. At generation 600, only three strategies remained in the pool: <strong>I</strong> (932), <strong>C4</strong> (692) and <strong>O</strong> (374). Not long before that the populations of <strong>I</strong> and <strong>C4</strong> peaked and started to decline, while <strong>O</strong> continued to rise. In direct confrontation, <strong>O</strong> always beats both <strong>I</strong> and <strong>C4</strong> 397:390 which proved to be decisive when other strategies were eliminated. After the thousandth generation, there were 1374 copies of <strong>O</strong> and only 625 copies of both its competitors combined.</p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_8.png?v=11244814e86aa91dd6ea30c1109391cd\" alt=\"\" width=\"360\" height=\"222\" /></p>\n<p><strong>Final Remarks<br /></strong></p>\n<p>I have organised the prisoner's dilemma tournament mainly for fun, but hopefully the results can illustrate few facts about non-zero sum games. At least in the final simulation we can see how short-term success needn't imply long-term survival. The failure of CliqueBots shows that it isn't much prudent to be too restrictive about whom one cooperates with. There is one more lesson that I have learned: the strategies which I considered most beautiful (<strong>U</strong> and <strong>C1</strong>) played poorly and were the first to be eliminated from the pool. Both <strong>U</strong> and <strong>C1</strong> tried to experiment with the opponent's behaviour and use the results to construct a working model thereof. But that didn't work in this setting: while those strategies were losing points experimenting, dumb mechanical tit-for-tats were maximising their gain. There are situations when the cost of obtaining knowledge is higher than the knowledge is worth, and this was one of such situations. (Of course, both discussed strategies could have used the knowledge they had more efficiently, but that wouldn't significantly help.)</p>\n<p>I have deliberately run fixed-length matches to make the competition more interesting. Random-length iterated PDs are domain of tit-for-tats and it would be hard to devise a better strategy; in the fixed-length case the players had to think about the correct turn when to defect first when playing a nice strategy and the answer isn't trivial. The most successful strategies started defecting on the 98th or 99th turns.</p>\n<p>The question whether reading LessWrong improves performance in similar games remains unsolved as the evidence gathered in the tournament was very weak. The difference between the LW and control groups wasn't big. Other problem is that I don't know much about the control group authors (except one of them whom I know personally); it's not clear what part of population the control group represents. The judgement is more complicated by the apparent facts that not all participants were trying to win: the nameless author of <strong>A</strong> has explicitly declared a different goal (namely, to punish defectorish strategies) and it's likely that <strong>L</strong> was intended to signal author's cynicism (I am specifically thinking about one of the four DefectBot's originators) rather than to succeed in the tournament. Furthermore, many strategies were send by new users and only 11 out of 26 strategy authors have karma over 500, which means that the LW group doesn't faithfully represent LessWrong active participants.</p>\n<p>&nbsp;</p>\n<hr />\n<p><sup><a name=\"pz1\"></a>1</sup> <em>Evolutionary</em> may be a misnomer. The tournament models natural selection, but no changes and therefore evolution occurs. I have kept the designation to maintain consistency with the earlier post.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 1, "be2Mh2bddQ6ZaBcti": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hamma4XgeNrsvAJv5", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 110, "baseScore": 150, "extendedScore": null, "score": 0.000296, "legacy": true, "legacyId": "9614", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 150, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>About two weeks ago I <a href=\"/lw/79r/prisoners_dilemma_as_a_game_theory_laboratory/\">announced</a> an open competition for LessWrong readers inspired by Robert Axelrod's famous <a href=\"http://en.wikipedia.org/wiki/Evolution_of_cooperation#Axelrod.27s_Tournaments\">tournaments</a>. The competitors had to submit a strategy which would play an iterated prisoner's dilemma of fixed length: first in the round-robin tournament where the strategy plays a hundred-turn match against each of its competitors exactly once, and second in the evolutionary tournament where the strategies are randomly paired against each other and their gain is translated in number of their copies present in next generation; the strategy with the highest number of copies after generation 100 wins. More details about the rules were described in the announcement. This post summarises the results.</p>\n<p><a id=\"more\"></a></p>\n<p><strong id=\"The_Zoo_of_Strategies\">The Zoo of Strategies</strong></p>\n<p>I have received 25 contest entries containing 21 distinct strategies. Those I have divided into six classes based on superficial similarities (except the last class, which is a catch-all category for everything which doesn't belong anywhere else, something like <em>adverbs</em> within the classification of parts of speech or now defunct <em>vermes</em> in the animal kingdom). The first class is formed by <strong><em>Tit-for-tat variants</em></strong>, probably the most obvious choice for a potentially successful strategy. Apparently so obvious that at least one commenter <a href=\"/lw/79r/prisoners_dilemma_as_a_game_theory_laboratory/4p3f\">declared</a> high confidence that tit-for-tat will make more than half of the strategy pool. That was actually a good example of misplaced confidence, since the number of received tit-for-tat variants (where I put anything which behaves like tit-for-tat except for isolated deviations) was only six, two of them being identical and thus counted as one. Moreover there wasn't a single true tit-for-tatter among the contestants; the closest we got was</p>\n<p><strong>A</strong> (-, -): <em>On the first turn of each match, cooperate. On every other turn, with probability 0.0000004839, cooperate; otherwise play the move that the opponent played on the immediately preceding turn.</em></p>\n<p>(In the presentation of strategies, the letter in bold serves as a unique identificator. The following parentheses include the name of the strategy \u2014 if the author has provided one \u2014 and the name of the author. I use the author's original description of the strategy when possible. If that's too long, an abbreviated paraphrase is given. If I found the original description ambiguous, I may give a slightly reformulated version based on subsequent clarifications with the author.) The author of <strong>A</strong> was the only one who requested his/her name should be withheld and the strategy is nameless, so both arguments in the bracket are empty. The reason for the obscure probability was to make the strategy unique. The author says:</p>\n<blockquote>\n<p>I wish to enter a trivial variation on the tit-for-tat strategy. (The trivial variation is to force the strategy to be unique; I wish to punish defectorish strategies by having lots of tit-for-tat-style strategies in the pool.)</p>\n</blockquote>\n<p>This was perhaps a slight abuse of rules, but since I am responsible for failing to make the rules immune to abuse, I had to accept the strategy as it is. Anyway, it turned out that the trivial variation was needless for the stated purpose.</p>\n<p>The remaining strategies from this class were more or less standard with <strong>B</strong> being the most obvious choice.</p>\n<p><strong>B</strong> (-, <a href=\"/user/Alexei\">Alexei</a>): <em>Tit-for-Tat, but always defect on last turn.</em></p>\n<p><strong>C</strong> (-, <a href=\"/user/Caerbannog\">Caerbannog</a>): <em>Tit-for-tat with 20% chance of forgiving after opponent's defection. Defect on the last turn.</em></p>\n<p><strong>D</strong> (-, <a href=\"/user/fubarobfusco\">fubarobfusco</a> and <a href=\"/user/DuncanS\">DuncanS</a>): <em>Tit-for-tat with 10% chance of forgiving.</em></p>\n<p><strong>E</strong> (-, <a href=\"/user/Jem\">Jem</a>): <em>First two turns cooperate. Later tit-for-tat with chance of forgiving equal to 1/2<sup>x</sup> where x is equal to number of opponent's defections after own cooperations. Last turn defect.</em></p>\n<p>The next category of strategies I call <strong><em>Avengers</em></strong>. The Avengers play a nice strategy until the opponent's defective behaviour reaches a specified threshold. After that they switch to irrevocable defection.</p>\n<p><strong>F</strong> (-, <a href=\"/user/Eugine_Nier/\">Eugine_Nier</a>): <em>Standard Tit-for-Tat with the following modifications: 1) Always defect on the last move. 2) Once the other player defects 5 times, switch to all defect.</em></p>\n<p><strong>G</strong> (-, <a href=\"/user/rwallace\">rwallace</a>): <em>1. Start with vanilla tit-for-tat. 2. When the opponent has defected a total of three times this match, switch to always defect for the rest of the match. 3. If the number of rounds is known and fixed, defect on the last round.</em></p>\n<p><strong>H</strong> (-, <a href=\"/user/Michaelos\">Michaelos</a>): <em>Tit for Tat, with two exceptions: 1: Ignore all other considerations and always defect on the final iteration. 2: After own defection and opponent's cooperation, 50 percent of the time, cooperate. The other 50 percent of the time, always defect for the rest of the game.<br></em><br><strong>I</strong> (-, <a href=\"/user/malthrin/\">malthrin</a>): <em>if OpponentDefectedBefore(7) then MyMove=D else if n&gt;98 then MyMove=D else MyMove=OpponentLastMove</em></p>\n<p><strong>J</strong> (<em>Vengeful Cheater</em>, <a href=\"/user/Vaniver/\">Vaniver</a>): <em>Set Rage = 0. Turn 1: cooperate. Turn 2-99: If the opponent defected last round, set Rage to 1. (If they cooperate, no change.) If Rage is 1, defect. Else, cooperate. Turn 100: defect.</em><br><br><strong>K</strong> (<em>Grim Trigger</em>, <a href=\"/user/shinoteki/\">shinoteki</a>): <em>Cooperate if and only if the opponent has not defected in the current match.</em></p>\n<p>Then we have a special \"class\" of <em><strong>DefectBots</strong></em> (a name stolen from <a href=\"/lw/767/decision_theory_paradox_pd_with_three_implies/\">Orthonormal's excellent article</a>) consisting of only one single strategy. But this is by far the most popular strategy&nbsp;\u2014 I have received it in four copies. Unfortunately for DefectBots, according to the rules they have to be included in the pool only once. The peculiar name for the single DefectBot comes from wedrifid:</p>\n<p><strong>L</strong> (<em>Fuck them all!</em>, <a href=\"/user/MileyCyrus/\">MileyCyrus</a> and <a href=\"/user/wedrifid/\">wedrifid</a> and <a href=\"/user/vespiacic/\">vespiacic</a> and <a href=\"/user/peter_hurford/\">peter_hurford</a>): <em>Defect.</em></p>\n<p>Then, we have a rather heterogenous class of <em><strong>Lists</strong></em>, whose only common thing is that their play is specified by a rather long list of instructions.</p>\n<p><strong>M</strong> (-, <a href=\"/user/JesusPetry/\">JesusPetry</a>): <em>Turn 1: cooperate. Turns 2 to 21: tit-for-tat. Turn 22: defect. Turn 23: cooperate. Turn 24: if opponent cooperated in turns 21 and 22, cooperate; otherwise, do whatever the opponent did on previous turn. Then tit-for-tat, the scenario from turns 22-24 repeats itself in turns 35-37, 57-59, 73-75. Turns 99 and 100: defect.</em></p>\n<p><strong>N</strong> (-, <a href=\"/user/FAWS\">FAWS</a>): <em>1. Cooperate on the first turn. 2. If the opponent has defected at least 3 times: Defect for the rest of the match. 3. Play tit for tat unless specified otherwise. 4. If the opponent has cooperated on the first 20 turns: Randomly choose a turn between 21 and 30. Otherwise continue with 11. 5. If the opponent defects after turn 20, but before the chosen turn comes up: Play CDC the next three turns, then continue with 12. 6. Defect on the chosen turn if the opponent hasn't defected before. 7. If the opponent defects on the same turn: Cooperate, then continue with 12. 8. If the opponent defects on the next turn: Cooperate, then continue with 11. 9. If the opponent defects on the turn after that (i. e. the second turn after we defected): Cooperate, then continue with 12. 10. If the opponent cooperates on every turn up to and including two turns after the defection: Defect until the opponent defects, then cooperate twice and continue with 11. 11. Defect on the last two turns (99 and 100). 12. Defect on the last two turns unless the opponent defected exactly once.</em><br><br><strong>O</strong> (<em>Second Chance</em>, <a href=\"/user/benelliott\">benelliott</a>): <em>Second Chance is defined by 5 rules. When more than one rule applies to the current situation, always defer to the one with the lower number. Rules: 1. Co-operate on move 1, defect on moves 98, 99 and 100. 2. If Second Chance has co-operated at least 4 times (excluding the most recent move) and in every case co-operation has been followed by defection from the other strategy, then always defect from now on. 3. If Second Chance has co-operated at least 8 times, and defected at least 10 times (excluding the previous move), then calculate x, the proportion of the time that co-operation has been followed by co-operation and y, the proportion of the time that defection has been followed by co-operation. If 4x &lt; 6y + 1 then defect until that changes. 4. If the number of times the opposing strategy has defected (including the most recent move) is a multiple of 4, co-operate. 5. Do whatever the opposing strategy did on the previous move.</em></p>\n<p>The next class are <em><strong>CliqueBots</strong></em> (once more a name from the Orthonormal's post). CliqueBots try to identify whether their opponent is a copy of themselves and then cooperate. If they identify a foreign opponent, they usually become pretty nasty.</p>\n<p><strong>P</strong> (<em>Simple Identity ChecK</em>, <a href=\"/user/red75/\">red75</a>): <em>Move 1: cooperate. Moves 2 to 57: tit-for-tat. Move 58 - defect. Move 59 - if moves 0-57 were coop-coop and 58 was defect-defect then coop else defect. Moves 60-100: if my move 59 was coop then tit-for-tat else defect.<br></em></p>\n<p><strong>Q</strong> (<em>EvilAlliance</em>, <a href=\"/user/ArisKatsaris/\">ArisKatsaris</a>): <em>Start with 5 defections. On later turns, if the opponent had cooperated at least once in the first 5 turns or defected ever since, defect; else cooperate.</em></p>\n<p>The remaining strategies are <strong><em>Unclassified</em></strong>, since they have little in common. Here they come:</p>\n<p><strong>R</strong> (<em>Probe &amp; Punish</em>, <a href=\"/user/Eneasz\">Eneasz</a>): <em>1. Cooperate for a turn. 2. Cooperate for a second turn. If opponent cooperated on the second turn, continue to cooperate in every subsequent turn until the opponent defects. 3. If/when the opponent defects, defect for 12 consecutive rounds. 4. Return to 1, repeat.</em></p>\n<p><strong>S</strong> (<em>Win-stay lose-shift</em>, <a href=\"/user/nerzhin/\">Nerzhin</a>): <em>It cooperates if and only if it and the opponent both played the same strategy on the last round.</em></p>\n<p>The author of <strong>S</strong> says he's adopted the strategy from Nowak and Sigmund, Nature 364 pp. 56-58.</p>\n<p><strong>T</strong> (<em>Tit for Two Tats</em>, <a href=\"/user/DataPacRat/\">DataPacRat</a>): <a href=\"http://en.wikipedia.org/wiki/Tit_for_two_tats#Tit_for_two_tats\"><em>As the name suggests.</em></a></p>\n<p><strong>U</strong> (<em>RackBlockShooter</em>, <a href=\"/user/Nic_Smith\">Nic_Smith</a>): <em>The code is available <a href=\"http://ariarule.com/files/rbs.html\">here</a>.</em></p>\n<p>This one is a real beast, with code more that twice as long as that of its competitors. It actually models the opponent's behaviour as a random process specified by two parameters: probability of cooperation when the second strategy (which is the RackBlockShooter) also cooperates, and probability of cooperation when RBS defects. This was acausal: the parameters depend on RBS's behaviour in the same turn, not in the preceding one. Moreover, RBS stores whole probability distributions of those parameters and correctly updates them in an orthodox Bayesian manner after each turn. To further complicate things, if RBS thinks that the opponent is itself, it cooperates in the CliqueBottish style.</p>\n<p>And finally, there's a strategy included by default:</p>\n<p><strong>Z</strong> (<em>Fully Random</em>, default): <em>Cooperate with 50% probability.</em></p>\n<p><strong id=\"The_Round_Robin_Tournament\">The Round-Robin Tournament</strong></p>\n<p>Each strategy had played 21 matches where a total maximum of 14,700 points could theoretically be won. A more reasonable threshold was 8,400 points: an award for any member of a pool consisting solely of nice strategies (i.e. those which never defect first). No strategy reached this optimum. The standings are given in the following table (<em>columns from left: strategy identifier, matches won, matches drawn, matches lost, total points</em>).</p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_1.png\" alt=\"\" width=\"112\" height=\"349\"></p>\n<p>The winner is malthrin's unnamed strategy. Second and third place are occupied by Eugine_Nier's modified tit-for-tat and benelliott's <em>Second Chance</em>, respectively. Congratulations!</p>\n<p>Here are results of all matches (<em>view the picture in full size by right-clicking</em>).</p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_2.png?v=9fff80f7f2b67544a2f94de8ed92df21\" alt=\"\" width=\"733\" height=\"185\"></p>\n<p>Is there something interesting to say about the results? Well, there were three strategies which performed worse than random: the DefectBot <strong>L</strong>, the CliqueBot <strong>Q</strong> (which effectively acted like a DefectBot) and the complex strategy <strong>U</strong>. As expected, very nasty strategies were unsuccessful. On the other hand, totally nice strategies weren't much successful either. No member of the set {<strong>A</strong>, <strong>D</strong>, <strong>K</strong>, <strong>R</strong>, <strong>S</strong>, <strong>T</strong>} got into the first five. Not defecting on the last turn was a mistake, defecting on the last two turns seemed even a better choice. As for the somewhat arbitrary classes, the average gains were as such:</p>\n<ol>\n<li>Tit-for-tat variants 7197.4</li>\n<li>Avengers 7144.3</li>\n<li>Lists 6551.3</li>\n<li>Unclassified 5963.8</li>\n<li>CliqueBots 4371.5</li>\n<li>DefectBots 3174.0</li>\n</ol>\n<p>Another probably obvious point is that in a non-zero sum game it doesn't matter too much how many opponents you beat in direct confrontation. The only undefeated strategies that won all their matches except one draw, <strong>L</strong> and <strong>Q</strong>, finished last.</p>\n<p><strong id=\"The_Evolutionary1_Tournament\">The Evolutionary<a href=\"#pz1\"><sup>1</sup></a> Tournament</strong></p>\n<p>The initial population consisted of 1,980 strategies, 90 of each kind. But that didn't last for long. The very nasty strategies started to die out rapidly and by generation 6 there were no representants of <strong>L</strong>, <strong>Q</strong>, <strong>U</strong> and <strong>Z</strong>. (I have to say I was very glad to see <strong>U</strong> extinct so early because with non-negligible number of <strong>U</strong>s in the pool the simulation run very, very slowly.) By generation 40, <strong>M</strong>, <strong>N</strong> and <strong>P</strong> were also gone. Unfortunately, 100 was too low as a number of generations to see any interesting effects associated with environmental change as different strategies become extinct (but see the next section for a longer simulation); the strategies successful in the round-robin tournament were generally successful here as well.</p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_0.png?v=c0ac98e434b3b7380a538814d5702d33\" alt=\"\" width=\"360\" height=\"226\"></p>\n<p>The graph shows the history of subpopulations (<em>TfT variants are blue, Avengers red, Lists green, CliqueBots purple, Unclassified strategies yellow, the DefectBot gray and FullyRandom is black</em>). The final standings (orderd by number of copies in generation 100) are summarised in the following table.</p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_3.png\" alt=\"\" width=\"552\" height=\"29\"></p>\n<p>The gold and silver medals are once again awarded to malthrin and Eugine_Nier. There is a change at the third place which is now occupied by Caerbannog's 20%-forgiving tit-for-tat.</p>\n<p>One remarkable fact is the absolute failure of CliqueBots which were without doubt designed to succeed in the evolutionary tournament. The idea was, perhaps, that CliqueBots win by cooperating among themselves and punish the others by defecting. That may be a working meta-strategy once you are dominant in the population, but for players starting as small minority it is suicidal. It would be a pretty bad idea even if most contestants were CliqueBots: while TfT variants generally cooperate with each other regardless of slight differences between their source codes, CliqueBots are losing points in defectionful matches even against other CliqueBots except those of their own species.</p>\n<p><strong id=\"The_Control_Group\">The Control Group<br></strong></p>\n<p>I run the same experiment with readers of my blog and thus I have a control group to match against the LW competitors. The comparison is done simply by putting all strategies together in an even larger tournament. Before showing the results, let me introduce the control group strategies.</p>\n<p><strong>C1</strong>: <em>Let p be the relative frequency of opponent's cooperations (after my cooperation if I have cooperated last turn / after my defection if I have defected). If the relative frequency can't be calculated, let p = 1/2. Let E<sub>c</sub> = 5p - 7 and E<sub>d</sub> = 6p - 6. Cooperate with probability exp E<sub>c</sub> / (exp E<sub>c</sub> + exp E<sub>d</sub>).</em></p>\n<p>This strategy was in many respects similar to strategy <strong>U</strong>: mathematically elegant, but very unsuccessful.</p>\n<p><strong>C2</strong>: <em>Defect if and only if the opponent had defected more than twice or if it defected on the first turn.</em></p>\n<p><strong>C3</strong>: <em>If the opponent played the same move on the last and last but one turn, play that. Else, play the opposite what I have played last turn. On turns 1 and 2 play defect, cooperate.<br></em></p>\n<p><strong>C4</strong>: <em>Cooperate on the first three turns, defect on the last two. Else, cooperate if the opponent has cooperated at least 85% of the time.</em></p>\n<p><strong>C5</strong>: <em>Cooperate on the first three turns, defect on the last turn. Else, cooperate with probability equal to the proportion of opponent's cooperations.</em></p>\n<p><strong>C6</strong>: <em>Tit-for-tat, but cooperate after the opponent's first defection.</em></p>\n<p><strong>C7</strong>: <em>On the first turn play randomly, on turns 2n and 2n+1 play what the opponent played on turn n.</em></p>\n<p><strong>C8</strong>: <em>Defect, if the opponent defected on two out of three preceding turns, or if I have defected on the last turn and cooperated on the last but one turn, or if I haven't defected in preceding 10 turns. On turns 20, 40, 60 and 80 cooperate always.</em></p>\n<p><strong>C9</strong>: <em>Have two substrategies: 1. tit for two tats and 2. defect after opponent's first defection. Begin with 1. After each tenth turn change the substrategy if the gain in the last ten turns was between 16 and 34 points. If the substrategy is changed, reset all defection counters.</em></p>\n<p><strong>C10</strong>: <em>Turns 1 and 2: cooperate. Turns 3-29: tit for tat. Turns 30-84: defect if the opponent defected more than seven times, else tit for tat. Turn 85: defect. Turns 86 and 87: defect if the opponent defected more than seven times, else cooperate. Turns 88-97: If the opponent defected on turn 87 or more than four times during the match, defect. Else cooperate. Turns 98-100: defect.</em></p>\n<p><strong>C11</strong>: <em>Turn 1: cooperate. Turn 2: tit for tat. Turn 100: defect. Else, if the opponent defected on the last and (I have cooperated or the opponent has defected on the last but one turn) defect, else cooperate.</em></p>\n<p>Two round-robin tournament standings are given in the following tables (two tournaments were played to illustrate the r\u00f4le of randomness).</p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_4.png?v=b971bcdc16d1c82cdf1d72345e40aff3\" alt=\"\" width=\"146\" height=\"525\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src=\"http://images.lesswrong.com/t3_7f2_5.png?v=a8dfa43ee91e4a7272b8a5dbcfc4f81e\" alt=\"\" width=\"146\" height=\"525\"></p>\n<p>You can see how the new strategies had changed the standings. The original winner, <strong>I</strong>, now hovers around rather unremarkable 15th position, 10th among LW strategies. The average score for the LW strategies (calculated from the second tournament) is 9702.9, for the control group only 9296.9. How much this means that reading LW improves game-theoretical skills I leave to the readers' judgement.</p>\n<p>In the evolutionary setting the strategy <strong>I</strong> recovered back its leading position. Here are the 100th generation populations and the graph:</p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_6.png?v=fee9d7b9d9451a6da9d30b03bac65348\" alt=\"\" width=\"832\" height=\"29\"></p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_7.png?v=21f07517ad3710d49bd543c199c17d2a\" alt=\"\" width=\"360\" height=\"223\"></p>\n<p>(In the graph, LW strategies are green and the control group strategies are red.)</p>\n<p>It is relatively clear that the situation after hudred generations is not equilibrium and further changes would ensue if the simulation continued. Therefore I have run one more simulation, this time lasting for thousand of generations. This was more interesting, since more strategies died out. The first was <strong>C10</strong> which was gone after generation 125. Around generation 300 there were only four surviving strategies from the control group (two of them moribund) and twelve from the original LW pool (two of them seriously endangered, too.) The leaders were <strong>I</strong> (with population 539), <strong>C4</strong> (432) and <strong>C11</strong> (183). The last one went through a steady slow decline and was soon overtaken by <strong>O</strong>, which assumed the third position around generation 420. At generation 600, only three strategies remained in the pool: <strong>I</strong> (932), <strong>C4</strong> (692) and <strong>O</strong> (374). Not long before that the populations of <strong>I</strong> and <strong>C4</strong> peaked and started to decline, while <strong>O</strong> continued to rise. In direct confrontation, <strong>O</strong> always beats both <strong>I</strong> and <strong>C4</strong> 397:390 which proved to be decisive when other strategies were eliminated. After the thousandth generation, there were 1374 copies of <strong>O</strong> and only 625 copies of both its competitors combined.</p>\n<p><img src=\"http://images.lesswrong.com/t3_7f2_8.png?v=11244814e86aa91dd6ea30c1109391cd\" alt=\"\" width=\"360\" height=\"222\"></p>\n<p><strong id=\"Final_Remarks\">Final Remarks<br></strong></p>\n<p>I have organised the prisoner's dilemma tournament mainly for fun, but hopefully the results can illustrate few facts about non-zero sum games. At least in the final simulation we can see how short-term success needn't imply long-term survival. The failure of CliqueBots shows that it isn't much prudent to be too restrictive about whom one cooperates with. There is one more lesson that I have learned: the strategies which I considered most beautiful (<strong>U</strong> and <strong>C1</strong>) played poorly and were the first to be eliminated from the pool. Both <strong>U</strong> and <strong>C1</strong> tried to experiment with the opponent's behaviour and use the results to construct a working model thereof. But that didn't work in this setting: while those strategies were losing points experimenting, dumb mechanical tit-for-tats were maximising their gain. There are situations when the cost of obtaining knowledge is higher than the knowledge is worth, and this was one of such situations. (Of course, both discussed strategies could have used the knowledge they had more efficiently, but that wouldn't significantly help.)</p>\n<p>I have deliberately run fixed-length matches to make the competition more interesting. Random-length iterated PDs are domain of tit-for-tats and it would be hard to devise a better strategy; in the fixed-length case the players had to think about the correct turn when to defect first when playing a nice strategy and the answer isn't trivial. The most successful strategies started defecting on the 98th or 99th turns.</p>\n<p>The question whether reading LessWrong improves performance in similar games remains unsolved as the evidence gathered in the tournament was very weak. The difference between the LW and control groups wasn't big. Other problem is that I don't know much about the control group authors (except one of them whom I know personally); it's not clear what part of population the control group represents. The judgement is more complicated by the apparent facts that not all participants were trying to win: the nameless author of <strong>A</strong> has explicitly declared a different goal (namely, to punish defectorish strategies) and it's likely that <strong>L</strong> was intended to signal author's cynicism (I am specifically thinking about one of the four DefectBot's originators) rather than to succeed in the tournament. Furthermore, many strategies were send by new users and only 11 out of 26 strategy authors have karma over 500, which means that the LW group doesn't faithfully represent LessWrong active participants.</p>\n<p>&nbsp;</p>\n<hr>\n<p><sup><a name=\"pz1\"></a>1</sup> <em>Evolutionary</em> may be a misnomer. The tournament models natural selection, but no changes and therefore evolution occurs. I have kept the designation to maintain consistency with the earlier post.</p>\n<p>&nbsp;</p>", "sections": [{"title": "The Zoo of Strategies", "anchor": "The_Zoo_of_Strategies", "level": 1}, {"title": "The Round-Robin Tournament", "anchor": "The_Round_Robin_Tournament", "level": 1}, {"title": "The Evolutionary1 Tournament", "anchor": "The_Evolutionary1_Tournament", "level": 1}, {"title": "The Control Group", "anchor": "The_Control_Group", "level": 1}, {"title": "Final Remarks", "anchor": "Final_Remarks", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "171 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 171, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vJAFXz6DWYnKq4Mue", "HT8jwNJ6vH7p9gaTT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2011-09-06T00:46:35.112Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-06T01:12:25.235Z", "modifiedAt": null, "url": null, "title": "[LINK] People are biased against creativity", "slug": "link-people-are-biased-against-creativity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.233Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "minderbinder", "createdAt": "2011-07-20T16:48:05.262Z", "isAdmin": false, "displayName": "minderbinder"}, "userId": "SPcZb9HWJg5nsvt4E", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3ceHCsiAWSb6Z9GyJ/link-people-are-biased-against-creativity", "pageUrlRelative": "/posts/3ceHCsiAWSb6Z9GyJ/link-people-are-biased-against-creativity", "linkUrl": "https://www.lesswrong.com/posts/3ceHCsiAWSb6Z9GyJ/link-people-are-biased-against-creativity", "postedAtFormatted": "Tuesday, September 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20People%20are%20biased%20against%20creativity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20People%20are%20biased%20against%20creativity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ceHCsiAWSb6Z9GyJ%2Flink-people-are-biased-against-creativity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20People%20are%20biased%20against%20creativity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ceHCsiAWSb6Z9GyJ%2Flink-people-are-biased-against-creativity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ceHCsiAWSb6Z9GyJ%2Flink-people-are-biased-against-creativity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 199, "htmlBody": "<p>A lot of people probably saw this on hacker news but I thought I'd share it anyway - <a href=\"http://www.news.cornell.edu/stories/Aug11/ILRCreativityBias.html\">People are biased against creative ideas, studies find</a></p>\n<p>To sum up, most people dislike uncertainty so much that they'll reject pretty much anything new, good or not. The article states that \"Anti-creativity bias can be so subtle that people are unaware of it, which can interfere with their ability to recognize a creative idea.\" By \"creative idea,\" I of course mean <a href=\"/lw/vm/lawful_creativity/\">lawful creativity</a> - the article seems to suggest that at a certain point, <em>every</em> creative suggestion starts to sound about as useful as \"let's put pictures of purple unicorns on the wall to help ourselves be more productive,\" if you're biased enough.</p>\n<p>What's a good way to fight this? Obviously solving the problem of&nbsp;<em>being creative</em> is a totally different matter. But I would suggest the usual \"if you were a different person injected into your own life to improve things\" approach and start by taking every single suggestion seriously and thinking it through as if you were only dealing with the issue for the very first time, and then as time went on, improve at making quick unbiased evaluations of creative ideas.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3ceHCsiAWSb6Z9GyJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 3, "extendedScore": null, "score": 7.653508348885288e-07, "legacy": true, "legacyId": "9678", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KKLQp934n77cfZpPn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-06T02:21:43.451Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Lens That Sees Its Flaws", "slug": "seq-rerun-the-lens-that-sees-its-flaws", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.260Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eWzaJPWAiCsK4bvGf/seq-rerun-the-lens-that-sees-its-flaws", "pageUrlRelative": "/posts/eWzaJPWAiCsK4bvGf/seq-rerun-the-lens-that-sees-its-flaws", "linkUrl": "https://www.lesswrong.com/posts/eWzaJPWAiCsK4bvGf/seq-rerun-the-lens-that-sees-its-flaws", "postedAtFormatted": "Tuesday, September 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Lens%20That%20Sees%20Its%20Flaws&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Lens%20That%20Sees%20Its%20Flaws%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeWzaJPWAiCsK4bvGf%2Fseq-rerun-the-lens-that-sees-its-flaws%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Lens%20That%20Sees%20Its%20Flaws%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeWzaJPWAiCsK4bvGf%2Fseq-rerun-the-lens-that-sees-its-flaws", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeWzaJPWAiCsK4bvGf%2Fseq-rerun-the-lens-that-sees-its-flaws", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p>Today's post, <a href=\"/lw/jm/the_lens_that_sees_its_flaws/\">The Lens That Sees Its Flaws</a> was originally published on 23 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Part of what makes humans different from other animals is our own ability to reason about our reasoning. Mice do not think about the cognitive algorithms that generate their belief that the cat is hunting them. Our ability to think about what sort of thought processes would lead to correct beliefs is what gave rise to Science. This ability makes our admittedly flawed minds much more powerful.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/7gg/seq_rerun_what_is_evidence/\">What is Evidence?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eWzaJPWAiCsK4bvGf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 7.653733591098198e-07, "legacy": true, "legacyId": "9681", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["46qnWRSR7L2eyNbMA", "Q9b7iahJrLheMBXas", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-06T03:46:35.252Z", "modifiedAt": null, "url": null, "title": "Case Study: Reading Edge's financial filings", "slug": "case-study-reading-edge-s-financial-filings", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:27.396Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PmrD2T6F82RkRkhQv/case-study-reading-edge-s-financial-filings", "pageUrlRelative": "/posts/PmrD2T6F82RkRkhQv/case-study-reading-edge-s-financial-filings", "linkUrl": "https://www.lesswrong.com/posts/PmrD2T6F82RkRkhQv/case-study-reading-edge-s-financial-filings", "postedAtFormatted": "Tuesday, September 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Case%20Study%3A%20Reading%20Edge's%20financial%20filings&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACase%20Study%3A%20Reading%20Edge's%20financial%20filings%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPmrD2T6F82RkRkhQv%2Fcase-study-reading-edge-s-financial-filings%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Case%20Study%3A%20Reading%20Edge's%20financial%20filings%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPmrD2T6F82RkRkhQv%2Fcase-study-reading-edge-s-financial-filings", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPmrD2T6F82RkRkhQv%2Fcase-study-reading-edge-s-financial-filings", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2124, "htmlBody": "<p><a name=\"TOC\"></a></p>\n<ul>\n<li><a href=\"#the-problem\"><span class=\"toc-section-number\">1</span> The problem</a></li>\n<li><a href=\"#the-solution\"><span class=\"toc-section-number\">2</span> The solution</a> \n<ul>\n<li><a href=\"#which-charity\"><span class=\"toc-section-number\">2.1</span> Which charity?</a> \n<ul>\n<li><a href=\"#the-filings\"><span class=\"toc-section-number\">2.1.1</span> The filings</a></li>\n<li><a href=\"#overview\"><span class=\"toc-section-number\">2.1.2</span> Overview</a></li>\n<li><a href=\"#income\"><span class=\"toc-section-number\">2.1.3</span> Income</a></li>\n<li><a href=\"#expenses\"><span class=\"toc-section-number\">2.1.4</span> Expenses</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#final-thoughts\"><span class=\"toc-section-number\">3</span> Final thoughts</a></li>\n<li><a href=\"#other-examples\"><span class=\"toc-section-number\">4</span> Other examples</a></li>\n</ul>\n<blockquote>\n<p>Abstract: an example of how to read charity filings, using <a href=\"http://edge.org/\">Edge.org</a></p>\n</blockquote>\n<h1 id=\"the-problem\"><a id=\"more\"></a><br /></h1>\n<h1><a href=\"#TOC\"><span class=\"header-section-number\">1</span> The problem</a></h1>\n<p>Recently, on <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\"><code>#lesswrong</code></a>:</p>\n<blockquote><strong>epitron&gt;</strong> and you're right, Edge was a bit better back when people wrote essays, and the mailing list of smart people critiqued them <br /> <strong>epitron&gt;</strong> i guess we can chalk this up to the fall of journalism. or more likely, the way the financial system broke (which Lanier talks about) <br /> <strong>gwern&gt;</strong> Edge seems quite profitable, publishing prestigious pop intellectual books <br /> <strong>epitron&gt;</strong> o_O edge doesn't look very profitable to me. they just redesigned their website, after like 15 years <br /><strong>gwern&gt;</strong> the videos, the dinners, the books... <br /> <strong>epitron&gt;</strong> I think advertising pop-science books by contributors is the only way to bribe them to contribute <br /> <strong>epitron&gt;</strong> hmm apparently Brockman has a thing for videotaping conversations with smart people. he's been doing it since the 60's <br /><strong>epitron&gt;</strong> and publishing books is not a very profitable venture <br /> <strong>epitron&gt;</strong> maybe it's somewhat lucrative if you already have an audience who's waiting to buy up the next book... so that you don't have to advertise it <br /> <strong>epitron&gt;</strong> actually, Edge is a nonprofit. They have to publish quarterly reports, don't they?</blockquote>\n<p>Yes! That&rsquo;s quite true - <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Edge_Foundation,_Inc.\">Edge Foundation, Inc.</a> is in fact a non-profit, and a <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/501%28c%29_organization\">501(c)</a>(3) charity to boot. So we <em>can</em> answer these questions.</p>\n<h1 id=\"the-solution\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> The solution</a></h1>\n<p>But how are we to do this?</p>\n<p>As in previous examples, we have to know <em>where</em> such filings can be found. We ask the great Google: <a href=\"https://encrypted.google.com/search?num=100&amp;q=501c3%20filings\">&ldquo;501c3 filing&rdquo;</a>, and it tells us: <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/GuideStar\">GuideStar.org</a>. To quote Wikipedia:</p>\n<blockquote>\n<p>&ldquo;Anyone who is interested in reviewing a nonprofit&rsquo;s recent <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/IRS_tax_forms#990\">Forms 990</a> can register at <a href=\"https://www2.guidestar.org/\">Guidestar.org</a> and download them for free.&rdquo;</p>\n</blockquote>\n<p>We <a href=\"https://commerce.guidestar.org/GuideStar/newaccount.aspx\">register</a> (it&rsquo;s the normal signup form), and now we can search and look at the Form 990s.</p>\n<h2 id=\"which-charity\"><a href=\"#TOC\"><span class=\"header-section-number\">2.1</span> Which charity?</a></h2>\n<p>&lsquo;Edge&rsquo; turns up 5,933 results; &lsquo;Edge.org&rsquo; turns up 0; &lsquo;Edge Foundation&rsquo; turns up 354. We&rsquo;ll start with that last one. There turn out to be 4 or 5 &lsquo;Edge Foundation Inc&rsquo; on the first page, so here we have to either look at every one or apply some outside knowledge.</p>\n<p>In this case, as Epitron mentioned, the Foundation is powered by book agent Brockman, and where is the American book industry famously concentrated? In New York City. Exactly <a href=\"https://www2.guidestar.org/organizations/13-3528667/edge-foundation.aspx\">1</a> of those 4 is in NYC. If we look at the filings, this is in fact the right Edge Foundation.</p>\n<h3 id=\"the-filings\"><a href=\"#TOC\"><span class=\"header-section-number\">2.1.1</span> The filings</a></h3>\n<p>GuideStar has 3 Form 990s available for free:</p>\n<ul>\n<li><a href=\"http://www.guidestar.org/FinDocuments/2008/133/528/2008-133528667-04d54cd1-F.pdf\">2008</a></li>\n<li><a href=\"http://www.guidestar.org/FinDocuments/2009/133/528/2009-133528667-05f0fda4-F.pdf\">2009</a></li>\n<li><a href=\"http://www.guidestar.org/FinDocuments/2010/133/528/2010-133528667-072c12f4-F.pdf\">2010</a></li>\n</ul>\n<p>We&rsquo;re only really interested in the current situation, so we look at the 2010 filing.</p>\n<h3 id=\"overview\"><a href=\"#TOC\"><span class=\"header-section-number\">2.1.2</span> Overview</a></h3>\n<p>Tax filings have a bad reputation for being impenetrable, but like programming, they&rsquo;re fairly logical; you need to pay attention to the labels and follow the lines with your eyes, and then they&rsquo;re pretty clear.</p>\n<p>The first page contains all the highest-level figures and give us the overall look at what kind of charity this is.</p>\n<ol style=\"list-style-type: decimal\">\n<li>Income<sup><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></sup>: $159,000</li>\n<li>Expenses<sup><a id=\"fnref2\" class=\"footnoteRef\" href=\"#fn2\">2</a></sup>: $191,000</li>\n<li>Net assets<sup><a id=\"fnref3\" class=\"footnoteRef\" href=\"#fn3\">3</a></sup>: $192,000</li>\n</ol>\n<p>Just from these 3 numbers, what might we infer? (As in the Girl Scouts essay, it&rsquo;s a good idea to pause and reflect. What does one expect, what is one looking for?)</p>\n<ul>\n<li>\n<p>It&rsquo;s a <em>small</em> charity, people-wise</p>\nEven if they are accepting relatively nominal incomes for NYC, like $50k, there simply cannot be more than 2 or 3 employees. And if they work for free, there still can&rsquo;t be more than 30 or 50, simply because there would be the usual corporate/business overhead for the computers, utilities, insurance, etc. Girl Scouts&rsquo;s NYC headquarters might spend $159k just on legal advice or investment fees. However, Edge is something like twice as large as another charity whose filings I&rsquo;ve read through, the Lifeboat Foundation, so there&rsquo;s always room to get smaller.</li>\n<li>\n<p>Expenses &gt; Income</p>\nWikipedia says Edge was incorporated in 1988<sup><a id=\"fnref4\" class=\"footnoteRef\" href=\"#fn4\">4</a></sup>, which suggests Edge has a regular cycle of increases and decreases in its net worth. 2010 would seem to be in a &lsquo;decrease&rsquo; part of the cycle. This is support for the book publishing theory - donations tend to be more consistent than the occasional delay-filled infusion of royalties from a published book.</li>\n<li>\n<p>Edge has built up a substantial cushion of assets - an entire year&rsquo;s revenue.</p>\n<p>Like above, this suggests a regular excess of revenue. It&rsquo;s also possibly interesting from an organizational point of view: why such reserves? It&rsquo;s not as if Edge maintains an expensive infrastructure where the slightest disruption is disastrous (eg. the power grid). This suggests maybe Edge simply can&rsquo;t think of any way to responsibly spend it. There is only so much one can spend on a fancy dinner/banquet before it is obscene, and Edge&rsquo;s participants tend to the meritocratic, and might object to overly lavish settings. Of course, this could just be a warning sign of some skulduggery.</p>\n</li>\n</ul>\n<h3 id=\"income\"><a href=\"#TOC\"><span class=\"header-section-number\">2.1.3</span> Income</a></h3>\n<p>After page 1, we&rsquo;ll skip around a little. I&rsquo;m interested in where their money is coming from. We have to skip all the way down to page 12 before we see anything about their <em>income</em>. Baldly on line 1 of Part XIV-A on page 12, we see &lsquo;Book contract&rsquo; and $83,777.</p>\n<p>Well, that explains a lot! It&rsquo;s not listed as royalties, and it&rsquo;s a fairly regular looking number, so we can probably infer that this money is a <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Advance_against_royalties\">book advance</a>, and a remarkably high one - Edge has produced many books by this point, so this advance would not be gambling but a good indicator of how much the publisher thinks they will sell. At a dollar or two in royalties per book, that implies sales in the scores of thousands; perhaps not best seller list, but a very good show indeed</p>\n<p>That leaves ~$75,000 of the $159,000 unaccounted for. We continue skimming until we hit page 15, the second page of &lsquo;Schedule B&rsquo;/&lsquo;Schedule of Contributors&rsquo;. There are two entries:</p>\n<ul>\n<li>&lsquo;Enhanced Education&rsquo;, $50,000 Address is in the <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/British_Virgin_Islands\">British Virgin Islands</a>; this is interesting, inasmuch as the British Virgin Islands are one of the leading <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Corporate_haven#Commonwealth_.26_Western_Europe\">corporate havens</a> and the listed address is shared by a <a href=\"http://www.taxexemptworld.com/organizations/st-thomas-vi-virgin-islands.asp\">great many</a> other non-profits.</li>\n<li>&lsquo;Parasolholdings&rsquo;, $25,000 This strange entry doesn&rsquo;t list a real address - just &lsquo;c/o Edge Fdn&rsquo;. Googling doesn&rsquo;t help at all; there are a number of &lsquo;Parasol Holdings&rsquo; worldwide, none of which have much information online. (One New Zealand entity provides copious filings&hellip; all of which say nothing whatsoever beyond the shareholders&rsquo; names &amp; addresses.)</li>\n</ul>\n<p>Bingo. So Edge&rsquo;s entire income is derived from its book contract and two odd-looking donators.</p>\n<p>If we check the 2009 filings, Enhanced Education donated $53,000 that year, but Parasolholdings did not; instead, $25,000 came from &lsquo;J.E. Safra&rsquo; (again &lsquo;c/o Edge Fdn&rsquo;). This is more helpful; a J.E. Safra was the father of billionaire <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Edmond_Safra\">Edmond Safra</a>, but this seems to be a red herring (that J.E. is dead) - retrying the Google search as &lsquo;edge safra&rsquo; leads us straight to <a href=\"http://www.edge.org/memberbio/jacob_e_safra\">a biography</a> on Edge.org:</p>\n<blockquote>\n<p>&lsquo;Jacob E. &ldquo;Jacqui&rdquo; Safra, a Swiss investor, is the Chairman of Encyclopedia Britannica, and owner of Spring Mountain Vineyards, a large wine-growing estate located in Saint Helena, California.&rsquo;</p>\n</blockquote>\n<p>Well, that settles Parasolholdings - it&rsquo;s some sort of tax dodge or corporate cutout for Jacob. Nothing wrong with that. Enhanced Education remains a mystery; it does not show up in Google, GuideStar, or a few other places I checked.</p>\n<h3 id=\"expenses\"><a href=\"#TOC\"><span class=\"header-section-number\">2.1.4</span> Expenses</a></h3>\n<p>We backtrack to the beginning. The first hit is page 6, where we find the employment data: Part VIII, 1. Remember the observation that the expenses could not support many employees? It is borne out, with Edge having just 3 employees, each working a quarter of an hour a week, for free. (The secretary, Katinka Mason, doubles as the book-keeper too.) Very laudable.</p>\n<p>Page 7 tantalizes us with Part IX-A, lines 1&ndash;2, but they prove a bust:</p>\n<ol style=\"list-style-type: decimal\">\n<li>&lsquo;Development of communication vehicles for the literary world over the Internet&rsquo;, $172,999</li>\n<li>&lsquo;Publication concerning Internet communication vehicles&rsquo;, $13,144</li>\n</ol>\n<p>The first is just the canned mission statement you would have noticed elsewhere in the filing. The second is opaque; expenses somehow related to the book advance? Page 8 tempts us with a different number, the expenses at $178,226 (different from the just quoted #1), but no more details.</p>\n<p>Page 10 gives us interesting details like the founding day of Edge.org, and expenses from 2010&ndash;2007:</p>\n<ul>\n<li>2007: $123,672</li>\n<li>2008: $99,217</li>\n<li>2009: $98,249</li>\n<li>2010: $178,226</li>\n</ul>\n<p>Notice the cyclical trend here - high to low to high. Maybe this is the explanation for the cash reserve previously mentioned: there might be a $20,000 and then an $80,000 surplus in the two belt-tightened years, that would make up at least half of the reserve.</p>\n<p>Back to page 1 for the detailed expenses:</p>\n<ul>\n<li>Accounting fees: $5,227</li>\n<li>Other professional fees: $14,700</li>\n<li>Depreciation &amp; depletion: $13,144</li>\n<li>&ldquo;Travel, conferences, and meetings&rdquo;: $100,861</li>\n<li>Other expenses: $57,438</li>\n</ul>\n<p>Page 18 and page 20 finally give us some meaty details about the Edge.org setup. On an &lsquo;unadjusted cost or basis&rsquo;, Edge spent:</p>\n<ol style=\"list-style-type: decimal\">\n<li>$1,299 on &lsquo;equipment&rsquo;</li>\n<li>$70,038<sup><a id=\"fnref5\" class=\"footnoteRef\" href=\"#fn5\">5</a></sup> on &lsquo;computer software&rsquo; (must be a Microsoft shop)</li>\n<li>$55,034<sup><a id=\"fnref6\" class=\"footnoteRef\" href=\"#fn6\">6</a></sup> on &lsquo;computer equipment&rsquo;</li>\n<li>Total: $126,371</li>\n</ol>\n<p>Of that, a percentage becomes the depreciation &amp; depletion above. And the &lsquo;Other expenses&rsquo;? From page 20, 2010-only:</p>\n<ol style=\"list-style-type: decimal\">\n<li>Web hosting: $8,993</li>\n<li>Web newsletter: $6,000</li>\n<li>Miscellaneous: $902</li>\n<li>Video hosting: $4,500</li>\n<li>Interview transcription: $4,068</li>\n<li>Software and hardware: $14,092</li>\n<li>NYS Dept of Law filing fees: $50</li>\n<li>Office supplies: $1,333</li>\n<li>Website development: $17,500</li>\n<li>Total: $57,438</li>\n</ol>\n<p>While Edge spent a fair bit on all the technical stuff and on the basic overhead of a corporation, it spent even more on &lsquo;meetings&rsquo; - all the dinners and functions that make up the grist for the mill-website. Which is pretty much as one would expect.</p>\n<p>Some of the expenses seem on the high side; Edge.org seems to be a fairly undemanding website, something which could be hosted very cheaply on a cloud service like <a href=\"http://en.wikipedia.org/wiki/Amazon_AWS\">Amazon AWS</a> (and one that could be done almost entirely as a <a href=\"http://en.wikipedia.org/wiki/Static_site\">static site</a> and hosted even more cheaply on <a href=\"http://en.wikipedia.org/wiki/Amazon_S3\">Amazon S3</a>) - a hosting bill of $750 a month seems a bit high to me, especially when that isn&rsquo;t even the cost of hosting videos, a separate $375 a month. The hardware expense of $55,000 seems <em>extremely</em> high if they are just purchasing laptops, but would make sense if these were purchases of cameras or big audio-visual rigs; there&rsquo;s no way to tell just from the filing. The software expenses, though, seem amazingly high; with costs like $70,000, it might be worthwhile to investigate the gratis or Free options for video processing (what I assume they are paying for - if that&rsquo;s just for things like Microsoft Windows or Microsoft Word&hellip;).</p>\n<h1 id=\"final-thoughts\"><a href=\"#TOC\"><span class=\"header-section-number\">3</span> Final thoughts</a></h1>\n<p>So what picture has emerged? From the numbers, I read Edge as this: &lsquo;a pretty well-run quasi-public social club for interesting people that has really nice dinners and which turns them into profitable books to pay for it all&rsquo;.</p>\n<h1 id=\"other-examples\"><a href=\"#TOC\"><span class=\"header-section-number\">4</span> Other examples</a></h1>\n<p>Looking at financial filings for charities can be interesting from the points of view of sustainability and efficiency. Here are some links where people attempt lay analysis similar to the foregoing:</p>\n<ul>\n<li><a href=\"/!Wikipedia\">ALCOR</a>: <a href=\"/!Wikipedia\">Mike Darwin</a>, <a href=\"http://chronopause.com/index.php/2011/07/11/the-armories-of-the-latter-day-laputas-part-5/\">&ldquo;The Armories of the Latter Day Laputas, Part 5&rdquo;</a></li>\n<li>\n<p><a href=\"/!Wikipedia\">JSTOR</a></p>\n<ol style=\"list-style-type: decimal\">\n<li><a href=\"http://lists.wikimedia.org/pipermail/wikien-l/2011-July/109234.html\">myself</a></li>\n<li><a href=\"http://www.generalist.org.uk/blog/2011/jstor-where-does-your-money-go/\">Andrew Gray</a></li>\n</ol></li>\n<li>\n<p><a href=\"/!Wikipedia\">SIAI</a>: <a href=\"/lw/5il/siai_an_examination/\">&ldquo;SIAI - An Examination&rdquo;</a>, Brandon Reinhart</p>\n</li>\n</ul>\n<p><a name=\"footnotes\"></a></p>\n<hr />\n<ol>\n<li id=\"fn1\">\n<p>Part 1, line 12 <a class=\"footnoteBackLink\" title=\"Jump back to footnote 1\" href=\"#fnref1\">\u21a9</a></p>\n</li>\n<li id=\"fn2\">\n<p>Part 1, line 26 <a class=\"footnoteBackLink\" title=\"Jump back to footnote 2\" href=\"#fnref2\">\u21a9</a></p>\n</li>\n<li id=\"fn3\">\n<p>Section &lsquo;I&rsquo;, above Part 1 <a class=\"footnoteBackLink\" title=\"Jump back to footnote 3\" href=\"#fnref3\">\u21a9</a></p>\n</li>\n<li id=\"fn4\">\n<p>21 August 1988, apparently, since that&rsquo;s the date listed in 1a of Part XIV on page 10; but I&rsquo;m getting ahead of myself. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 4\" href=\"#fnref4\">\u21a9</a></p>\n</li>\n<li id=\"fn5\">\n<p>This is adding up 5 entries. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 5\" href=\"#fnref5\">\u21a9</a></p>\n</li>\n<li id=\"fn6\">\n<p>Sum of 2 entries. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 6\" href=\"#fnref6\">\u21a9</a></p>\n</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PmrD2T6F82RkRkhQv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 23, "extendedScore": null, "score": 7.654012947891234e-07, "legacy": true, "legacyId": "9682", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a name=\"TOC\"></a></p>\n<ul>\n<li><a href=\"#the-problem\"><span class=\"toc-section-number\">1</span> The problem</a></li>\n<li><a href=\"#the-solution\"><span class=\"toc-section-number\">2</span> The solution</a> \n<ul>\n<li><a href=\"#which-charity\"><span class=\"toc-section-number\">2.1</span> Which charity?</a> \n<ul>\n<li><a href=\"#the-filings\"><span class=\"toc-section-number\">2.1.1</span> The filings</a></li>\n<li><a href=\"#overview\"><span class=\"toc-section-number\">2.1.2</span> Overview</a></li>\n<li><a href=\"#income\"><span class=\"toc-section-number\">2.1.3</span> Income</a></li>\n<li><a href=\"#expenses\"><span class=\"toc-section-number\">2.1.4</span> Expenses</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#final-thoughts\"><span class=\"toc-section-number\">3</span> Final thoughts</a></li>\n<li><a href=\"#other-examples\"><span class=\"toc-section-number\">4</span> Other examples</a></li>\n</ul>\n<blockquote>\n<p>Abstract: an example of how to read charity filings, using <a href=\"http://edge.org/\">Edge.org</a></p>\n</blockquote>\n<h1 id=\"the-problem\"><a id=\"more\"></a><br></h1>\n<h1 id=\"1_The_problem\"><a href=\"#TOC\"><span class=\"header-section-number\">1</span> The problem</a></h1>\n<p>Recently, on <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\"><code>#lesswrong</code></a>:</p>\n<blockquote><strong>epitron&gt;</strong> and you're right, Edge was a bit better back when people wrote essays, and the mailing list of smart people critiqued them <br> <strong>epitron&gt;</strong> i guess we can chalk this up to the fall of journalism. or more likely, the way the financial system broke (which Lanier talks about) <br> <strong>gwern&gt;</strong> Edge seems quite profitable, publishing prestigious pop intellectual books <br> <strong>epitron&gt;</strong> o_O edge doesn't look very profitable to me. they just redesigned their website, after like 15 years <br><strong>gwern&gt;</strong> the videos, the dinners, the books... <br> <strong>epitron&gt;</strong> I think advertising pop-science books by contributors is the only way to bribe them to contribute <br> <strong>epitron&gt;</strong> hmm apparently Brockman has a thing for videotaping conversations with smart people. he's been doing it since the 60's <br><strong>epitron&gt;</strong> and publishing books is not a very profitable venture <br> <strong>epitron&gt;</strong> maybe it's somewhat lucrative if you already have an audience who's waiting to buy up the next book... so that you don't have to advertise it <br> <strong>epitron&gt;</strong> actually, Edge is a nonprofit. They have to publish quarterly reports, don't they?</blockquote>\n<p>Yes! That\u2019s quite true - <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Edge_Foundation,_Inc.\">Edge Foundation, Inc.</a> is in fact a non-profit, and a <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/501%28c%29_organization\">501(c)</a>(3) charity to boot. So we <em>can</em> answer these questions.</p>\n<h1 id=\"2_The_solution\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> The solution</a></h1>\n<p>But how are we to do this?</p>\n<p>As in previous examples, we have to know <em>where</em> such filings can be found. We ask the great Google: <a href=\"https://encrypted.google.com/search?num=100&amp;q=501c3%20filings\">\u201c501c3 filing\u201d</a>, and it tells us: <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/GuideStar\">GuideStar.org</a>. To quote Wikipedia:</p>\n<blockquote>\n<p>\u201cAnyone who is interested in reviewing a nonprofit\u2019s recent <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/IRS_tax_forms#990\">Forms 990</a> can register at <a href=\"https://www2.guidestar.org/\">Guidestar.org</a> and download them for free.\u201d</p>\n</blockquote>\n<p>We <a href=\"https://commerce.guidestar.org/GuideStar/newaccount.aspx\">register</a> (it\u2019s the normal signup form), and now we can search and look at the Form 990s.</p>\n<h2 id=\"2_1_Which_charity_\"><a href=\"#TOC\"><span class=\"header-section-number\">2.1</span> Which charity?</a></h2>\n<p>\u2018Edge\u2019 turns up 5,933 results; \u2018Edge.org\u2019 turns up 0; \u2018Edge Foundation\u2019 turns up 354. We\u2019ll start with that last one. There turn out to be 4 or 5 \u2018Edge Foundation Inc\u2019 on the first page, so here we have to either look at every one or apply some outside knowledge.</p>\n<p>In this case, as Epitron mentioned, the Foundation is powered by book agent Brockman, and where is the American book industry famously concentrated? In New York City. Exactly <a href=\"https://www2.guidestar.org/organizations/13-3528667/edge-foundation.aspx\">1</a> of those 4 is in NYC. If we look at the filings, this is in fact the right Edge Foundation.</p>\n<h3 id=\"2_1_1_The_filings\"><a href=\"#TOC\"><span class=\"header-section-number\">2.1.1</span> The filings</a></h3>\n<p>GuideStar has 3 Form 990s available for free:</p>\n<ul>\n<li><a href=\"http://www.guidestar.org/FinDocuments/2008/133/528/2008-133528667-04d54cd1-F.pdf\">2008</a></li>\n<li><a href=\"http://www.guidestar.org/FinDocuments/2009/133/528/2009-133528667-05f0fda4-F.pdf\">2009</a></li>\n<li><a href=\"http://www.guidestar.org/FinDocuments/2010/133/528/2010-133528667-072c12f4-F.pdf\">2010</a></li>\n</ul>\n<p>We\u2019re only really interested in the current situation, so we look at the 2010 filing.</p>\n<h3 id=\"2_1_2_Overview\"><a href=\"#TOC\"><span class=\"header-section-number\">2.1.2</span> Overview</a></h3>\n<p>Tax filings have a bad reputation for being impenetrable, but like programming, they\u2019re fairly logical; you need to pay attention to the labels and follow the lines with your eyes, and then they\u2019re pretty clear.</p>\n<p>The first page contains all the highest-level figures and give us the overall look at what kind of charity this is.</p>\n<ol style=\"list-style-type: decimal\">\n<li>Income<sup><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></sup>: $159,000</li>\n<li>Expenses<sup><a id=\"fnref2\" class=\"footnoteRef\" href=\"#fn2\">2</a></sup>: $191,000</li>\n<li>Net assets<sup><a id=\"fnref3\" class=\"footnoteRef\" href=\"#fn3\">3</a></sup>: $192,000</li>\n</ol>\n<p>Just from these 3 numbers, what might we infer? (As in the Girl Scouts essay, it\u2019s a good idea to pause and reflect. What does one expect, what is one looking for?)</p>\n<ul>\n<li>\n<p>It\u2019s a <em>small</em> charity, people-wise</p>\nEven if they are accepting relatively nominal incomes for NYC, like $50k, there simply cannot be more than 2 or 3 employees. And if they work for free, there still can\u2019t be more than 30 or 50, simply because there would be the usual corporate/business overhead for the computers, utilities, insurance, etc. Girl Scouts\u2019s NYC headquarters might spend $159k just on legal advice or investment fees. However, Edge is something like twice as large as another charity whose filings I\u2019ve read through, the Lifeboat Foundation, so there\u2019s always room to get smaller.</li>\n<li>\n<p>Expenses &gt; Income</p>\nWikipedia says Edge was incorporated in 1988<sup><a id=\"fnref4\" class=\"footnoteRef\" href=\"#fn4\">4</a></sup>, which suggests Edge has a regular cycle of increases and decreases in its net worth. 2010 would seem to be in a \u2018decrease\u2019 part of the cycle. This is support for the book publishing theory - donations tend to be more consistent than the occasional delay-filled infusion of royalties from a published book.</li>\n<li>\n<p>Edge has built up a substantial cushion of assets - an entire year\u2019s revenue.</p>\n<p>Like above, this suggests a regular excess of revenue. It\u2019s also possibly interesting from an organizational point of view: why such reserves? It\u2019s not as if Edge maintains an expensive infrastructure where the slightest disruption is disastrous (eg. the power grid). This suggests maybe Edge simply can\u2019t think of any way to responsibly spend it. There is only so much one can spend on a fancy dinner/banquet before it is obscene, and Edge\u2019s participants tend to the meritocratic, and might object to overly lavish settings. Of course, this could just be a warning sign of some skulduggery.</p>\n</li>\n</ul>\n<h3 id=\"2_1_3_Income\"><a href=\"#TOC\"><span class=\"header-section-number\">2.1.3</span> Income</a></h3>\n<p>After page 1, we\u2019ll skip around a little. I\u2019m interested in where their money is coming from. We have to skip all the way down to page 12 before we see anything about their <em>income</em>. Baldly on line 1 of Part XIV-A on page 12, we see \u2018Book contract\u2019 and $83,777.</p>\n<p>Well, that explains a lot! It\u2019s not listed as royalties, and it\u2019s a fairly regular looking number, so we can probably infer that this money is a <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Advance_against_royalties\">book advance</a>, and a remarkably high one - Edge has produced many books by this point, so this advance would not be gambling but a good indicator of how much the publisher thinks they will sell. At a dollar or two in royalties per book, that implies sales in the scores of thousands; perhaps not best seller list, but a very good show indeed</p>\n<p>That leaves ~$75,000 of the $159,000 unaccounted for. We continue skimming until we hit page 15, the second page of \u2018Schedule B\u2019/\u2018Schedule of Contributors\u2019. There are two entries:</p>\n<ul>\n<li>\u2018Enhanced Education\u2019, $50,000 Address is in the <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/British_Virgin_Islands\">British Virgin Islands</a>; this is interesting, inasmuch as the British Virgin Islands are one of the leading <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Corporate_haven#Commonwealth_.26_Western_Europe\">corporate havens</a> and the listed address is shared by a <a href=\"http://www.taxexemptworld.com/organizations/st-thomas-vi-virgin-islands.asp\">great many</a> other non-profits.</li>\n<li>\u2018Parasolholdings\u2019, $25,000 This strange entry doesn\u2019t list a real address - just \u2018c/o Edge Fdn\u2019. Googling doesn\u2019t help at all; there are a number of \u2018Parasol Holdings\u2019 worldwide, none of which have much information online. (One New Zealand entity provides copious filings\u2026 all of which say nothing whatsoever beyond the shareholders\u2019 names &amp; addresses.)</li>\n</ul>\n<p>Bingo. So Edge\u2019s entire income is derived from its book contract and two odd-looking donators.</p>\n<p>If we check the 2009 filings, Enhanced Education donated $53,000 that year, but Parasolholdings did not; instead, $25,000 came from \u2018J.E. Safra\u2019 (again \u2018c/o Edge Fdn\u2019). This is more helpful; a J.E. Safra was the father of billionaire <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Edmond_Safra\">Edmond Safra</a>, but this seems to be a red herring (that J.E. is dead) - retrying the Google search as \u2018edge safra\u2019 leads us straight to <a href=\"http://www.edge.org/memberbio/jacob_e_safra\">a biography</a> on Edge.org:</p>\n<blockquote>\n<p>\u2018Jacob E. \u201cJacqui\u201d Safra, a Swiss investor, is the Chairman of Encyclopedia Britannica, and owner of Spring Mountain Vineyards, a large wine-growing estate located in Saint Helena, California.\u2019</p>\n</blockquote>\n<p>Well, that settles Parasolholdings - it\u2019s some sort of tax dodge or corporate cutout for Jacob. Nothing wrong with that. Enhanced Education remains a mystery; it does not show up in Google, GuideStar, or a few other places I checked.</p>\n<h3 id=\"2_1_4_Expenses\"><a href=\"#TOC\"><span class=\"header-section-number\">2.1.4</span> Expenses</a></h3>\n<p>We backtrack to the beginning. The first hit is page 6, where we find the employment data: Part VIII, 1. Remember the observation that the expenses could not support many employees? It is borne out, with Edge having just 3 employees, each working a quarter of an hour a week, for free. (The secretary, Katinka Mason, doubles as the book-keeper too.) Very laudable.</p>\n<p>Page 7 tantalizes us with Part IX-A, lines 1\u20132, but they prove a bust:</p>\n<ol style=\"list-style-type: decimal\">\n<li>\u2018Development of communication vehicles for the literary world over the Internet\u2019, $172,999</li>\n<li>\u2018Publication concerning Internet communication vehicles\u2019, $13,144</li>\n</ol>\n<p>The first is just the canned mission statement you would have noticed elsewhere in the filing. The second is opaque; expenses somehow related to the book advance? Page 8 tempts us with a different number, the expenses at $178,226 (different from the just quoted #1), but no more details.</p>\n<p>Page 10 gives us interesting details like the founding day of Edge.org, and expenses from 2010\u20132007:</p>\n<ul>\n<li>2007: $123,672</li>\n<li>2008: $99,217</li>\n<li>2009: $98,249</li>\n<li>2010: $178,226</li>\n</ul>\n<p>Notice the cyclical trend here - high to low to high. Maybe this is the explanation for the cash reserve previously mentioned: there might be a $20,000 and then an $80,000 surplus in the two belt-tightened years, that would make up at least half of the reserve.</p>\n<p>Back to page 1 for the detailed expenses:</p>\n<ul>\n<li>Accounting fees: $5,227</li>\n<li>Other professional fees: $14,700</li>\n<li>Depreciation &amp; depletion: $13,144</li>\n<li>\u201cTravel, conferences, and meetings\u201d: $100,861</li>\n<li>Other expenses: $57,438</li>\n</ul>\n<p>Page 18 and page 20 finally give us some meaty details about the Edge.org setup. On an \u2018unadjusted cost or basis\u2019, Edge spent:</p>\n<ol style=\"list-style-type: decimal\">\n<li>$1,299 on \u2018equipment\u2019</li>\n<li>$70,038<sup><a id=\"fnref5\" class=\"footnoteRef\" href=\"#fn5\">5</a></sup> on \u2018computer software\u2019 (must be a Microsoft shop)</li>\n<li>$55,034<sup><a id=\"fnref6\" class=\"footnoteRef\" href=\"#fn6\">6</a></sup> on \u2018computer equipment\u2019</li>\n<li>Total: $126,371</li>\n</ol>\n<p>Of that, a percentage becomes the depreciation &amp; depletion above. And the \u2018Other expenses\u2019? From page 20, 2010-only:</p>\n<ol style=\"list-style-type: decimal\">\n<li>Web hosting: $8,993</li>\n<li>Web newsletter: $6,000</li>\n<li>Miscellaneous: $902</li>\n<li>Video hosting: $4,500</li>\n<li>Interview transcription: $4,068</li>\n<li>Software and hardware: $14,092</li>\n<li>NYS Dept of Law filing fees: $50</li>\n<li>Office supplies: $1,333</li>\n<li>Website development: $17,500</li>\n<li>Total: $57,438</li>\n</ol>\n<p>While Edge spent a fair bit on all the technical stuff and on the basic overhead of a corporation, it spent even more on \u2018meetings\u2019 - all the dinners and functions that make up the grist for the mill-website. Which is pretty much as one would expect.</p>\n<p>Some of the expenses seem on the high side; Edge.org seems to be a fairly undemanding website, something which could be hosted very cheaply on a cloud service like <a href=\"http://en.wikipedia.org/wiki/Amazon_AWS\">Amazon AWS</a> (and one that could be done almost entirely as a <a href=\"http://en.wikipedia.org/wiki/Static_site\">static site</a> and hosted even more cheaply on <a href=\"http://en.wikipedia.org/wiki/Amazon_S3\">Amazon S3</a>) - a hosting bill of $750 a month seems a bit high to me, especially when that isn\u2019t even the cost of hosting videos, a separate $375 a month. The hardware expense of $55,000 seems <em>extremely</em> high if they are just purchasing laptops, but would make sense if these were purchases of cameras or big audio-visual rigs; there\u2019s no way to tell just from the filing. The software expenses, though, seem amazingly high; with costs like $70,000, it might be worthwhile to investigate the gratis or Free options for video processing (what I assume they are paying for - if that\u2019s just for things like Microsoft Windows or Microsoft Word\u2026).</p>\n<h1 id=\"3_Final_thoughts\"><a href=\"#TOC\"><span class=\"header-section-number\">3</span> Final thoughts</a></h1>\n<p>So what picture has emerged? From the numbers, I read Edge as this: \u2018a pretty well-run quasi-public social club for interesting people that has really nice dinners and which turns them into profitable books to pay for it all\u2019.</p>\n<h1 id=\"4_Other_examples\"><a href=\"#TOC\"><span class=\"header-section-number\">4</span> Other examples</a></h1>\n<p>Looking at financial filings for charities can be interesting from the points of view of sustainability and efficiency. Here are some links where people attempt lay analysis similar to the foregoing:</p>\n<ul>\n<li><a href=\"/!Wikipedia\">ALCOR</a>: <a href=\"/!Wikipedia\">Mike Darwin</a>, <a href=\"http://chronopause.com/index.php/2011/07/11/the-armories-of-the-latter-day-laputas-part-5/\">\u201cThe Armories of the Latter Day Laputas, Part 5\u201d</a></li>\n<li>\n<p><a href=\"/!Wikipedia\">JSTOR</a></p>\n<ol style=\"list-style-type: decimal\">\n<li><a href=\"http://lists.wikimedia.org/pipermail/wikien-l/2011-July/109234.html\">myself</a></li>\n<li><a href=\"http://www.generalist.org.uk/blog/2011/jstor-where-does-your-money-go/\">Andrew Gray</a></li>\n</ol></li>\n<li>\n<p><a href=\"/!Wikipedia\">SIAI</a>: <a href=\"/lw/5il/siai_an_examination/\">\u201cSIAI - An Examination\u201d</a>, Brandon Reinhart</p>\n</li>\n</ul>\n<p><a name=\"footnotes\"></a></p>\n<hr>\n<ol>\n<li id=\"fn1\">\n<p>Part 1, line 12 <a class=\"footnoteBackLink\" title=\"Jump back to footnote 1\" href=\"#fnref1\">\u21a9</a></p>\n</li>\n<li id=\"fn2\">\n<p>Part 1, line 26 <a class=\"footnoteBackLink\" title=\"Jump back to footnote 2\" href=\"#fnref2\">\u21a9</a></p>\n</li>\n<li id=\"fn3\">\n<p>Section \u2018I\u2019, above Part 1 <a class=\"footnoteBackLink\" title=\"Jump back to footnote 3\" href=\"#fnref3\">\u21a9</a></p>\n</li>\n<li id=\"fn4\">\n<p>21 August 1988, apparently, since that\u2019s the date listed in 1a of Part XIV on page 10; but I\u2019m getting ahead of myself. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 4\" href=\"#fnref4\">\u21a9</a></p>\n</li>\n<li id=\"fn5\">\n<p>This is adding up 5 entries. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 5\" href=\"#fnref5\">\u21a9</a></p>\n</li>\n<li id=\"fn6\">\n<p>Sum of 2 entries. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 6\" href=\"#fnref6\">\u21a9</a></p>\n</li>\n</ol>", "sections": [{"title": "1 The problem", "anchor": "1_The_problem", "level": 1}, {"title": "2 The solution", "anchor": "2_The_solution", "level": 1}, {"title": "2.1 Which charity?", "anchor": "2_1_Which_charity_", "level": 2}, {"title": "2.1.1 The filings", "anchor": "2_1_1_The_filings", "level": 3}, {"title": "2.1.2 Overview", "anchor": "2_1_2_Overview", "level": 3}, {"title": "2.1.3 Income", "anchor": "2_1_3_Income", "level": 3}, {"title": "2.1.4 Expenses", "anchor": "2_1_4_Expenses", "level": 3}, {"title": "3 Final thoughts", "anchor": "3_Final_thoughts", "level": 1}, {"title": "4 Other examples", "anchor": "4_Other_examples", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qqhdj3W3vSfB5E9ss"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-06T12:42:55.999Z", "modifiedAt": "2022-05-17T13:19:10.955Z", "url": null, "title": "Latex support?", "slug": "latex-support", "viewCount": null, "lastCommentedAt": "2013-04-05T18:39:46.333Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "snarles", "createdAt": "2009-06-01T03:48:38.132Z", "isAdmin": false, "displayName": "snarles"}, "userId": "YsmFaM5MdsDW8GNop", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JRocWETozby8KvC3N/latex-support", "pageUrlRelative": "/posts/JRocWETozby8KvC3N/latex-support", "linkUrl": "https://www.lesswrong.com/posts/JRocWETozby8KvC3N/latex-support", "postedAtFormatted": "Tuesday, September 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Latex%20support%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALatex%20support%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJRocWETozby8KvC3N%2Flatex-support%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Latex%20support%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJRocWETozby8KvC3N%2Flatex-support", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJRocWETozby8KvC3N%2Flatex-support", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 27, "htmlBody": "<p>Apologies since I am almost sure this has been brought up before.&nbsp; Are there any plans for some sort of LateX or MathML functionality on the site?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JRocWETozby8KvC3N", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 7.65576890155439e-07, "legacy": true, "legacyId": "9698", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2011-09-06T12:42:55.999Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-06T13:01:40.297Z", "modifiedAt": null, "url": null, "title": "How memory works - video lecture from Stanford", "slug": "how-memory-works-video-lecture-from-stanford", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.258Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K9zxqX4ya2FqBPPGC/how-memory-works-video-lecture-from-stanford", "pageUrlRelative": "/posts/K9zxqX4ya2FqBPPGC/how-memory-works-video-lecture-from-stanford", "linkUrl": "https://www.lesswrong.com/posts/K9zxqX4ya2FqBPPGC/how-memory-works-video-lecture-from-stanford", "postedAtFormatted": "Tuesday, September 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20memory%20works%20-%20video%20lecture%20from%20Stanford&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20memory%20works%20-%20video%20lecture%20from%20Stanford%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK9zxqX4ya2FqBPPGC%2Fhow-memory-works-video-lecture-from-stanford%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20memory%20works%20-%20video%20lecture%20from%20Stanford%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK9zxqX4ya2FqBPPGC%2Fhow-memory-works-video-lecture-from-stanford", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK9zxqX4ya2FqBPPGC%2Fhow-memory-works-video-lecture-from-stanford", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>Good intro lecture to both high-level concepts (long vs short, declarative etc) and anatomical aspects of memory.</p>\n<p><a href=\"http://www.youtube.com/watch?v=a_HfSnQqeyY\">http://www.youtube.com/watch?v=a_HfSnQqeyY</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K9zxqX4ya2FqBPPGC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 7.655830262075184e-07, "legacy": true, "legacyId": "9699", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-06T17:37:16.931Z", "modifiedAt": null, "url": null, "title": "Meetup : Houston Meetup 9/11", "slug": "meetup-houston-meetup-9-11", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:25.334Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cog", "createdAt": "2011-04-25T04:58:53.803Z", "isAdmin": false, "displayName": "Cog"}, "userId": "xkp87vCZ56dp2tWnN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yeQvCJQt9tj7sjQB3/meetup-houston-meetup-9-11", "pageUrlRelative": "/posts/yeQvCJQt9tj7sjQB3/meetup-houston-meetup-9-11", "linkUrl": "https://www.lesswrong.com/posts/yeQvCJQt9tj7sjQB3/meetup-houston-meetup-9-11", "postedAtFormatted": "Tuesday, September 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Houston%20Meetup%209%2F11&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Houston%20Meetup%209%2F11%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyeQvCJQt9tj7sjQB3%2Fmeetup-houston-meetup-9-11%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Houston%20Meetup%209%2F11%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyeQvCJQt9tj7sjQB3%2Fmeetup-houston-meetup-9-11", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyeQvCJQt9tj7sjQB3%2Fmeetup-houston-meetup-9-11", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 85, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/31'>Houston Meetup 9/11</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 September 2011 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2010 Commerce St, Houston, Tx. 77002</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After a short break (I was on vacation and then catching up from being on vacation), the Houston hackerspace meetup will be meeting again this Sunday at 2 PM. I will be giving a presentation on the history of psychology, where it has been, and where it is going. Some type of board game will follow.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/31'>Houston Meetup 9/11</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yeQvCJQt9tj7sjQB3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 7.656732880602828e-07, "legacy": true, "legacyId": "9700", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Houston_Meetup_9_11\">Discussion article for the meetup : <a href=\"/meetups/31\">Houston Meetup 9/11</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 September 2011 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2010 Commerce St, Houston, Tx. 77002</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After a short break (I was on vacation and then catching up from being on vacation), the Houston hackerspace meetup will be meeting again this Sunday at 2 PM. I will be giving a presentation on the history of psychology, where it has been, and where it is going. Some type of board game will follow.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Houston_Meetup_9_111\">Discussion article for the meetup : <a href=\"/meetups/31\">Houston Meetup 9/11</a></h2>", "sections": [{"title": "Discussion article for the meetup : Houston Meetup 9/11", "anchor": "Discussion_article_for_the_meetup___Houston_Meetup_9_11", "level": 1}, {"title": "Discussion article for the meetup : Houston Meetup 9/11", "anchor": "Discussion_article_for_the_meetup___Houston_Meetup_9_111", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-06T19:53:47.934Z", "modifiedAt": null, "url": null, "title": "Another 'prize' suggestion.", "slug": "another-prize-suggestion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.319Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2JZA3ZyBiMNgBhMA8/another-prize-suggestion", "pageUrlRelative": "/posts/2JZA3ZyBiMNgBhMA8/another-prize-suggestion", "linkUrl": "https://www.lesswrong.com/posts/2JZA3ZyBiMNgBhMA8/another-prize-suggestion", "postedAtFormatted": "Tuesday, September 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20'prize'%20suggestion.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20'prize'%20suggestion.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2JZA3ZyBiMNgBhMA8%2Fanother-prize-suggestion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20'prize'%20suggestion.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2JZA3ZyBiMNgBhMA8%2Fanother-prize-suggestion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2JZA3ZyBiMNgBhMA8%2Fanother-prize-suggestion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<p>Following this&nbsp;<a href=\"http://lesswrong.com/lw/7f5/what_are_good_topics_for_literature_review_prizes/\">http://lesswrong.com/lw/7f5/what_are_good_topics_for_literature_review_prizes/</a>&nbsp;methodology&nbsp;and applying it to&nbsp;<a href=\"http://lesswrong.com/lw/7fv/hacking_on_lesswrong_just_got_easier/\">http://lesswrong.com/lw/7fv/hacking_on_lesswrong_just_got_easier/</a><br />it would seem like a good idea to create a prize for a tutorial on LW website hacking: e.g. what do you need to learn (Pylons, some Jquery (just a guess)), how to work with the repository, style guide, etc.. We'd get more people helping the site a bit. What do you think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2JZA3ZyBiMNgBhMA8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 18, "extendedScore": null, "score": 7.657180039459087e-07, "legacy": true, "legacyId": "9701", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8LfkhgWkqp2gTqug4", "SRdvYJrDNvWkpcm8F"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-06T21:13:05.226Z", "modifiedAt": null, "url": null, "title": "Free research help, editing and article downloads for LessWrong", "slug": "free-research-help-editing-and-article-downloads-for", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:49.445Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q3wQGNicZZQPCmJXZ/free-research-help-editing-and-article-downloads-for", "pageUrlRelative": "/posts/q3wQGNicZZQPCmJXZ/free-research-help-editing-and-article-downloads-for", "linkUrl": "https://www.lesswrong.com/posts/q3wQGNicZZQPCmJXZ/free-research-help-editing-and-article-downloads-for", "postedAtFormatted": "Tuesday, September 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Free%20research%20help%2C%20editing%20and%20article%20downloads%20for%20LessWrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFree%20research%20help%2C%20editing%20and%20article%20downloads%20for%20LessWrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3wQGNicZZQPCmJXZ%2Ffree-research-help-editing-and-article-downloads-for%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Free%20research%20help%2C%20editing%20and%20article%20downloads%20for%20LessWrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3wQGNicZZQPCmJXZ%2Ffree-research-help-editing-and-article-downloads-for", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3wQGNicZZQPCmJXZ%2Ffree-research-help-editing-and-article-downloads-for", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 307, "htmlBody": "<p><strong>Update</strong>: Please use the <a href=\"/r/discussion/tag/help_desk/\">most recent thread</a>.</p>\n<p>The&nbsp;<a href=\"http://groups.google.com/group/lw-public-goods-team\">LW Public Goods Team</a> wants to encourage useful research projects (as well other kinds of projects) for the LW community. If you're interested in doing this kind of work, you might run into a problem that is best solved by good outside assistance. Without assistance you might get discouraged and stop working on the project or never even start it. We want to help you avoid that. Do you</p>\n<ul>\n<li>Not know how to interpret a finding and want help figuring it out?</li>\n<li>Need access to a particular paper and need someone with a library subscription to download it for you?</li>\n<li>Need someone to edit your writing?</li>\n<li>Not even know what you're having trouble with, but you know is that you're stuck and need someone to troubleshoot you?</li>\n</ul>\n<p>Then, we want to help!</p>\n<p>How do you request such help? For now, I think the best way is to post to the discussion section about your problem. That way other interested people can also provide help and be interested in your research. If you feel uncomfortable doing this, you may post to the&nbsp;<a href=\"http://groups.google.com/group/lw-public-goods-team\">public goods team mailing list</a>&nbsp;(lw-public-goods-team@googlegroups.com) or if it's not too long after this was posted, post in the comments.</p>\n<p>I personally commit to doing at least 3 hours a week of tasks like these for people doing LessWrong related projects (assuming demand for it; I'll be keeping a log) for at least the next month.&nbsp;<a href=\"/user/Morendil\">Morendil</a>&nbsp;has committed to doing at least an hour of this and&nbsp;<a href=\"/user/atucker\">atucker</a>&nbsp;has promised to some as well.</p>\n<p>Our goal is to find out whether this kind of help is effective and encourages people.&nbsp;If this kind of assistance turns out to be valuable, we'll continue to offer it.</p>\n<p>If you would like to volunteer some time (a little or a lot), say so in the comments!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q3wQGNicZZQPCmJXZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 56, "baseScore": 75, "extendedScore": null, "score": 0.0001594925013353096, "legacy": true, "legacyId": "9702", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 442, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-06T21:43:21.967Z", "modifiedAt": null, "url": null, "title": "[Question] What's your Elevator Pitch For Rationality?", "slug": "question-what-s-your-elevator-pitch-for-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:25.494Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3dABgWRMbcrS9sKme/question-what-s-your-elevator-pitch-for-rationality", "pageUrlRelative": "/posts/3dABgWRMbcrS9sKme/question-what-s-your-elevator-pitch-for-rationality", "linkUrl": "https://www.lesswrong.com/posts/3dABgWRMbcrS9sKme/question-what-s-your-elevator-pitch-for-rationality", "postedAtFormatted": "Tuesday, September 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BQuestion%5D%20What's%20your%20Elevator%20Pitch%20For%20Rationality%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BQuestion%5D%20What's%20your%20Elevator%20Pitch%20For%20Rationality%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3dABgWRMbcrS9sKme%2Fquestion-what-s-your-elevator-pitch-for-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BQuestion%5D%20What's%20your%20Elevator%20Pitch%20For%20Rationality%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3dABgWRMbcrS9sKme%2Fquestion-what-s-your-elevator-pitch-for-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3dABgWRMbcrS9sKme%2Fquestion-what-s-your-elevator-pitch-for-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 304, "htmlBody": "<p>You're talking with someone you like, and they ask you what you mean by rationality, or why you keep going to LessWrong meetups. Or you meet someone who might be interested in the site.</p>\n<p>What do you say to them? If you had to explain to someone what LW-style rationality is in 30 seconds, how would you do it? What's your elevator pitch? Has anyone had any success with a particular pitch?</p>\n<p><strong>My Current Pitch:</strong></p>\n<p>My current best one, made up on the spot, lacking any foreplanning, basically consists of:</p>\n<p>\"Basically, our brains are pretty bad at forming accurate beliefs, and bad in fairly systematic ways. I could show you one, if you want.\"</p>\n<p>Playing <a href=\"/lw/iw/positive_bias_look_into_the_dark/\">the triplet game</a> with them, then revealing that the numbers just need to be ascending</p>\n<p>Upon failure, \"Basically, your brain just doesn't look for examples that disprove your hypothesis, so you didn't notice that it could have a been a more general rule. There are a bunch of others, and I'm interested in learning about them so that I can correct for them.\"</p>\n<p><strong>My Thoughts on That:</strong></p>\n<p>It's massively effective at convincing people that cognitive biases exist (when they're in the 80% that fails, which has always been the case for me so far), but pretty much entirely useless as a rationality pitch. It doesn't explain at all why people should care about having accurate beliefs, and takes it as a given that that would be important.</p>\n<p>It's also far too dry and unfun (compared to say, Methods), and has the unfortunate side effect of making people feel like they've gotten tricked. It makes it look non-cultish though.</p>\n<p>I suspect that other people can do better, and I'll comment later with one that I actually put thought into. There's a pretty good chance that I'll use a few of the more upvoted ones and see how they go over.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"7ow6EFpypbH4hzFuz": 1, "rnvHPB3X2TiD5NMwY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3dABgWRMbcrS9sKme", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 26, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "9662", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>You're talking with someone you like, and they ask you what you mean by rationality, or why you keep going to LessWrong meetups. Or you meet someone who might be interested in the site.</p>\n<p>What do you say to them? If you had to explain to someone what LW-style rationality is in 30 seconds, how would you do it? What's your elevator pitch? Has anyone had any success with a particular pitch?</p>\n<p><strong id=\"My_Current_Pitch_\">My Current Pitch:</strong></p>\n<p>My current best one, made up on the spot, lacking any foreplanning, basically consists of:</p>\n<p>\"Basically, our brains are pretty bad at forming accurate beliefs, and bad in fairly systematic ways. I could show you one, if you want.\"</p>\n<p>Playing <a href=\"/lw/iw/positive_bias_look_into_the_dark/\">the triplet game</a> with them, then revealing that the numbers just need to be ascending</p>\n<p>Upon failure, \"Basically, your brain just doesn't look for examples that disprove your hypothesis, so you didn't notice that it could have a been a more general rule. There are a bunch of others, and I'm interested in learning about them so that I can correct for them.\"</p>\n<p><strong id=\"My_Thoughts_on_That_\">My Thoughts on That:</strong></p>\n<p>It's massively effective at convincing people that cognitive biases exist (when they're in the 80% that fails, which has always been the case for me so far), but pretty much entirely useless as a rationality pitch. It doesn't explain at all why people should care about having accurate beliefs, and takes it as a given that that would be important.</p>\n<p>It's also far too dry and unfun (compared to say, Methods), and has the unfortunate side effect of making people feel like they've gotten tricked. It makes it look non-cultish though.</p>\n<p>I suspect that other people can do better, and I'll comment later with one that I actually put thought into. There's a pretty good chance that I'll use a few of the more upvoted ones and see how they go over.</p>", "sections": [{"title": "My Current Pitch:", "anchor": "My_Current_Pitch_", "level": 1}, {"title": "My Thoughts on That:", "anchor": "My_Thoughts_on_That_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "38 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rmAbiEKQDpDnZzcRf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-07T03:55:22.580Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] How Much Evidence Does It Take?", "slug": "seq-rerun-how-much-evidence-does-it-take", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RjdL6w9StpDTNgGc7/seq-rerun-how-much-evidence-does-it-take", "pageUrlRelative": "/posts/RjdL6w9StpDTNgGc7/seq-rerun-how-much-evidence-does-it-take", "linkUrl": "https://www.lesswrong.com/posts/RjdL6w9StpDTNgGc7/seq-rerun-how-much-evidence-does-it-take", "postedAtFormatted": "Wednesday, September 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20How%20Much%20Evidence%20Does%20It%20Take%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20How%20Much%20Evidence%20Does%20It%20Take%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRjdL6w9StpDTNgGc7%2Fseq-rerun-how-much-evidence-does-it-take%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20How%20Much%20Evidence%20Does%20It%20Take%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRjdL6w9StpDTNgGc7%2Fseq-rerun-how-much-evidence-does-it-take", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRjdL6w9StpDTNgGc7%2Fseq-rerun-how-much-evidence-does-it-take", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 196, "htmlBody": "<p>Today's post, <a href=\"/lw/jn/how_much_evidence_does_it_take/\">How Much Evidence Does It Take?</a> was originally published on 24 September 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If you are considering one hypothesis out of many, or that hypothesis is more implausible than others, or you wish to know with greater confidence, you will need more evidence. Ignoring this rule will cause you to jump to a belief without enough evidence, and thus be wrong.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/7gx/seq_rerun_the_lens_that_sees_its_flaws/\">The Lens That Sees Its Flaws</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RjdL6w9StpDTNgGc7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 7.658757809335317e-07, "legacy": true, "legacyId": "9706", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nj8JKFoLSMEmD3RGp", "eWzaJPWAiCsK4bvGf", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-07T05:24:47.540Z", "modifiedAt": null, "url": null, "title": "Safety can be dangerous", "slug": "safety-can-be-dangerous", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:37.417Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZyesYcGM3CbuEe5Cw/safety-can-be-dangerous", "pageUrlRelative": "/posts/ZyesYcGM3CbuEe5Cw/safety-can-be-dangerous", "linkUrl": "https://www.lesswrong.com/posts/ZyesYcGM3CbuEe5Cw/safety-can-be-dangerous", "postedAtFormatted": "Wednesday, September 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Safety%20can%20be%20dangerous&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASafety%20can%20be%20dangerous%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZyesYcGM3CbuEe5Cw%2Fsafety-can-be-dangerous%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Safety%20can%20be%20dangerous%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZyesYcGM3CbuEe5Cw%2Fsafety-can-be-dangerous", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZyesYcGM3CbuEe5Cw%2Fsafety-can-be-dangerous", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 744, "htmlBody": "<p>In 2005, Hurricane Rita caused 111 deaths.&nbsp; 3 deaths were caused by the hurricane.&nbsp; <a href=\"http://meeting.chestpubs.org/cgi/content/abstract/130/4/124S-c\">90 were caused by the mass evacuation</a>.</p>\n<p>The FDA is supposed to approve new drugs and procedures if the expected benefits outweigh the expected costs.&nbsp; If they actually did this, their errors on both sides (approvals of bad drugs vs. rejections of good drugs) would be roughly equal.&nbsp; The most-publicized drug withdrawal in the past 10 years was that of Vioxx, which the <a href=\"http://www.druginjurylaw.com/Vioxx-FDA-Memo.html\">FDA estimated killed a total of 5165 people over 5 years</a>.&nbsp; This suggests that the best drug that the FDA rejected during that decade could have saved 1000 people/year.&nbsp; During that decade, many drugs were (or could have been) approved that might save more than that many lives every year.&nbsp; Gleevec (invented 1993, approved 2001) is believed to <a href=\"http://www.sciencedaily.com/releases/2007/12/071210094338.htm\">save about 10,000 lives a year</a>.&nbsp; Herceptin (invented in the 1980s, began human trials 1991, approved for some patients in 1998, more in 2006, and more in 2010) was estimated to <a href=\"http://www.guardian.co.uk/society/2007/jan/05/cancercare.health\">save 1,000 lives a year in the United Kingdom</a>, which would translate to 5,000 lives a year in the US.&nbsp; Patients on Apibaxan (discovered in 2006, not yet approved) have <a href=\"http://www.emaxhealth.com/1275/apixaban-beats-warfarin-stroke-prevention-lives-saved\">11% fewer deaths</a> from stroke than patients on warfarin, and stroke causes about <a href=\"http://www.strokecenter.org/patients/stats.htm\">140,000 deaths/year in the US</a>.&nbsp; To stay below the expected drug-rejection error level of 1000 people/year, given just these three drugs (and assuming that Apibaxan pans out and can save 5,000 lives/year), the FDA would need to have a faulty-rejection rate F such that F(10000) + F(5000) + F(5000) &lt; 1000, F &lt; 5%.&nbsp; This seems unlikely.</p>\n<p>ADDED:&nbsp; One area where this affects me every day is in branching software repositories.&nbsp; Every software developer agrees that branching the repository head for test versions and for production versions is good practice.&nbsp; Yet branching causes, I would estimate, at least half of our problems with test and production releases.&nbsp; It is common for me to be delayed one to three days while someone figures out that the software isn't running because they issued a patch on one branch and forgot to update the trunk, or forgot to update other development or test versions that are on separate branches.&nbsp; I don't believe in branching anymore - I think we would have fewer bugs if we just did all development on the trunk, and checked out the code when it worked.&nbsp; Branching is good for humongous projects where you have public releases that you can't patch on the head, like Firefox or Linux.&nbsp; But it's out of place for in-house projects where you <em>can </em>just patch the head and re-checkout.&nbsp; The evidence for this in my personal experience as a software developer is overwhelming; yet whenever I suggest not branching, I'm met with incredulity.</p>\n<p>Exercise for the reader:&nbsp; Find other cases where cautionary measures are &lt;EDIT&gt;taken past the point of marginal utility&lt;/EDIT&gt;.</p>\n<p>ADDED: I think that this is the problem:&nbsp; You have observed a distribution of outcome utilities from some category of event followed by you taking some action A.&nbsp; You observe a new instance of this event.&nbsp; You want to predict the outcome utility of action A for this event.</p>\n<p>Some categories have a power-law outcome distribution with a negative exponent <em>b</em>, indicating there are fewer events of large importance: number of events of size U = e<sup>c - bU</sup>.&nbsp; Assume that you don't observe all possible values of U.&nbsp; Events of importance &lt; U0 are too small to observe; and events with large U are very uncommon.&nbsp; It is then difficult to tell whether the category has a power-law distribution without a lot of previous observations.</p>\n<p>If a lot of event categories have a distribution like this, where big impacts are bad, and they are usually insignificant but sometimes catastrophic, then it's likely rational to treat these events as if they will be catastrophic.&nbsp; And if you don't have enough observations to know if the distribution is a power-law, or something else, it's rational to treat it as if it were a power-law distribution to be safe.</p>\n<p>Could this account for the human risk-aversion \"bias\"?</p>\n<p>If you are the FDA, you are faced with situations where the utility distribution is probably such a power-law distribution <em>mirrored around zero</em>, so there are a few events with very high utility (save lots of lives), and a similar number of events with the negative of that utility (lose that many lives).&nbsp; I would guess that situations like that are rare in our ancestral environment, though I don't know.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZyesYcGM3CbuEe5Cw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 11, "extendedScore": null, "score": 7.659050822266182e-07, "legacy": true, "legacyId": "9707", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-07T07:46:54.386Z", "modifiedAt": null, "url": null, "title": "Journal article about politics and mindkilling", "slug": "journal-article-about-politics-and-mindkilling", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:39.059Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CronoDAS", "createdAt": "2009-02-27T04:42:19.587Z", "isAdmin": false, "displayName": "CronoDAS"}, "userId": "Q2oaNonArzibx5cQN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pyP8wR28fmgeHf27o/journal-article-about-politics-and-mindkilling", "pageUrlRelative": "/posts/pyP8wR28fmgeHf27o/journal-article-about-politics-and-mindkilling", "linkUrl": "https://www.lesswrong.com/posts/pyP8wR28fmgeHf27o/journal-article-about-politics-and-mindkilling", "postedAtFormatted": "Wednesday, September 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Journal%20article%20about%20politics%20and%20mindkilling&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJournal%20article%20about%20politics%20and%20mindkilling%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpyP8wR28fmgeHf27o%2Fjournal-article-about-politics-and-mindkilling%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Journal%20article%20about%20politics%20and%20mindkilling%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpyP8wR28fmgeHf27o%2Fjournal-article-about-politics-and-mindkilling", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpyP8wR28fmgeHf27o%2Fjournal-article-about-politics-and-mindkilling", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 379, "htmlBody": "<p>I just found a link to a paper written in 2003 by Geoffrey L. Cohen of Yale University.</p>\n<p>\"<a href=\"https://eee.uci.edu/10s/55550/home/Cohen+2003.pdf\">Party over Policy: The Dominating Impact of Group Influence on Political Beliefs</a>\"</p>\n<p>Abstract:</p>\n<blockquote>Four studies demonstrated both the power of group influence in persuasion and people&rsquo;s blindness to it. Even under conditions of effortful processing, attitudes toward a social policy depended almost exclusively upon the stated position of one&rsquo;s political party. This effect overwhelmed the impact of both the policy&rsquo;s objective content and participants&rsquo; ideological beliefs (Studies 1&ndash;3), and it was driven by a shift in the assumed factual qualities of the policy and in its perceived moral connotations (Study 4). Nevertheless, participants denied having been influenced by their political group, although they believed that other individuals, especially their ideological adversaries, would be so influenced. The underappreciated role of social identity in persuasion is discussed.</blockquote>\n<p>That's written in journal-ese, so I'll post a translation from <a href=\"http://www.washingtonpost.com/blogs/ezra-klein/post/why-did-the-gop-turn-against-stimulus-ask-a-psychologist/2011/08/25/gIQADugV6J_blog.html?wprss=ezra-klein\">the article I found that contained the link</a>:</p>\n<blockquote>\n<p>My favorite <a href=\"https://eee.uci.edu/10s/55550/home/Cohen+2003.pdf\" target=\"_blank\">study</a> (pdf) in this space was by Yale&rsquo;s Geoffrey Cohen. He had a control group of liberals and conservatives look at a generous welfare reform proposal and a harsh welfare reform proposal. As expected, liberals preferred the generous plan and conservatives favored the more stringent option. Then he had another group of liberals and conservatives look at the same plans, but this time, the plans were associated with parties.</p>\n<p>Both liberals and conservatives followed their parties, even when their parties disagreed with their preferences. So when Democrats were said to favor the stringent welfare reform, for example, liberals went right along. Three scary sentences from the piece: &ldquo;When reference group information was available, participants gave no weight to objective policy content, and instead assumed the position of their group as their own. This effect was as strong among people who were knowledgeable about welfare as it was among people who were not. Finally, participants persisted in the belief that they had formed their attitude autonomously even in the two group information conditions where they had not.&rdquo;</p>\n</blockquote>\n<p>Also, the final study conducted had subjects write editorials either in support of or against a single policy proposal. The differences in how people responded in the \"no group information\" condition and the \"my political party supports / opposes\" conditions are also illuminating...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DdgSyQoZXjj3KnF4N": 1, "FkzScn5byCs9PxGsA": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pyP8wR28fmgeHf27o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 41, "extendedScore": null, "score": 0.000127, "legacy": true, "legacyId": "9712", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-07T08:45:37.355Z", "modifiedAt": null, "url": null, "title": "The beginnings of a test for Rationality Quotient", "slug": "the-beginnings-of-a-test-for-rationality-quotient", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:09.673Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cgQcj48SRkkeoWNRy/the-beginnings-of-a-test-for-rationality-quotient", "pageUrlRelative": "/posts/cgQcj48SRkkeoWNRy/the-beginnings-of-a-test-for-rationality-quotient", "linkUrl": "https://www.lesswrong.com/posts/cgQcj48SRkkeoWNRy/the-beginnings-of-a-test-for-rationality-quotient", "postedAtFormatted": "Wednesday, September 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20beginnings%20of%20a%20test%20for%20Rationality%20Quotient&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20beginnings%20of%20a%20test%20for%20Rationality%20Quotient%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcgQcj48SRkkeoWNRy%2Fthe-beginnings-of-a-test-for-rationality-quotient%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20beginnings%20of%20a%20test%20for%20Rationality%20Quotient%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcgQcj48SRkkeoWNRy%2Fthe-beginnings-of-a-test-for-rationality-quotient", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcgQcj48SRkkeoWNRy%2Fthe-beginnings-of-a-test-for-rationality-quotient", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 133, "htmlBody": "<p>In their <a href=\"http://web.mac.com/kstanovich/Site/Research_on_Reasoning_files/Stanovich_Handbook.pdf\">2011 chapter</a> for the <em>Cambridge Handbook of Intelligence</em>, Stanovich et al. review the evidence suggesting that intelligence and rationality are not the same thing and that rationality is often more important than intelligence. They then lament the fact that there are no standard tests for measuring one's \"Rationality Quotient.\" Then they take a few steps toward such a thing by suggesting some important rationality skills (actively open-minded thinking, fine-grained emotional regulation, tendency to seek information and fully process it, etc.) and rationality 'mindware' (probability theory, scientific process, economic thinking, etc.).</p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Stanovich-et-als-rationality-test-beginnings.pdf\">Here</a> are those pages in particular: first a graphic of some important rationality skills and mindware, and then a table of the components of rational thought: rationality components, relevant literature citations, and example word problems that would test for each rationality component.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cgQcj48SRkkeoWNRy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 24, "extendedScore": null, "score": 7.659709007066114e-07, "legacy": true, "legacyId": "9714", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-07T11:26:20.877Z", "modifiedAt": null, "url": null, "title": "A 2011 summary of modern intelligence tests", "slug": "a-2011-summary-of-modern-intelligence-tests", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:25.600Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7gptRck2aNE3t3est/a-2011-summary-of-modern-intelligence-tests", "pageUrlRelative": "/posts/7gptRck2aNE3t3est/a-2011-summary-of-modern-intelligence-tests", "linkUrl": "https://www.lesswrong.com/posts/7gptRck2aNE3t3est/a-2011-summary-of-modern-intelligence-tests", "postedAtFormatted": "Wednesday, September 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%202011%20summary%20of%20modern%20intelligence%20tests&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%202011%20summary%20of%20modern%20intelligence%20tests%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7gptRck2aNE3t3est%2Fa-2011-summary-of-modern-intelligence-tests%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%202011%20summary%20of%20modern%20intelligence%20tests%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7gptRck2aNE3t3est%2Fa-2011-summary-of-modern-intelligence-tests", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7gptRck2aNE3t3est%2Fa-2011-summary-of-modern-intelligence-tests", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<p>...and the theories of intelligence they use to measure 'intelligence'. <a href=\"http://books.google.com/books?id=FtYeTcNwzQ4C&amp;lpg=PA34&amp;dq=%22pass%20theory%20of%20cognitive%20functioning%22%20%22planning%20attention%20simultaneous%22&amp;pg=PA34#v=onepage&amp;q&amp;f=false\">Here</a>, from the new (and very good)&nbsp;<em><a href=\"http://www.amazon.com/Cambridge-Handbook-Intelligence-Handbooks-Psychology/dp/052173911X/\">Cambridge Handbook of Intelligence</a></em>.</p>\n<p>Bonus fun fact from chapter 3: \"Persons with higher IQs apparently are also likely to be taller and have more body symmetry than persons with lower ability scores.\" [Silventoinen et al. (2006); Prokosch &amp; Miller (2006)]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4cKQgA4S7xfNeeWXg": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7gptRck2aNE3t3est", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 7.660235827900017e-07, "legacy": true, "legacyId": "9716", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-09-07T12:46:43.535Z", "modifiedAt": null, "url": null, "title": "Doing medical research with (a lot) of personal money [link]", "slug": "doing-medical-research-with-a-lot-of-personal-money-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:24.827Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GdxC6aqz47hSbZYd3/doing-medical-research-with-a-lot-of-personal-money-link", "pageUrlRelative": "/posts/GdxC6aqz47hSbZYd3/doing-medical-research-with-a-lot-of-personal-money-link", "linkUrl": "https://www.lesswrong.com/posts/GdxC6aqz47hSbZYd3/doing-medical-research-with-a-lot-of-personal-money-link", "postedAtFormatted": "Wednesday, September 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Doing%20medical%20research%20with%20(a%20lot)%20of%20personal%20money%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoing%20medical%20research%20with%20(a%20lot)%20of%20personal%20money%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGdxC6aqz47hSbZYd3%2Fdoing-medical-research-with-a-lot-of-personal-money-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Doing%20medical%20research%20with%20(a%20lot)%20of%20personal%20money%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGdxC6aqz47hSbZYd3%2Fdoing-medical-research-with-a-lot-of-personal-money-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGdxC6aqz47hSbZYd3%2Fdoing-medical-research-with-a-lot-of-personal-money-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 53, "htmlBody": "<p><a href=\"http://www.bloomberg.com/news/2011-09-07/mogul-using-own-100-million-in-race-to-cure-daughter-prompts-novartis-aid.html\">http://www.bloomberg.com/news/2011-09-07/mogul-using-own-100-million-in-race-to-cure-daughter-prompts-novartis-aid.html</a></p>\n<p>I think this an interesting data point (though obviously a single one) of what money can do when directed <a href=\"/lw/nb/something_to_protect/\">with a lot of drive</a> to a particular medical issue. Are there other examples of this? I wonder what kind of results can be achieved if this level of effort was directed at cryonics.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GdxC6aqz47hSbZYd3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "9718", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SGR4GxFK7KmW7ckCB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}