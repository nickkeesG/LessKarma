{"results": [{"createdAt": null, "postedAt": "2014-08-19T07:52:29.425Z", "modifiedAt": null, "url": null, "title": "Meetup : September Rationality Dojo - Fixed and Growth Mindset", "slug": "meetup-september-rationality-dojo-fixed-and-growth-mindset", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:05.540Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MelbourneLW", "createdAt": "2014-07-15T07:42:47.692Z", "isAdmin": false, "displayName": "MelbourneLW"}, "userId": "fnAEhR2GNN3t6PmFc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BPvmjkdRGLJBC4vGr/meetup-september-rationality-dojo-fixed-and-growth-mindset", "pageUrlRelative": "/posts/BPvmjkdRGLJBC4vGr/meetup-september-rationality-dojo-fixed-and-growth-mindset", "linkUrl": "https://www.lesswrong.com/posts/BPvmjkdRGLJBC4vGr/meetup-september-rationality-dojo-fixed-and-growth-mindset", "postedAtFormatted": "Tuesday, August 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20September%20Rationality%20Dojo%20-%20Fixed%20and%20Growth%20Mindset&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20September%20Rationality%20Dojo%20-%20Fixed%20and%20Growth%20Mindset%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBPvmjkdRGLJBC4vGr%2Fmeetup-september-rationality-dojo-fixed-and-growth-mindset%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20September%20Rationality%20Dojo%20-%20Fixed%20and%20Growth%20Mindset%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBPvmjkdRGLJBC4vGr%2Fmeetup-september-rationality-dojo-fixed-and-growth-mindset", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBPvmjkdRGLJBC4vGr%2Fmeetup-september-rationality-dojo-fixed-and-growth-mindset", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 233, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13h'>September Rationality Dojo - Fixed and Growth Mindset</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 September 2014 03:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Ross House Association, 247-251 Flinders Lane, Melbourne</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>[ATTN: Please remember the new location for the dojos: the Jenny Florence Room, Level 3, Ross House at 247 Flinders Lane, Melbourne.</p>\n\n<p>3:30pm start / arrival - formal dojo activities will commence at 4:00pm.]</p>\n\n<p>The Less Wrong Sunday Rationality Dojos are crafted to be serious self-improvement sessions for those committed to the Art of Rationality and personal growth. Each month a community member will run a session involving a presentation of content, discussion, and exercises.</p>\n\n<p>Continuing the succession of immensely successful dojos, Helen will run a session on fixed and growth mindset.</p>\n\n<p>As always, we will review the personal goals we committed to at the previous Dojo (I will have done X by the next Dojo). Scott Fowler recorded the commitments, if you didn't make it but would like to add your own goal to the records, send him a message (shokwave.sf@gmail.com).</p>\n\n<p>The Dojo is likely to run for 2-3 hours, after which some people will get dinner together.</p>\n\n<p>If you have any trouble finding the venue or getting in, call Louise on 0419 192 367.</p>\n\n<p>If you would like to present at a future Dojo or suggest a topic, please fill it in on the Rationality Dojo Roster: <a href=\"http://is.gd/dojoroster\" rel=\"nofollow\">http://is.gd/dojoroster</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13h'>September Rationality Dojo - Fixed and Growth Mindset</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BPvmjkdRGLJBC4vGr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.938585763623389e-06, "legacy": true, "legacyId": "26949", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___September_Rationality_Dojo___Fixed_and_Growth_Mindset\">Discussion article for the meetup : <a href=\"/meetups/13h\">September Rationality Dojo - Fixed and Growth Mindset</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 September 2014 03:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Ross House Association, 247-251 Flinders Lane, Melbourne</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>[ATTN: Please remember the new location for the dojos: the Jenny Florence Room, Level 3, Ross House at 247 Flinders Lane, Melbourne.</p>\n\n<p>3:30pm start / arrival - formal dojo activities will commence at 4:00pm.]</p>\n\n<p>The Less Wrong Sunday Rationality Dojos are crafted to be serious self-improvement sessions for those committed to the Art of Rationality and personal growth. Each month a community member will run a session involving a presentation of content, discussion, and exercises.</p>\n\n<p>Continuing the succession of immensely successful dojos, Helen will run a session on fixed and growth mindset.</p>\n\n<p>As always, we will review the personal goals we committed to at the previous Dojo (I will have done X by the next Dojo). Scott Fowler recorded the commitments, if you didn't make it but would like to add your own goal to the records, send him a message (shokwave.sf@gmail.com).</p>\n\n<p>The Dojo is likely to run for 2-3 hours, after which some people will get dinner together.</p>\n\n<p>If you have any trouble finding the venue or getting in, call Louise on 0419 192 367.</p>\n\n<p>If you would like to present at a future Dojo or suggest a topic, please fill it in on the Rationality Dojo Roster: <a href=\"http://is.gd/dojoroster\" rel=\"nofollow\">http://is.gd/dojoroster</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___September_Rationality_Dojo___Fixed_and_Growth_Mindset1\">Discussion article for the meetup : <a href=\"/meetups/13h\">September Rationality Dojo - Fixed and Growth Mindset</a></h2>", "sections": [{"title": "Discussion article for the meetup : September Rationality Dojo - Fixed and Growth Mindset", "anchor": "Discussion_article_for_the_meetup___September_Rationality_Dojo___Fixed_and_Growth_Mindset", "level": 1}, {"title": "Discussion article for the meetup : September Rationality Dojo - Fixed and Growth Mindset", "anchor": "Discussion_article_for_the_meetup___September_Rationality_Dojo___Fixed_and_Growth_Mindset1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-20T13:18:02.863Z", "modifiedAt": null, "url": null, "title": "\"Follow your dreams\" as a case study in incorrect thinking", "slug": "follow-your-dreams-as-a-case-study-in-incorrect-thinking", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:07.090Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EJ4eT72cEp7ijvQem/follow-your-dreams-as-a-case-study-in-incorrect-thinking", "pageUrlRelative": "/posts/EJ4eT72cEp7ijvQem/follow-your-dreams-as-a-case-study-in-incorrect-thinking", "linkUrl": "https://www.lesswrong.com/posts/EJ4eT72cEp7ijvQem/follow-your-dreams-as-a-case-study-in-incorrect-thinking", "postedAtFormatted": "Wednesday, August 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Follow%20your%20dreams%22%20as%20a%20case%20study%20in%20incorrect%20thinking&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Follow%20your%20dreams%22%20as%20a%20case%20study%20in%20incorrect%20thinking%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEJ4eT72cEp7ijvQem%2Ffollow-your-dreams-as-a-case-study-in-incorrect-thinking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Follow%20your%20dreams%22%20as%20a%20case%20study%20in%20incorrect%20thinking%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEJ4eT72cEp7ijvQem%2Ffollow-your-dreams-as-a-case-study-in-incorrect-thinking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEJ4eT72cEp7ijvQem%2Ffollow-your-dreams-as-a-case-study-in-incorrect-thinking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 750, "htmlBody": "<p>This post doesn't contain any new ideas that LWers don't already know. It's more of an attempt to organize my thoughts and have a writeup for future reference.</p>\n<p>Here's a great&nbsp;<a href=\"http://qntm.org/daemons\">quote</a> from Sam Hughes, giving some examples of good and bad advice:</p>\n<blockquote>\n<p>\"You and your gaggle of girlfriends had a saying at university,\" he tells her. \"'Drink through it'. Breakups, hangovers, finals. I have never encountered a shorter, worse, more densely&nbsp;<em>bad</em>&nbsp;piece of advice.\" Next he goes into their bedroom for a moment. He returns with four running shoes. \"You did the right thing by waiting for me. Probably the first right thing you've done in the last twenty-four hours. I subscribe, as you know, to a different mantra. So we're going to run.\"</p>\n</blockquote>\n<p>The typical advice given to young people who want to succeed in highly competitive areas, like sports, writing, music, or making video games, is to \"follow your dreams\". I think that advice is up there with \"drink through it\" in terms of sheer destructive potential. If it was replaced with \"don't bother following your dreams\" every time it was uttered, the world might become a happier place.</p>\n<p>The amazing thing about \"follow your dreams\" is that thinking about it uncovers a sort of perfect storm of biases. It's <a href=\"http://eev.ee/blog/2012/04/09/php-a-fractal-of-bad-design/\">fractally wrong</a>, like PHP, where the big picture is wrong and every small piece is also wrong in its own unique way.</p>\n<p>The big culprit is, of course, <a href=\"http://en.wikipedia.org/wiki/Optimism_bias\">optimism bias</a> due to perceived control. I will succeed because I'm me, the special person at the center of my experience. That's the same bias that leads us to overestimate our chances of finishing the thesis on time, or having a successful marriage, or any number of other things. Thankfully, we have a really good debiasing technique for this particular bias, known as <a href=\"http://en.wikipedia.org/wiki/Reference_class_forecasting\">reference class forecasting</a>, or inside vs outside view. What if your friend Bob was a slightly better guitar player than you? Would you bet a lot of money on Bob making it big like Jimi Hendrix? The question is laughable, but then so is betting the years of your own life, with a smaller chance of success than Bob.</p>\n<p>That still leaves many questions unanswered, though. Why do people offer such advice in the first place, why do other people follow it, and what can be done about it?</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Survivorship_bias\">Survivorship bias</a> is one big reason we constantly hear successful people telling us to \"follow our dreams\". Successful people doesn't really know why they are successful, so they attribute it to their hard work and not giving up. The media amplifies that message, while millions of failures go unreported because they're not celebrities, even though they try just as hard. So we hear about successes disproportionately, in comparison to how often they actually happen, and that colors our expectations of our own future success. Sadly, I don't know of any good debiasing techniques for this error, other than just reminding yourself that it's an error.</p>\n<p>When someone has invested a lot of time and effort into following their dream, it feels harder to give up due to the <a href=\"http://en.wikipedia.org/wiki/Sunk_costs\">sunk cost fallacy</a>. That happens even with very stupid dreams, like the dream of winning at the casino, that were obviously installed by someone else for their own profit. So when you feel convinced that you'll eventually make it big in writing or music, you can remind yourself that compulsive gamblers feel the same way, and that feeling something doesn't make it true.</p>\n<p>Of course there are good dreams and bad dreams. Some people have dreams that don't tease them for years with empty promises, but actually start paying off in a predictable time frame. The main difference between the two kinds of dream is the difference between positive-sum games, a.k.a. productive occupations, and zero-sum games, a.k.a. popularity contests. Sebastian Marshall's post&nbsp;<a href=\"http://sebastianmarshall.com/positive-sum-games-dont-require-natural-talent\">Positive Sum Games Don't Require Natural Talent</a>&nbsp;makes the same point, and advises you to choose a game where you can be successful without outcompeting 99% of other players.</p>\n<p>The really interesting question to me right now is, what sets someone on the path of investing everything in a hopeless dream? Maybe it's a small success at an early age, followed by some random encouragement from others, and then you're locked in. Is there any hope for thinking back to that moment, or set of moments, and making a little twist to put yourself on a happier path? I usually don't advise people to change their desires, but in this case it seems to be the right thing to do.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EJ4eT72cEp7ijvQem", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 43, "extendedScore": null, "score": 0.000133, "legacy": true, "legacyId": "26962", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-20T14:36:17.955Z", "modifiedAt": null, "url": null, "title": "Polling Thread", "slug": "polling-thread-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:05.338Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QezE6pwRxDsZhydWv/polling-thread-0", "pageUrlRelative": "/posts/QezE6pwRxDsZhydWv/polling-thread-0", "linkUrl": "https://www.lesswrong.com/posts/QezE6pwRxDsZhydWv/polling-thread-0", "postedAtFormatted": "Wednesday, August 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Polling%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolling%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQezE6pwRxDsZhydWv%2Fpolling-thread-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Polling%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQezE6pwRxDsZhydWv%2Fpolling-thread-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQezE6pwRxDsZhydWv%2Fpolling-thread-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 224, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">The next installment of the&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/jju/polling_thread/\">Polling Thread</a>.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">This is your chance to ask your multiple choice question you always wanted to throw in. Get qualified numeric feedback to your comments. Post fun polls.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">These are the rules:</p>\n<ol style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">\n<li>Each poll goes into its own top level comment and may be commented there.</li>\n<li>You must at least vote all polls that were posted earlier than you own. This ensures participation in all polls and also limits the total number of polls. You may of course vote without posting a poll.</li>\n<li>Your poll should include a 'don't know' option (to avoid conflict with 2). I don't know whether we need to add a troll catch option here but we will see.</li>\n</ol>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">If you don't know how to make a poll in a comment look at the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Comment_formatting#Polls\">Poll Markup Help</a>.</p>\n<hr style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" />\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">This is a somewhat regular thread. If it is successful I may post again. Or you may. In that case do the following :</p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">\n<li>Use \"Polling Thread\" in the title.</li>\n<li>Copy the rules.</li>\n<li>Add the tag \"poll\".</li>\n<li>Link to this Thread or a previous Thread.</li>\n<li>Create a top-level comment saying 'Discussion of this thread goes here; all other top-level comments should be polls or similar'</li>\n<li>Add a second top-level comment with an initial poll to start participation.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QezE6pwRxDsZhydWv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 1.9417145082717693e-06, "legacy": true, "legacyId": "26963", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sqeXx4dPiRixdRxdb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-20T17:25:35.026Z", "modifiedAt": null, "url": null, "title": "Thought experiments on simplicity in logical probability", "slug": "thought-experiments-on-simplicity-in-logical-probability", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:02.976Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FHisTJwPjpcMXEXDx/thought-experiments-on-simplicity-in-logical-probability", "pageUrlRelative": "/posts/FHisTJwPjpcMXEXDx/thought-experiments-on-simplicity-in-logical-probability", "linkUrl": "https://www.lesswrong.com/posts/FHisTJwPjpcMXEXDx/thought-experiments-on-simplicity-in-logical-probability", "postedAtFormatted": "Wednesday, August 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thought%20experiments%20on%20simplicity%20in%20logical%20probability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThought%20experiments%20on%20simplicity%20in%20logical%20probability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFHisTJwPjpcMXEXDx%2Fthought-experiments-on-simplicity-in-logical-probability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thought%20experiments%20on%20simplicity%20in%20logical%20probability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFHisTJwPjpcMXEXDx%2Fthought-experiments-on-simplicity-in-logical-probability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFHisTJwPjpcMXEXDx%2Fthought-experiments-on-simplicity-in-logical-probability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 936, "htmlBody": "<p>A common feature of many proposed logical priors is a preference for simple sentences over complex ones. This is sort of like an extension of Occam's razor into math. Simple things are more likely to be true. So, as it is said, \"why not?\"</p>\n<p>&nbsp;</p>\n<p>Well, the analogy has some wrinkles - unlike hypothetical rules for the world, logical sentences do not form a mutually exclusive set. Instead, for every sentence A there is a sentence not-A with pretty much the same complexity, and probability 1-P(A). So you can't make the probability smaller for all complex sentences, because their negations are also complex sentences! If you don't have any information that discriminates between them, A and not-A will both get probability 1/2 no matter how complex they get.</p>\n<p>But if our agent knows something that breaks the symmetry between A and not-A, like that A belongs to a mutually exclusive and exhaustive set of sentences with differing complexities, then it can assign higher probabilities to simpler sentences in this set without breaking the rules of probability. Except, perhaps, the rule about not making up information.</p>\n<p><strong>The question: is the simpler answer really more likely to be true than the more complicated answer, or is this just a delusion? If so, is it for some ontologically basic reason, or for a contingent and explainable reason?</strong></p>\n<p>&nbsp;</p>\n<p>There are two complications to draw your attention to. The first is in what we mean by complexity. Although it would be nice to use the Kolmogorov complexity of any sentence, which is the length of the shortest program that prints the sentence, such a thing is uncomputable by the kind of agent we want to build in the real world. The only thing our real-world agent is assured of seeing is the length of the sentence as-is. We can also find something in between Kolmogorov complexity and length by doing a brief search for short programs that print the sentence - this meaning is what is usually meant in this article, and I'll call it \"apparent complexity.\"</p>\n<p>The second complication is in what exactly a simplicity prior is supposed to look like. In the case of Solomonoff induction the shape is exponential - more complicated hypotheses are exponentially less likely. But why not a power law? Why not even a Poisson distribution? Does the difficulty of answering this question mean that thinking that simpler sentences are more likely is a delusion after all?</p>\n<p>&nbsp;</p>\n<p>Thought experiments:</p>\n<p>1: Suppose our agent knew from a trusted source that some extremely complicated sum could only be equal to A, or to B, or to C, which are three expressions of differing complexity. What are the probabilities?</p>\n<p>&nbsp;</p>\n<p>Commentary: This is the most sparse form of the question. Not very helpful regarding the \"why,\" but handy to stake out the \"what.\" Do the probabilities follow a nice exponential curve? A power law? Or, since there are just the three known options, do they get equal consideration?</p>\n<p>This is all based off intuition, of course. What does intuition say when various knobs of this situation are tweaked - if the sum is of unknown complexity, or of complexity about that of C? If there are a hundred options, or countably many? Intuitively speaking, does it seem like favoring simpler sentences is an ontologically basic part of your logical prior?</p>\n<p>&nbsp;</p>\n<p>2: Consider subsequences of the digits of pi. If I give you a pair (n,m), you can tell me the m digits following the nth digit of pi.&nbsp;So if I start a sentence like \"the subsequence of digits of pi (10<sup>100</sup>, 10<sup>2</sup>) = \", do you expect to see simpler strings of digits on the right side? Is this a testable prediction about the properties of pi?</p>\n<p>&nbsp;</p>\n<p>Commentary: We know that there is always a short-ish program to produce the sequences, which is just to compute the relevant digits of pi. This sets a hard upper bound on the possible Kolmogorov complexity of sequences of pi (that grows logarithmically as you increase m and n), and past a certain m this will genuinely start restricting complicated sequences, and thus favoring \"all zeros\" - or does it?</p>\n<p>After all, this is weak tea compared to an exponential simplicity prior, for which the all-zero sequence would be hojillions of times more likely than a messy one. On the other hand, an exponential curve allows sequences with higher Kolmogorov complexity than the computation of the digits of pi.</p>\n<p>Does the low-level view outlined in the first paragraph above demonstrate that the exponential prior is bunk? Or can you derive one from the other with appropriate simplifications (keeping in mind Komogorov complexity vs. apparent complexity)? Does pi really contain more long simple strings than expected, and if not what's going on with our prior?</p>\n<p>&nbsp;</p>\n<p>3: Suppose I am writing an expression that I want to equal some number you know - that is, the sentence \"my expression = your number\" should be true. If I tell you the complexity of my expression, what can you infer about the likelihood of the above sentence?</p>\n<p>&nbsp;</p>\n<p>Commentary: If we had access to Kolmogorov complexity of your number, then we could completely rule out answers that were too K-simple to work. With only an approximation, it seems like we can still say that simple answers are <em>less</em>&nbsp;likely up to a point. Then as my expression gets more and more complicated, there are more and more available wrong answers (and, outside of the system a bit, it becomes less and less likely that I know what I'm doing), and so probability goes down.</p>\n<p>In the limit that my expression is much more complex than your number, does an elegant exponential distribution emerge from underlying considerations?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb25c": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FHisTJwPjpcMXEXDx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 1.9420022136585953e-06, "legacy": true, "legacyId": "26957", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-20T19:23:26.132Z", "modifiedAt": null, "url": null, "title": "Anthropics doesn't explain why the Cold War stayed Cold", "slug": "anthropics-doesn-t-explain-why-the-cold-war-stayed-cold", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:08.082Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KnaveOfAllTrades", "createdAt": "2012-07-20T02:08:23.538Z", "isAdmin": false, "displayName": "KnaveOfAllTrades"}, "userId": "FuACexYrBpyrMmz5C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EkmeEAB646Yf2DNzW/anthropics-doesn-t-explain-why-the-cold-war-stayed-cold", "pageUrlRelative": "/posts/EkmeEAB646Yf2DNzW/anthropics-doesn-t-explain-why-the-cold-war-stayed-cold", "linkUrl": "https://www.lesswrong.com/posts/EkmeEAB646Yf2DNzW/anthropics-doesn-t-explain-why-the-cold-war-stayed-cold", "postedAtFormatted": "Wednesday, August 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anthropics%20doesn't%20explain%20why%20the%20Cold%20War%20stayed%20Cold&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnthropics%20doesn't%20explain%20why%20the%20Cold%20War%20stayed%20Cold%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEkmeEAB646Yf2DNzW%2Fanthropics-doesn-t-explain-why-the-cold-war-stayed-cold%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anthropics%20doesn't%20explain%20why%20the%20Cold%20War%20stayed%20Cold%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEkmeEAB646Yf2DNzW%2Fanthropics-doesn-t-explain-why-the-cold-war-stayed-cold", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEkmeEAB646Yf2DNzW%2Fanthropics-doesn-t-explain-why-the-cold-war-stayed-cold", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2547, "htmlBody": "<p><em>(<strong>Epistemic status:</strong> There are some lines of argument that I haven&rsquo;t even started here, which potentially defeat the thesis advocated here. I don&rsquo;t go into them because this is already too long or I can&rsquo;t explain them adequately without derailing the main thesis. Similarly some continuations of chains of argument and counterargument begun here are terminated in the interest of focussing on the lower-order counterarguments. Overall this piece probably overstates my confidence in its thesis. It is quite possible this post will be torn to pieces in the comments&mdash;possibly by my own aforementioned elided considerations. That&rsquo;s good too.)</em><br /><br /><span style=\"text-decoration: underline;\"><strong>I</strong></span><br /><br />George VI, King of the United Kingdom, had five siblings. That is, the father of current Queen Elizabeth II had as many siblings as on a typical human hand. (This paragraph is true, and is not a trick; in particular, the second sentence of this paragraph really is trying to disambiguate and help convey the fact in question and relate it to prior knowledge, rather than introduce an opening for some sleight of hand so I can laugh at you later, or whatever fear such a suspiciously simple proposition might engender.)<br /><br />Let it be known.<br /><br /><span style=\"text-decoration: underline;\"><strong>II</strong></span><br /><br />Exactly one of the following stories is true:<br /><br /><strong>Story One</strong><br /><br />Recently I hopped on Facebook and saw the following post:<br /><em><br />&ldquo;I notice that I am confused about why a nuclear war never occurred. Like, I think (knowing only the very little I know now) that if you had asked me, at the start of the Cold War or something, the probability that it would eventually lead to a nuclear war, I would've said it was moderately likely. So what's up with that?&rdquo;</em><br /><br />The post had 14 likes. In the comments, the most-Liked explanation was:<br /><br /><em>&ldquo;anthropically you are considerably more likely to live in a world where there never was a fullscale nuclear war&rdquo;</em><br /><br />That comment had 17 Likes. The second-most-liked comment that offered an explanation had 4 Likes.<br /><br /><strong>Story Two</strong><br /><a id=\"more\"></a><br />Recently I hopped on Facebook and saw the following post:<br /><br /><em>&ldquo;I notice that I am confused about why George VI only had five siblings. Like, I think (knowing only the very little I know now) that if you had asked me, at his birth or something, the probability that he would eventually have had only five siblings and no more, I would've said it was moderately unlikely. So what's up with that?&rdquo;</em><br /><br />The post had 14 likes. In the comments, the most-Liked explanation was:<br /><br /><em>&ldquo;anthropically you are considerably more likely to live in a world where George VI never had more siblings&rdquo;</em><br /><br />That comment had 17 Likes. The second-most-liked comment that offered an explanation had 4 Likes.<br /><br />~<br /><br />Which of the stories is true?<br /><br /><span style=\"text-decoration: underline;\"><strong>III</strong></span><br /><br />It wasn&rsquo;t a trick question; the first story was of course true, with the second false. (If you didn&rsquo;t think it was a trick but still guessed otherwise, then I want you to tell me how the heck I can get discussions like the second story on my feed.)<br /><br />Even if one disagrees with it as an explanation, invoking anthropics in the context of a nuclear exchange is a familiar part of the conversational landscape by now, but it most certainly is not in the case of George VI&rsquo;s lack of further siblings. This smacks to me of a contextual bias; we treat nuclear exchanges as qualitatively different from George VI&rsquo;s siblings, with respect to anthropics explanations, but I am suspicious of this distinction.<a name=\"triple_asterisk_top\"></a><a href=\"#triple_asterisk_bottom\">***</a><br /><br />The obvious defence in face of my suspicion is to point out that the qualitative distinction arises from the nuclear exchange being an &lsquo;observer-killing&rsquo; event, whereas George VI having more siblings is not. In this case, &lsquo;observer-killing&rsquo; would mean that the event seems incompatible with the observer&rsquo;s existence (i.e. you wouldn&rsquo;t exist in a world that had suffered a nuclear exchange since this would significantly affect world history), rather than that the nuclear exchange directly would have killed the would-be observer.<br /><br />(At this point, I would like to remind you that George VI had five siblings; the same number as you presumably have fingers on each of your hands.)<br /><br />(Also, note that the Facebook thread was talking about a nuclear exchange, not a nuclear extinction event. Indeed, I have often seen it claimed that there were never actually enough nuclear weapons in the world for extinction to even take place. I am not convinced this aside makes a difference to the justifiability of anthropic explanations, but I note it since it is <em>at least</em> as hard to justify anthropic explanations with a non-extinction nuclear exchange as it is with a potential extinction event.)<br /><br />However, this defence does not hold when we examine the motivation for anthropic evidence in more detail. The reason a nuclear exchange seems observer-killing is that the history of a world with a nuclear exchange will not feature you being alive in 2014, for example due to economic or civilizational setback caused by the nuclear exchange. More precisely,<br /><br />Probability(You exist | No nuclear exchange) &gt; Probability(You exist | Nuclear exchange),<br /><br />whereas<br /><br />Probability(You exist | George VI had five (or fewer) siblings) = Probability(You exist | George VI had six or more siblings).<br /><br />The idea of anthropics evidence is motivated by the epistemological principle that one must condition, in a Bayesian update, on all information (evidence) available to oneself, including one&rsquo;s own existence. Anthropic evidence is unconventional; it arises from <a href=\"/lw/2l6/taking_ideas_seriously/\">taking this epistemological principle seriously</a> to an uncommon extent.<br /><br />(Fix it in your mind that George VI had five siblings.)<br /><br />However, taking this principle even further, and bearing in mind that you (now) know that George VI had five siblings, I might ask you now why George VI didn&rsquo;t have more siblings.<br /><br />&hellip;and since we now know that George VI <em>didn&rsquo;t</em> have more siblings, we obtain<br /><br />Probability(You exist [and know that George VI had exactly five siblings] | George VI had more than five siblings) = 0<br /><br />since the &lsquo;Evidence&rsquo; in our likelihood Probability(Evidence | Hypothesis) now includes your knowledge that George VI had exactly five siblings.<br /><br />Oh look, I just made &lsquo;George VI had five siblings&rsquo; an observer-killing event.<br /><br />Or less sensationally: Insomuch as one can explain the absence of a nuclear exchange by one&rsquo;s existence, you can now also explain George VI&rsquo;s exact number of siblings, or any other part of one&rsquo;s knowledge.<br /><br />In fact, &lsquo;anthropic effects&rsquo; or &lsquo;survivorship bias&rsquo; is a fully general &lsquo;explanation&rsquo; for why <em>anything</em> is the case rather than some contradictory fact of the matter. This is a strong form of actualism that, when presented in such terms, is generally rejected (or at least, which I would expect to be rejected by some of the people deploying anthropic explanations).<br /><br />I am skeptical of counterarguments along the lines of &lsquo;but you, the observer, completely fail to exist in the event of a nuclear exchange, whereas only some tiny part of your knowledge is lost if George VI has more siblings&rsquo;. I am not sure on what grounds one would justify making a point of treating &lsquo;being alive&rsquo; (which is a fuzzy human concept) as the relevant point of divergence, and exclude &lsquo;being alive but slightly different&rsquo;, and the whole line of argument reeks of the kind of anthropocentric epistemology that has lead to (for example) &lsquo;Hrm, looks like the observation of a conscious entity causes wavefunction collapse.&rsquo;<br /><span style=\"text-decoration: underline;\"><strong><br />IV</strong></span><br /><br />I realize that the extreme interpretation of &lsquo;condition on all information&rsquo; that I have invoked here looks very pedantic, and one suspects that it might be more Clever than wise. After all, even if the argument presented for anthropics evidence is refuted by my considerations, there might be other legitimate processes for harvesting anthropics evidence that <em>do</em> allow &lsquo;survivorship bias&rsquo; to be an appropriate explanation for &lsquo;why wasn&rsquo;t there a nuclear exchange&rsquo;.<br /><br />Another angle of offence on anthropics explanations in this context gives me more confidence that anthropic explanations are inappropriate (and thereby more confidence that the particular argument I gave above holds):<br /><br />Even if one insists that there is a relevant qualitative difference between a nuclear exchange and George VI&rsquo;s siblings, according to some criteria, one can think of any number of similar questions that do not feel anthropics-appropriate but which fall on the same side of the criteria as a nuclear exchange.<br /><br />For example, if one thinks the qualitative difference is the possibility of civilizational collapse or economic setback, then one can explain the absence of World War III without reference to history, merely with one&rsquo;s existence. That might, in fact, seem legitimate, but then we have to explain why <em>the first two</em> World Wars took place.<br /><br />Similarly if one thinks the qualitative difference is that more people existed for one&rsquo;s <a href=\"/lw/jh4/anthropic_atheism/\">anthropic soul</a> to be epiphenomenally bound to in worlds without a nuclear exchange than worlds with one; again one has to explain why we are in a model where any setbacks have happened.<br /><br />It seems very &lsquo;mysterious&rsquo; that when not talking about Conventionally Anthropic-y Thingies like nuclear exchanges, we try to explain questions of history or geopolitics or fertility using the typical, direct, causal considerations of the relevant fields. But when nuclear weapons come up, we switch into Anthropic Thinking and abandon, say, geopolitical or game theoretic explanations in favour of observer selection effects.<br /><br /><span style=\"text-decoration: underline;\"><strong>V</strong></span><br /><br />Another perspective that views anthropic explanations unfavourably is a pragmatic account that begins by considering what we want when asking a question like the one posed in the Facebook post. It seems to me that the post is basically asking for one of the following:<br /><br />(A) <strong>Coincidence</strong>: Evidence that the absence of a nuclear exchange was coincidental, in the sense that there were no identifiable causal factors for the nuclear exchange not happening, or at least that any such factors are not relevant (e.g. not helpful for making predictions about future calamities, not helpful for understanding international relations, etc.)<br /><br />(B) <strong>Faulty model</strong>: Evidence that there are relevant factors that the original poster overlooked or weighed incorrectly that, if weighed correctly, would decrease the probability one would give at the start of the Cold War for a subsequent nuclear exchange<br /><br />In case (A), we want to know so that we can confirm that there is nothing actionable to be learned from the incorrect prediction. In case (B), we want to know so that we can learn from any systematic mistakes that might crop up in our understanding of such situations. An anthropic explanation advances neither of these projects.<br /><br />Imagine that you are drenched if and only if somebody turned on a sprinkler next to you. We could represent this graphically by &lsquo;Somebody turns on sprinkler&rsquo;--&gt;&rsquo;Water is sprinkled towards you&rsquo;--&gt;&rsquo;You get soaked&rsquo;--&gt;&rsquo;You outragedly ask why you are soaked&rsquo;, with each of the successive conditional probabilities being one (certainty). It would not be a particularly useful reply to the question (&ldquo;Why am I soaked?!&rdquo;) for someone to say, &ldquo;Well, in every model in which you&rsquo;re <em>not</em> soaked, you don&rsquo;t think to ask that question.&rdquo; It is trivially true that there is perfect correlation between these two events, but this is not the causal information being sought. The observation that you ask the question if and only if you are soaked encodes a correlation, but this is not the aspect of the graph we&rsquo;re interested in. In fact, this observer selection explanation arises <em>entirely</em> from regular, causal, useful factors downstream.<br /><br />So long as you believe that your weight of experience is distributed among your instantiations according to some prior on initial conditions and causal laws governing the redistribution of weight of experience thereafter (for example, if one starts with a prior over quantum wavefunctions then allocates weight of experience within each wavefunction according to its evolution and the Born rule), then anthropic explanations are lossy compressions of causal explanations, as with the sprinkler; insomuch as any event is meaningfully explainable, there must be a causal explanation.<br /><br />(This is equivalent to the deep point that the Doomsday Argument or the Great Filter only rephrase our priors in ways that <em>seem</em> significant, so that updating on them is a mistake of updating twice on evidence&mdash;or, more accurately, updating on one&rsquo;s priors!<a name=\"triple_octothorpe_top\"></a><a href=\"#triple_octothorpe_bottom\">###</a>)<br /><br />Now, there <em>might</em> be rules for allocating weight of experience that somehow favour anthropic perspectives. But there seems to be no reason to expect any unbiased set of rules to favour anthropic perspectives specifically, or even to support anthropics rather than penalize it, and my conjunctive probability that there exist such unbiased supporting sets of rules <em>and</em> that the people explaining the absence of a nuclear exchange with those unbiased rules in mind&hellip;is not very high.<br /><br /><span style=\"text-decoration: underline;\"><strong>VI</strong></span><br /><br />I&rsquo;ve given several reasons to be skeptical of anthropic explanations like that quoted early in this post. Even if the considerations here are non-exhaustive or incomplete, they suggest that the matter is more involved and less clear-cut than many seem to believe.<br /><br />On the other hand, possibly the pattern of agreement in the comments on the Facebook post was about showing off understanding (or even just having heard of) anthropic arguments, or rewarding Cleverness, rather than endorsement of anthropics as the One True Explanation. Maybe I&rsquo;m not actually much less confident than others about anthropic explanations, and I misread the situation?<br /><br /><a name=\"triple_asterisk_bottom\"></a><a href=\"#triple_asterisk_top\">***</a>There is at least one unmentioned line of potential redemption that I see for the instinct to treat these cases as qualitatively different, but for the sake of allocating attention to the points raised here, in the interest of not jumping a few rungs up <a href=\"http://en.wikipedia.org/wiki/Lie-to-children#Wittgenstein.27s_ladder\">the ladder</a>, and because I have not explored that avenue so well, I shall pass over it. Ideally I shall explain the line in question eventually, but first I would prefer to build up the preceding rungs. If I see anyone raise it anyway, I shall publicly award them many Knave points.<br /><br /><a name=\"triple_octothorpe_bottom\"></a><a href=\"#triple_octothorpe_top\">###</a>This also leaves an opening; since we are not Bayesian reasoners, it might be practical to try to construct priors after-the-fact from considerations such as observer selection effects. But then we are in murky enough territory that it is not clear why we should give much weight to observer selection effects compared to the countless other types of evidence we can learn from&mdash;or indeed that we should take selection effects into account at all. This point deserves further thought, though.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EkmeEAB646Yf2DNzW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 12, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "26966", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q8jyAdRYbieK8PtfT", "93oeqzF7ZEKbd9jdx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-21T05:05:11.156Z", "modifiedAt": null, "url": null, "title": "Productivity thoughts from Matt Fallshaw", "slug": "productivity-thoughts-from-matt-fallshaw", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:03.115Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7XPbHTMwqgLgryF4v/productivity-thoughts-from-matt-fallshaw", "pageUrlRelative": "/posts/7XPbHTMwqgLgryF4v/productivity-thoughts-from-matt-fallshaw", "linkUrl": "https://www.lesswrong.com/posts/7XPbHTMwqgLgryF4v/productivity-thoughts-from-matt-fallshaw", "postedAtFormatted": "Thursday, August 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Productivity%20thoughts%20from%20Matt%20Fallshaw&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProductivity%20thoughts%20from%20Matt%20Fallshaw%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7XPbHTMwqgLgryF4v%2Fproductivity-thoughts-from-matt-fallshaw%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Productivity%20thoughts%20from%20Matt%20Fallshaw%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7XPbHTMwqgLgryF4v%2Fproductivity-thoughts-from-matt-fallshaw", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7XPbHTMwqgLgryF4v%2Fproductivity-thoughts-from-matt-fallshaw", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 702, "htmlBody": "<p>At the 2014 Effective Altruism Summit in Berkeley a few weeks ago, I had the pleasure of talking to Matt Fallshaw about the things he does to be more effective. &nbsp;Matt is a founder of <a href=\"http://trikeapps.com/\">Trike Apps</a> (the consultancy that built Less Wrong), a founder of <a href=\"http://bellroy.com\">Bellroy</a>, and a polyphasic sleeper. &nbsp;Notes on our conversation follow.</p>\n<p>Matt recommends having a system for acquiring habits. &nbsp;He recommends separating collection from processing; that is, if you have an idea for a new habit you want to acquire, you should record the idea at the time you have it and then think about actually implementing it at some future time. &nbsp;Matt recommends doing this through a weekly review. &nbsp;He recommends vetting your collection to see what habits seem actually worth acquiring, then for those habits you actually want to acquire, coming up with a compassionate, reasonable plan for how you're going to acquire the habit.</p>\n<p>(Previously on LW: <a href=\"/lw/ita/how_habits_work_and_how_you_may_control_them/\">How habits work and how you may control them</a>,&nbsp;<a href=\"/lw/hub/common_failure_modes_in_habit_formation/\">Common failure modes in habit formation</a>.)</p>\n<p>The most difficult kind of habit for me to acquire is that of random-access situation-response habits, e.g. \"if I'm having a hard time focusing, read my <a href=\"/lw/egr/personal_information_management/\">notebook entry</a>&nbsp;that lists techniques for improving focus\". &nbsp;So I asked Matt if he had any habit formation advice for this particular situation. &nbsp;Matt recommended trying to actually execute the habit I wanted as many times as possible, even in an artificial context. &nbsp;Steve Pavlina describes the technique <a href=\"http://www.stevepavlina.com/blog/2006/04/how-to-get-up-right-away-when-your-alarm-goes-off/\">here</a>. &nbsp;Matt recommends making your habit execution as emotionally salient as possible. &nbsp;His example: Let's say you're trying to become less of a prick. &nbsp;Someone starts a conversation with you and you notice yourself experiencing the kind of emotions you experience before you start acting like a prick. &nbsp;So you spend several minutes explaining to them the episode of disagreeableness you felt coming on and how you're trying to become less of a prick before proceeding with the conversation. &nbsp;If all else fails, Matt recommends setting a recurring alarm on your phone that reminds you of the habit you're trying to acquire, although he acknowledges that this can be expensive.</p>\n<p>Part of your plan should include a check to make sure you actually stick with your new habit. &nbsp;But you don't want a check that's overly intrusive. &nbsp;Matt recommends keeping an Anki deck with a card for each of your habits. &nbsp;Then during your weekly review session, you can review the cards Anki recommends for you. &nbsp;For each card, you can rate the degree to which you've been sticking with the habit it refers to and do something to revitalize the habit if you haven't been executing it. &nbsp;Matt recommends writing the cards in a form of a concrete question, e.g. for a speed reading habit, a question could be \"Did you speed read the last 5 things you read?\" &nbsp;If you haven't been executing a particular habit, check to see if it has a clear, identifiable trigger.</p>\n<p>Ideally your weekly review will come at a time you feel particularly \"agenty\" (see also:&nbsp;<a href=\"/lw/ii9/reflective_control/\">Reflective Control</a>). &nbsp;So you may wish to schedule it at a time during the week when you tend to feel especially effective and energetic. &nbsp;Consuming caffeine before your weekly review is another idea.</p>\n<p>When running in to seemingly intractable problems related to your personal effectiveness, habits, etc., Matt recommends taking a step back to brainstorm and try to think of creative solutions. &nbsp;He says that oftentimes people will write off a task as \"impossible\" if they aren't able to come up with a solution in 30 seconds. &nbsp;He recommends setting a 5-minute timer.</p>\n<p>In terms of habits worth acquiring, Matt is a fan of speed reading, Getting Things Done, and the Theory of Constraints (especially useful for larger projects).</p>\n<p>Matt has found that through aggressive habit acquisition, he's been able to experience a sort of compound return on the habits he's acquired: by acquiring habits that give him additional time and mental energy, he's been able to reinvest some of that additional time and mental energy in to the acquisition of even more useful habits. &nbsp;Matt doesn't think he's especially smart or high-willpower relative to the average person in the Less Wrong community, and credits this compounding for the reputation he's acquired for being a badass.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"udPbn9RthmgTtHMiG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7XPbHTMwqgLgryF4v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 1.9431920140315625e-06, "legacy": true, "legacyId": "26973", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5wMTZLZZmZEbXdoMD", "PYgGpmmk3wQhSt6Yv", "stDijTKuto52M5wQT", "9TCH6qtKkfSnjjieP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-21T11:10:59.654Z", "modifiedAt": null, "url": null, "title": "Why we should err in both directions", "slug": "why-we-should-err-in-both-directions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:04.810Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "owencb", "createdAt": "2013-05-12T09:01:14.360Z", "isAdmin": false, "displayName": "owencb"}, "userId": "QDNJ93vrjoaRBesk2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qZWhaAEDayZx2rXXM/why-we-should-err-in-both-directions", "pageUrlRelative": "/posts/qZWhaAEDayZx2rXXM/why-we-should-err-in-both-directions", "linkUrl": "https://www.lesswrong.com/posts/qZWhaAEDayZx2rXXM/why-we-should-err-in-both-directions", "postedAtFormatted": "Thursday, August 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20we%20should%20err%20in%20both%20directions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20we%20should%20err%20in%20both%20directions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZWhaAEDayZx2rXXM%2Fwhy-we-should-err-in-both-directions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20we%20should%20err%20in%20both%20directions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZWhaAEDayZx2rXXM%2Fwhy-we-should-err-in-both-directions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZWhaAEDayZx2rXXM%2Fwhy-we-should-err-in-both-directions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1779, "htmlBody": "<p><em>Crossposted from the <a href=\"http://www.fhi.ox.ac.uk/why-we-should-err-in-both-directions/\">Global Priorities Project</a></em></p>\n<p>This is an introduction to the principle that when we are making decisions under uncertainty, we should choose so that we may err in either direction. We justify the principle, explore the relation with Umeshisms, and look at applications in priority-setting.</p>\n<h2>Some trade-offs</h2>\n<p>How much should you spend on your bike lock? A cheaper lock saves you money at the cost of security.</p>\n<p>How long should you spend weighing up which charity to donate to before choosing one? Longer means less time for doing other useful things, but you&rsquo;re more likely to make a good choice.</p>\n<p>How early should you aim to arrive at the station for your train? Earlier means less chance of missing it, but more time hanging around at the station.</p>\n<p>Should you be willing to undertake risky projects, or stick only to safe ones? The safer your threshold, the more confident you can be that you won&rsquo;t waste resources, but some of the best opportunities may have a degree of risk, and you might be able to achieve a lot more with a weaker constraint.</p>\n<h2><strong>The principle</strong></h2>\n<p>We face trade-offs and make judgements all the time, and inevitably we sometimes make bad calls. In some cases we should have known better; sometimes we are just unlucky. As well as trying to make fewer mistakes, we should try to minimise the damage from the mistakes that we do make.</p>\n<p>Here&rsquo;s a rule which can be useful in helping you do this:</p>\n<p style=\"padding-left: 30px;\">When making decisions that lie along a spectrum, you should choose so that you think you have some chance of being off from the best choice in each direction.</p>\n<p>We could call this principle <em>erring in both directions</em>. It might seem counterintuitive -- isn&rsquo;t it worse to not even know what direction you&rsquo;re wrong in? -- but it&rsquo;s based on some fairly straightforward economics. I give a non-technical sketch of a proof at the end, but the essence is: if you&rsquo;re not going to be perfect, you want to be close to perfect, and this is best achieved by putting your actual choice near the middle of your error bar.</p>\n<p>So the principle suggests that you should aim to arrive at the station with a bit of time wasted, but not so much that you won&rsquo;t miss the train even if something goes wrong.</p>\n<h3>Refinements</h3>\n<p>Just saying that you should have some chance of erring in either direction isn&rsquo;t enough to tell you what you should actually choose. It can be a useful warning sign in the cases where you&rsquo;re going substantially wrong, though, and as these are the most important cases to fix it has some use in this form.</p>\n<p>A more careful analysis would tell you that at the best point on the spectrum, a small change in your decision produces about as much expected benefit as expected cost. In ideal circumstances we can use this to work out exactly where on the spectrum we should be (in some cases more than one point may fit this, so you need to compare them directly). In practice it is often hard to estimate the marginal benefits and costs well enough for this to be useful approach. So although it is theoretically optimal, you will only sometimes want to try to apply this version.</p>\n<p>Say in our train example that you found missing the train as bad as 100 minutes waiting at the station. Then you want to leave time so that an extra minute of safety margin gives you a 1% reduction in the absolute chance of missing the train.</p>\n<p>For instance, say your options in the train case look like this:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>Safety margin (min)</td>\n<td>1</td>\n<td>2</td>\n<td>3</td>\n<td>4</td>\n<td>5</td>\n<td>6</td>\n<td>7</td>\n<td>8</td>\n<td>9</td>\n<td>10</td>\n<td>11</td>\n<td>12</td>\n<td>13</td>\n<td>14</td>\n<td>15</td>\n</tr>\n<tr>\n<td>Chance of missing train (%)</td>\n<td>50</td>\n<td>30</td>\n<td>15</td>\n<td>8</td>\n<td>5</td>\n<td>3</td>\n<td>2</td>\n<td>1.5</td>\n<td>1.1</td>\n<td>0.8</td>\n<td>0.6</td>\n<td>0.4</td>\n<td>0.3</td>\n<td>0.2</td>\n<td>0.1</td>\n</tr>\n</tbody>\n</table>\n<p>Then the optimal safety margin to leave is somewhere between 6 and 7 minutes: this is where the marginal minute leads to a 1% reduction in the chance of missing the train.</p>\n<h2>Predictions and track records</h2>\n<p>So far, we've phrased the idea in terms of the predicted outcomes of actions. Another more well-known perspective on the idea looks at events that have already happened. For example:</p>\n<ul>\n<li>&ldquo;If you've never missed a flight, you're spending too much time in airports.&rdquo;</li>\n<li><a href=\"http://arcanesentiment.blogspot.co.uk/2010/01/umeshism.html\">&ldquo;If your code never has bugs, you&rsquo;re being too careful.&rdquo;</a></li>\n</ul>\n<p>These formulations, <a href=\"http://www.scottaaronson.com/blog/?p=40\">dubbed 'Umeshisms'</a>, only work for decisions that you make multiple times, so that you can gather a track record.</p>\n<p>An advantage of applying the principle to track records is that it&rsquo;s more obvious when you&rsquo;re going wrong. Introspection can be hard.</p>\n<p>You can even apply the principle to track records of decisions which don&rsquo;t look like they are choosing from a spectrum. For example it is given as advice in the game of bridge: if you don&rsquo;t sometimes double the stakes on hands which eventually go against you, you&rsquo;re not doubling enough. Although doubling or not is a binary choice, erring in both directions still works because &lsquo;how often to do double&rsquo; is a trait that roughly falls on a spectrum.</p>\n<h2>Failures</h2>\n<p>There are some circumstances where the principle may not apply.</p>\n<p>First, if you think the correct point is at one extreme of the available spectrum. For instance nobody says &lsquo;if you&rsquo;re not worried about going to jail, you&rsquo;re not committing enough armed robberies&rsquo;, because we think the best number of armed robberies to commit is probably zero.</p>\n<p>Second, if the available points in the spectrum are discrete and few in number. Take the example of the bike locks. Perhaps there are only three options available: the Cheap-o lock (&pound;5), the Regular lock (&pound;20), and the Super lock (&pound;50). You might reasonably decide on the Regular lock, thinking that maybe the Super lock is better, but that the Cheap-o one certainly isn&rsquo;t. When you buy the Regular lock, you&rsquo;re pretty sure you&rsquo;re not buying a lock that&rsquo;s too tough. But since only two of the locks are good candidates, there is no decision you <em>could </em>make which tries to err in both directions.</p>\n<p>Third, in the case of evaluating track records, it may be that your record isn&rsquo;t long enough to expect to have seen errors in both directions, even if they should both come up eventually. If you haven&rsquo;t flown that many times, you could well be spending the right amount of time -- or even too little -- in airports, even if you&rsquo;ve never missed a flight.</p>\n<p>Finally, a warning about a case where the principle is not supposed to apply. It shouldn&rsquo;t be applied directly to try to equalise the probability of being wrong in either direction, without taking any account of magnitude of loss. So for example if someone says you should err on the side of caution by getting an early train to your job interview, it might look as though that were in conflict with the idea of erring in both directions. But normally what&rsquo;s meant is that you should have a higher probability of failing in one direction (wasting time by taking an earlier train than needed), <em>because</em> the consequences of failing in the other direction (missing the interview) are much higher.</p>\n<h2>Conclusions and applications to prioritisation</h2>\n<p>Seeking to err in both directions can provide a useful tool in helping to form better judgements in uncertain situations. Many people may already have internalised key points, but it can be useful to have a label to facilitate discussion. Additionally, having a clear principle can help you to apply it in cases where you might not have noticed it was relevant.</p>\n<p>How might this principle apply to priority-setting? It suggests that:</p>\n<ul>\n<li>You should spend enough time and resources on the prioritisation itself that you think some of time may have been wasted (for example you should spend a while at the end without changing your mind much), but not so much that you are totally confident you have the right answer.</li>\n<li>If you are unsure what discount rate to use, you should choose one so that you think that it could be either too high or too low.</li>\n<li>If you don&rsquo;t know how strongly to weigh fragile cost-effectiveness estimates against more robust evidence, you should choose a level so that you might be over- or under-weighing them.</li>\n<li>When you are providing a best-guess estimate, you should choose a figure which could plausibly be wrong either way.</li>\n</ul>\n<p>And one on track records:</p>\n<ul>\n<li>Suppose you&rsquo;ve made lots of grants. Then if you&rsquo;ve never backed a project which has failed, you&rsquo;re probably too risk-averse in your grantmaking.</li>\n</ul>\n<h3>Questions for readers</h3>\n<p>Do you know any other useful applications of this idea? Do you know anywhere where it seems to break? Can anyone work out easier-to-apply versions, and the circumstances in which they are valid?</p>\n<h2>Appendix: a sketch proof of the principle</h2>\n<p>Assume the true graph of value (on the vertical axis) against the decision you make (on the horizontal axis, representing the spectrum) is smooth, looking something like this: &nbsp; <img class=\"alignnone wp-image-792 size-full\" src=\"http://www.effective-altruism.com/wp-content/uploads/2014/08/pic.png\" alt=\"pic\" width=\"623\" height=\"229\" /></p>\n<p>The highest value is achieved at d, so this is where you&rsquo;d like to be. But assume you don&rsquo;t know quite where d is. Say your best guess is that d=g. But you think it&rsquo;s quite possible that d&gt;g, and quite unlikely that d&lt;g. Should you choose g?</p>\n<p>Suppose we compare g to g&rsquo;, which is just a little bit bigger than g. If d&gt;g, then switching from g to g&rsquo; would be moving up the slope on the left of the diagram, which is an improvement. If d=g then it would be better to stick with g, but it doesn&rsquo;t make so much difference because the curve is fairly flat at the top. And if g were bigger than d, we&rsquo;d be moving down the slope on the right of the diagram, which is worse for g&rsquo; -- but this scenario was deemed unlikely.</p>\n<p>Aggregating the three possibilities, we found that two of them were better for sticking with g, but in one of these (d=g) it didn&rsquo;t matter very much, and the other (d&lt;g) just wasn&rsquo;t very likely. In contrast, the third case (d&gt;g) was reasonably likely, and noticeably better for g&rsquo; than g. So overall we should prefer g&rsquo; to g.</p>\n<p>In fact we&rsquo;d want to continue moving until the marginal upside from going slightly higher was equal to the marginal downside; this would have to involve a non-trivial chance that we are going too high. So our choice should have a chance of failure in either direction. This completes the (sketch) proof.</p>\n<p><strong>Note</strong>: There was an assumption of smoothness in this argument. I suspect it may be possible to get slightly stronger conclusions or work from slightly weaker assumptions, but I&rsquo;m not certain what the most general form of this argument is. It is often easier to build a careful argument in specific cases.</p>\n<p><em>Acknowledgements: thanks to Ryan Carey, Max Dalton, and Toby Ord for useful comments and suggestions.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qZWhaAEDayZx2rXXM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 1.9438146510635836e-06, "legacy": true, "legacyId": "26977", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Crossposted from the <a href=\"http://www.fhi.ox.ac.uk/why-we-should-err-in-both-directions/\">Global Priorities Project</a></em></p>\n<p>This is an introduction to the principle that when we are making decisions under uncertainty, we should choose so that we may err in either direction. We justify the principle, explore the relation with Umeshisms, and look at applications in priority-setting.</p>\n<h2 id=\"Some_trade_offs\">Some trade-offs</h2>\n<p>How much should you spend on your bike lock? A cheaper lock saves you money at the cost of security.</p>\n<p>How long should you spend weighing up which charity to donate to before choosing one? Longer means less time for doing other useful things, but you\u2019re more likely to make a good choice.</p>\n<p>How early should you aim to arrive at the station for your train? Earlier means less chance of missing it, but more time hanging around at the station.</p>\n<p>Should you be willing to undertake risky projects, or stick only to safe ones? The safer your threshold, the more confident you can be that you won\u2019t waste resources, but some of the best opportunities may have a degree of risk, and you might be able to achieve a lot more with a weaker constraint.</p>\n<h2 id=\"The_principle\"><strong>The principle</strong></h2>\n<p>We face trade-offs and make judgements all the time, and inevitably we sometimes make bad calls. In some cases we should have known better; sometimes we are just unlucky. As well as trying to make fewer mistakes, we should try to minimise the damage from the mistakes that we do make.</p>\n<p>Here\u2019s a rule which can be useful in helping you do this:</p>\n<p style=\"padding-left: 30px;\">When making decisions that lie along a spectrum, you should choose so that you think you have some chance of being off from the best choice in each direction.</p>\n<p>We could call this principle <em>erring in both directions</em>. It might seem counterintuitive -- isn\u2019t it worse to not even know what direction you\u2019re wrong in? -- but it\u2019s based on some fairly straightforward economics. I give a non-technical sketch of a proof at the end, but the essence is: if you\u2019re not going to be perfect, you want to be close to perfect, and this is best achieved by putting your actual choice near the middle of your error bar.</p>\n<p>So the principle suggests that you should aim to arrive at the station with a bit of time wasted, but not so much that you won\u2019t miss the train even if something goes wrong.</p>\n<h3 id=\"Refinements\">Refinements</h3>\n<p>Just saying that you should have some chance of erring in either direction isn\u2019t enough to tell you what you should actually choose. It can be a useful warning sign in the cases where you\u2019re going substantially wrong, though, and as these are the most important cases to fix it has some use in this form.</p>\n<p>A more careful analysis would tell you that at the best point on the spectrum, a small change in your decision produces about as much expected benefit as expected cost. In ideal circumstances we can use this to work out exactly where on the spectrum we should be (in some cases more than one point may fit this, so you need to compare them directly). In practice it is often hard to estimate the marginal benefits and costs well enough for this to be useful approach. So although it is theoretically optimal, you will only sometimes want to try to apply this version.</p>\n<p>Say in our train example that you found missing the train as bad as 100 minutes waiting at the station. Then you want to leave time so that an extra minute of safety margin gives you a 1% reduction in the absolute chance of missing the train.</p>\n<p>For instance, say your options in the train case look like this:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>Safety margin (min)</td>\n<td>1</td>\n<td>2</td>\n<td>3</td>\n<td>4</td>\n<td>5</td>\n<td>6</td>\n<td>7</td>\n<td>8</td>\n<td>9</td>\n<td>10</td>\n<td>11</td>\n<td>12</td>\n<td>13</td>\n<td>14</td>\n<td>15</td>\n</tr>\n<tr>\n<td>Chance of missing train (%)</td>\n<td>50</td>\n<td>30</td>\n<td>15</td>\n<td>8</td>\n<td>5</td>\n<td>3</td>\n<td>2</td>\n<td>1.5</td>\n<td>1.1</td>\n<td>0.8</td>\n<td>0.6</td>\n<td>0.4</td>\n<td>0.3</td>\n<td>0.2</td>\n<td>0.1</td>\n</tr>\n</tbody>\n</table>\n<p>Then the optimal safety margin to leave is somewhere between 6 and 7 minutes: this is where the marginal minute leads to a 1% reduction in the chance of missing the train.</p>\n<h2 id=\"Predictions_and_track_records\">Predictions and track records</h2>\n<p>So far, we've phrased the idea in terms of the predicted outcomes of actions. Another more well-known perspective on the idea looks at events that have already happened. For example:</p>\n<ul>\n<li>\u201cIf you've never missed a flight, you're spending too much time in airports.\u201d</li>\n<li><a href=\"http://arcanesentiment.blogspot.co.uk/2010/01/umeshism.html\">\u201cIf your code never has bugs, you\u2019re being too careful.\u201d</a></li>\n</ul>\n<p>These formulations, <a href=\"http://www.scottaaronson.com/blog/?p=40\">dubbed 'Umeshisms'</a>, only work for decisions that you make multiple times, so that you can gather a track record.</p>\n<p>An advantage of applying the principle to track records is that it\u2019s more obvious when you\u2019re going wrong. Introspection can be hard.</p>\n<p>You can even apply the principle to track records of decisions which don\u2019t look like they are choosing from a spectrum. For example it is given as advice in the game of bridge: if you don\u2019t sometimes double the stakes on hands which eventually go against you, you\u2019re not doubling enough. Although doubling or not is a binary choice, erring in both directions still works because \u2018how often to do double\u2019 is a trait that roughly falls on a spectrum.</p>\n<h2 id=\"Failures\">Failures</h2>\n<p>There are some circumstances where the principle may not apply.</p>\n<p>First, if you think the correct point is at one extreme of the available spectrum. For instance nobody says \u2018if you\u2019re not worried about going to jail, you\u2019re not committing enough armed robberies\u2019, because we think the best number of armed robberies to commit is probably zero.</p>\n<p>Second, if the available points in the spectrum are discrete and few in number. Take the example of the bike locks. Perhaps there are only three options available: the Cheap-o lock (\u00a35), the Regular lock (\u00a320), and the Super lock (\u00a350). You might reasonably decide on the Regular lock, thinking that maybe the Super lock is better, but that the Cheap-o one certainly isn\u2019t. When you buy the Regular lock, you\u2019re pretty sure you\u2019re not buying a lock that\u2019s too tough. But since only two of the locks are good candidates, there is no decision you <em>could </em>make which tries to err in both directions.</p>\n<p>Third, in the case of evaluating track records, it may be that your record isn\u2019t long enough to expect to have seen errors in both directions, even if they should both come up eventually. If you haven\u2019t flown that many times, you could well be spending the right amount of time -- or even too little -- in airports, even if you\u2019ve never missed a flight.</p>\n<p>Finally, a warning about a case where the principle is not supposed to apply. It shouldn\u2019t be applied directly to try to equalise the probability of being wrong in either direction, without taking any account of magnitude of loss. So for example if someone says you should err on the side of caution by getting an early train to your job interview, it might look as though that were in conflict with the idea of erring in both directions. But normally what\u2019s meant is that you should have a higher probability of failing in one direction (wasting time by taking an earlier train than needed), <em>because</em> the consequences of failing in the other direction (missing the interview) are much higher.</p>\n<h2 id=\"Conclusions_and_applications_to_prioritisation\">Conclusions and applications to prioritisation</h2>\n<p>Seeking to err in both directions can provide a useful tool in helping to form better judgements in uncertain situations. Many people may already have internalised key points, but it can be useful to have a label to facilitate discussion. Additionally, having a clear principle can help you to apply it in cases where you might not have noticed it was relevant.</p>\n<p>How might this principle apply to priority-setting? It suggests that:</p>\n<ul>\n<li>You should spend enough time and resources on the prioritisation itself that you think some of time may have been wasted (for example you should spend a while at the end without changing your mind much), but not so much that you are totally confident you have the right answer.</li>\n<li>If you are unsure what discount rate to use, you should choose one so that you think that it could be either too high or too low.</li>\n<li>If you don\u2019t know how strongly to weigh fragile cost-effectiveness estimates against more robust evidence, you should choose a level so that you might be over- or under-weighing them.</li>\n<li>When you are providing a best-guess estimate, you should choose a figure which could plausibly be wrong either way.</li>\n</ul>\n<p>And one on track records:</p>\n<ul>\n<li>Suppose you\u2019ve made lots of grants. Then if you\u2019ve never backed a project which has failed, you\u2019re probably too risk-averse in your grantmaking.</li>\n</ul>\n<h3 id=\"Questions_for_readers\">Questions for readers</h3>\n<p>Do you know any other useful applications of this idea? Do you know anywhere where it seems to break? Can anyone work out easier-to-apply versions, and the circumstances in which they are valid?</p>\n<h2 id=\"Appendix__a_sketch_proof_of_the_principle\">Appendix: a sketch proof of the principle</h2>\n<p>Assume the true graph of value (on the vertical axis) against the decision you make (on the horizontal axis, representing the spectrum) is smooth, looking something like this: &nbsp; <img class=\"alignnone wp-image-792 size-full\" src=\"http://www.effective-altruism.com/wp-content/uploads/2014/08/pic.png\" alt=\"pic\" width=\"623\" height=\"229\"></p>\n<p>The highest value is achieved at d, so this is where you\u2019d like to be. But assume you don\u2019t know quite where d is. Say your best guess is that d=g. But you think it\u2019s quite possible that d&gt;g, and quite unlikely that d&lt;g. Should you choose g?</p>\n<p>Suppose we compare g to g\u2019, which is just a little bit bigger than g. If d&gt;g, then switching from g to g\u2019 would be moving up the slope on the left of the diagram, which is an improvement. If d=g then it would be better to stick with g, but it doesn\u2019t make so much difference because the curve is fairly flat at the top. And if g were bigger than d, we\u2019d be moving down the slope on the right of the diagram, which is worse for g\u2019 -- but this scenario was deemed unlikely.</p>\n<p>Aggregating the three possibilities, we found that two of them were better for sticking with g, but in one of these (d=g) it didn\u2019t matter very much, and the other (d&lt;g) just wasn\u2019t very likely. In contrast, the third case (d&gt;g) was reasonably likely, and noticeably better for g\u2019 than g. So overall we should prefer g\u2019 to g.</p>\n<p>In fact we\u2019d want to continue moving until the marginal upside from going slightly higher was equal to the marginal downside; this would have to involve a non-trivial chance that we are going too high. So our choice should have a chance of failure in either direction. This completes the (sketch) proof.</p>\n<p><strong>Note</strong>: There was an assumption of smoothness in this argument. I suspect it may be possible to get slightly stronger conclusions or work from slightly weaker assumptions, but I\u2019m not certain what the most general form of this argument is. It is often easier to build a careful argument in specific cases.</p>\n<p><em>Acknowledgements: thanks to Ryan Carey, Max Dalton, and Toby Ord for useful comments and suggestions.</em></p>", "sections": [{"title": "Some trade-offs", "anchor": "Some_trade_offs", "level": 1}, {"title": "The principle", "anchor": "The_principle", "level": 1}, {"title": "Refinements", "anchor": "Refinements", "level": 2}, {"title": "Predictions and track records", "anchor": "Predictions_and_track_records", "level": 1}, {"title": "Failures", "anchor": "Failures", "level": 1}, {"title": "Conclusions and applications to prioritisation", "anchor": "Conclusions_and_applications_to_prioritisation", "level": 1}, {"title": "Questions for readers", "anchor": "Questions_for_readers", "level": 2}, {"title": "Appendix: a sketch proof of the principle", "anchor": "Appendix__a_sketch_proof_of_the_principle", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-21T14:15:55.051Z", "modifiedAt": null, "url": null, "title": "An example of deadly non-general AI", "slug": "an-example-of-deadly-non-general-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:06.818Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zJDMY9KSPQjiGNfA2/an-example-of-deadly-non-general-ai", "pageUrlRelative": "/posts/zJDMY9KSPQjiGNfA2/an-example-of-deadly-non-general-ai", "linkUrl": "https://www.lesswrong.com/posts/zJDMY9KSPQjiGNfA2/an-example-of-deadly-non-general-ai", "postedAtFormatted": "Thursday, August 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20example%20of%20deadly%20non-general%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20example%20of%20deadly%20non-general%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzJDMY9KSPQjiGNfA2%2Fan-example-of-deadly-non-general-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20example%20of%20deadly%20non-general%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzJDMY9KSPQjiGNfA2%2Fan-example-of-deadly-non-general-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzJDMY9KSPQjiGNfA2%2Fan-example-of-deadly-non-general-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 317, "htmlBody": "<p>In a <a href=\"/r/discussion/lw/ksa/the_metaphormyth_of_general_intelligence/\">previous post</a>, I mused that we might be focusing too much on general intelligences, and that the route to powerful and dangerous intelligences might go through much more specialised intelligences instead. Since it's easier to reason with an example, here is a potentially deadly narrow AI (partially due to Toby Ord). Feel free to comment and improve on it, or suggest you own example.</p>\n<p>It's the standard \"pathological goal AI\" but only a narrow intelligence. Imagine a medicine designing super-AI with the goal of reducing human mortality in 50 years - i.e. massively reducing human population in the next 49 years. It's a narrow intelligence, so it has access only to a huge amount of human biological and epidemiological research. It must gets its drugs past FDA approval; this requirement is encoded as certain physical reactions (no death, some health improvements) to people taking the drugs over the course of a few years.</p>\n<p>Then it seems trivial for it to design a drug that would have no negative impact for the first few years, and then causes sterility or death. Since it wants to spread this to as many humans as possible, it would probably design something that interacted with common human pathogens - colds, flues - in order to spread the impact, rather than affecting only those that took the disease.</p>\n<p>Now, this narrow intelligence is less threatening than if it had general intelligence - where it could also plan for possible human countermeasures and such - but it seems sufficiently dangerous on its own that we can't afford to worry only about general intelligences. Some of the \"AI superpowers\" that Nick mentions in his book (intelligence amplification, strategizing, social manipulation, hacking, technology research, economic productivity) could be enough to cause devastation on their own, even if the AI never developed other abilities.</p>\n<p>We still could be destroyed by a machine that we outmatch in almost every area.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zJDMY9KSPQjiGNfA2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 21, "extendedScore": null, "score": 1.944129540636366e-06, "legacy": true, "legacyId": "26978", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["i9gFjtHcMSrPHkcLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-21T14:49:50.430Z", "modifiedAt": null, "url": null, "title": "Another type of intelligence explosion", "slug": "another-type-of-intelligence-explosion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:04.870Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uWAW2kDQg8JYvGAyD/another-type-of-intelligence-explosion", "pageUrlRelative": "/posts/uWAW2kDQg8JYvGAyD/another-type-of-intelligence-explosion", "linkUrl": "https://www.lesswrong.com/posts/uWAW2kDQg8JYvGAyD/another-type-of-intelligence-explosion", "postedAtFormatted": "Thursday, August 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20type%20of%20intelligence%20explosion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20type%20of%20intelligence%20explosion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuWAW2kDQg8JYvGAyD%2Fanother-type-of-intelligence-explosion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20type%20of%20intelligence%20explosion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuWAW2kDQg8JYvGAyD%2Fanother-type-of-intelligence-explosion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuWAW2kDQg8JYvGAyD%2Fanother-type-of-intelligence-explosion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p>I've <a href=\"/r/discussion/lw/ksa/the_metaphormyth_of_general_intelligence/\">argued</a> that we might have to worry about <a href=\"/r/discussion/lw/kte/an_example_of_deadly_nongeneral_ai/\">dangerous non-general intelligences</a>. In a series of back and forth with Wei Dai, we agreed that some level of general intelligence (such as that humans seem to possess) seemed to be a great advantage, though possibly one with diminishing returns. Therefore a dangerous AI could be one with great narrow intelligence in one area, and a little bit of general intelligence in others.</p>\n<p>The traditional view of an intelligence explosion is that of an AI that knows how to do X, suddenly getting (much) better at doing X, to a level beyond human capacity. Call this the <em>gain of aptitude&nbsp;</em>intelligence explosion. We can prepare for that, maybe, by tracking the AI's ability level and seeing if it shoots up.</p>\n<p>But the example above hints at another kind of potentially dangerous intelligence explosion. That of a very intelligent but narrow AI that suddenly gains intelligence across other domains. Call this the <em>gain of function</em> intelligence explosion. If we're not looking specifically for it, it may not trigger any warnings - the AI might still be dumber than the average human in other domains. But this might be enough, when combined with its narrow superintelligence, to make it deadly. We can't ignore the toaster that starts babbling.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uWAW2kDQg8JYvGAyD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 28, "extendedScore": null, "score": 1.944187314847208e-06, "legacy": true, "legacyId": "26979", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["i9gFjtHcMSrPHkcLp", "zJDMY9KSPQjiGNfA2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-21T17:07:43.674Z", "modifiedAt": null, "url": null, "title": "Fighting Biases and Bad Habits like Boggarts", "slug": "fighting-biases-and-bad-habits-like-boggarts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:03.144Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "palladias", "createdAt": "2012-04-03T13:45:53.766Z", "isAdmin": false, "displayName": "palladias"}, "userId": "Bv2LXWzZf96WGpqJ5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jn3R6ybSrMYguDBNw/fighting-biases-and-bad-habits-like-boggarts", "pageUrlRelative": "/posts/jn3R6ybSrMYguDBNw/fighting-biases-and-bad-habits-like-boggarts", "linkUrl": "https://www.lesswrong.com/posts/jn3R6ybSrMYguDBNw/fighting-biases-and-bad-habits-like-boggarts", "postedAtFormatted": "Thursday, August 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fighting%20Biases%20and%20Bad%20Habits%20like%20Boggarts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFighting%20Biases%20and%20Bad%20Habits%20like%20Boggarts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjn3R6ybSrMYguDBNw%2Ffighting-biases-and-bad-habits-like-boggarts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fighting%20Biases%20and%20Bad%20Habits%20like%20Boggarts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjn3R6ybSrMYguDBNw%2Ffighting-biases-and-bad-habits-like-boggarts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjn3R6ybSrMYguDBNw%2Ffighting-biases-and-bad-habits-like-boggarts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 603, "htmlBody": "<p><strong>TL;DR: </strong>Building humor into your habits for spotting and correcting errors makes the fix more enjoyable, easier to talk about and receive social support, and limits the danger of a contempt spiral.&nbsp;</p>\n<p>&nbsp;</p>\n<p>One of the most reliably bad decisions I've made on a regular basis is the choice to stay awake (well, \"awake\") and on the internet past the point where I can get work done, or even have much fun. &nbsp;I went through a spell where I even fell asleep on the couch more nights than not, unable to muster the will or judgement to get up and go downstairs to bed.</p>\n<p>I could remember (even sometimes in the moment) that this was a bad pattern, but, the more tired I was, the more tempting it was to think that I should just&nbsp;<em>buckle down</em> and&nbsp;<em>apply more willpower</em> to be more awake and get more out of my computer time. &nbsp;Going to bed was&nbsp;<em>a</em> solution, but it was hard for it not to feel (to my sleepy brain and my normal one) like a bit of a cop out.</p>\n<p>Only two things helped me really keep this failure mode in check. &nbsp;One was setting a hard bedtime (<a href=\"https://www.beeminder.com/palladias/goals/bedtime\">and beeminding it</a>) <a href=\"http://www.patheos.com/blogs/unequallyyoked/2012/12/for-i-am-a-woman-under-authority.html\">as part of my sacrifice for Advent</a>. &nbsp; But the other key tool (which has lasted me long past Advent) is the gif below.</p>\n<p><a href=\"https://www.youtube.com/watch?v=j9akQL-MhHI\"><img class=\"aligncenter wp-image-10135\" src=\"http://wp.production.patheos.com/blogs/unequallyyoked/files/2014/08/sleep-eating-ice-cream.gif\" alt=\"sleep eating ice cream\" width=\"305\" height=\"172\" /></a></p>\n<p>The poor kid struggling to eat his ice cream cone, even in the face of his exhaustion, is hilarious. &nbsp;And not too far off the portrait of me around 2am scrolling through my Feedly.</p>\n<p>Thinking about how&nbsp;<em>stupid</em> or&nbsp;<em>ineffective</em> or&nbsp;<em>insufficiently&nbsp;strong-willed</em> I'm being makes it hard for me to do anything that feels like a retreat from my current course of action. &nbsp;I want to master the situation and prove I'm stronger. &nbsp;But catching on to the fact that my current situation (of my own making or not) is ridiculous, makes it easier to laugh, shrug, and move on.</p>\n<p>I think the difference is that it's easy for me to feel contemptuous of myself when frustrated, and easy to feel fond when amused.</p>\n<p>I've tried to strike the new emotional tone when I'm working on catching and correcting other errors. &nbsp;(e.g \"Stupid, you should have known to leave more time to make the appointment! &nbsp;Planning fallacy!\" &nbsp;becomes \"Heh, I guess you thought that adding two \"trivially short\" errands was a closed set, and must remain 'trivially short.' &nbsp;That's a pretty silly error.\")</p>\n<p>In the first case, noticing and correcting an error feels punitive, since it's quickly followed by a hefty dose of flagellation, but the second comes with a quick laugh and a easier shift to a growth mindset framing. &nbsp;Funny stories about errors are also easier to tell, increasing the chance my friends can help catch me out next time, or that I'll be better at spotting the error just by keeping it fresh in my memory.&nbsp;Not to mention, in order to get the joke, I tend to look for a more specific cause of the error than stupid/lazy/etc.</p>\n<p>As far as I can tell, it also helps that&nbsp;<em>amusement</em> is a pretty different feeling than the ones that tend to be active when I'm falling into error (frustration, anger, feeling trapped, impatience, etc). &nbsp;So, for a couple of seconds at least, I'm&nbsp;<em>out</em> of the rut and now need to actively&nbsp;<em>return</em> to it to stay stuck.&nbsp;</p>\n<p>In the heat of the moment of anger/akrasia/etc is a bad time to figure out what's funny, but, if you're reflecting on your errors after the fact, in a moment of consolation, it's easier to go back armed with a helpful reframing, ready to cast <em>Riddikulus</em>!</p>\n<p>&nbsp;</p>\n<p><em><a href=\"http://www.patheos.com/blogs/unequallyyoked/2014/08/fighting-bad-habits-and-biases-like-boggarts.html\">Crossposted from my personal blog, Unequally Yoked.</a></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jn3R6ybSrMYguDBNw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 46, "extendedScore": null, "score": 0.000292, "legacy": true, "legacyId": "26980", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-22T04:17:28.493Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington, D.C.: Museums Meetup", "slug": "meetup-washington-d-c-museums-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinZ", "createdAt": "2009-07-08T20:34:05.168Z", "isAdmin": false, "displayName": "RobinZ"}, "userId": "eTMojvi4f2z3pDfsc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZFtoZNQtMtXX3GyM3/meetup-washington-d-c-museums-meetup", "pageUrlRelative": "/posts/ZFtoZNQtMtXX3GyM3/meetup-washington-d-c-museums-meetup", "linkUrl": "https://www.lesswrong.com/posts/ZFtoZNQtMtXX3GyM3/meetup-washington-d-c-museums-meetup", "postedAtFormatted": "Friday, August 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%2C%20D.C.%3A%20Museums%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%2C%20D.C.%3A%20Museums%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZFtoZNQtMtXX3GyM3%2Fmeetup-washington-d-c-museums-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%2C%20D.C.%3A%20Museums%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZFtoZNQtMtXX3GyM3%2Fmeetup-washington-d-c-museums-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZFtoZNQtMtXX3GyM3%2Fmeetup-washington-d-c-museums-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13i'>Washington, D.C.: Museums Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 August 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be congregating between 3:00 and 3:30 in the Kogod Courtyard of the National Portrait Gallery (8th and F Sts or 8th and G Sts NW, go straight past the information desk from either entrance), and heading out after that to look at exhibits. Those present will decide on museums to visit (possibly including the National Portrait Gallery itself) and inform anyone joining belatedly of location and direction of travel via the lesswrong-dc Google Group.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13i'>Washington, D.C.: Museums Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZFtoZNQtMtXX3GyM3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.945563695075999e-06, "legacy": true, "legacyId": "26988", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington__D_C___Museums_Meetup\">Discussion article for the meetup : <a href=\"/meetups/13i\">Washington, D.C.: Museums Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 August 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be congregating between 3:00 and 3:30 in the Kogod Courtyard of the National Portrait Gallery (8th and F Sts or 8th and G Sts NW, go straight past the information desk from either entrance), and heading out after that to look at exhibits. Those present will decide on museums to visit (possibly including the National Portrait Gallery itself) and inform anyone joining belatedly of location and direction of travel via the lesswrong-dc Google Group.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington__D_C___Museums_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/13i\">Washington, D.C.: Museums Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington, D.C.: Museums Meetup", "anchor": "Discussion_article_for_the_meetup___Washington__D_C___Museums_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington, D.C.: Museums Meetup", "anchor": "Discussion_article_for_the_meetup___Washington__D_C___Museums_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-22T04:48:49.875Z", "modifiedAt": null, "url": null, "title": "Memory is Everything ", "slug": "memory-is-everything", "viewCount": null, "lastCommentedAt": "2017-12-09T06:42:02.368Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Qwake", "createdAt": "2014-07-21T05:54:10.040Z", "isAdmin": false, "displayName": "Qwake"}, "userId": "NJhDXz8RGsWkLNEKq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iihJFC4r5oGW43b6F/memory-is-everything", "pageUrlRelative": "/posts/iihJFC4r5oGW43b6F/memory-is-everything", "linkUrl": "https://www.lesswrong.com/posts/iihJFC4r5oGW43b6F/memory-is-everything", "postedAtFormatted": "Friday, August 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Memory%20is%20Everything%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMemory%20is%20Everything%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiihJFC4r5oGW43b6F%2Fmemory-is-everything%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Memory%20is%20Everything%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiihJFC4r5oGW43b6F%2Fmemory-is-everything", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiihJFC4r5oGW43b6F%2Fmemory-is-everything", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 509, "htmlBody": "<p>I have found (there is some (evidence)[http://mentalfloss.com/article/52586/why-do-our-best-ideas-come-us-shower] to suggest this) that showers are a great place to think. While I am taking a shower I find that I can think about things in a whole new perspective and it's very refreshing. Well today, while I was taking a shower, an interesting thing popped into my head. Memory is everything. Your memory contains you, it contains your thoughts, it contains your own unique perception of reality. Imagine going to bed tonight and waking up with absolutely no memory of your past. Would you still consider that person yourself? There is no question that our memories/experiences influence our behavior in every possible way. If you were born in a different environment with different stimuli you would've responded to your environment differently and became a different person. How different? I don't want to get involved in the nature/nurture debate but I think there is no question that humans are influenced by their environment. How are humans influenced by our environment? Through learning from our past experiences, which are contained in our memory. I'm getting off topic and I have no idea what my point is... So I propose a thought experiment!</p>\n<p>&nbsp;</p>\n<p><span style=\"white-space:pre\"> </span>Omega the supercomputer gives you 3 Options. Option 1 is for you to pay Omega $1,000,000,000 and Omega will grant you unlimited utility potential for 1 week in which Omega will basically provide to your every wish. You will have absolutely no memory of the experience after the week is up. Option 2 is for Omega to pay you $1,000,000,000 but you must be willing to suffer unlimited negative utility potential for a week (you will not be harmed physically or mentally you will simply experience excruciating pain). You will also have absolutely memory of this experience after the week (your subconscious will also not be affected). Finally, Option 3 is simply to refuse Option 1 and 2 and maintain the status quo.</p>\n<p>&nbsp;</p>\n<p><span style=\"white-space:pre\"> </span>At first glance, it may seem that Option 2 is simply not choosable. It seems insane to subject yourself to torture when you have the option of nirvana. But it requires more thought than that. If you compare Option 1 to Option 2 after the week is up there is no difference between the options except that Option 2 nets you 2 billion dollars compared to Option 1. In both Options you have absolutely no memory of either weeks. The question that I'm trying to put forward in this thought experiment is this. If you have no memory of an experience does that experience still matter? Is it worth experiencing something for the experience alone or is it the memory of an experience that matters? Those are some questions that I have been thinking about lately. Any feedback or criticism is appreciated.</p>\n<p>One last thing, if you are interested in the concept and importance of memory two excellent movies on the subject are [Memento](http://www.imdb.com/title/tt0209144/) and [Eternal Sunshine of the Spotless Mind](http://www.imdb.com/title/tt0338013/0). I know they both of these movies aren't scientific but I thought them to be very intriguing and thought provoking. &nbsp; &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iihJFC4r5oGW43b6F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": -5, "extendedScore": null, "score": -4e-06, "legacy": true, "legacyId": "26974", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-22T15:25:34.102Z", "modifiedAt": null, "url": null, "title": "Conservation of Expected Jury Probability", "slug": "conservation-of-expected-jury-probability", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:03.091Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nYAS4oPpkzTwghwei/conservation-of-expected-jury-probability", "pageUrlRelative": "/posts/nYAS4oPpkzTwghwei/conservation-of-expected-jury-probability", "linkUrl": "https://www.lesswrong.com/posts/nYAS4oPpkzTwghwei/conservation-of-expected-jury-probability", "postedAtFormatted": "Friday, August 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Conservation%20of%20Expected%20Jury%20Probability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConservation%20of%20Expected%20Jury%20Probability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnYAS4oPpkzTwghwei%2Fconservation-of-expected-jury-probability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Conservation%20of%20Expected%20Jury%20Probability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnYAS4oPpkzTwghwei%2Fconservation-of-expected-jury-probability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnYAS4oPpkzTwghwei%2Fconservation-of-expected-jury-probability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 344, "htmlBody": "<p>The New York Times has a <a href=\"http://www.nytimes.com/interactive/2014/08/20/nyregion/jury-duty-quiz.html\">calculator</a> to explain how getting on a jury works.  They have a slider at the top indicating how likely each of the two lawyers think you are to side with them, and as you answer questions it moves around.  For example, if you select that your occupation is \"blue collar\" then it says \"more likely to side with plaintiff\" while \"white collar\" gives \"more likely to side with defendant\".  As you give it more information the pointer labeled \"you\" slides back and forth, representing the lawyers' ongoing revision of their estimates of you.  Let's see what this looks like.</p>\n<p><!-- dt {margin-top: 10px; margin-bottom: 5px;} --></p>\n<dl> <dt>Initial</dt> <dd><img src=\"http://www.jefftk.com/nyt-jury-duty-unknown.png\" border=\"1\" alt=\"\" /></dd> <dt>Selecting \"Over 30\"</dt> <dd><img src=\"http://www.jefftk.com/nyt-jury-duty-over-30.png\" border=\"1\" alt=\"\" /></dd> <dt>Selecting \"Under 30\"</dt> <dd><img src=\"http://www.jefftk.com/nyt-jury-duty-under-30.png\" border=\"1\" alt=\"\" /></dd> </dl>\n<p>For several other questions, however, the options aren't matched.  If your household income is under $50k then it will give you \"more likely to side with plaintiff\" while if it's over $50k then it will say \"no effect on either lawyer\".  This is not how <a href=\"/lw/ii/conservation_of_expected_evidence/\">conservation of expected evidence</a> works: if learning something pushes you in one direction, then learning its opposite has to push you in the other.</p>\n<p>Let's try this with some numbers.  Say people's leanings are:  \n<table style=\"margin: 20px\" border=\"1\" cellpadding=\"5\">\n<tbody>\n<tr>\n<th>income</th> <th>probability of siding with plaintiff</th> <th>probability of siding with defendant</th>\n</tr>\n<tr>\n<td>&gt;$50k</td>\n<td>50%</td>\n<td>50%</td>\n</tr>\n<tr>\n<td>&lt;$50k</td>\n<td>70%</td>\n<td>30%</td>\n</tr>\n</tbody>\n</table>\nBefore asking you your income the lawyers' best guess is you're equally likely to be earning &gt;$50k as &lt;$50k because $50k's the median [1].  This means they'd guess you're 60% likely to side with the plaintiff: half the people in your position earn over &gt;$50k and will be approximately evenly split while the other half of people who could be in your position earn under &lt;$50k and would favor the plaintiff 70-30, and averaging these two cases gives us 60%.</p>\n<p>So the lawyers best guess for you is that you're at 60%, and then they ask the question.  If you say \"&gt;$50k\" then they update their estimate for you down to 50%, if you say \"&lt;$50k\" they update it up to 70%.  \"No effect on either lawyer\" can't be an option here unless the question gives no information.</p>\n<p><br /> [1]  Almost; the median income in the US in 2012 was      $51k. (<a href=\"http://www.census.gov/prod/2013pubs/acsbr12-02.pdf\">pdf</a>)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"tZsfB6WfpRy6kFb6q": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nYAS4oPpkzTwghwei", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 15, "extendedScore": null, "score": 1.946703571524181e-06, "legacy": true, "legacyId": "26992", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jiBFC7DcCrZjGmZnJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-22T15:38:29.362Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-36", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:02.874Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wohB3zidvHYfMhbyz/weekly-lw-meetups-36", "pageUrlRelative": "/posts/wohB3zidvHYfMhbyz/weekly-lw-meetups-36", "linkUrl": "https://www.lesswrong.com/posts/wohB3zidvHYfMhbyz/weekly-lw-meetups-36", "postedAtFormatted": "Friday, August 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwohB3zidvHYfMhbyz%2Fweekly-lw-meetups-36%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwohB3zidvHYfMhbyz%2Fweekly-lw-meetups-36", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwohB3zidvHYfMhbyz%2Fweekly-lw-meetups-36", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 543, "htmlBody": "<p><strong>This summary was posted to LW Main on August 15th. The following week's summary is <a href=\"/lw/ktt/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/135\">[Atlanta] MIRIxAtlanta - Decision Theory 2:&nbsp;<span class=\"date\">17 August 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/12t\">Bratislava:&nbsp;<span class=\"date\">18 August 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/12s\">Helsinki Book Blanket Meetup:&nbsp;<span class=\"date\">16 August 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/zr\">Houston, TX:&nbsp;<span class=\"date\">13 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/12r\">Perth, Australia: Sunday lunch:&nbsp;<span class=\"date\">17 August 2014 12:00PM</span></a></li>\n<li><a href=\"/meetups/138\">Perth, Australia: Games night:&nbsp;<span class=\"date\">02 September 2014 06:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">16 August 2025 01:30PM</span></a></li>\n<li><a href=\"/meetups/131\">[Cambridge MA] The Psychology of Video Games:&nbsp;<span class=\"date\">17 August 2014 03:30PM</span></a></li>\n<li><a href=\"/meetups/136\">Canberra: Cooking for LessWrongers:&nbsp;<span class=\"date\">22 August 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/137\">[Moscow] Regular Moscow Meetup:&nbsp;<span class=\"date\">17 August 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/12u\">Sydney Meetup - August:&nbsp;<span class=\"date\">27 August 2014 06:30PM</span></a></li>\n<li><a href=\"/meetups/130\">[Utrecht] Cognitive Biases:&nbsp;<span class=\"date\">23 August 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/132\">Washington, D.C.: Fun &amp; Games Meetup:&nbsp;<span class=\"date\">17 August 2014 03:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Buffalo.2C_NY\">Buffalo</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Moscow.2C_Russia\">Moscow</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Sydney\">Sydney</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tel_Aviv.2C_Israel\">Tel Aviv</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Warsaw.2C_Poland\">Warsaw</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wohB3zidvHYfMhbyz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.946725628450816e-06, "legacy": true, "legacyId": "26911", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aDeaxfLsdK8iwyNdQ", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-22T21:46:01.254Z", "modifiedAt": null, "url": null, "title": "[LINK] Physicist Carlo Rovelli on Modern Physics Research", "slug": "link-physicist-carlo-rovelli-on-modern-physics-research", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:04.336Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DnQ795GQqjLYhApzY/link-physicist-carlo-rovelli-on-modern-physics-research", "pageUrlRelative": "/posts/DnQ795GQqjLYhApzY/link-physicist-carlo-rovelli-on-modern-physics-research", "linkUrl": "https://www.lesswrong.com/posts/DnQ795GQqjLYhApzY/link-physicist-carlo-rovelli-on-modern-physics-research", "postedAtFormatted": "Friday, August 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Physicist%20Carlo%20Rovelli%20on%20Modern%20Physics%20Research&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Physicist%20Carlo%20Rovelli%20on%20Modern%20Physics%20Research%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDnQ795GQqjLYhApzY%2Flink-physicist-carlo-rovelli-on-modern-physics-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Physicist%20Carlo%20Rovelli%20on%20Modern%20Physics%20Research%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDnQ795GQqjLYhApzY%2Flink-physicist-carlo-rovelli-on-modern-physics-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDnQ795GQqjLYhApzY%2Flink-physicist-carlo-rovelli-on-modern-physics-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 381, "htmlBody": "<p><a href=\"http://blogs.scientificamerican.com/cross-check/2014/08/21/quantum-gravity-expert-says-philosophical-superficiality-has-harmed-physics/\">A blog post in Scientific American</a>, well worth reading. Rovelli is a researcher in Loop Quantum Gravity.</p>\n<p>Some quotes:</p>\n<p style=\"margin: 0px 0px 25px; padding: 0px; border: 0px; outline: 0px; font-size: 16px; vertical-align: baseline; line-height: 24px; color: #222222; font-family: Georgia, ApresTT, Prelude, Verdana, san-serif; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background: transparent;\">Horgan</strong>: Do multiverse theories and quantum gravity theories deserve to be taken seriously if they cannot be falsified?</p>\n<p style=\"margin: 0px 0px 25px; padding: 0px; border: 0px; outline: 0px; font-size: 16px; vertical-align: baseline; line-height: 24px; color: #222222; font-family: Georgia, ApresTT, Prelude, Verdana, san-serif; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background: transparent;\">Rovelli</strong>: No.</p>\n<p style=\"margin: 0px 0px 25px; padding: 0px; border: 0px; outline: 0px; font-size: 16px; vertical-align: baseline; line-height: 24px; color: #222222; font-family: Georgia, ApresTT, Prelude, Verdana, san-serif; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background: transparent;\">Horgan</strong>: What&rsquo;s your opinion of the recent philosophy-bashing by Stephen Hawking, Lawrence Krauss and Neil deGrasse Tyson?</p>\n<p style=\"margin: 0px 0px 25px; padding: 0px; border: 0px; outline: 0px; font-size: 16px; vertical-align: baseline; line-height: 24px; color: #222222; font-family: Georgia, ApresTT, Prelude, Verdana, san-serif; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background: transparent;\">Rovelli</strong>: Seriously: I think they are stupid in this. &nbsp; I have admiration for them in other things, but here they have gone really wrong. &nbsp;Look: Einstein, Heisenberg, Newton, Bohr&hellip;. and many many others of the greatest scientists of all times, much greater than the names you mention, of course, read philosophy, learned from philosophy, and could have never done the great science they did without the input they got from philosophy, as they claimed repeatedly.&nbsp; You see: the scientists that talk philosophy down are simply superficial: they have a philosophy (usually some ill-digested mixture of Popper and Kuhn) and think that this is the &ldquo;true&rdquo; philosophy, and do not realize that this has limitations.</p>\n<p style=\"margin: 0px 0px 25px; padding: 0px; border: 0px; outline: 0px; font-size: 16px; vertical-align: baseline; line-height: 24px; color: #222222; font-family: Georgia, ApresTT, Prelude, Verdana, san-serif; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background: transparent;\">Horgan</strong>: Can science attain absolute truth?</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p style=\"margin: 0px 0px 25px; padding: 0px; border: 0px; outline: 0px; font-size: 16px; vertical-align: baseline; line-height: 24px; color: #222222; font-family: Georgia, ApresTT, Prelude, Verdana, san-serif; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background: transparent;\">Rovelli</strong>: I have no idea what &ldquo;absolute truth&rdquo; means. I think that science is the attitude of those who find funny the people saying they know something is absolute truth. &nbsp;Science is the awareness that our knowledge is constantly uncertain. &nbsp;What I know is that there are plenty of things that science does not understand yet. And science is the best tool found so far for reaching reasonably reliable knowledge.</p>\n<p style=\"margin: 0px 0px 25px; padding: 0px; border: 0px; outline: 0px; font-size: 16px; vertical-align: baseline; line-height: 24px; color: #222222; font-family: Georgia, ApresTT, Prelude, Verdana, san-serif; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background: transparent;\">Horgan</strong>: Do you believe in God?</p>\n<p style=\"margin: 0px 0px 25px; padding: 0px; border: 0px; outline: 0px; font-size: 16px; vertical-align: baseline; line-height: 24px; color: #222222; font-family: Georgia, ApresTT, Prelude, Verdana, san-serif; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background: transparent;\">Rovelli</strong>: No.&nbsp; But perhaps I should qualify the answer, because like this it is bit too rude and simplistic. I do not understand what &ldquo;to believe in God&rdquo; means. The people that &ldquo;believe in God&rdquo; seem like Martians to me. &nbsp;I do not understand them. &nbsp;I suppose this means that I &ldquo;do not believe in God&rdquo;.&nbsp;If the question is whether I think that there is a person who has created Heavens and Earth, and responds to our prayers, then definitely my answer is no, with much certainty.</p>\n<p style=\"margin: 0px 0px 25px; padding: 0px; border: 0px; outline: 0px; font-size: 16px; vertical-align: baseline; line-height: 24px; color: #222222; font-family: Georgia, ApresTT, Prelude, Verdana, san-serif; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background: transparent;\">Horgan</strong>: Are science and religion compatible?</p>\n<p style=\"margin: 0px 0px 25px; padding: 0px; border: 0px; outline: 0px; font-size: 16px; vertical-align: baseline; line-height: 24px; color: #222222; font-family: Georgia, ApresTT, Prelude, Verdana, san-serif; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background: transparent;\">Rovelli</strong>: Of course yes: you can be great in solving Maxwell&rsquo;s equations and pray to God in the evening.&nbsp; But there is an unavoidable clash between science and certain religions, especially some forms of Christianity and Islam, those that pretend to be repositories of &ldquo;absolute Truths.&rdquo;</p>\n<p style=\"margin: 0px 0px 25px; padding: 0px; border: 0px; outline: 0px; font-size: 16px; vertical-align: baseline; line-height: 24px; color: #222222; font-family: Georgia, ApresTT, Prelude, Verdana, san-serif; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DnQ795GQqjLYhApzY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 8, "extendedScore": null, "score": 1.947353213727591e-06, "legacy": true, "legacyId": "26994", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-22T22:23:34.215Z", "modifiedAt": null, "url": null, "title": "Study: In giving charity, let not your right hand...", "slug": "study-in-giving-charity-let-not-your-right-hand", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:03.111Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "homunq", "createdAt": "2009-05-08T21:56:32.475Z", "isAdmin": false, "displayName": "homunq"}, "userId": "FZoKpxRbKw5g32eiY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GTjCfH7BcEYh9zXMs/study-in-giving-charity-let-not-your-right-hand", "pageUrlRelative": "/posts/GTjCfH7BcEYh9zXMs/study-in-giving-charity-let-not-your-right-hand", "linkUrl": "https://www.lesswrong.com/posts/GTjCfH7BcEYh9zXMs/study-in-giving-charity-let-not-your-right-hand", "postedAtFormatted": "Friday, August 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Study%3A%20In%20giving%20charity%2C%20let%20not%20your%20right%20hand...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStudy%3A%20In%20giving%20charity%2C%20let%20not%20your%20right%20hand...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGTjCfH7BcEYh9zXMs%2Fstudy-in-giving-charity-let-not-your-right-hand%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Study%3A%20In%20giving%20charity%2C%20let%20not%20your%20right%20hand...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGTjCfH7BcEYh9zXMs%2Fstudy-in-giving-charity-let-not-your-right-hand", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGTjCfH7BcEYh9zXMs%2Fstudy-in-giving-charity-let-not-your-right-hand", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 252, "htmlBody": "<p>So, <a href=\"http://web.missouri.edu/~segerti/capstone/Slacktivism.pdf\">here's the study</a>&sup1;:</p>\n<p>It's veterans' day in Canada. As any good Canadian knows, you're supposed to wear a poppy to show you support the veterans (it has something to do with Flanders Field). As people enter a concourse on the university, a person there does one of three things: gives them a poppy to wear on their clothes; gives them an envelope to carry and tells them (truthfully) that there's a poppy inside; or gives them nothing. Then, after they've crossed the concourse, another person asks them if they want to put donations in a box to support Canadian war veterans.</p>\n<p>Who do you think gives the most?</p>\n<p>...</p>\n<p>If you guessed that it's the people who got the poppy inside the envelope, you're right. 78% of them gave, for an overall average donation of $0.86. That compares to 58% of the people wearing the poppy, for an average donation of $0.34; and 56% of those with no poppy, for an average of $0.15.</p>\n<p>Why did the envelope holders give the most? Unlike the no-poppy group, they had been reminded of the expectation of supporting veterans; but unlike the poppy-wearers, they hadn't been given an easy, cost-free means of demonstrating their support.</p>\n<p>I think this research has obvious applications, both to fundraising and to self-hacking. It also validates the bible quote (Matthew 6:3) which is the title of this article.</p>\n<p>&sup1;<span style=\"font-family: Arial, sans-serif; color: #6611cc;\"><span style=\"font-size: 11.818181991577148px;\"> </span></span>The Nature of Slacktivism: How the Social Observability of an Initial Act of Token Support Affects Subsequent Prosocial Action;&nbsp;K&nbsp;Kristofferson,&nbsp;K White,&nbsp;J Peloza&nbsp;- Journal of Consumer Research, 2014</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GTjCfH7BcEYh9zXMs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 1.94741735191806e-06, "legacy": true, "legacyId": "26995", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-23T04:19:55.882Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Reconstituting", "slug": "meetup-urbana-champaign-reconstituting", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C359ArrQCLafGnCNR/meetup-urbana-champaign-reconstituting", "pageUrlRelative": "/posts/C359ArrQCLafGnCNR/meetup-urbana-champaign-reconstituting", "linkUrl": "https://www.lesswrong.com/posts/C359ArrQCLafGnCNR/meetup-urbana-champaign-reconstituting", "postedAtFormatted": "Saturday, August 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Reconstituting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Reconstituting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC359ArrQCLafGnCNR%2Fmeetup-urbana-champaign-reconstituting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Reconstituting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC359ArrQCLafGnCNR%2Fmeetup-urbana-champaign-reconstituting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC359ArrQCLafGnCNR%2Fmeetup-urbana-champaign-reconstituting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13j'>Urbana-Champaign: Reconstituting</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 August 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">206 S. Cedar St, 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In the puddles and pools formed by the fall rains, meetup groups that seemed dead end their arid hibernation and emerge from the earth.</p>\n\n<p>Come join us for the first meetup of the new school year. Board games and light discussion are likely. A starter topic: increasing exposure to positive black swans.</p>\n\n<p><a href=\"https://groups.google.com/forum/#!forum/lesswrong-urbana-champaign\">Google group</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13j'>Urbana-Champaign: Reconstituting</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C359ArrQCLafGnCNR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.9480262403054665e-06, "legacy": true, "legacyId": "26998", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Reconstituting\">Discussion article for the meetup : <a href=\"/meetups/13j\">Urbana-Champaign: Reconstituting</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 August 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">206 S. Cedar St, 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In the puddles and pools formed by the fall rains, meetup groups that seemed dead end their arid hibernation and emerge from the earth.</p>\n\n<p>Come join us for the first meetup of the new school year. Board games and light discussion are likely. A starter topic: increasing exposure to positive black swans.</p>\n\n<p><a href=\"https://groups.google.com/forum/#!forum/lesswrong-urbana-champaign\">Google group</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Reconstituting1\">Discussion article for the meetup : <a href=\"/meetups/13j\">Urbana-Champaign: Reconstituting</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Reconstituting", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Reconstituting", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Reconstituting", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Reconstituting1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-23T20:14:20.279Z", "modifiedAt": null, "url": null, "title": "[Link] Feynman lectures on physics", "slug": "link-feynman-lectures-on-physics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:05.495Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "vpeitoi89GPGat77P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vSEeWCfyidFufcTAs/link-feynman-lectures-on-physics", "pageUrlRelative": "/posts/vSEeWCfyidFufcTAs/link-feynman-lectures-on-physics", "linkUrl": "https://www.lesswrong.com/posts/vSEeWCfyidFufcTAs/link-feynman-lectures-on-physics", "postedAtFormatted": "Saturday, August 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Feynman%20lectures%20on%20physics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Feynman%20lectures%20on%20physics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvSEeWCfyidFufcTAs%2Flink-feynman-lectures-on-physics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Feynman%20lectures%20on%20physics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvSEeWCfyidFufcTAs%2Flink-feynman-lectures-on-physics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvSEeWCfyidFufcTAs%2Flink-feynman-lectures-on-physics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 37, "htmlBody": "<p>The Feynman lectures on physics <a href=\"http://www.feynmanlectures.caltech.edu/\">are now available</a> to read online for free. This is a classic resource for not just learning physics also but also the process of science and the mindset of a scientific rationalist.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vSEeWCfyidFufcTAs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 17, "extendedScore": null, "score": 1.949658625924923e-06, "legacy": true, "legacyId": "27000", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-24T04:19:58.661Z", "modifiedAt": null, "url": null, "title": "Meetup : Any one interested in Milwaukee meetup?", "slug": "meetup-any-one-interested-in-milwaukee-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "zerotimer", "createdAt": "2013-10-12T20:14:24.421Z", "isAdmin": false, "displayName": "zerotimer"}, "userId": "DBngec8BYGxh3oNBB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9mxf2x5ayHpm9Yzb3/meetup-any-one-interested-in-milwaukee-meetup", "pageUrlRelative": "/posts/9mxf2x5ayHpm9Yzb3/meetup-any-one-interested-in-milwaukee-meetup", "linkUrl": "https://www.lesswrong.com/posts/9mxf2x5ayHpm9Yzb3/meetup-any-one-interested-in-milwaukee-meetup", "postedAtFormatted": "Sunday, August 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Any%20one%20interested%20in%20Milwaukee%20meetup%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Any%20one%20interested%20in%20Milwaukee%20meetup%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9mxf2x5ayHpm9Yzb3%2Fmeetup-any-one-interested-in-milwaukee-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Any%20one%20interested%20in%20Milwaukee%20meetup%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9mxf2x5ayHpm9Yzb3%2Fmeetup-any-one-interested-in-milwaukee-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9mxf2x5ayHpm9Yzb3%2Fmeetup-any-one-interested-in-milwaukee-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13k'>Any one interested in Milwaukee meetup?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 September 2014 05:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rock Bottom, 740 N Plankinton Ave, Milwaukee, WI 53203</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We can discuss the options for meetup place and date but is anyone here interested in milwaukee meetup. I dont think it has happened before.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13k'>Any one interested in Milwaukee meetup?</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9mxf2x5ayHpm9Yzb3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.950490177350603e-06, "legacy": true, "legacyId": "27001", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Any_one_interested_in_Milwaukee_meetup_\">Discussion article for the meetup : <a href=\"/meetups/13k\">Any one interested in Milwaukee meetup?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 September 2014 05:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rock Bottom, 740 N Plankinton Ave, Milwaukee, WI 53203</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We can discuss the options for meetup place and date but is anyone here interested in milwaukee meetup. I dont think it has happened before.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Any_one_interested_in_Milwaukee_meetup_1\">Discussion article for the meetup : <a href=\"/meetups/13k\">Any one interested in Milwaukee meetup?</a></h2>", "sections": [{"title": "Discussion article for the meetup : Any one interested in Milwaukee meetup?", "anchor": "Discussion_article_for_the_meetup___Any_one_interested_in_Milwaukee_meetup_", "level": 1}, {"title": "Discussion article for the meetup : Any one interested in Milwaukee meetup?", "anchor": "Discussion_article_for_the_meetup___Any_one_interested_in_Milwaukee_meetup_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-24T08:07:31.980Z", "modifiedAt": null, "url": null, "title": "Announcing The Effective Altruism Forum", "slug": "announcing-the-effective-altruism-forum", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:04.293Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RyanCarey", "createdAt": "2011-04-27T00:19:14.586Z", "isAdmin": false, "displayName": "RyanCarey"}, "userId": "CBkbKSCEzEK2kLQww", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iD742Sb8bxnbGgM2D/announcing-the-effective-altruism-forum", "pageUrlRelative": "/posts/iD742Sb8bxnbGgM2D/announcing-the-effective-altruism-forum", "linkUrl": "https://www.lesswrong.com/posts/iD742Sb8bxnbGgM2D/announcing-the-effective-altruism-forum", "postedAtFormatted": "Sunday, August 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Announcing%20The%20Effective%20Altruism%20Forum&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnnouncing%20The%20Effective%20Altruism%20Forum%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiD742Sb8bxnbGgM2D%2Fannouncing-the-effective-altruism-forum%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Announcing%20The%20Effective%20Altruism%20Forum%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiD742Sb8bxnbGgM2D%2Fannouncing-the-effective-altruism-forum", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiD742Sb8bxnbGgM2D%2Fannouncing-the-effective-altruism-forum", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 450, "htmlBody": "<p>The&nbsp;Effective Altruism Forum&nbsp;will be launched at effective-altruism.com on September 10, British time.</p>\n<p>Now seems like a good time time to discuss why we might need an&nbsp;Effective Altruism Forum, and how it might compare to LessWrong.</p>\n<p><strong>About the Effective Altruism Forum</strong></p>\n<p>The motivation for the Effective Altruism Forum is to improve the quality of effective altruist discussion and coordination. A big part of this is to give many of the useful features of LessWrong to effective altruists, including:</p>\n<p>&nbsp;</p>\n<ul>\n<li>Archived, searchable content (this will begin with archived content from effective-altruism.com)</li>\n<li>Meetups</li>\n<li>Nested comments</li>\n<li>A karma system</li>\n<li>A dynamically upated list of external effective altruist blogs</li>\n<li>Introductory materials (this will begin with <a href=\"/effectivealtruism.org/resources/#reading\">these</a> articles)</li>\n</ul>\n<p>&nbsp;</p>\n<p>The&nbsp;Effective Altruism Forum&nbsp;has been designed by Mihai Badic. Over the last month, it has been developed by Trike Apps, who have built the new site using the LessWrong codebase. I'm glad to report that it is now basically ready, looks nice, and is easy to use.</p>\n<p>I expect that at the new forum, as on the effective altruist <a href=\"https://www.facebook.com/groups/effective.altruists/\">Facebook</a> and <a href=\"http://www.reddit.com/r/smartgiving\">Reddit</a> pages, people will want to discuss the which intellectual procedures to use to pick effective actions. I also expect some proposals of effective altruist projects, and offers of resources. So users of the new forum will share LessWrong's interest in instrumental and epistemic rationality. On the other hand, I expect that few of its users will want to discuss the technical aspects of artificial intelligence, anthropics or decision theory, and to the extent that they do so, they will want to do it at LessWrong. As a result, I &nbsp;expect the new forum to cause:</p>\n<p>&nbsp;</p>\n<ul>\n<li>A bunch of materials on effective altruism and instrumental rationality to be collated for new effective altruists</li>\n<li>Discussion of old LessWrong materials to resurface</li>\n<li>A slight increase to the number of users of LessWrong, possibly offset by some users spending more of their time posting at the new forum.</li>\n</ul>\n<p>&nbsp;</p>\n<p>At least initially, the new forum won't have a wiki or a Main/Discussion split and won't have any institutional affiliations.</p>\n<p><strong>Next Steps:</strong></p>\n<p>It's really important to make sure that the&nbsp;Effective Altruism Forum&nbsp;is established with a beneficial culture. If people want to help that process by writing some seed materials, to be posted around the time of the site's launch, then they can contact me at ry [dot] duff [at] gmail.com. Alternatively, they can wait a short while until they automatically receive posting priveleges.</p>\n<p>It's also important that the Effective Altruism Forum helps the shared goals of rationalists and effective altruists, and has net positive effects on LessWrong in particular. Any suggestions for improving the odds of success for the effective altruism forum are most welcome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iD742Sb8bxnbGgM2D", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 44, "extendedScore": null, "score": 1.9508800317794367e-06, "legacy": true, "legacyId": "27002", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The&nbsp;Effective Altruism Forum&nbsp;will be launched at effective-altruism.com on September 10, British time.</p>\n<p>Now seems like a good time time to discuss why we might need an&nbsp;Effective Altruism Forum, and how it might compare to LessWrong.</p>\n<p><strong id=\"About_the_Effective_Altruism_Forum\">About the Effective Altruism Forum</strong></p>\n<p>The motivation for the Effective Altruism Forum is to improve the quality of effective altruist discussion and coordination. A big part of this is to give many of the useful features of LessWrong to effective altruists, including:</p>\n<p>&nbsp;</p>\n<ul>\n<li>Archived, searchable content (this will begin with archived content from effective-altruism.com)</li>\n<li>Meetups</li>\n<li>Nested comments</li>\n<li>A karma system</li>\n<li>A dynamically upated list of external effective altruist blogs</li>\n<li>Introductory materials (this will begin with <a href=\"/effectivealtruism.org/resources/#reading\">these</a> articles)</li>\n</ul>\n<p>&nbsp;</p>\n<p>The&nbsp;Effective Altruism Forum&nbsp;has been designed by Mihai Badic. Over the last month, it has been developed by Trike Apps, who have built the new site using the LessWrong codebase. I'm glad to report that it is now basically ready, looks nice, and is easy to use.</p>\n<p>I expect that at the new forum, as on the effective altruist <a href=\"https://www.facebook.com/groups/effective.altruists/\">Facebook</a> and <a href=\"http://www.reddit.com/r/smartgiving\">Reddit</a> pages, people will want to discuss the which intellectual procedures to use to pick effective actions. I also expect some proposals of effective altruist projects, and offers of resources. So users of the new forum will share LessWrong's interest in instrumental and epistemic rationality. On the other hand, I expect that few of its users will want to discuss the technical aspects of artificial intelligence, anthropics or decision theory, and to the extent that they do so, they will want to do it at LessWrong. As a result, I &nbsp;expect the new forum to cause:</p>\n<p>&nbsp;</p>\n<ul>\n<li>A bunch of materials on effective altruism and instrumental rationality to be collated for new effective altruists</li>\n<li>Discussion of old LessWrong materials to resurface</li>\n<li>A slight increase to the number of users of LessWrong, possibly offset by some users spending more of their time posting at the new forum.</li>\n</ul>\n<p>&nbsp;</p>\n<p>At least initially, the new forum won't have a wiki or a Main/Discussion split and won't have any institutional affiliations.</p>\n<p><strong id=\"Next_Steps_\">Next Steps:</strong></p>\n<p>It's really important to make sure that the&nbsp;Effective Altruism Forum&nbsp;is established with a beneficial culture. If people want to help that process by writing some seed materials, to be posted around the time of the site's launch, then they can contact me at ry [dot] duff [at] gmail.com. Alternatively, they can wait a short while until they automatically receive posting priveleges.</p>\n<p>It's also important that the Effective Altruism Forum helps the shared goals of rationalists and effective altruists, and has net positive effects on LessWrong in particular. Any suggestions for improving the odds of success for the effective altruism forum are most welcome.</p>", "sections": [{"title": "About the Effective Altruism Forum", "anchor": "About_the_Effective_Altruism_Forum", "level": 1}, {"title": "Next Steps:", "anchor": "Next_Steps_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-25T11:14:52.992Z", "modifiedAt": null, "url": null, "title": "Open thread, 25-31 August 2014", "slug": "open-thread-25-31-august-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:09.634Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jaime2000", "createdAt": "2013-07-29T01:47:35.714Z", "isAdmin": false, "displayName": "jaime2000"}, "userId": "z4AsqwchcTtf37Xj8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZbApyaLw4PeamCtEF/open-thread-25-31-august-2014", "pageUrlRelative": "/posts/ZbApyaLw4PeamCtEF/open-thread-25-31-august-2014", "linkUrl": "https://www.lesswrong.com/posts/ZbApyaLw4PeamCtEF/open-thread-25-31-august-2014", "postedAtFormatted": "Monday, August 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%2025-31%20August%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%2025-31%20August%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZbApyaLw4PeamCtEF%2Fopen-thread-25-31-august-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%2025-31%20August%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZbApyaLw4PeamCtEF%2Fopen-thread-25-31-august-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZbApyaLw4PeamCtEF%2Fopen-thread-25-31-august-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/ksc/open_thread_1824_august_2014/\">Previous open thread</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px; font-weight: bold;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px; font-weight: bold;\"><br /></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">3.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should be posted in Discussion, and not Main.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">4.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should start on Monday, and end on Sunday.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZbApyaLw4PeamCtEF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.953672078848224e-06, "legacy": true, "legacyId": "27016", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 229, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["r2tPvypDm6Da6unPe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-26T01:38:20.167Z", "modifiedAt": null, "url": null, "title": "Persistent Idealism", "slug": "persistent-idealism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:07.500Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FWZSGzsyvXijNBtG5/persistent-idealism", "pageUrlRelative": "/posts/FWZSGzsyvXijNBtG5/persistent-idealism", "linkUrl": "https://www.lesswrong.com/posts/FWZSGzsyvXijNBtG5/persistent-idealism", "postedAtFormatted": "Tuesday, August 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Persistent%20Idealism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APersistent%20Idealism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFWZSGzsyvXijNBtG5%2Fpersistent-idealism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Persistent%20Idealism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFWZSGzsyvXijNBtG5%2Fpersistent-idealism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFWZSGzsyvXijNBtG5%2Fpersistent-idealism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 380, "htmlBody": "<p>When I talk to people about <a href=\"http://www.washingtonpost.com/blogs/wonkblog/wp/2013/05/31/join-wall-street-save-the-world/\">earning to give</a>, it's common to hear worries about \"backsliding\".  Yes, you say you're going to go make a lot of money and donate it, but once you're surrounded by rich coworkers spending heavily on cars, clothes, and nights out, will you follow through?  Working at a <a href=\"https://www.facebook.com/groups/effective.altruists/permalink/727166167339743/?comment_id=733509863372040\">greedy company in a selfishness-promoting culture you could easily become corrupted</a> and lose initial values and motivation.</p>\n<p>First off, this is a totally reasonable concern.  People do change, and we are pulled towards thinking like the people around us.  I see two main ways of working against this:</p>\n<ol>\n<li>Be <a href=\"http://www.jefftk.com/p/make-your-giving-public\">public       with your giving</a>.  Make <a href=\"http://www.jefftk.com/p/giving-half\">visible</a> <a href=\"http://www.jefftk.com/p/joining-giving-what-we-can\">commitments</a> and then <a href=\"http://www.jefftk.com/donations\">list your       donations</a>.  This means that you can't slowly slip away from       giving; either you publish updates saying you're not going to do       what you said you would, or you just stop updating and your       pages become stale.  By making a public promise you've given       friends permission to notice that you've stopped and ask \"what       changed?\" </li>\n<li> Don't just surround yourself with coworkers.  Keep in touch with       friends and family.  Spend some time with other people in the       effective altruism movement.  You could throw yourself entirely       into your work, maximizing income while sending occasional       substantial checks to GiveWell's top picks, but without some       ongoing engagement with the community and the research this       doesn't seem likely to last. </li>\n</ol>\n<p>One implication of the \"won't you drift away\" objection, however, is often that if instead of going into earning to give you become an activist then you'll remain true to your values.  I'm not so sure about this: many people who are really into activism and radical change in their 20s have become much less ambitious and idealistic by their 30s.  You can call it \"burning out\" or \"selling out\" but decreasing idealism with age is very common.  This doesn't mean people earning to give don't have to worry about losing their motivation&mdash;in fact it points the opposite way&mdash;but this isn't a danger unique to the \"go work at something lucrative\" approach.  Trying honestly to do the most good possible is far from the default in our society, and wherever you are there's going to be pressure to do the easy thing, the normal thing, and stop putting so much effort into altruism.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FWZSGzsyvXijNBtG5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 19, "extendedScore": null, "score": 1.9551563785388293e-06, "legacy": true, "legacyId": "27020", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-26T02:47:16.924Z", "modifiedAt": null, "url": null, "title": "The immediate real-world uses of Friendly AI research", "slug": "the-immediate-real-world-uses-of-friendly-ai-research", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:04.545Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ancientcampus", "createdAt": "2011-11-09T20:20:29.779Z", "isAdmin": false, "displayName": "ancientcampus"}, "userId": "BGcNEeTWb7Qxcf5Pc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NT3xSsFTsQdvCahJT/the-immediate-real-world-uses-of-friendly-ai-research", "pageUrlRelative": "/posts/NT3xSsFTsQdvCahJT/the-immediate-real-world-uses-of-friendly-ai-research", "linkUrl": "https://www.lesswrong.com/posts/NT3xSsFTsQdvCahJT/the-immediate-real-world-uses-of-friendly-ai-research", "postedAtFormatted": "Tuesday, August 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20immediate%20real-world%20uses%20of%20Friendly%20AI%20research&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20immediate%20real-world%20uses%20of%20Friendly%20AI%20research%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNT3xSsFTsQdvCahJT%2Fthe-immediate-real-world-uses-of-friendly-ai-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20immediate%20real-world%20uses%20of%20Friendly%20AI%20research%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNT3xSsFTsQdvCahJT%2Fthe-immediate-real-world-uses-of-friendly-ai-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNT3xSsFTsQdvCahJT%2Fthe-immediate-real-world-uses-of-friendly-ai-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 481, "htmlBody": "<p style=\"margin-bottom: 0in;\">Much of the glamor and attention paid toward Friendly AI is focused on the misty-future event of a super-intelligent general AI, and how we can prevent it from repurposing our atoms to better run Quake 2. Until very recently, that was the full breadth of the field in my mind. I recently realized that dumber, narrow AI is a real thing today, helpfully choosing advertisements for me and running my 401K. As such, making automated programs safe to let loose on the real world is not just a problem to solve as a favor for the people of tomorrow, but something with immediate real-world advantages that has indeed already been going on for quite some time. Veterans in the field surely already understand this, so this post is directed at people like me, with a passing and disinterested understanding of the point of Friendly AI research, and outlines an argument that the field may be useful right now, even if you believe that an evil AI overlord is not on the list of things to worry about in the next 40 years.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Let's look at the stock market. High-Frequency Trading is the practice of using computer programs to make fast trades constantly throughout the day, and accounts for more than half of all equity trades in the US. So, the economy today is already in the hands of a bunch of very narrow AIs buying and selling to each other. And as you may or may not already know, this has already caused problems. In the &ldquo;2010 Flash Crash&rdquo;, the Dow Jones suddenly and mysteriously hit a massive plummet only to mostly recover within a few minutes. The reasons for this were of course complicated, but it boiled down to a couple red flags triggering in numerous programs, setting off a cascade of wacky trades.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The long-term damage was not catastrophic to society at large (though I'm sure a couple fortunes were made and lost that day), but it illustrates the need for safety measures as we hand over more and more responsibility and power to processes that require little human input. It might be a blue moon before anyone makes true general AI, but adaptive city traffic-light systems are entirely plausible in upcoming years.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">To me, Friendly AI isn't solely about making a human-like intelligence that doesn't hurt us &ndash; we need techniques for testing automated programs, predicting how they will act when let loose on the world, and how they'll act when faced with unpredictable situations. Indeed, when framed like that, it looks less like a field for &ldquo;the singularitarian cultists at LW&rdquo;, and more like a narrow-but-important specialty in which quite a bit of money might be made.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">After all, I want my self-driving car.</p>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><em>(To the actual researchers in FAI &ndash; I'm sorry if I'm stretching the field's definition to include more than it does or should. If so, please correct me.)</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NT3xSsFTsQdvCahJT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "27024", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-26T03:03:57.026Z", "modifiedAt": null, "url": null, "title": "Knightian uncertainty: a rejection of the MMEU rule", "slug": "knightian-uncertainty-a-rejection-of-the-mmeu-rule", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:05.499Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SEov2u5Y7mJTPaTLK/knightian-uncertainty-a-rejection-of-the-mmeu-rule", "pageUrlRelative": "/posts/SEov2u5Y7mJTPaTLK/knightian-uncertainty-a-rejection-of-the-mmeu-rule", "linkUrl": "https://www.lesswrong.com/posts/SEov2u5Y7mJTPaTLK/knightian-uncertainty-a-rejection-of-the-mmeu-rule", "postedAtFormatted": "Tuesday, August 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Knightian%20uncertainty%3A%20a%20rejection%20of%20the%20MMEU%20rule&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKnightian%20uncertainty%3A%20a%20rejection%20of%20the%20MMEU%20rule%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEov2u5Y7mJTPaTLK%2Fknightian-uncertainty-a-rejection-of-the-mmeu-rule%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Knightian%20uncertainty%3A%20a%20rejection%20of%20the%20MMEU%20rule%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEov2u5Y7mJTPaTLK%2Fknightian-uncertainty-a-rejection-of-the-mmeu-rule", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEov2u5Y7mJTPaTLK%2Fknightian-uncertainty-a-rejection-of-the-mmeu-rule", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4481, "htmlBody": "<p>Recently, I found myself in a conversation with someone advocating the use of <a href=\"http://en.wikipedia.org/wiki/Knightian_uncertainty\">Knightian uncertainty</a>. He (who I'm anonymizing as Sir Percy) made suggestions that are useful to most bounded reasoners, and which can <a href=\"/lw/klg/knightian_uncertainty_in_a_bayesian_framework/\">be integrated into a Bayesian framework</a>. He also claimed preferences that depend upon his Knightian uncertainty and that he's not an expected utility maximizer. Further, he claimed that Bayesian reasoning cannot capture his preferences. Specifically, Sir Percy said he maximizes&nbsp;<em>minimum</em> expected utility given his Knightian uncertainty, using what I will refer to as the \"MMEU rule\" to make decisions.</p>\n<p>In my <a href=\"/lw/koj/knightian_uncertainty_bayesian_agents_and_the/\">previous post</a>, I showed that Bayesian expected utility maximizers <em>can</em> exhibit behavior in accordance with his preferences. Two such reasoners, Paranoid Perry and Cautious Caul, were explored. These hypothetical agents demonstrate that it is possible for Bayesians to be \"ambiguity averse\", e.g. to avoid certain types of uncertainty.</p>\n<p>But Perry and Caul are unnatural agents using strange priors. Is this because we are twisting the Bayesian framework to represent behavior it is ill-suited to emulate? Or does the strangeness of Perry and Caul merely reveal a strangeness in the MMEU rule?</p>\n<p>In this post, I'll argue the latter: maximization of minimum expected utility is not a good decision rule, for the same reason that Perry and Caul seem irrational. My rejection of the MMEU rule will follow from my rejections of Perry and Caul.</p>\n<p><a id=\"more\"></a></p>\n<h1>A rejection of Perry</h1>\n<p>I understand the appeal of Paranoid Perry, the agent that assumes ambiguity is resolved adversarially. Humans often keep the worst-case in mind, avoid big gambles, and make sure that they'll be OK even if everything goes wrong. Perry promises to capture some of this intuitively reasonable behavior.</p>\n<p>Unfortunately, this promise is not kept. From the description of the MMEU rule, you might think that Perry is forgoing high utility in the average case to ensure moderate utility in the worst case. But this is not the case: Perry willingly takes huge gambles, so long as those gambles are resolved by \"normal\" uncertainty rather than \"adversarial\" uncertainty.</p>\n<p>Allow me to reiterate: <em>Maximizing minimum expected utility does not ensure that you do well in the worst case.</em> It merely selects a <em>single type</em> of uncertainty against which to play defensively, and then gambles against the rest. To illustrate, consider the following Game of Drawers:</p>\n<p><em>There are two boxes, Box 1 and Box 2. Each box has two Drawers, drawer A and drawer B. Each drawer contains a bet, as follows:</em></p>\n<pre><code>1A. 99% lose $1000, 1% gain $99300 (expectation: $3)\n1B. Gain 2$</code></pre>\n<pre><code>\n2A: 99% lose $1000, 1% gain $99500 (expectation: $5)\n2B. Gain $10\n</code></pre>\n<p><em>You face one of the boxes (you do not know which) and you must choose one of the drawers. Which do you choose?</em></p>\n<p>Imagine that&nbsp;you have \"ambiguity\" (\"Knightian uncertainty\") about which box you face, but that you believe the gambles inside the boxes are fair: this is a setup analogous to the <a href=\"http://en.wikipedia.org/wiki/Ellsberg_paradox\">Ellsberg urn game</a>, except that it gives the opposite intuition.</p>\n<p>In the Game of Drawers, I expect most people would choose drawer B (and win either $2 or $10). However, Paranoid Perry (and Cautious Caul, and Sir Percy)&nbsp;would choose drawer A.</p>\n<p><em>In the Game of Drawers, Perry acts such that 99% of the time it loses $1000.</em></p>\n<p>Wasn't Paranoid Perry supposed to reason so that it does well in the worst case? What went wrong?</p>\n<p>Paranoid Perry reasons that <em>nature</em> gets to pick which box it faces, and that nature will force Perry into the worse box. Box 1 is strictly worse than Box 2, so Perry expects to face Box 1. And drawer A in Box 1 has higher expected utility than drawer B in Box 1, so Perry takes drawer A, a gamble that loses Perry $1000 99% of the time!</p>\n<p>Perry has <em>no inclination</em> to avoid big gambles. Perry isn't making sure that the worst scenario is acceptable, it's is maximizing <em>expected</em> utility in the worst scenario <em>that nature can pick</em>. Within that least convenient world, Perry (and Caul, and Sir Percy) take standard Bayesian gambles.</p>\n<p>Bayesian gambles may seem reckless, but Perry is not the solution: Perry simply divides uncertainty into two classes, and treats one of them too defensively and the other just as recklessly as a Bayesian would. Two flaws don't make a feature.</p>\n<p>Perry <em>assumes</em>&nbsp;that some of its uncertainty gets resolved unfavorably by nature <em>regardless of whether or not it is</em>. In some domains this captures caution, yes. In others, it's just a silly waste. As soon as nature <em>actually</em> starts acting adversarial, Bayesians become cautious too<em>.</em> The difference is that Bayesians are not <em>forced</em> to act as if one arbitrary segment of my uncertainty is adversarially resolved.</p>\n<p>Perry believes&nbsp;&mdash;&nbsp;with absolute certainty, disregarding all evidence no matter how strong &mdash;&nbsp;that Nature never cuts any slack.</p>\n<h1>A rejection of Caul</h1>\n<p>I understand, too, the appeal of Cautious Caul. Caul reasons about multiple possible worldparts, and attempts to ensure that the Caul-sliver in the least convenient worldpart does well enough. Instead of insisting that convenient things <em>can't</em> happen (like Perry), Caul only <em>cares</em> about the inconvenient parts. Perhaps this better captures our intuition that people should be less reckless than Bayesians?</p>\n<p>Expected utility maximizers happily trade utility in one branch of the possibility space for proportional utility in another branch, regardless of which branch had higher utility in the first place. Some people have moral intuitions that say this is wrong, and that we should be unwilling to trade utility away from unfortunate branches and into fortunate branches.</p>\n<p>But this moral intuition is flawed, in a way that reveals confusion both about worst cases and about utility.</p>\n<p>There's a huge difference between what the average person consider to be a worst case scenario (e.g., losing the bet) and what a Bayesian considers to be the worst case scenario (e.g. physics has been lying to you and this is about to turn into the worst possible world). Or, to put things glibly, the human version of worst case is \"you lose the bet\", whereas the Bayesian version of worst case is \"the bet was a lie and now everybody will be tortured forever\".</p>\n<p>You can't maximize the actual worst case in any reasonably complex system. There are some small systems (say, software used to control trains) where people actually do worry about the&nbsp;<em>absolute </em>worst case, but upon inspection, these are consistent with expected utility maximization. A train crash is pretty costly.</p>\n<p>And, in fact, expected utility maximization can capture caution <em>in general</em>. We don't need Cautious Caul in order to act with appropriate caution in a Bayesian framework. Caution is not implemented by new decision rules, it is implemented in the conversion from money (or whatever) to utility. Allow me to illustrate:</p>\n<p>Suppose that the fates are about offer me a bet and then roll a thousand-sided die. The die seems fair (to me) and my information gives me a uniform probability distribution over values between 1 and 1000: my probability mass is about to split into a thousand shards of equal measure. Before the die is rolled, I am given a choice between two options:</p>\n<ol>\n<li>No matter what the die rolls, I get $100.</li>\n<li>If the die rolls a 1, I pay $898. Otherwise, I get $101.</li>\n</ol>\n<p>The second package results in more expected money (by $1), but I would choose the former. Why? Losing $898 is more bad than the extra dollars are good. I'm more than happy to burn one expected dollar in order to avoid the branch where I have to pay $898. In this case, I act cautious in the intuitive sense&nbsp;&mdash;<span style=\"font-family: arial, sans-serif; color: #545454;\"><span style=\"line-height: 18.200000762939453px;\">&nbsp;</span></span><em>and I do this as an expected utility maximizer</em>. How? Well, consider the following bet instead:</p>\n<ol>\n<li>No matter what the die rolls, I get 100 utility.</li>\n<li>If the die rolls a 1, I lose 898 utility. Otherwise, I gain 101 utility.</li>\n</ol>\n<p>Now I pick bet 2 in a heartbeat. What changed? Utils have <em>already factored in</em> everything that I care about.</p>\n<p>I am risk-neutral in utils. If I am loss-averse, then the fact that the losing version of me will experience a sharp pang of frustration and perhaps a lasting depression and lowered productivity has <em>already been factored in</em> to the calculation. It's not like I lose 898 utility and <em>then</em> feel bad about it: the bad feelings are included in the number. The fact that all the <em>other</em> versions of me (who get 101 utility) will feel a little sad and remorseful, and will feel a little frustrated because the world is unfair, has already been factored into <em>their</em> utility numbers: it's not like they see that they got 101 utility and <em>then</em> feel remorse. (Similarly, their relief and glee has already been rolled into the numbers too.)</p>\n<p>The intuition that we shouldn't trade utility from unfortunate branches mostly stems from a misunderstanding of utility. Utility <em>already</em>&nbsp;takes into account your egalitarianism, your preference curve for dollars, and so on. Once these things are accounted for, you <em>should </em>trade utility from unfortunate branches into fortunate branches: if this feels bad to you, then you haven't got your utility calculations quite right.</p>\n<p>Expected utility maximizers can be cautious. They can avoid ruinous bets. They can play it safe. But all of this behavior is encoded in the utility numbers: we don't need Cautious Caul to exhibit these preferences.</p>\n<h1>A rejection of the MMEU rule</h1>\n<p>Bets come with stigma. They are traditionally only offered to humans by other humans, and anyone offering a bet is usually either a con artist or a philosophy professor. \"Reject bets by default\" is a good heuristic for most people.</p>\n<p>Advocates of Bayesian reasoning talk about accepting bets without flinching, and that can seem strange. I think this comes down to a fundamental misunderstanding between colloquial bets and Bayesian bets.</p>\n<p>Colloquial bets are offered by skeevy con artists who probably know something you don't. Bayesian bets, on the other hand, arise whenever the agent must make a decision. \"Rejecting the bet\" is not an option: inaction is a choice. You have to weigh all available actions (including \"stall\" or \"gather more information\") and bet on which one will serve you best.</p>\n<p>This mismatch, I think, is responsible for quite a bit of most people's discomfort with Bayesian decisions. That said, Bayesians are <em>also</em> willing to make really big gambles, gambles which look crazy to most people (who are risk- and loss-averse). Bayesians claim that risk- and loss-aversion are biases that should be overcome, and that we should [shut up and multiply](http://wiki.lesswrong.com/wiki/Shut_up_and_multiply), but this only exacerbates the discomfort.</p>\n<p>As such, there's a lot of appeal to a decision rule that looks out for you in the \"worst case\" and lets you turn down bets instead of making crazy gambles like those Bayesians. The concepts of \"Knightian uncertainty\" and \"the MMEU rule\" appeal to this intuition.</p>\n<p>But the MMEU rule doesn't work as advertised. And finally, I'm in a position to articulate my objection, in three parts.</p>\n<hr />\n<p><strong>The MMEU rule fails to grant me caution.</strong> Maximizing minimum expected utility does not help me do well in the worst case. It only helps me pick out types of uncertainty that I expect are adversarial, and maximize my odds given that <em>that</em> uncertainty will be resolved disfavorably.</p>\n<p>Which is a <em>little</em> bit like assuming the worst. I can look at the special uncertainty and say \"imagine this part is resolved adversarially, what happens?\" But I can't do this with <em>all</em> my uncertainty, because there's always <em>some</em> chance that reality has been lying to me and everything is about to get weird. MMEU manages this by limiting its caution to an arbitrary subset of its possibility. This is a poor approximation of caution.</p>\n<p>The MMEU rule is <em>not</em> allowing me to reason as if the world might turn against me. Rather, it's <em>forcing</em> me to act as if <em>with certainty</em> an arbitrary segment of my uncertainty will be resolved disfavorably. I'm all for hedging my bets, and I'm very much in favor of playing defensively when there is an Adversary on the field. I can be just as paranoid as Paranoid Perry, <em>given appropriate reason</em>. I'm happy to identify the parts of nature that often resolve disfavorably and hedge the relevant bets. But when nature proves unbiased, I play the odds. Minimum expected utility maximizers are forced to play defensively forever, no matter how hard nature tries to do them favors.</p>\n<p>More importantly, though, <em>new decision rules aren't how you capture caution</em>. Remember the game of drawers? The MMEU rule just&nbsp;doesn't correspond to our intuition sense of caution. The way to avoid ruinous bets is not to assume that nature is out to get you. It's to <em>adjust the utilities appropriately.</em></p>\n<p>Imagine the following variant of Sir Percy's coin toss:</p>\n<ol>\n<li>Pay $1,000,000 to be paid $2,000,001 if the coin came up heads</li>\n<li>Pay $1,000,000 to be paid $2,000,001 if the coin came up tails</li>\n</ol>\n<p>I would refuse both bets individually, and accept their conjunction. <em>But not because I can't assign a consistent credence to the event \"the coin came up heads\"</em>, that's ridiculous. <em>Not because I fail to attempt to maximize utility</em>, that's ridiculous too. I reject each bet individually because <em>dollars aren't utility</em>. If you convert the dollars into my utils, you'll see that the downside of either bet taken individually outweighs its upside, but that the downside of both bets taken together is $0 (with an upside of $1).</p>\n<p>So yes, I want to be cautious sometimes. Yes, I can reject bets individually that I accept together. I am completely comfortable rejecting many seemingly-good bets. But the MMEU rule is not the thing which grants me these powers.</p>\n<p><strong>The MMEU rule fails to grant me humility.</strong> One of the original motivations of the MMEU rule is that, as humans, we don't always know what our credence <em>should</em> be (if we were using all our information correctly and were able to consider more hypotheses and so on). In the&nbsp;<a href=\"/lw/kcl/knightian_uncertainty_and_ambiguity_aversion/\">unbalanced tennis game</a>, we know that our credence for \"Anabel wins\" should be either really high or really low, but we don't know which.</p>\n<p>I can, of course, recognize this fact as a bounded Bayesian reasoner, without any need for a new decision rule. It is useful for me to recognize that my credences are fuzzy and context dependent and that they <em>would</em> be very different if I was a better Bayesian, but I don't need a new decision rule to model these things. In fact, the MMEU rule makes it <em>harder</em> for me to reason about what my credence should be.</p>\n<p>Imagine you know the unbalanced tennis game has already occurred, and that your friend (who you trust completely) has handed you a complicated logical sentence that is true if and only if Anabel won. You haven't figured out whether the sentence is true yet (you could see it going either way), but now you seem justified in saying your credence should be <em>either</em> 0 or 1 (though you don't know which yet).</p>\n<p>But if your credence for \"Anabel won\" is either 0% or 100% and you have Knightian uncertainty about which, then you're going to have a bad time. If the <a href=\"/lw/koj/knightian_uncertainty_bayesian_agents_and_the/\">eccentric bookie from earlier</a> tries to offer you a bet on a player <em>of your choice</em>, then there are no odds the bookie can offer that would make you take the bet.</p>\n<p>Allow me to repeat: if you think the tennis game has already occurred, and have Knightian uncertainty as to whether your credence for \"Anabel won\" is 0% or 100%, then if you <em>actually</em> use the MMEU rule, you would refuse a bet with 1,000,000,000 to 1 odds in favor of the player of your choice.</p>\n<p>Yes, I have meta-level probability distributions over my future credences for object-level events. I am not a perfect Bayesian (nor even a very good one). I regularly misuse the information I have. It is useful for me to be able to reason about what my credence <em>should</em> be, if only to combat various biases such as overconfidence and base rate neglect.</p>\n<p>But the MMEU rule doesn't help me with any of these things. In fact, the MMEU rule only makes it <em>harder</em> for me to reason about what my credence should be. It's a broken tool for a problem that I already know how to address.</p>\n<p><strong>The MMEU rule sees its uncertainty in the world.</strong> Above all, using the MMEU rule requires that you see some of your uncertainty as part of the <em>world</em>, as part of the territory rather than the map. How is the world-uncertainty separated from the mind-uncertainty? Why should I treat them as different types of thing? The MMEU rule divides uncertainty into two arbitrary tasks, and the distinction fails to grant me any useful tools.</p>\n<p>I already know how to treat my credences as imprecise: I widen my error bars and expect my beliefs to change (even though I <a href=\"/lw/ii/conservation_of_expected_evidence/\">can't predict how</a>). But I still treat the resulting imprecise credences as normal uncertainty. In order to pretend that Knightian uncertainty is fundamentally different from normal uncertainty, we have assume that it lives in the territory rather than the map. It has to either <em>be controlled by an external process</em> (as Perry believes) or <em>have external significance</em> (as Caul believes).</p>\n<p>This seems crazy. Insofar as my credences are biased, I will strive to adjust accordingly. But no matter what I do, they will remain imprecise, and I have to deal with this as best I can. Claiming that the imprecision denotes the Adversarial hand of Nature, or that the imprecision denotes actual Worldparts over which I have preferences, doesn't help me address the real problem.</p>\n<hr />\n<p>The MMEU rule fails to solve the problems it set out to solve. And I don't <em>need</em> it to solve those problems &mdash; I already know how to do that with the tools I have.</p>\n<p>Most of the advice from the Knightian uncertainty camp is good. It is good to realize that your credences are imprecise. You should often expect to be surprised. In many domains, you should widen your error bars. But I <a href=\"/lw/klg/knightian_uncertainty_in_a_bayesian_framework/\">already know how to do these things</a>.</p>\n<p>Sometimes, it is good to reject bets. Often, it is good to delay decisions and seek more information, and to make sure that you do well in the worst case. <em>But I already know how to do these things.</em> I already know how to translate dollars into utilities such that ruinous bets become unappealing.</p>\n<p>If the label \"Knightian uncertainty\" is useful to you, then use it. I won't protest if you want to stick that label on your own imprecision, on your own inability to consider all of the hypotheses that your evidence supports, or on your own expectation that the future will surprise you no matter how long you deliberate. I personally don't think that \"Knightian uncertainty\" is a useful label for these things, because it is one label that tries to do too much. But if it's useful to you, then use it.</p>\n<p>But don't try to tell me that you should treat it differently! To treat it differently is to act like your uncertainty is in the world, not in you.</p>\n<p>If nature starts acting adversarial, then identify the parts of reality that nature gets to control and assume they'll act against you. I'll be behind you all the way. If there's an Adversary around, I'll be paranoid as hell. But throughout it all, I'll be maximizing expected utility.</p>\n<p>Anything else is either silly, or a misunderstanding of the label \"utility\".</p>\n<h1>When MMEU is useful anyway</h1>\n<p>The MMEU rule is not fit to be a general decision rule in idealized agents, for all the reasons listed above. Expected utility maximization may seem reckless, and MMEU rule attempts to offer a fix. However, the proposed answer is to divide uncertainty into two categories, and then be both excessively defensive and excessively reckless <em>at the same time</em>. Unfortunately, two flaws don't make a feature.</p>\n<p>It may appear that a correct decision rule lies somewhere in the middle, somewhere between the \"reckless\" and \"defensive\" extremes. Don't be fooled: Bayesian expected utility maximizers naturally grow defensive as they learn that the world is adversarial, and caution can be written into the utility numbers. If ever it looks like your preferences are best met by doing anything other than maximizing expected utility, then you've misplaced your \"utility\" label.</p>\n<p>But, unfortunately for us, we are humans living in the real world, and we happen to have misplaced all our utility labels.</p>\n<p>Nobody is offering you bets with payoffs written in clearly delineated utilities. In fact, almost all of the bets that you are offered by nature are delineated in things like money, time, attention, friendship, or various goods and services. Most of us experience diminishing marginal returns on most goods, and most of us are risk averse. As such, na&iuml;ve Bayesian-style gambling for money or time or attention or any other good is usually a pretty plan.</p>\n<p>Almost all of the bets offered to us by other humans&nbsp;are worse, as they tend to come with ulterior motives attached. Unless you really know what you are doing, na&iuml;ve Bayesian-style gambling at a Casino will get you into a whole lot of trouble.</p>\n<p>Furthermore, we are humans. We use a bunch of faulty heuristics, and we are rife with biases. We're <a href=\"/lw/ii/conservation_of_expected_evidence/\">overconfident</a>. We succumb to the <a href=\"http://en.wikipedia.org/wiki/Planning_fallacy\">planning fallacy</a>. <a href=\"http://en.wikipedia.org/wiki/Planning_fallacy\">People often don't distinguish between their expected case and their best case</a>. When people are facing a bet and you ask them to consider the worst case, they consider things like losing the bet, and they <em>don't</em> consider things like reality being turned into an eternal hellscape because the laws of physics were just kidding. So while it doesn't make sense for an <em>idealized</em> reasoner to try to maximize utility in the worst case, it may&nbsp;make sense for <em>humans</em> to act that way.</p>\n<p>If you find that the MMEU rule is a good heuristic for you, then use it. But when you do, remember why you need it: because humans are overconfident, and because most goods have diminishing returns. If we could fully debias you and correctly compute the utility of each action available to you (including actions like \"don't take the bet\" or \"stall\", and including preferences for security and stability), then expected utility maximization would be the only sane decision rule to use.</p>\n<p>Finally, there <em>are</em> times when we might want to treat uncertainty like it's in the world rather than in our heads. Suppose, for example, that you believe the Many Worlds interpretation of quantum mechanics. It is possible to have preferences over Everett branches that don't treat quantum uncertainty like internal uncertainty, and this isn't necessarily crazy. For example, you could have preferences stating that <em>any</em> non-zero Everett branch in which humanity survives is extremely valuable. In this case, you might be willing to work very hard to expand the branch where humanity survives from zero to something, but be unwilling to work proportionally hard to expand it from large to slightly larger. If you're <a href=\"http://en.wikipedia.org/wiki/Planning_fallacy\">VNM-rational</a>, this indicates that you treat quantum uncertainty differently from mental uncertainty.</p>\n<p>This doesn't mean you should use the MMEU rule over quantum uncertainty, by any means: Cautious Caul is crazy. But it is useful to remember that whenever something uncertainty-ish&nbsp;<em>is</em> in the world, you might end up doing things that don't look like expected utility maximization, and this <em>can </em>be rational.</p>\n<h1>A closing anecdote</h1>\n<p>My response to someone actually using the MMEU rule depends upon their answer to a simple question:</p>\n<blockquote>\n<p>Why ain't you rich?</p>\n</blockquote>\n<p>If they sigh and say \"because nature resolves all my ambiguity, and nature hates me\", then I will show them all the money that I won when playing exactly the same games as them, and I will say</p>\n<blockquote>\n<p>But nature <em>doesn't</em> hate you! In all the games where we had reason to believe that nature was adversarial (like when that bookie scanned our brains and came back two days later offering bets that looked really nice at first glance), I played just as defensively as you did, and I did just as well as you did. I'm behind you all the way when it looks like nature has stacked things against us. But in other games, nature <em>hasn't</em> been adversarial! Remember all those times we played Sir Percy's coin toss? Look how much richer I became!</p>\n</blockquote>\n<p>But this agent will only shake their head and say \"I'm sorry, but you don't understand. I <em>know</em> that nature is adversarial, and I am <em>absolutely certain</em> that every shred of ambiguity allowed by my credence distribution will be used against me. I acknowledge that nature is not acting adversarial against <em>you</em>, and I envy you for your luck, but nature <em>is</em> adversarial against me, and I'm eeking out as much utility as I can.\"</p>\n<p>And to that, I will only shake my head and depart, mourning for their broken priors.</p>\n<p>If, instead, the agent answers \"I may not be rich in <em>this</em> worldpart, but there are other worldparts that showed up in my credence distribution where I am richer than you\", then I will shrug.</p>\n<blockquote>\n<p>I care for my Everett-brothers as much as you care for your credence-brothers, and that caring was factored into my utility calculations. And yet still, I am richer.</p>\n</blockquote>\n<p>\"Indeed\", the agent will respond with a sage nod. \"But while you care for your Everett-brothers according to their measure, I care <em>only</em> about the least convenient world consistent with my credence distribution: so yes, I am poorer here, but it is fine, because I am richer there.\"</p>\n<blockquote>\n<p>Well, maybe. You've maximized the minimum odds, but that doesn't mean that your least convenient sliver did <em>well</em>. Back when we played the Game of Drawers, the sliver of you that faced Box 1 probably lost one thousand dollars, while the sliver of me that faced Box 1 definitely gained two bucks.</p>\n</blockquote>\n<p>\"Perhaps. But in expectation over my credence distribution, that sliver of me has more money.\"</p>\n<blockquote>\n<p>But in expectation <em>overall</em>, considering that Box 2 also exists, I did better than you.</p>\n</blockquote>\n<p>\"I understand how you find it strange, but these are my preferences. I care only about the world with the worst odds that happens to fit in my credence distribution.\"</p>\n<p>\"Consider the bet with the thousand-sided quantum die\", the agent will continue. \"In the least convenient world of that game, you lost 898 utility, and there is a version of me asking how you could let yourself fail so.\"</p>\n<blockquote>\n<p>That Everett-brother of mine knew the risks. His suffering and my sorrow was factored into the utility calculations. Even after adjusting for loss aversion and risk aversion and my preferences for egalitarianism, he traded his utils to us one-for-one or better. He would make the trade again in a heartbeat, as would I to others.</p>\n</blockquote>\n<p>\"In that least convenient world\", the agent will reply, \"my sliver is asking yours, 'and what of your Everett-brothers, who profited so from your despair, knowing that you would be left suffering in these depths. Do you think they shed tears for you?'\"</p>\n<blockquote>\n<p>Don't worry,</p>\n</blockquote>\n<p>I'll answer, in the plethora of expected worlds where I am richer.</p>\n<blockquote>\n<p><a href=\"http://i.imgur.com/TK9zjDF.gif\">We do</a>.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 1, "dPPATLhRmhdJtJM2t": 1, "3uE2pXvbcnS9nnZRE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SEov2u5Y7mJTPaTLK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 40, "extendedScore": null, "score": 0.00012, "legacy": true, "legacyId": "27019", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Recently, I found myself in a conversation with someone advocating the use of <a href=\"http://en.wikipedia.org/wiki/Knightian_uncertainty\">Knightian uncertainty</a>. He (who I'm anonymizing as Sir Percy) made suggestions that are useful to most bounded reasoners, and which can <a href=\"/lw/klg/knightian_uncertainty_in_a_bayesian_framework/\">be integrated into a Bayesian framework</a>. He also claimed preferences that depend upon his Knightian uncertainty and that he's not an expected utility maximizer. Further, he claimed that Bayesian reasoning cannot capture his preferences. Specifically, Sir Percy said he maximizes&nbsp;<em>minimum</em> expected utility given his Knightian uncertainty, using what I will refer to as the \"MMEU rule\" to make decisions.</p>\n<p>In my <a href=\"/lw/koj/knightian_uncertainty_bayesian_agents_and_the/\">previous post</a>, I showed that Bayesian expected utility maximizers <em>can</em> exhibit behavior in accordance with his preferences. Two such reasoners, Paranoid Perry and Cautious Caul, were explored. These hypothetical agents demonstrate that it is possible for Bayesians to be \"ambiguity averse\", e.g. to avoid certain types of uncertainty.</p>\n<p>But Perry and Caul are unnatural agents using strange priors. Is this because we are twisting the Bayesian framework to represent behavior it is ill-suited to emulate? Or does the strangeness of Perry and Caul merely reveal a strangeness in the MMEU rule?</p>\n<p>In this post, I'll argue the latter: maximization of minimum expected utility is not a good decision rule, for the same reason that Perry and Caul seem irrational. My rejection of the MMEU rule will follow from my rejections of Perry and Caul.</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"A_rejection_of_Perry\">A rejection of Perry</h1>\n<p>I understand the appeal of Paranoid Perry, the agent that assumes ambiguity is resolved adversarially. Humans often keep the worst-case in mind, avoid big gambles, and make sure that they'll be OK even if everything goes wrong. Perry promises to capture some of this intuitively reasonable behavior.</p>\n<p>Unfortunately, this promise is not kept. From the description of the MMEU rule, you might think that Perry is forgoing high utility in the average case to ensure moderate utility in the worst case. But this is not the case: Perry willingly takes huge gambles, so long as those gambles are resolved by \"normal\" uncertainty rather than \"adversarial\" uncertainty.</p>\n<p>Allow me to reiterate: <em>Maximizing minimum expected utility does not ensure that you do well in the worst case.</em> It merely selects a <em>single type</em> of uncertainty against which to play defensively, and then gambles against the rest. To illustrate, consider the following Game of Drawers:</p>\n<p><em>There are two boxes, Box 1 and Box 2. Each box has two Drawers, drawer A and drawer B. Each drawer contains a bet, as follows:</em></p>\n<pre><code>1A. 99% lose $1000, 1% gain $99300 (expectation: $3)\n1B. Gain 2$</code></pre>\n<pre><code>\n2A: 99% lose $1000, 1% gain $99500 (expectation: $5)\n2B. Gain $10\n</code></pre>\n<p><em>You face one of the boxes (you do not know which) and you must choose one of the drawers. Which do you choose?</em></p>\n<p>Imagine that&nbsp;you have \"ambiguity\" (\"Knightian uncertainty\") about which box you face, but that you believe the gambles inside the boxes are fair: this is a setup analogous to the <a href=\"http://en.wikipedia.org/wiki/Ellsberg_paradox\">Ellsberg urn game</a>, except that it gives the opposite intuition.</p>\n<p>In the Game of Drawers, I expect most people would choose drawer B (and win either $2 or $10). However, Paranoid Perry (and Cautious Caul, and Sir Percy)&nbsp;would choose drawer A.</p>\n<p><em>In the Game of Drawers, Perry acts such that 99% of the time it loses $1000.</em></p>\n<p>Wasn't Paranoid Perry supposed to reason so that it does well in the worst case? What went wrong?</p>\n<p>Paranoid Perry reasons that <em>nature</em> gets to pick which box it faces, and that nature will force Perry into the worse box. Box 1 is strictly worse than Box 2, so Perry expects to face Box 1. And drawer A in Box 1 has higher expected utility than drawer B in Box 1, so Perry takes drawer A, a gamble that loses Perry $1000 99% of the time!</p>\n<p>Perry has <em>no inclination</em> to avoid big gambles. Perry isn't making sure that the worst scenario is acceptable, it's is maximizing <em>expected</em> utility in the worst scenario <em>that nature can pick</em>. Within that least convenient world, Perry (and Caul, and Sir Percy) take standard Bayesian gambles.</p>\n<p>Bayesian gambles may seem reckless, but Perry is not the solution: Perry simply divides uncertainty into two classes, and treats one of them too defensively and the other just as recklessly as a Bayesian would. Two flaws don't make a feature.</p>\n<p>Perry <em>assumes</em>&nbsp;that some of its uncertainty gets resolved unfavorably by nature <em>regardless of whether or not it is</em>. In some domains this captures caution, yes. In others, it's just a silly waste. As soon as nature <em>actually</em> starts acting adversarial, Bayesians become cautious too<em>.</em> The difference is that Bayesians are not <em>forced</em> to act as if one arbitrary segment of my uncertainty is adversarially resolved.</p>\n<p>Perry believes&nbsp;\u2014&nbsp;with absolute certainty, disregarding all evidence no matter how strong \u2014&nbsp;that Nature never cuts any slack.</p>\n<h1 id=\"A_rejection_of_Caul\">A rejection of Caul</h1>\n<p>I understand, too, the appeal of Cautious Caul. Caul reasons about multiple possible worldparts, and attempts to ensure that the Caul-sliver in the least convenient worldpart does well enough. Instead of insisting that convenient things <em>can't</em> happen (like Perry), Caul only <em>cares</em> about the inconvenient parts. Perhaps this better captures our intuition that people should be less reckless than Bayesians?</p>\n<p>Expected utility maximizers happily trade utility in one branch of the possibility space for proportional utility in another branch, regardless of which branch had higher utility in the first place. Some people have moral intuitions that say this is wrong, and that we should be unwilling to trade utility away from unfortunate branches and into fortunate branches.</p>\n<p>But this moral intuition is flawed, in a way that reveals confusion both about worst cases and about utility.</p>\n<p>There's a huge difference between what the average person consider to be a worst case scenario (e.g., losing the bet) and what a Bayesian considers to be the worst case scenario (e.g. physics has been lying to you and this is about to turn into the worst possible world). Or, to put things glibly, the human version of worst case is \"you lose the bet\", whereas the Bayesian version of worst case is \"the bet was a lie and now everybody will be tortured forever\".</p>\n<p>You can't maximize the actual worst case in any reasonably complex system. There are some small systems (say, software used to control trains) where people actually do worry about the&nbsp;<em>absolute </em>worst case, but upon inspection, these are consistent with expected utility maximization. A train crash is pretty costly.</p>\n<p>And, in fact, expected utility maximization can capture caution <em>in general</em>. We don't need Cautious Caul in order to act with appropriate caution in a Bayesian framework. Caution is not implemented by new decision rules, it is implemented in the conversion from money (or whatever) to utility. Allow me to illustrate:</p>\n<p>Suppose that the fates are about offer me a bet and then roll a thousand-sided die. The die seems fair (to me) and my information gives me a uniform probability distribution over values between 1 and 1000: my probability mass is about to split into a thousand shards of equal measure. Before the die is rolled, I am given a choice between two options:</p>\n<ol>\n<li>No matter what the die rolls, I get $100.</li>\n<li>If the die rolls a 1, I pay $898. Otherwise, I get $101.</li>\n</ol>\n<p>The second package results in more expected money (by $1), but I would choose the former. Why? Losing $898 is more bad than the extra dollars are good. I'm more than happy to burn one expected dollar in order to avoid the branch where I have to pay $898. In this case, I act cautious in the intuitive sense&nbsp;\u2014<span style=\"font-family: arial, sans-serif; color: #545454;\"><span style=\"line-height: 18.200000762939453px;\">&nbsp;</span></span><em>and I do this as an expected utility maximizer</em>. How? Well, consider the following bet instead:</p>\n<ol>\n<li>No matter what the die rolls, I get 100 utility.</li>\n<li>If the die rolls a 1, I lose 898 utility. Otherwise, I gain 101 utility.</li>\n</ol>\n<p>Now I pick bet 2 in a heartbeat. What changed? Utils have <em>already factored in</em> everything that I care about.</p>\n<p>I am risk-neutral in utils. If I am loss-averse, then the fact that the losing version of me will experience a sharp pang of frustration and perhaps a lasting depression and lowered productivity has <em>already been factored in</em> to the calculation. It's not like I lose 898 utility and <em>then</em> feel bad about it: the bad feelings are included in the number. The fact that all the <em>other</em> versions of me (who get 101 utility) will feel a little sad and remorseful, and will feel a little frustrated because the world is unfair, has already been factored into <em>their</em> utility numbers: it's not like they see that they got 101 utility and <em>then</em> feel remorse. (Similarly, their relief and glee has already been rolled into the numbers too.)</p>\n<p>The intuition that we shouldn't trade utility from unfortunate branches mostly stems from a misunderstanding of utility. Utility <em>already</em>&nbsp;takes into account your egalitarianism, your preference curve for dollars, and so on. Once these things are accounted for, you <em>should </em>trade utility from unfortunate branches into fortunate branches: if this feels bad to you, then you haven't got your utility calculations quite right.</p>\n<p>Expected utility maximizers can be cautious. They can avoid ruinous bets. They can play it safe. But all of this behavior is encoded in the utility numbers: we don't need Cautious Caul to exhibit these preferences.</p>\n<h1 id=\"A_rejection_of_the_MMEU_rule\">A rejection of the MMEU rule</h1>\n<p>Bets come with stigma. They are traditionally only offered to humans by other humans, and anyone offering a bet is usually either a con artist or a philosophy professor. \"Reject bets by default\" is a good heuristic for most people.</p>\n<p>Advocates of Bayesian reasoning talk about accepting bets without flinching, and that can seem strange. I think this comes down to a fundamental misunderstanding between colloquial bets and Bayesian bets.</p>\n<p>Colloquial bets are offered by skeevy con artists who probably know something you don't. Bayesian bets, on the other hand, arise whenever the agent must make a decision. \"Rejecting the bet\" is not an option: inaction is a choice. You have to weigh all available actions (including \"stall\" or \"gather more information\") and bet on which one will serve you best.</p>\n<p>This mismatch, I think, is responsible for quite a bit of most people's discomfort with Bayesian decisions. That said, Bayesians are <em>also</em> willing to make really big gambles, gambles which look crazy to most people (who are risk- and loss-averse). Bayesians claim that risk- and loss-aversion are biases that should be overcome, and that we should [shut up and multiply](http://wiki.lesswrong.com/wiki/Shut_up_and_multiply), but this only exacerbates the discomfort.</p>\n<p>As such, there's a lot of appeal to a decision rule that looks out for you in the \"worst case\" and lets you turn down bets instead of making crazy gambles like those Bayesians. The concepts of \"Knightian uncertainty\" and \"the MMEU rule\" appeal to this intuition.</p>\n<p>But the MMEU rule doesn't work as advertised. And finally, I'm in a position to articulate my objection, in three parts.</p>\n<hr>\n<p><strong>The MMEU rule fails to grant me caution.</strong> Maximizing minimum expected utility does not help me do well in the worst case. It only helps me pick out types of uncertainty that I expect are adversarial, and maximize my odds given that <em>that</em> uncertainty will be resolved disfavorably.</p>\n<p>Which is a <em>little</em> bit like assuming the worst. I can look at the special uncertainty and say \"imagine this part is resolved adversarially, what happens?\" But I can't do this with <em>all</em> my uncertainty, because there's always <em>some</em> chance that reality has been lying to me and everything is about to get weird. MMEU manages this by limiting its caution to an arbitrary subset of its possibility. This is a poor approximation of caution.</p>\n<p>The MMEU rule is <em>not</em> allowing me to reason as if the world might turn against me. Rather, it's <em>forcing</em> me to act as if <em>with certainty</em> an arbitrary segment of my uncertainty will be resolved disfavorably. I'm all for hedging my bets, and I'm very much in favor of playing defensively when there is an Adversary on the field. I can be just as paranoid as Paranoid Perry, <em>given appropriate reason</em>. I'm happy to identify the parts of nature that often resolve disfavorably and hedge the relevant bets. But when nature proves unbiased, I play the odds. Minimum expected utility maximizers are forced to play defensively forever, no matter how hard nature tries to do them favors.</p>\n<p>More importantly, though, <em>new decision rules aren't how you capture caution</em>. Remember the game of drawers? The MMEU rule just&nbsp;doesn't correspond to our intuition sense of caution. The way to avoid ruinous bets is not to assume that nature is out to get you. It's to <em>adjust the utilities appropriately.</em></p>\n<p>Imagine the following variant of Sir Percy's coin toss:</p>\n<ol>\n<li>Pay $1,000,000 to be paid $2,000,001 if the coin came up heads</li>\n<li>Pay $1,000,000 to be paid $2,000,001 if the coin came up tails</li>\n</ol>\n<p>I would refuse both bets individually, and accept their conjunction. <em>But not because I can't assign a consistent credence to the event \"the coin came up heads\"</em>, that's ridiculous. <em>Not because I fail to attempt to maximize utility</em>, that's ridiculous too. I reject each bet individually because <em>dollars aren't utility</em>. If you convert the dollars into my utils, you'll see that the downside of either bet taken individually outweighs its upside, but that the downside of both bets taken together is $0 (with an upside of $1).</p>\n<p>So yes, I want to be cautious sometimes. Yes, I can reject bets individually that I accept together. I am completely comfortable rejecting many seemingly-good bets. But the MMEU rule is not the thing which grants me these powers.</p>\n<p><strong>The MMEU rule fails to grant me humility.</strong> One of the original motivations of the MMEU rule is that, as humans, we don't always know what our credence <em>should</em> be (if we were using all our information correctly and were able to consider more hypotheses and so on). In the&nbsp;<a href=\"/lw/kcl/knightian_uncertainty_and_ambiguity_aversion/\">unbalanced tennis game</a>, we know that our credence for \"Anabel wins\" should be either really high or really low, but we don't know which.</p>\n<p>I can, of course, recognize this fact as a bounded Bayesian reasoner, without any need for a new decision rule. It is useful for me to recognize that my credences are fuzzy and context dependent and that they <em>would</em> be very different if I was a better Bayesian, but I don't need a new decision rule to model these things. In fact, the MMEU rule makes it <em>harder</em> for me to reason about what my credence should be.</p>\n<p>Imagine you know the unbalanced tennis game has already occurred, and that your friend (who you trust completely) has handed you a complicated logical sentence that is true if and only if Anabel won. You haven't figured out whether the sentence is true yet (you could see it going either way), but now you seem justified in saying your credence should be <em>either</em> 0 or 1 (though you don't know which yet).</p>\n<p>But if your credence for \"Anabel won\" is either 0% or 100% and you have Knightian uncertainty about which, then you're going to have a bad time. If the <a href=\"/lw/koj/knightian_uncertainty_bayesian_agents_and_the/\">eccentric bookie from earlier</a> tries to offer you a bet on a player <em>of your choice</em>, then there are no odds the bookie can offer that would make you take the bet.</p>\n<p>Allow me to repeat: if you think the tennis game has already occurred, and have Knightian uncertainty as to whether your credence for \"Anabel won\" is 0% or 100%, then if you <em>actually</em> use the MMEU rule, you would refuse a bet with 1,000,000,000 to 1 odds in favor of the player of your choice.</p>\n<p>Yes, I have meta-level probability distributions over my future credences for object-level events. I am not a perfect Bayesian (nor even a very good one). I regularly misuse the information I have. It is useful for me to be able to reason about what my credence <em>should</em> be, if only to combat various biases such as overconfidence and base rate neglect.</p>\n<p>But the MMEU rule doesn't help me with any of these things. In fact, the MMEU rule only makes it <em>harder</em> for me to reason about what my credence should be. It's a broken tool for a problem that I already know how to address.</p>\n<p><strong>The MMEU rule sees its uncertainty in the world.</strong> Above all, using the MMEU rule requires that you see some of your uncertainty as part of the <em>world</em>, as part of the territory rather than the map. How is the world-uncertainty separated from the mind-uncertainty? Why should I treat them as different types of thing? The MMEU rule divides uncertainty into two arbitrary tasks, and the distinction fails to grant me any useful tools.</p>\n<p>I already know how to treat my credences as imprecise: I widen my error bars and expect my beliefs to change (even though I <a href=\"/lw/ii/conservation_of_expected_evidence/\">can't predict how</a>). But I still treat the resulting imprecise credences as normal uncertainty. In order to pretend that Knightian uncertainty is fundamentally different from normal uncertainty, we have assume that it lives in the territory rather than the map. It has to either <em>be controlled by an external process</em> (as Perry believes) or <em>have external significance</em> (as Caul believes).</p>\n<p>This seems crazy. Insofar as my credences are biased, I will strive to adjust accordingly. But no matter what I do, they will remain imprecise, and I have to deal with this as best I can. Claiming that the imprecision denotes the Adversarial hand of Nature, or that the imprecision denotes actual Worldparts over which I have preferences, doesn't help me address the real problem.</p>\n<hr>\n<p>The MMEU rule fails to solve the problems it set out to solve. And I don't <em>need</em> it to solve those problems \u2014 I already know how to do that with the tools I have.</p>\n<p>Most of the advice from the Knightian uncertainty camp is good. It is good to realize that your credences are imprecise. You should often expect to be surprised. In many domains, you should widen your error bars. But I <a href=\"/lw/klg/knightian_uncertainty_in_a_bayesian_framework/\">already know how to do these things</a>.</p>\n<p>Sometimes, it is good to reject bets. Often, it is good to delay decisions and seek more information, and to make sure that you do well in the worst case. <em>But I already know how to do these things.</em> I already know how to translate dollars into utilities such that ruinous bets become unappealing.</p>\n<p>If the label \"Knightian uncertainty\" is useful to you, then use it. I won't protest if you want to stick that label on your own imprecision, on your own inability to consider all of the hypotheses that your evidence supports, or on your own expectation that the future will surprise you no matter how long you deliberate. I personally don't think that \"Knightian uncertainty\" is a useful label for these things, because it is one label that tries to do too much. But if it's useful to you, then use it.</p>\n<p>But don't try to tell me that you should treat it differently! To treat it differently is to act like your uncertainty is in the world, not in you.</p>\n<p>If nature starts acting adversarial, then identify the parts of reality that nature gets to control and assume they'll act against you. I'll be behind you all the way. If there's an Adversary around, I'll be paranoid as hell. But throughout it all, I'll be maximizing expected utility.</p>\n<p>Anything else is either silly, or a misunderstanding of the label \"utility\".</p>\n<h1 id=\"When_MMEU_is_useful_anyway\">When MMEU is useful anyway</h1>\n<p>The MMEU rule is not fit to be a general decision rule in idealized agents, for all the reasons listed above. Expected utility maximization may seem reckless, and MMEU rule attempts to offer a fix. However, the proposed answer is to divide uncertainty into two categories, and then be both excessively defensive and excessively reckless <em>at the same time</em>. Unfortunately, two flaws don't make a feature.</p>\n<p>It may appear that a correct decision rule lies somewhere in the middle, somewhere between the \"reckless\" and \"defensive\" extremes. Don't be fooled: Bayesian expected utility maximizers naturally grow defensive as they learn that the world is adversarial, and caution can be written into the utility numbers. If ever it looks like your preferences are best met by doing anything other than maximizing expected utility, then you've misplaced your \"utility\" label.</p>\n<p>But, unfortunately for us, we are humans living in the real world, and we happen to have misplaced all our utility labels.</p>\n<p>Nobody is offering you bets with payoffs written in clearly delineated utilities. In fact, almost all of the bets that you are offered by nature are delineated in things like money, time, attention, friendship, or various goods and services. Most of us experience diminishing marginal returns on most goods, and most of us are risk averse. As such, na\u00efve Bayesian-style gambling for money or time or attention or any other good is usually a pretty plan.</p>\n<p>Almost all of the bets offered to us by other humans&nbsp;are worse, as they tend to come with ulterior motives attached. Unless you really know what you are doing, na\u00efve Bayesian-style gambling at a Casino will get you into a whole lot of trouble.</p>\n<p>Furthermore, we are humans. We use a bunch of faulty heuristics, and we are rife with biases. We're <a href=\"/lw/ii/conservation_of_expected_evidence/\">overconfident</a>. We succumb to the <a href=\"http://en.wikipedia.org/wiki/Planning_fallacy\">planning fallacy</a>. <a href=\"http://en.wikipedia.org/wiki/Planning_fallacy\">People often don't distinguish between their expected case and their best case</a>. When people are facing a bet and you ask them to consider the worst case, they consider things like losing the bet, and they <em>don't</em> consider things like reality being turned into an eternal hellscape because the laws of physics were just kidding. So while it doesn't make sense for an <em>idealized</em> reasoner to try to maximize utility in the worst case, it may&nbsp;make sense for <em>humans</em> to act that way.</p>\n<p>If you find that the MMEU rule is a good heuristic for you, then use it. But when you do, remember why you need it: because humans are overconfident, and because most goods have diminishing returns. If we could fully debias you and correctly compute the utility of each action available to you (including actions like \"don't take the bet\" or \"stall\", and including preferences for security and stability), then expected utility maximization would be the only sane decision rule to use.</p>\n<p>Finally, there <em>are</em> times when we might want to treat uncertainty like it's in the world rather than in our heads. Suppose, for example, that you believe the Many Worlds interpretation of quantum mechanics. It is possible to have preferences over Everett branches that don't treat quantum uncertainty like internal uncertainty, and this isn't necessarily crazy. For example, you could have preferences stating that <em>any</em> non-zero Everett branch in which humanity survives is extremely valuable. In this case, you might be willing to work very hard to expand the branch where humanity survives from zero to something, but be unwilling to work proportionally hard to expand it from large to slightly larger. If you're <a href=\"http://en.wikipedia.org/wiki/Planning_fallacy\">VNM-rational</a>, this indicates that you treat quantum uncertainty differently from mental uncertainty.</p>\n<p>This doesn't mean you should use the MMEU rule over quantum uncertainty, by any means: Cautious Caul is crazy. But it is useful to remember that whenever something uncertainty-ish&nbsp;<em>is</em> in the world, you might end up doing things that don't look like expected utility maximization, and this <em>can </em>be rational.</p>\n<h1 id=\"A_closing_anecdote\">A closing anecdote</h1>\n<p>My response to someone actually using the MMEU rule depends upon their answer to a simple question:</p>\n<blockquote>\n<p>Why ain't you rich?</p>\n</blockquote>\n<p>If they sigh and say \"because nature resolves all my ambiguity, and nature hates me\", then I will show them all the money that I won when playing exactly the same games as them, and I will say</p>\n<blockquote>\n<p>But nature <em>doesn't</em> hate you! In all the games where we had reason to believe that nature was adversarial (like when that bookie scanned our brains and came back two days later offering bets that looked really nice at first glance), I played just as defensively as you did, and I did just as well as you did. I'm behind you all the way when it looks like nature has stacked things against us. But in other games, nature <em>hasn't</em> been adversarial! Remember all those times we played Sir Percy's coin toss? Look how much richer I became!</p>\n</blockquote>\n<p>But this agent will only shake their head and say \"I'm sorry, but you don't understand. I <em>know</em> that nature is adversarial, and I am <em>absolutely certain</em> that every shred of ambiguity allowed by my credence distribution will be used against me. I acknowledge that nature is not acting adversarial against <em>you</em>, and I envy you for your luck, but nature <em>is</em> adversarial against me, and I'm eeking out as much utility as I can.\"</p>\n<p>And to that, I will only shake my head and depart, mourning for their broken priors.</p>\n<p>If, instead, the agent answers \"I may not be rich in <em>this</em> worldpart, but there are other worldparts that showed up in my credence distribution where I am richer than you\", then I will shrug.</p>\n<blockquote>\n<p>I care for my Everett-brothers as much as you care for your credence-brothers, and that caring was factored into my utility calculations. And yet still, I am richer.</p>\n</blockquote>\n<p>\"Indeed\", the agent will respond with a sage nod. \"But while you care for your Everett-brothers according to their measure, I care <em>only</em> about the least convenient world consistent with my credence distribution: so yes, I am poorer here, but it is fine, because I am richer there.\"</p>\n<blockquote>\n<p>Well, maybe. You've maximized the minimum odds, but that doesn't mean that your least convenient sliver did <em>well</em>. Back when we played the Game of Drawers, the sliver of you that faced Box 1 probably lost one thousand dollars, while the sliver of me that faced Box 1 definitely gained two bucks.</p>\n</blockquote>\n<p>\"Perhaps. But in expectation over my credence distribution, that sliver of me has more money.\"</p>\n<blockquote>\n<p>But in expectation <em>overall</em>, considering that Box 2 also exists, I did better than you.</p>\n</blockquote>\n<p>\"I understand how you find it strange, but these are my preferences. I care only about the world with the worst odds that happens to fit in my credence distribution.\"</p>\n<p>\"Consider the bet with the thousand-sided quantum die\", the agent will continue. \"In the least convenient world of that game, you lost 898 utility, and there is a version of me asking how you could let yourself fail so.\"</p>\n<blockquote>\n<p>That Everett-brother of mine knew the risks. His suffering and my sorrow was factored into the utility calculations. Even after adjusting for loss aversion and risk aversion and my preferences for egalitarianism, he traded his utils to us one-for-one or better. He would make the trade again in a heartbeat, as would I to others.</p>\n</blockquote>\n<p>\"In that least convenient world\", the agent will reply, \"my sliver is asking yours, 'and what of your Everett-brothers, who profited so from your despair, knowing that you would be left suffering in these depths. Do you think they shed tears for you?'\"</p>\n<blockquote>\n<p>Don't worry,</p>\n</blockquote>\n<p>I'll answer, in the plethora of expected worlds where I am richer.</p>\n<blockquote>\n<p><a href=\"http://i.imgur.com/TK9zjDF.gif\">We do</a>.</p>\n</blockquote>", "sections": [{"title": "A rejection of Perry", "anchor": "A_rejection_of_Perry", "level": 1}, {"title": "A rejection of Caul", "anchor": "A_rejection_of_Caul", "level": 1}, {"title": "A rejection of the MMEU rule", "anchor": "A_rejection_of_the_MMEU_rule", "level": 1}, {"title": "When MMEU is useful anyway", "anchor": "When_MMEU_is_useful_anyway", "level": 1}, {"title": "A closing anecdote", "anchor": "A_closing_anecdote", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bukfeDZPHDHYRyBLB", "GBpdBPS8CqcC425tq", "iDYaCJ3o3Q7ypriTF", "jiBFC7DcCrZjGmZnJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-26T03:27:01.706Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta August meetup - Media representations", "slug": "meetup-atlanta-august-meetup-media-representations", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Adele_L", "createdAt": "2012-05-25T06:52:13.187Z", "isAdmin": false, "displayName": "Adele_L"}, "userId": "5cAXqfacg2fkQPK8j", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/55Q2g6EJ66cdCFQzs/meetup-atlanta-august-meetup-media-representations", "pageUrlRelative": "/posts/55Q2g6EJ66cdCFQzs/meetup-atlanta-august-meetup-media-representations", "linkUrl": "https://www.lesswrong.com/posts/55Q2g6EJ66cdCFQzs/meetup-atlanta-august-meetup-media-representations", "postedAtFormatted": "Tuesday, August 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20August%20meetup%20-%20Media%20representations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20August%20meetup%20-%20Media%20representations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F55Q2g6EJ66cdCFQzs%2Fmeetup-atlanta-august-meetup-media-representations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20August%20meetup%20-%20Media%20representations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F55Q2g6EJ66cdCFQzs%2Fmeetup-atlanta-august-meetup-media-representations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F55Q2g6EJ66cdCFQzs%2Fmeetup-atlanta-august-meetup-media-representations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13l'>Atlanta August meetup - Media representations</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 August 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Unit L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Oge says:</p>\n\n<p>====================================</p>\n\n<p>It\u2019s another video night! Bret Victor is a Web designer who believes that thinkers and creators should have instant feedback when using their tools i.e. computers.\n<a href=\"http://worrydream.com/\" rel=\"nofollow\">http://worrydream.com/</a>\nLet\u2019s watch a video of his titled, \u201cMedia for Thinking the Unthinkable\u201d then have a discussion to share the different representations we use for comprehending the ineffable in our daily lives.</p>\n\n<p>====================================</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13l'>Atlanta August meetup - Media representations</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "55Q2g6EJ66cdCFQzs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.9553433647611954e-06, "legacy": true, "legacyId": "27026", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_August_meetup___Media_representations\">Discussion article for the meetup : <a href=\"/meetups/13l\">Atlanta August meetup - Media representations</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 August 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Unit L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Oge says:</p>\n\n<p>====================================</p>\n\n<p>It\u2019s another video night! Bret Victor is a Web designer who believes that thinkers and creators should have instant feedback when using their tools i.e. computers.\n<a href=\"http://worrydream.com/\" rel=\"nofollow\">http://worrydream.com/</a>\nLet\u2019s watch a video of his titled, \u201cMedia for Thinking the Unthinkable\u201d then have a discussion to share the different representations we use for comprehending the ineffable in our daily lives.</p>\n\n<p>====================================</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_August_meetup___Media_representations1\">Discussion article for the meetup : <a href=\"/meetups/13l\">Atlanta August meetup - Media representations</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta August meetup - Media representations", "anchor": "Discussion_article_for_the_meetup___Atlanta_August_meetup___Media_representations", "level": 1}, {"title": "Discussion article for the meetup : Atlanta August meetup - Media representations", "anchor": "Discussion_article_for_the_meetup___Atlanta_August_meetup___Media_representations1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-26T17:29:42.661Z", "modifiedAt": null, "url": null, "title": "Changes to my workflow", "slug": "changes-to-my-workflow", "viewCount": null, "lastCommentedAt": "2019-01-09T04:34:55.891Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DDDNqq7QuwoseDvgf/changes-to-my-workflow", "pageUrlRelative": "/posts/DDDNqq7QuwoseDvgf/changes-to-my-workflow", "linkUrl": "https://www.lesswrong.com/posts/DDDNqq7QuwoseDvgf/changes-to-my-workflow", "postedAtFormatted": "Tuesday, August 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Changes%20to%20my%20workflow&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChanges%20to%20my%20workflow%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDDDNqq7QuwoseDvgf%2Fchanges-to-my-workflow%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Changes%20to%20my%20workflow%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDDDNqq7QuwoseDvgf%2Fchanges-to-my-workflow", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDDDNqq7QuwoseDvgf%2Fchanges-to-my-workflow", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1616, "htmlBody": "<p>About 18 months ago I made a <a href=\"/lw/fux/my_workflow/\">post here</a> on my workflow. I've received a handful of requests for follow-up, so I thought I would make another post detailing changes since then. I expect this post to be less useful than the last one.</p>\n<p>For the most part, the overall outline has remained pretty stable and feels very similar to 18 months ago. Things not mentioned below have mostly stayed the same. I believe that the total effect of continued changes have been continued but much smaller improvements, though it is hard to tell (as opposed to the last changes, which were more clearly improvements).</p>\n<p>Based on comparing time logging records I seem to now do substantially more work on average, but there are many other changes during this period that could explain the change (including changes in time logging). Changes other than work output are much harder to measure; I feel like they are positive but I wouldn't be surprised if this were an illusion.</p>\n<h2>Splitting days:</h2>\n<p>I now regularly divide my day into two halves, and treat the two halves as separate units. I plan each separately and reflect on each separately. I divide them by an hour long period of reflecting on the morning, relaxing for 5-10 minutes, napping for 25-30 minutes, processing my emails, and planning the evening. I find that this generally makes me more productive and happier about the day. Splitting my days is often difficult due to engagements in the middle of the day, and I don't have a good solution to that.</p>\n<h2>WasteNoTime:</h2>\n<p>I have longstanding objections to explicitly rationing internet use (since it seems either indicative of a broader problem that should be resolved directly, or else to serve a useful function that would be unwise to remove). That said, I now use the extension WasteNoTime to limit my consumption of blogs, webcomics, facebook, news sites, browser games, etc., to 10 minutes each half-day. This has cut the amount of time I spend browsing the internet from an average of 30-40 minutes to an average of 10-15 minutes. It doesn't seem to have been replaced by lower-quality leisure, but by a combination of work and higher-quality leisure.</p>\n<p>Similarly, I <a href=\"https://chrome.google.com/webstore/detail/kill-news-feed/hjobfcedfgohjkaieocljfcppjbkglfd?hl=en\">turned off the newsfeed</a> in facebook, which I found to improve the quality of my internet time in general (the primary issue was that I would sometimes be distracted by the newsfeed while sending messages over facebook, which wasn't my favorite way to use up wastenotime minutes).</p>\n<p>I also tried StayFocusd, but ended up adopting WasteNoTime because of the ability to set limits per half-day (via \"At work\" and \"not at work\" timers) rather than per-day. I find that the main upside is cutting off the tail of derping (e.g. getting sucked into a blog comment thread, or looking into a particularly engrossing issue), and for this purpose per half-day timers are much more effective.</p>\n<h2>Email discipline:</h2>\n<p>I set gmail to archive all emails on arrival and assign them the special label \"In.\" This lets me to search for emails and compose emails, using the normal gmail interface, without being notified of new arrivals. I process the items with label \"in\" (typically turning emails into todo items to be processed by the same system that deals with other todo items) at the beginning of each half day. Each night I scan my email quickly for items that require urgent attention.&nbsp;</p>\n<h2>Todo lists / reminders:</h2>\n<p>I continue to use todo lists for each half day and for a range of special conditions. I now check these lists at the beginning of each half day rather than before going to bed.</p>\n<p>I also maintain a third list of \"reminders.\" These are things that I want to be reminded of periodically, organized by day; each morning I look at the day's reminders and think about them briefly. Each of them is copied and filed under a future day. If I feel like I remember a thing well I file it in far in the future, if I feel like I don't remember it well I file it in the near future.</p>\n<p>Over the last month most of these reminders have migrated to be in the form \"If X, then Y,\" e.g. \"If I agree to do something for someone, then pause, say `actually I should think about it for a few minutes to make sure I have time,' and set a 5 minute timer that night to think about it more clearly.\" These are designed to fix problems that I notice when reflecting on the day. This is a recommendation from CFAR folks, which seems to be working well, though is the newest part of the system and least tested.</p>\n<h2>Isolating \"todos\":</h2>\n<p>I now attempt to isolate things that probably need doing, but don't seem maximally important; I aim to do them only on every 5th day, and only during one half-day. If I can't finish them in this time, I will typically delay them 5 days. When they spill over to other days, I try to at least keep them to one half-day or the other. I don't know if this helps, but it feels better to have isolated unproductive-feeling blocks of time rather than scattering it throughout the week.</p>\n<p>I don't do this very rigidly. I expect the overall level of discipline I have about it is comparable to or lower than a normal office worker who has a clearer division between their personal time and work time.</p>\n<h2>Toggl:</h2>\n<p>I now use&nbsp;<a href=\"https://www.toggl.com/\">Toggl</a>&nbsp;for detailed time tracking. Katja Grace and I experimented with about half a dozen other systems (Harvest, Yast, Klok, Freckle, Lumina, I expect others I'm forgetting) before settling on Toggl. It has a depressing number of flaws, but ends up winning for me by making it very fast to start and switch timers which is probably the most important criterion for me. It also offers reviews that work out well with what I want to look at.</p>\n<p>I find the main value adds from detailed time tracking are:</p>\n<p>1. Knowing how long I've spent on projects, especially long-term projects. My intuitive estimates are often off by more than a factor of 2, even for things taking 80 hours; this can lead me to significantly underestimate the costs of taking on some kinds of projects, and it can also lead me to think an activity is unproductive instead of productive by overestimating how long I've actually spent on it.</p>\n<p>2. Accurate breakdowns of time in a day, which guide efforts at improving my day-to-day routine. They probably also make me feel more motivated about working, and improve focus during work.</p>\n<h2>Reflection / improvement:</h2>\n<p>Reflection is now a smaller fraction of my time, down from 10% to 3-5%, based on diminishing returns to finding stuff to improve. Another 3-5% is now redirected into longer-term projects to improve particular aspects of my life (I maintain a list of possible improvements, roughly sorted by goodness). Examples: buying new furniture, improvements to my diet (Holden's&nbsp;<a href=\"http://powersmoothie.org/power-smoothie-recipes/#PeanutButter\">powersmoothie</a>&nbsp;is great), improvements to my sleep (low doses of melatonin seem good). At the moment the list of possible improvements is long enough that adding to the list is less valuable than doing things on the list.</p>\n<p>I have equivocated a lot about how much of my time should go into this sort of thing. My best guess is the number should be higher.</p>\n<h2>-Pomodoros:</h2>\n<p>I don't use pomodoros at all any more. I still have periods of uninterrupted work, often of comparable length, for individual tasks. This change wasn't extremely carefully considered, it mostly just happened. I find explicit time logging (such that I must consciously change the timer before changing tasks) seems to work as a substitute in many cases. I also maintain the habit of writing down candidate distractions and then attending to them later (if at all).</p>\n<p>For larger tasks I find that I often prefer longer blocks of unrestricted working time. I continue to use Alinof timer to manage these blocks of uninterrupted work.</p>\n<h2>-Catch:</h2>\n<p>Catch disappeared, and I haven't found a replacement that I find comparably useful. (It's also not that high on the list of priorities.) I now just send emails to myself, but I do it much less often.</p>\n<h2>-Beeminder:</h2>\n<p>I no longer use beeminder. This again wasn't super-considered, though it was based on a very rough impression of overhead being larger than the short-term gains. I think beeminder was helpful for setting up a number of habits which have persisted (especially with respect to daily routine and regular focused work), and my long-term averages continue to satisfy my old beeminder goals.</p>\n<h2>Project outlines:</h2>\n<p>I now organize notes about each project I am working on in a more standardized way, with \"Queue of todos,\" \"Current workspace,\" and \"Data\" as the three subsections. I'm not thrilled by this system, but it seems to be an improvement over the previous informal arrangement. In particular, having a workspace into which I can easily write thoughts without thinking about where they fit, and only later sorting them into the data section once it's clearer how they fit in, decreases the activation energy of using the system. I now use Toggl rather than maintaining time logs by hand.</p>\n<h2>Randomized trials:</h2>\n<p>As described in my last post I tried various randomized trials (esp. of effects of exercise, stimulant use, and sleep on mood, cognitive performance, and productive time). I have found extracting meaningful data from these trials to be extremely difficult, due to straightforward issues with signal vs. noise. There are a number of tests which I still do expect to yield meaningful data, but I've increased my estimates for the expensiveness of useful tests substantially, and they've tended to fall down the priority list. For some things I've just decided to do them without the data, since my best guess is positive in expectation and the data is too expensive to acquire.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DDDNqq7QuwoseDvgf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 45, "extendedScore": null, "score": 0.000124, "legacy": true, "legacyId": "27032", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>About 18 months ago I made a <a href=\"/lw/fux/my_workflow/\">post here</a> on my workflow. I've received a handful of requests for follow-up, so I thought I would make another post detailing changes since then. I expect this post to be less useful than the last one.</p>\n<p>For the most part, the overall outline has remained pretty stable and feels very similar to 18 months ago. Things not mentioned below have mostly stayed the same. I believe that the total effect of continued changes have been continued but much smaller improvements, though it is hard to tell (as opposed to the last changes, which were more clearly improvements).</p>\n<p>Based on comparing time logging records I seem to now do substantially more work on average, but there are many other changes during this period that could explain the change (including changes in time logging). Changes other than work output are much harder to measure; I feel like they are positive but I wouldn't be surprised if this were an illusion.</p>\n<h2 id=\"Splitting_days_\">Splitting days:</h2>\n<p>I now regularly divide my day into two halves, and treat the two halves as separate units. I plan each separately and reflect on each separately. I divide them by an hour long period of reflecting on the morning, relaxing for 5-10 minutes, napping for 25-30 minutes, processing my emails, and planning the evening. I find that this generally makes me more productive and happier about the day. Splitting my days is often difficult due to engagements in the middle of the day, and I don't have a good solution to that.</p>\n<h2 id=\"WasteNoTime_\">WasteNoTime:</h2>\n<p>I have longstanding objections to explicitly rationing internet use (since it seems either indicative of a broader problem that should be resolved directly, or else to serve a useful function that would be unwise to remove). That said, I now use the extension WasteNoTime to limit my consumption of blogs, webcomics, facebook, news sites, browser games, etc., to 10 minutes each half-day. This has cut the amount of time I spend browsing the internet from an average of 30-40 minutes to an average of 10-15 minutes. It doesn't seem to have been replaced by lower-quality leisure, but by a combination of work and higher-quality leisure.</p>\n<p>Similarly, I <a href=\"https://chrome.google.com/webstore/detail/kill-news-feed/hjobfcedfgohjkaieocljfcppjbkglfd?hl=en\">turned off the newsfeed</a> in facebook, which I found to improve the quality of my internet time in general (the primary issue was that I would sometimes be distracted by the newsfeed while sending messages over facebook, which wasn't my favorite way to use up wastenotime minutes).</p>\n<p>I also tried StayFocusd, but ended up adopting WasteNoTime because of the ability to set limits per half-day (via \"At work\" and \"not at work\" timers) rather than per-day. I find that the main upside is cutting off the tail of derping (e.g. getting sucked into a blog comment thread, or looking into a particularly engrossing issue), and for this purpose per half-day timers are much more effective.</p>\n<h2 id=\"Email_discipline_\">Email discipline:</h2>\n<p>I set gmail to archive all emails on arrival and assign them the special label \"In.\" This lets me to search for emails and compose emails, using the normal gmail interface, without being notified of new arrivals. I process the items with label \"in\" (typically turning emails into todo items to be processed by the same system that deals with other todo items) at the beginning of each half day. Each night I scan my email quickly for items that require urgent attention.&nbsp;</p>\n<h2 id=\"Todo_lists___reminders_\">Todo lists / reminders:</h2>\n<p>I continue to use todo lists for each half day and for a range of special conditions. I now check these lists at the beginning of each half day rather than before going to bed.</p>\n<p>I also maintain a third list of \"reminders.\" These are things that I want to be reminded of periodically, organized by day; each morning I look at the day's reminders and think about them briefly. Each of them is copied and filed under a future day. If I feel like I remember a thing well I file it in far in the future, if I feel like I don't remember it well I file it in the near future.</p>\n<p>Over the last month most of these reminders have migrated to be in the form \"If X, then Y,\" e.g. \"If I agree to do something for someone, then pause, say `actually I should think about it for a few minutes to make sure I have time,' and set a 5 minute timer that night to think about it more clearly.\" These are designed to fix problems that I notice when reflecting on the day. This is a recommendation from CFAR folks, which seems to be working well, though is the newest part of the system and least tested.</p>\n<h2 id=\"Isolating__todos__\">Isolating \"todos\":</h2>\n<p>I now attempt to isolate things that probably need doing, but don't seem maximally important; I aim to do them only on every 5th day, and only during one half-day. If I can't finish them in this time, I will typically delay them 5 days. When they spill over to other days, I try to at least keep them to one half-day or the other. I don't know if this helps, but it feels better to have isolated unproductive-feeling blocks of time rather than scattering it throughout the week.</p>\n<p>I don't do this very rigidly. I expect the overall level of discipline I have about it is comparable to or lower than a normal office worker who has a clearer division between their personal time and work time.</p>\n<h2 id=\"Toggl_\">Toggl:</h2>\n<p>I now use&nbsp;<a href=\"https://www.toggl.com/\">Toggl</a>&nbsp;for detailed time tracking. Katja Grace and I experimented with about half a dozen other systems (Harvest, Yast, Klok, Freckle, Lumina, I expect others I'm forgetting) before settling on Toggl. It has a depressing number of flaws, but ends up winning for me by making it very fast to start and switch timers which is probably the most important criterion for me. It also offers reviews that work out well with what I want to look at.</p>\n<p>I find the main value adds from detailed time tracking are:</p>\n<p>1. Knowing how long I've spent on projects, especially long-term projects. My intuitive estimates are often off by more than a factor of 2, even for things taking 80 hours; this can lead me to significantly underestimate the costs of taking on some kinds of projects, and it can also lead me to think an activity is unproductive instead of productive by overestimating how long I've actually spent on it.</p>\n<p>2. Accurate breakdowns of time in a day, which guide efforts at improving my day-to-day routine. They probably also make me feel more motivated about working, and improve focus during work.</p>\n<h2 id=\"Reflection___improvement_\">Reflection / improvement:</h2>\n<p>Reflection is now a smaller fraction of my time, down from 10% to 3-5%, based on diminishing returns to finding stuff to improve. Another 3-5% is now redirected into longer-term projects to improve particular aspects of my life (I maintain a list of possible improvements, roughly sorted by goodness). Examples: buying new furniture, improvements to my diet (Holden's&nbsp;<a href=\"http://powersmoothie.org/power-smoothie-recipes/#PeanutButter\">powersmoothie</a>&nbsp;is great), improvements to my sleep (low doses of melatonin seem good). At the moment the list of possible improvements is long enough that adding to the list is less valuable than doing things on the list.</p>\n<p>I have equivocated a lot about how much of my time should go into this sort of thing. My best guess is the number should be higher.</p>\n<h2 id=\"_Pomodoros_\">-Pomodoros:</h2>\n<p>I don't use pomodoros at all any more. I still have periods of uninterrupted work, often of comparable length, for individual tasks. This change wasn't extremely carefully considered, it mostly just happened. I find explicit time logging (such that I must consciously change the timer before changing tasks) seems to work as a substitute in many cases. I also maintain the habit of writing down candidate distractions and then attending to them later (if at all).</p>\n<p>For larger tasks I find that I often prefer longer blocks of unrestricted working time. I continue to use Alinof timer to manage these blocks of uninterrupted work.</p>\n<h2 id=\"_Catch_\">-Catch:</h2>\n<p>Catch disappeared, and I haven't found a replacement that I find comparably useful. (It's also not that high on the list of priorities.) I now just send emails to myself, but I do it much less often.</p>\n<h2 id=\"_Beeminder_\">-Beeminder:</h2>\n<p>I no longer use beeminder. This again wasn't super-considered, though it was based on a very rough impression of overhead being larger than the short-term gains. I think beeminder was helpful for setting up a number of habits which have persisted (especially with respect to daily routine and regular focused work), and my long-term averages continue to satisfy my old beeminder goals.</p>\n<h2 id=\"Project_outlines_\">Project outlines:</h2>\n<p>I now organize notes about each project I am working on in a more standardized way, with \"Queue of todos,\" \"Current workspace,\" and \"Data\" as the three subsections. I'm not thrilled by this system, but it seems to be an improvement over the previous informal arrangement. In particular, having a workspace into which I can easily write thoughts without thinking about where they fit, and only later sorting them into the data section once it's clearer how they fit in, decreases the activation energy of using the system. I now use Toggl rather than maintaining time logs by hand.</p>\n<h2 id=\"Randomized_trials_\">Randomized trials:</h2>\n<p>As described in my last post I tried various randomized trials (esp. of effects of exercise, stimulant use, and sleep on mood, cognitive performance, and productive time). I have found extracting meaningful data from these trials to be extremely difficult, due to straightforward issues with signal vs. noise. There are a number of tests which I still do expect to yield meaningful data, but I've increased my estimates for the expensiveness of useful tests substantially, and they've tended to fall down the priority list. For some things I've just decided to do them without the data, since my best guess is positive in expectation and the data is too expensive to acquire.</p>\n<p>&nbsp;</p>", "sections": [{"title": "Splitting days:", "anchor": "Splitting_days_", "level": 1}, {"title": "WasteNoTime:", "anchor": "WasteNoTime_", "level": 1}, {"title": "Email discipline:", "anchor": "Email_discipline_", "level": 1}, {"title": "Todo lists / reminders:", "anchor": "Todo_lists___reminders_", "level": 1}, {"title": "Isolating \"todos\":", "anchor": "Isolating__todos__", "level": 1}, {"title": "Toggl:", "anchor": "Toggl_", "level": 1}, {"title": "Reflection / improvement:", "anchor": "Reflection___improvement_", "level": 1}, {"title": "-Pomodoros:", "anchor": "_Pomodoros_", "level": 1}, {"title": "-Catch:", "anchor": "_Catch_", "level": 1}, {"title": "-Beeminder:", "anchor": "_Beeminder_", "level": 1}, {"title": "Project outlines:", "anchor": "Project_outlines_", "level": 1}, {"title": "Randomized trials:", "anchor": "Randomized_trials_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 14}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["E4gud47NNqgtsEeEr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-26T18:00:31.094Z", "modifiedAt": null, "url": null, "title": "Reverse engineering of belief structures", "slug": "reverse-engineering-of-belief-structures", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:06.982Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stefan_Schubert", "createdAt": "2013-12-26T16:42:04.883Z", "isAdmin": false, "displayName": "Stefan_Schubert"}, "userId": "6omuoq9oQuy3KQzG9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ek5kGPyMCMsi7H4DH/reverse-engineering-of-belief-structures", "pageUrlRelative": "/posts/ek5kGPyMCMsi7H4DH/reverse-engineering-of-belief-structures", "linkUrl": "https://www.lesswrong.com/posts/ek5kGPyMCMsi7H4DH/reverse-engineering-of-belief-structures", "postedAtFormatted": "Tuesday, August 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reverse%20engineering%20of%20belief%20structures&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReverse%20engineering%20of%20belief%20structures%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fek5kGPyMCMsi7H4DH%2Freverse-engineering-of-belief-structures%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reverse%20engineering%20of%20belief%20structures%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fek5kGPyMCMsi7H4DH%2Freverse-engineering-of-belief-structures", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fek5kGPyMCMsi7H4DH%2Freverse-engineering-of-belief-structures", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1390, "htmlBody": "<p style=\"margin-bottom: 0.825em;\"><em>(Cross-posted from <a href=\"http://stefanschubert.wordpress.com/2014/08/26/reverse-engineering-of-belief-structures/\">my blog</a>.)</em></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif; color: #222222;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px;\">Since some belief-forming processes are more reliable than others, learning by what processes different beliefs were formed is for several reasons very useful. Firstly, if we learn that someone's belief that p (where p is a <a href=\"http://en.wikipedia.org/wiki/Proposition\">proposition</a> such as \"the cat is on the mat\") was formed a reliable process, such as visual observation under ideal circumstances, we have reason to believe that p is probably true. Conversely, if we learn that the belief that p was formed by an unreliable process, such as <a href=\"http://en.wikipedia.org/wiki/Motivated_reasoning\">motivated reasoning</a>, we have no particular reason to believe that p is true (though it might be - by luck, as it were). Thus we can use knowledge about the process that gave rise to the belief that p to evaluate the chance that p is true.</span></span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">Secondly, we can use knowledge about belief-forming processes in our search for knowledge. If we learn that some alleged expert's beliefs are more often than not caused by unreliable processes, we are better off looking for other sources of knowledge. Or, if we learn that the beliefs we acquire under certain circumstances - say under emotional stress - tend to be caused by unreliable processes such as wishful thinking, we should cease to acquire beliefs under those circumstances.</span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">Thirdly, we can use knowledge about others' belief-forming processes to try to improve them. For instance, if it turns out that a famous scientist has used outdated methods to arrive at their experimental results, we can announce this publically. Such \"shaming\" can be a very effective means to scare people to use more reliable methods, and will typically not only have an effect on the shamed person, but also on others who learn about the case. (Obviously, shaming also has its disadvantages, but my impression is that it has played a very important historical role in the spreading of reliable scientific methods.)</span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif; color: #222222;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px;\">A useful way of inferring by what process a set of beliefs was formed is by looking at its structure. This is a very general method, but in this post I will focus on how we can infer that a certain set of beliefs most probably was formed by (politically) motivated cognition. Another use <a href=\"/lw/kpj/multiple_factor_explanations_should_not_appear/\">is covered here</a> and more will follow in future posts.</span></span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">Let me give two examples. Firstly, suppose that we give American voters the following four questions:</span></p>\n<ol>\n<li><span style=\"color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14.399999618530273px; line-height: 23.799999237060547px;\">Do expert scientists mostly agree that genetically modified foods are safe?</span></li>\n<li><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">Do expert scientists mostly agree that radioactive wastes from nuclear power can be safely disposed of in deep underground storage facilities?</span></li>\n<li><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">Do expert scientists mostly agree that global temperatures are rising due to human activities?</span></li>\n<li><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">Do expert scientists mostly agree that the \"intelligent design\" theory is false?</span></li>\n</ol>\n<p><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">The answer to all of these questions is \"yes\".* Now suppose that a disproportionate number of republicans answer \"yes\" to the first two questions, and \"no\" to the third and the fourth questions, and that a disproportionate number of democrats answer \"no\" to the first two questions, and \"yes\" to the third and the fourth questions. In the light of what we know about motivated cognition, these are very suspicious patterns or structures of beliefs, since that it is precisely the patterns we would expect them to arrive at given the hypothesis that they'll acquire whatever belief on empirical questions that suit their political preferences. Since no other plausibe hypothesis seem to be able to explain these patterns as well, this confirms this hypothesis. (Obviously, if we were to give the voters more questions and their answers would retain their one-sided structure, that would confirm the hypothesis even stronger.)</span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">Secondly, consider a policy question - say minimum wages - on which a number of empirical claims have bearing. For instance, these empirical claims might be that minimum wages significantly decrease employers' demand for new workers, that they cause inflation and that they significantly reduce workers' tendency to use public services (since they now earn more). Suppose that there are five such claims which tell in favour of minimum wages and five that tell against them, and that you think that each of them has a roughly 50 % chance of being true. Also, suppose that they are probabilistically independent of each other, so that learning that one of them is true does not affect the probabilities of the other claims.</span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">Now suppose that in a debate, all proponents of minimum wages defend all of the claims that tell in favour of minimum wages, and reject all of the claims that tell against them, and vice versa for the opponents of minimum wages. Now this is a very surprising pattern. It might of course be that one side is right across the board, but given your prior probability distribution (that the claims are independent and have a 50 % probability of being true) a more reasonable interpretation of the striking degree of coherence within both sides is, according to your lights, that they are both biased; that they are both using motivated cognition. <a href=\"/lw/gz/policy_debates_should_not_appear_onesided/\">(See also this post for more on this line of reasoning.)</a></span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">The difference between the first and the second case is that in the former, your hypothesis that the test-takers are biased is based on the fact that they are provably wrong on certain questions, whereas in the second case, you cannot point to any issue where any of the sides is provably wrong. However, the patterns of their claims are so improbable given the hypothesis that they have reviewed the evidence impartially, and so likely given the hypothesis of bias, that they nevertheless strongly confirms the latter. What they are saying is simply \"too good to be true\".</span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif; color: #222222;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px;\"><br /></span></span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">These kinds of arguments, in which you infer a belief-forming process from a structure of beliefs (i.e you reverse engineer the beliefs), have of course always been used. (A salient example is Marxist interpretations of \"bourgeois\" belief structures, which, Marx argued, supported their material interests to a suspiciously high degree.) Recent years have, however, seen a number of developments that should make them less speculative and more reliable and useful.</span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">Firstly, psychological research such as <a href=\"http://en.wikipedia.org/wiki/Amos_Tversky\">Tversky</a> and <a href=\"http://en.wikipedia.org/wiki/Daniel_Kahneman\">Kahneman's</a> has given us a much better picture of the mechanisms by which we acquire beliefs. Experiments have shown that we fall prey to an astonishing <a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\">list of biases</a> and identified which circumstances that are most likely to trigger them.&nbsp;</span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">Secondly, a much greater portion of our behaviour is now being recorded, especially on the Internet (where we spend an increasing share of our time). This obviously makes it much easier to spot suspicious patterns of beliefs.</span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">Thirdly, our algorithms for analyzing behaviour are quickly improving. <a href=\"http://www.businessinsider.com/five-labs-facebook-personality-test-2014-6\">FiveLabs recently launched a tool that analyzes</a> your big five personality traits on the basis of your Facebook posts. Granted, this tool does not seem completely accurate, and inferring bias promises to be a harder task (since the <a href=\"http://www.washingtonpost.com/news/morning-mix/wp/2014/06/16/creepy-facebook-personality-test-is-a-dream-for-advertisers-nightmare-for-privacy/\">correlations are more complicated than that between usage of exclamation marks and extraversion, or that betwen using words such as \"nightmare\" and \"sick of\" and neuroticism</a>). Nevertheless, better algorithms and more computer power will take us in the right direction.</span></p>\n<p style=\"margin-bottom: 0.825em;\">&nbsp;</p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif; color: #222222;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px;\">In my view, there is thus a large untapped potential to infer bias from the structure of people's beliefs, which in turn would be inferred from their online behaviour. In coming posts, I intend to flesh out my ideas on this in some more details. Any comments are welcome and might be incorporated in future posts.</span></span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif; color: #222222;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px;\">* The second and the third questions are taken from <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1549444\">a paper by Dan Kahan et al</a>, which refers to the US&nbsp;<a href=\"http://en.wikipedia.org/wiki/National_Academy_of_Sciences\">National Academy of Sciences (NAS)</a> assessment of expert scientists' views on these questions. Their study shows that many conservatives don't believe that experts agree on climate change, whereas a fair number of liberals think experts don't agree that nuclear storage is safe, confirming the hypothesis that people let their political preferences influence their empirical beliefs. The assessment of expert consensus on the <a href=\"http://en.wikipedia.org/wiki/Genetically_modified_food_controversies\">first</a> and <a href=\"http://en.wikipedia.org/wiki/List_of_scientific_bodies_explicitly_rejecting_Intelligent_design\">fourth</a> question are taken from Wikipedia.</span></span></p>\n<p style=\"margin-bottom: 0.825em;\"><span style=\"font-size: 14.399999618530273px; line-height: 23.799999237060547px; color: #222222; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;\">Asking people what they think about the expert consensus on these issues, rather than about the issues themselves, is good idea, since it's much easier to come to an agreement on what the true answer is on the former sort of question. (Of course, you can deny that professors from prestigious universities count as expert scientists, but that would be a quite extreme position that few people hold.)&nbsp;</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ek5kGPyMCMsi7H4DH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "27033", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YgbJq4WrZWXe5GvFY", "PeSzc9JTBxhaYRp9b"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-26T18:55:43.420Z", "modifiedAt": null, "url": null, "title": "[LINK] Could a Quantum Computer Have Subjective Experience?", "slug": "link-could-a-quantum-computer-have-subjective-experience", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:06.719Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7ZBH7b2okoqS2sf3h/link-could-a-quantum-computer-have-subjective-experience", "pageUrlRelative": "/posts/7ZBH7b2okoqS2sf3h/link-could-a-quantum-computer-have-subjective-experience", "linkUrl": "https://www.lesswrong.com/posts/7ZBH7b2okoqS2sf3h/link-could-a-quantum-computer-have-subjective-experience", "postedAtFormatted": "Tuesday, August 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Could%20a%20Quantum%20Computer%20Have%20Subjective%20Experience%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Could%20a%20Quantum%20Computer%20Have%20Subjective%20Experience%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ZBH7b2okoqS2sf3h%2Flink-could-a-quantum-computer-have-subjective-experience%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Could%20a%20Quantum%20Computer%20Have%20Subjective%20Experience%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ZBH7b2okoqS2sf3h%2Flink-could-a-quantum-computer-have-subjective-experience", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ZBH7b2okoqS2sf3h%2Flink-could-a-quantum-computer-have-subjective-experience", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 305, "htmlBody": "<p>Yet another exceptionally interesting <a href=\"http://www.scottaaronson.com/blog/?p=1951\">blog post</a> by Scott Aaronson, describing his talk at the&nbsp;Quantum Foundations of a Classical Universe workshop, videos of which should be posted soon. Despite the disclaimer \"<strong>My talk is for entertainment purposes only; it should not be taken seriously by anyone</strong>\", it raises several serious and semi-serious points about the nature of conscious experience and related paradoxes, which are generally overlooked by the philosophers, including Eliezer, because they have no relevant CS/QC expertise. For example:</p>\n<ul>\n<li>Is an&nbsp;<a href=\"http://en.wikipedia.org/wiki/Homomorphic_encryption#Fully_homomorphic_encryption\">FHE-encrypted</a>&nbsp;sim with a lost key conscious?</li>\n<li>If you \"untorture\" a reversible simulation, did it happen? What does the untorture feel like?</li>\n<li>Is Vaidman brain conscious? (You have to read the blog post to learn what it is, not going to spoil it.)</li>\n</ul>\n<p>Scott also suggests a model of consciousness which sort-of resolves the issues of cloning, identity and such, by introducing what he calls a \"digital abstraction layer\" (again, read the blog post to understand what he means by that). Our brains might be lacking such a layer and so be \"fundamentally unclonable\".&nbsp;</p>\n<p>Another interesting observation is that you never actually kill the cat in the Schroedinger's cat experiment, for a reasonable definition of \"kill\".</p>\n<p>There are several more mind-blowing insights in this \"entertainment purposes\" post/talk, related to the existence of p-zombies, consciousness of Boltzmann brains, the observed large-scale structure of the Universe and the \"reality\" of Tegmark IV.</p>\n<p>I certainly got the humbling experience that Scott is <a href=\"/lw/ua/the_level_above_mine/\">the level above mine</a>, and I would like to know if other people did, too.</p>\n<p>Finally, the standard <a href=\"/lw/h8n/litany_of_a_bright_dilettante/\">bright dilettante</a> caveat applies: if you think up a quick objection to what an expert in the area argues, and you yourself are not such an expert, the odds are extremely heavy that this objection is either silly or has been considered and addressed by the expert already.&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7ZBH7b2okoqS2sf3h", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 27, "extendedScore": null, "score": 1.956942316674124e-06, "legacy": true, "legacyId": "27034", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kXSETKZ3X9oidMozA", "SgGYcTmtLMu4rYnY9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-27T02:47:14.264Z", "modifiedAt": null, "url": null, "title": "Meetup : Canberra: Akrasia-busters!", "slug": "meetup-canberra-akrasia-busters", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielFilan", "createdAt": "2014-01-30T11:04:39.341Z", "isAdmin": false, "displayName": "DanielFilan"}, "userId": "DgsGzjyBXN8XSK22q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RQbpnJbfecLS9einW/meetup-canberra-akrasia-busters", "pageUrlRelative": "/posts/RQbpnJbfecLS9einW/meetup-canberra-akrasia-busters", "linkUrl": "https://www.lesswrong.com/posts/RQbpnJbfecLS9einW/meetup-canberra-akrasia-busters", "postedAtFormatted": "Wednesday, August 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Canberra%3A%20Akrasia-busters!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Canberra%3A%20Akrasia-busters!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRQbpnJbfecLS9einW%2Fmeetup-canberra-akrasia-busters%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Canberra%3A%20Akrasia-busters!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRQbpnJbfecLS9einW%2Fmeetup-canberra-akrasia-busters", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRQbpnJbfecLS9einW%2Fmeetup-canberra-akrasia-busters", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 142, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13m'>Canberra: Akrasia-busters!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 September 2014 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">108 North Road, Acton, ACT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Alexa will give a talk about using rewards to fight akrasia. Where is it better to set up external rewards and where should we focus on intrinsic motivation? Techniques include making short-term actions feel related to long-term ones as well as actually being so, to increase motivation and reduce boredom and apathy, and using intrinsic motivation to stop worrying about whether an instrumental action will work instead of just doing it.</p>\n\n<p>General meetup info:</p>\n\n<p>If you use Facebook, please join our group: <a href=\"https://www.facebook.com/groups/lwcanberra/\" rel=\"nofollow\">https://www.facebook.com/groups/lwcanberra/</a></p>\n\n<p>Structured meetups are held on the second Saturday and fourth Friday of each month from 6 pm until late in the CSIT building, room N101 (except for this meetup, which will be held in Bruce Hall).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13m'>Canberra: Akrasia-busters!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RQbpnJbfecLS9einW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.9577550169502532e-06, "legacy": true, "legacyId": "27037", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Canberra__Akrasia_busters_\">Discussion article for the meetup : <a href=\"/meetups/13m\">Canberra: Akrasia-busters!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 September 2014 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">108 North Road, Acton, ACT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Alexa will give a talk about using rewards to fight akrasia. Where is it better to set up external rewards and where should we focus on intrinsic motivation? Techniques include making short-term actions feel related to long-term ones as well as actually being so, to increase motivation and reduce boredom and apathy, and using intrinsic motivation to stop worrying about whether an instrumental action will work instead of just doing it.</p>\n\n<p>General meetup info:</p>\n\n<p>If you use Facebook, please join our group: <a href=\"https://www.facebook.com/groups/lwcanberra/\" rel=\"nofollow\">https://www.facebook.com/groups/lwcanberra/</a></p>\n\n<p>Structured meetups are held on the second Saturday and fourth Friday of each month from 6 pm until late in the CSIT building, room N101 (except for this meetup, which will be held in Bruce Hall).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Canberra__Akrasia_busters_1\">Discussion article for the meetup : <a href=\"/meetups/13m\">Canberra: Akrasia-busters!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Canberra: Akrasia-busters!", "anchor": "Discussion_article_for_the_meetup___Canberra__Akrasia_busters_", "level": 1}, {"title": "Discussion article for the meetup : Canberra: Akrasia-busters!", "anchor": "Discussion_article_for_the_meetup___Canberra__Akrasia_busters_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-27T10:42:14.690Z", "modifiedAt": null, "url": null, "title": "Meetup : Regular Moscow Meetup", "slug": "meetup-regular-moscow-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexander230", "createdAt": "2014-08-27T08:55:16.153Z", "isAdmin": false, "displayName": "Alexander230"}, "userId": "xqoKSJayCCtP5juLh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SDTcA4fCjyLDqNAss/meetup-regular-moscow-meetup", "pageUrlRelative": "/posts/SDTcA4fCjyLDqNAss/meetup-regular-moscow-meetup", "linkUrl": "https://www.lesswrong.com/posts/SDTcA4fCjyLDqNAss/meetup-regular-moscow-meetup", "postedAtFormatted": "Wednesday, August 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Regular%20Moscow%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Regular%20Moscow%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSDTcA4fCjyLDqNAss%2Fmeetup-regular-moscow-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Regular%20Moscow%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSDTcA4fCjyLDqNAss%2Fmeetup-regular-moscow-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSDTcA4fCjyLDqNAss%2Fmeetup-regular-moscow-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13n'>Regular Moscow Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 August 2014 02:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Here's our plan:</p>\n\n<ul>\n<li>A talk about Symbolic self-completion theory.</li>\n<li>A talk about Binaural beats.</li>\n<li>Training game.</li>\n<li>A talk about How to measure anything.</li>\n<li>Debates on Poppers' protocol.</li>\n</ul>\n\n<p>Details and schedule:\n<a href=\"https://lesswrong-ru.hackpad.com/-31--Ha1ZZZn9dPk\" rel=\"nofollow\">https://lesswrong-ru.hackpad.com/-31--Ha1ZZZn9dPk</a></p>\n\n<p>Yudcoins, positive reinforcement and pizza will all be present. If you've been to our meetups, you know what I'm talking about, and if you didn't, the best way to find out is to come and see for yourself.</p>\n\n<p>Info for newcomers:\nWe gather in the Yandex office, you need the first revolving door under the archway. Here is a guide how to get there:\n<a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">http://company.yandex.ru/contacts/redrose/</a></p>\n\n<p>Call Slava at +7(926)313-96-42 if you're late.\nWe start at 14:00 and stay until at least 19-20. Please pay attention that we only gather near the entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13n'>Regular Moscow Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SDTcA4fCjyLDqNAss", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.958574341754654e-06, "legacy": true, "legacyId": "27040", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Regular_Moscow_Meetup\">Discussion article for the meetup : <a href=\"/meetups/13n\">Regular Moscow Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 August 2014 02:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Here's our plan:</p>\n\n<ul>\n<li>A talk about Symbolic self-completion theory.</li>\n<li>A talk about Binaural beats.</li>\n<li>Training game.</li>\n<li>A talk about How to measure anything.</li>\n<li>Debates on Poppers' protocol.</li>\n</ul>\n\n<p>Details and schedule:\n<a href=\"https://lesswrong-ru.hackpad.com/-31--Ha1ZZZn9dPk\" rel=\"nofollow\">https://lesswrong-ru.hackpad.com/-31--Ha1ZZZn9dPk</a></p>\n\n<p>Yudcoins, positive reinforcement and pizza will all be present. If you've been to our meetups, you know what I'm talking about, and if you didn't, the best way to find out is to come and see for yourself.</p>\n\n<p>Info for newcomers:\nWe gather in the Yandex office, you need the first revolving door under the archway. Here is a guide how to get there:\n<a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">http://company.yandex.ru/contacts/redrose/</a></p>\n\n<p>Call Slava at +7(926)313-96-42 if you're late.\nWe start at 14:00 and stay until at least 19-20. Please pay attention that we only gather near the entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Regular_Moscow_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/13n\">Regular Moscow Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Regular Moscow Meetup", "anchor": "Discussion_article_for_the_meetup___Regular_Moscow_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Regular Moscow Meetup", "anchor": "Discussion_article_for_the_meetup___Regular_Moscow_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-27T20:11:30.657Z", "modifiedAt": null, "url": null, "title": "Meetup : London Meetup - Effective Altruism", "slug": "meetup-london-meetup-effective-altruism", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kerspoon", "createdAt": "2011-12-27T09:53:49.512Z", "isAdmin": false, "displayName": "kerspoon"}, "userId": "XvZ9yyyJNeDwWhECW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oz92DXZniEhbYiL8R/meetup-london-meetup-effective-altruism", "pageUrlRelative": "/posts/oz92DXZniEhbYiL8R/meetup-london-meetup-effective-altruism", "linkUrl": "https://www.lesswrong.com/posts/oz92DXZniEhbYiL8R/meetup-london-meetup-effective-altruism", "postedAtFormatted": "Wednesday, August 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Meetup%20-%20Effective%20Altruism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Meetup%20-%20Effective%20Altruism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foz92DXZniEhbYiL8R%2Fmeetup-london-meetup-effective-altruism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Meetup%20-%20Effective%20Altruism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foz92DXZniEhbYiL8R%2Fmeetup-london-meetup-effective-altruism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foz92DXZniEhbYiL8R%2Fmeetup-london-meetup-effective-altruism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 204, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13o'>London Meetup - Effective Altruism</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 August 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">The Shakespeare Inn, Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LessWrong London are having another awesome meetup and we would love for you to come!</p>\n\n<p>Since I have recently attended the UK and the USA effective altruism summit (<a href=\"http://www.effectivealtruismsummit.com/\" rel=\"nofollow\">http://www.effectivealtruismsummit.com/</a>) I thought that could be the topic of this event. I'll try and answer questions about it and say what went on when we gathered a load of charities together for a long weekend. I'm sure some people will want to split off and chat about other things and that is fine too.</p>\n\n<p>The plan is to meet at The Shakespeare Inn, 200m from Holborn Underground at 2pm on Sunday 31th August. We will officially finish at 4pm but many people stay longer. We will have a bit of paper with the LessWrong logo on it so you can find us easily.</p>\n\n<p>If you have any questions, or are thinking of coming, feel free to email me (james) at kerspoon+lw@gmail.com or call me on 07429552244. Otherwise, just turn up!</p>\n\n<p>Hope to see you there,\nJames</p>\n\n<p>P.S err on the side of turning-up, we're friendly, and it's fun :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13o'>London Meetup - Effective Altruism</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oz92DXZniEhbYiL8R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.959557050567383e-06, "legacy": true, "legacyId": "27041", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Meetup___Effective_Altruism\">Discussion article for the meetup : <a href=\"/meetups/13o\">London Meetup - Effective Altruism</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 August 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">The Shakespeare Inn, Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LessWrong London are having another awesome meetup and we would love for you to come!</p>\n\n<p>Since I have recently attended the UK and the USA effective altruism summit (<a href=\"http://www.effectivealtruismsummit.com/\" rel=\"nofollow\">http://www.effectivealtruismsummit.com/</a>) I thought that could be the topic of this event. I'll try and answer questions about it and say what went on when we gathered a load of charities together for a long weekend. I'm sure some people will want to split off and chat about other things and that is fine too.</p>\n\n<p>The plan is to meet at The Shakespeare Inn, 200m from Holborn Underground at 2pm on Sunday 31th August. We will officially finish at 4pm but many people stay longer. We will have a bit of paper with the LessWrong logo on it so you can find us easily.</p>\n\n<p>If you have any questions, or are thinking of coming, feel free to email me (james) at kerspoon+lw@gmail.com or call me on 07429552244. Otherwise, just turn up!</p>\n\n<p>Hope to see you there,\nJames</p>\n\n<p>P.S err on the side of turning-up, we're friendly, and it's fun :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_Meetup___Effective_Altruism1\">Discussion article for the meetup : <a href=\"/meetups/13o\">London Meetup - Effective Altruism</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Meetup - Effective Altruism", "anchor": "Discussion_article_for_the_meetup___London_Meetup___Effective_Altruism", "level": 1}, {"title": "Discussion article for the meetup : London Meetup - Effective Altruism", "anchor": "Discussion_article_for_the_meetup___London_Meetup___Effective_Altruism1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-27T22:52:00.221Z", "modifiedAt": null, "url": null, "title": "Rationalist house", "slug": "rationalist-house", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:37.197Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Elo", "createdAt": "2014-04-17T22:36:25.781Z", "isAdmin": false, "displayName": "Elo"}, "userId": "Qad7jGcRgP4BZMa5F", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EvjRHdJtM6quRd9Cy/rationalist-house", "pageUrlRelative": "/posts/EvjRHdJtM6quRd9Cy/rationalist-house", "linkUrl": "https://www.lesswrong.com/posts/EvjRHdJtM6quRd9Cy/rationalist-house", "postedAtFormatted": "Wednesday, August 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalist%20house&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalist%20house%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEvjRHdJtM6quRd9Cy%2Frationalist-house%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalist%20house%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEvjRHdJtM6quRd9Cy%2Frationalist-house", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEvjRHdJtM6quRd9Cy%2Frationalist-house", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 296, "htmlBody": "<p>\n<p style=\"margin-bottom: 0cm;\">At the Australia online hangout; one of the topics we discussed (before I fell asleep on camera for a bunch of people) Was writing a rationality TV show as an outreach task. &nbsp;Of course there being more ways for this to go wrong than right I figured its worth mentioning the ideas and getting some comments.</p>\n<p style=\"margin-bottom: 0cm;\">The strategy is to have a set of regular characters who's rationality behaviour seems nuts. &nbsp;Effectively sometimes because it is; when taken out of context. &nbsp;Then to have one \"blank\" person who tries to join - \"rationality house\". and work things out. &nbsp;My aim was to have each episode straw man a rationality behaviour and then steelman it. &nbsp;Where by the end of the episode it saves the day; makes someone happy; achieves a goal - or some other &lt;generic win-state&gt;.</p>\n<p style=\"margin-bottom: 0cm;\">Here is a list of notes of characters from the hangout or potential topics to talk about.</p>\n<p style=\"margin-bottom: 0cm;\">\n<ul>\n<li>No showers. Bacterial showers</li>\n<li>Stopwatches everywhere</li>\n<li>temperature controls everywhere, light controls.</li>\n<li>radical honesty person.</li>\n<li>Soylent only eating person</li>\n<li>born-again atheist</li>\n<li>bayesian person</li>\n<li>Polyphasic sleep cycles.</li>\n</ul>\n<div>I have not written much in my life and certainly never anything for TV but it sounds like a fun project. &nbsp;I figured I would pick a pilot idea; roll with it and see if I can make a script. &nbsp;I could probably also get Sydney folk to act for a first-round web-cast version.</div>\n<div><br /></div>\n<div>I was wondering if anyone had any other rationality topics that can be easily strawmanned then steelmanned worth adding to the list. &nbsp;And if anyone had experience worth sharing with writing for TV, as well as anyone interested in joining the project to write or be a sounding board...</div>\n<div><br /></div>\n<div><br /></div>\n</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EvjRHdJtM6quRd9Cy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 11, "extendedScore": null, "score": 1.9598342624758795e-06, "legacy": true, "legacyId": "27042", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-28T19:39:51.866Z", "modifiedAt": null, "url": null, "title": "Hal Finney has just died.", "slug": "hal-finney-has-just-died", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:45.421Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4aNCuqCmPMb6J3yox/hal-finney-has-just-died", "pageUrlRelative": "/posts/4aNCuqCmPMb6J3yox/hal-finney-has-just-died", "linkUrl": "https://www.lesswrong.com/posts/4aNCuqCmPMb6J3yox/hal-finney-has-just-died", "postedAtFormatted": "Thursday, August 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hal%20Finney%20has%20just%20died.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHal%20Finney%20has%20just%20died.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4aNCuqCmPMb6J3yox%2Fhal-finney-has-just-died%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hal%20Finney%20has%20just%20died.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4aNCuqCmPMb6J3yox%2Fhal-finney-has-just-died", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4aNCuqCmPMb6J3yox%2Fhal-finney-has-just-died", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 11, "htmlBody": "<p>Hal Finney has just died.</p>\n<p><a href=\"http://lists.extropy.org/pipermail/extropy-chat/2014-August/082585.html\">http://lists.extropy.org/pipermail/extropy-chat/2014-August/082585.html</a></p>\n<p><a href=\"/lw/1ab/dying_outside/\">http://lesswrong.com/lw/1ab/dying_outside/</a></p>\n<p>I'm very sad to see him go.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 2, "E9ihK6bA9YKkmJs2f": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4aNCuqCmPMb6J3yox", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 63, "extendedScore": null, "score": 0.000177, "legacy": true, "legacyId": "27044", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 63, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bshZiaLefDejvPKuS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-28T23:37:06.430Z", "modifiedAt": null, "url": null, "title": "Calibrating your probability estimates of world events: Russia vs Ukraine, 6 months later.", "slug": "calibrating-your-probability-estimates-of-world-events", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:09.563Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hmSBqMZhNxQv3fAw9/calibrating-your-probability-estimates-of-world-events", "pageUrlRelative": "/posts/hmSBqMZhNxQv3fAw9/calibrating-your-probability-estimates-of-world-events", "linkUrl": "https://www.lesswrong.com/posts/hmSBqMZhNxQv3fAw9/calibrating-your-probability-estimates-of-world-events", "postedAtFormatted": "Thursday, August 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Calibrating%20your%20probability%20estimates%20of%20world%20events%3A%20Russia%20vs%20Ukraine%2C%206%20months%20later.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACalibrating%20your%20probability%20estimates%20of%20world%20events%3A%20Russia%20vs%20Ukraine%2C%206%20months%20later.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhmSBqMZhNxQv3fAw9%2Fcalibrating-your-probability-estimates-of-world-events%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Calibrating%20your%20probability%20estimates%20of%20world%20events%3A%20Russia%20vs%20Ukraine%2C%206%20months%20later.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhmSBqMZhNxQv3fAw9%2Fcalibrating-your-probability-estimates-of-world-events", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhmSBqMZhNxQv3fAw9%2Fcalibrating-your-probability-estimates-of-world-events", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 334, "htmlBody": "<p>Some of the comments on the <a href=\"/lw/js0/link_poking_the_bear_podcast/\">link</a> by James_Miller exactly six months ago provided very specific estimates of how the events might turn out:</p>\n<p>James_Miller:</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">\n<ul>\n<li>The odds of Russian intervening militarily = 40%.</li>\n<li>The odds of the Russians losing the conventional battle (perhaps because of NATO intervention) conditional on them entering = 30%.</li>\n<li>The odds of the Russians resorting to nuclear weapons conditional on them losing the conventional battle = 20%.</li>\n</ul>\n</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">Me:</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify; padding-left: 30px;\">\"Russians intervening militarily\" could be anything from posturing to weapon shipments to a surgical strike to a Czechoslovakia-style tank-roll or Afghanistan invasion. My guess that the odds of the latter is below 5%.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">A bet between James_Miller and solipsist:</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify; padding-left: 30px;\">I will bet you $20 U.S. (mine) vs $100 (yours) that Russian tanks will be involved in combat in the Ukraine within 60 days. So in 60 days I will pay you $20 if I lose the bet, but you pay me $100 if I win.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">While it is hard to do any meaningful calibration based on a single event, there must be lessons to learn from it. Given that&nbsp;<a href=\"http://www.washingtonpost.com/world/russian-and-ukraine-troops-battle-in-south-prompting-fears-of-widescale-invasion/2014/08/28/04b614f4-9a6e-40f4-aa21-4f49104cf0e4_story.html\">Russian armored columns are said to capture key Ukrainian towns</a>&nbsp;today, the first part of&nbsp;<span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\">James_Miller's prediction has come true, even if it took 3 times longer than he estimated. </span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\">Note that even the most pessimistic person in that conversation (James) was probably too optimistic. My estimate of 5% appears way too low in retrospect, and I would probably bump it to 50% for a similar event in the future.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\">Now, given that the first prediction came true, how would one reevaluate the odds of the two further escalations he listed? I still feel that there is no way there will be a \"conventional battle\" between Russia and NATO, but having just been proven wrong makes me doubt my assumptions. If anything, maybe I should give more weight to what James_Miller (or at least Dan Carlin) has to say on the issue. And if I had any skin in the game, I would probably be even more cautious.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\"><br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hmSBqMZhNxQv3fAw9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 27, "extendedScore": null, "score": 0.000115, "legacy": true, "legacyId": "27045", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 165, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7xmsvrX7rmn4HnGXK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-29T02:11:14.866Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup: Lightning Talks", "slug": "meetup-west-la-meetup-lightning-talks-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jzfEms5XFAC9h2DPZ/meetup-west-la-meetup-lightning-talks-0", "pageUrlRelative": "/posts/jzfEms5XFAC9h2DPZ/meetup-west-la-meetup-lightning-talks-0", "linkUrl": "https://www.lesswrong.com/posts/jzfEms5XFAC9h2DPZ/meetup-west-la-meetup-lightning-talks-0", "postedAtFormatted": "Friday, August 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%3A%20Lightning%20Talks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%3A%20Lightning%20Talks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjzfEms5XFAC9h2DPZ%2Fmeetup-west-la-meetup-lightning-talks-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%3A%20Lightning%20Talks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjzfEms5XFAC9h2DPZ%2Fmeetup-west-la-meetup-lightning-talks-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjzfEms5XFAC9h2DPZ%2Fmeetup-west-la-meetup-lightning-talks-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13p'>West LA Meetup: Lightning Talks</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 September 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>How to Find Us: Go into the Del Taco. There will be a Rubik's Cube.\nParking is completely free. There is a sign that claims there is a 45-minute time limit, but it is not enforced.</p>\n\n<p>Discussion: This week we will try a new type of discussion. Everyone attending is encouraged to bring a 5-10 minute presentation (or lead a 5-10 minute discussion) on any rationality topic that they like. You are welcome to attend even if you do not want to bring a topic. There will be a small rationality related prize for those who choose to participate!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13p'>West LA Meetup: Lightning Talks</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jzfEms5XFAC9h2DPZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.9626696335779375e-06, "legacy": true, "legacyId": "27049", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup__Lightning_Talks\">Discussion article for the meetup : <a href=\"/meetups/13p\">West LA Meetup: Lightning Talks</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 September 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>How to Find Us: Go into the Del Taco. There will be a Rubik's Cube.\nParking is completely free. There is a sign that claims there is a 45-minute time limit, but it is not enforced.</p>\n\n<p>Discussion: This week we will try a new type of discussion. Everyone attending is encouraged to bring a 5-10 minute presentation (or lead a 5-10 minute discussion) on any rationality topic that they like. You are welcome to attend even if you do not want to bring a topic. There will be a small rationality related prize for those who choose to participate!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup__Lightning_Talks1\">Discussion article for the meetup : <a href=\"/meetups/13p\">West LA Meetup: Lightning Talks</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup: Lightning Talks", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup__Lightning_Talks", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup: Lightning Talks", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup__Lightning_Talks1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-29T05:17:41.332Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington, D.C.: CFAR Related Discussion (Parkour Postponed)", "slug": "meetup-washington-d-c-cfar-related-discussion-parkour", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinZ", "createdAt": "2009-07-08T20:34:05.168Z", "isAdmin": false, "displayName": "RobinZ"}, "userId": "eTMojvi4f2z3pDfsc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q7AaL4QsDCY8ND2hA/meetup-washington-d-c-cfar-related-discussion-parkour", "pageUrlRelative": "/posts/Q7AaL4QsDCY8ND2hA/meetup-washington-d-c-cfar-related-discussion-parkour", "linkUrl": "https://www.lesswrong.com/posts/Q7AaL4QsDCY8ND2hA/meetup-washington-d-c-cfar-related-discussion-parkour", "postedAtFormatted": "Friday, August 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%2C%20D.C.%3A%20CFAR%20Related%20Discussion%20(Parkour%20Postponed)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%2C%20D.C.%3A%20CFAR%20Related%20Discussion%20(Parkour%20Postponed)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ7AaL4QsDCY8ND2hA%2Fmeetup-washington-d-c-cfar-related-discussion-parkour%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%2C%20D.C.%3A%20CFAR%20Related%20Discussion%20(Parkour%20Postponed)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ7AaL4QsDCY8ND2hA%2Fmeetup-washington-d-c-cfar-related-discussion-parkour", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ7AaL4QsDCY8ND2hA%2Fmeetup-washington-d-c-cfar-related-discussion-parkour", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13q'>Washington, D.C.: CFAR Related Discussion (Parkour Postponed)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 August 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weather permitting, we will be meeting at the Navy Memorial to practice elementary parkour techniques. Otherwise, we will be meeting at the courtyard of the Portrait Gallery to discuss <a href=\"http://rationality.org/\" rel=\"nofollow\">CFAR</a>-related topics.</p>\n\n<p>Edit: The weather forecast includes a 40% chance of thunderstorms and near-certain high temperatures and humidity, so the parkour meetup is officially postponed.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13q'>Washington, D.C.: CFAR Related Discussion (Parkour Postponed)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q7AaL4QsDCY8ND2hA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.9629925769986652e-06, "legacy": true, "legacyId": "27052", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington__D_C___CFAR_Related_Discussion__Parkour_Postponed_\">Discussion article for the meetup : <a href=\"/meetups/13q\">Washington, D.C.: CFAR Related Discussion (Parkour Postponed)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 August 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weather permitting, we will be meeting at the Navy Memorial to practice elementary parkour techniques. Otherwise, we will be meeting at the courtyard of the Portrait Gallery to discuss <a href=\"http://rationality.org/\" rel=\"nofollow\">CFAR</a>-related topics.</p>\n\n<p>Edit: The weather forecast includes a 40% chance of thunderstorms and near-certain high temperatures and humidity, so the parkour meetup is officially postponed.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington__D_C___CFAR_Related_Discussion__Parkour_Postponed_1\">Discussion article for the meetup : <a href=\"/meetups/13q\">Washington, D.C.: CFAR Related Discussion (Parkour Postponed)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington, D.C.: CFAR Related Discussion (Parkour Postponed)", "anchor": "Discussion_article_for_the_meetup___Washington__D_C___CFAR_Related_Discussion__Parkour_Postponed_", "level": 1}, {"title": "Discussion article for the meetup : Washington, D.C.: CFAR Related Discussion (Parkour Postponed)", "anchor": "Discussion_article_for_the_meetup___Washington__D_C___CFAR_Related_Discussion__Parkour_Postponed_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-29T12:46:44.763Z", "modifiedAt": null, "url": null, "title": "Meetup : Bratislava", "slug": "meetup-bratislava", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BXzsCTMPe8w3wb48g/meetup-bratislava", "pageUrlRelative": "/posts/BXzsCTMPe8w3wb48g/meetup-bratislava", "linkUrl": "https://www.lesswrong.com/posts/BXzsCTMPe8w3wb48g/meetup-bratislava", "postedAtFormatted": "Friday, August 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bratislava&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bratislava%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBXzsCTMPe8w3wb48g%2Fmeetup-bratislava%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bratislava%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBXzsCTMPe8w3wb48g%2Fmeetup-bratislava", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBXzsCTMPe8w3wb48g%2Fmeetup-bratislava", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 31, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13r'>Bratislava</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 September 2014 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Bistro The Peach, Mari\u00e1nska 3, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The usual place, the usual time.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13r'>Bratislava</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BXzsCTMPe8w3wb48g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.9637707963648464e-06, "legacy": true, "legacyId": "27055", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bratislava\">Discussion article for the meetup : <a href=\"/meetups/13r\">Bratislava</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 September 2014 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Bistro The Peach, Mari\u00e1nska 3, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The usual place, the usual time.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bratislava1\">Discussion article for the meetup : <a href=\"/meetups/13r\">Bratislava</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bratislava", "anchor": "Discussion_article_for_the_meetup___Bratislava", "level": 1}, {"title": "Discussion article for the meetup : Bratislava", "anchor": "Discussion_article_for_the_meetup___Bratislava1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-29T15:43:55.925Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-10", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:04.338Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aDeaxfLsdK8iwyNdQ/weekly-lw-meetups-10", "pageUrlRelative": "/posts/aDeaxfLsdK8iwyNdQ/weekly-lw-meetups-10", "linkUrl": "https://www.lesswrong.com/posts/aDeaxfLsdK8iwyNdQ/weekly-lw-meetups-10", "postedAtFormatted": "Friday, August 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaDeaxfLsdK8iwyNdQ%2Fweekly-lw-meetups-10%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaDeaxfLsdK8iwyNdQ%2Fweekly-lw-meetups-10", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaDeaxfLsdK8iwyNdQ%2Fweekly-lw-meetups-10", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 533, "htmlBody": "<p><strong>This summary was posted to LessWrong Main on August 22nd. The following week's summary is <a href=\"/lw/kvl/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/13e\">Australia-wide mega-meetup:&nbsp;<span class=\"date\">24 August 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/139\">Hamburg - Diet:&nbsp;<span class=\"date\">29 August 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/zr\">Houston, TX:&nbsp;<span class=\"date\">13 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13d\">Perth, Australia: More Wrong:&nbsp;<span class=\"date\">23 August 2014 12:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/13g\">London Social - August 24th:&nbsp;<span class=\"date\">24 August 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13h\">[Melbourne] September Rationality Dojo - Fixed and Growth Mindset:&nbsp;<span class=\"date\">07 September 2014 03:30PM</span></a></li>\n<li><a href=\"/meetups/12u\">Sydney Meetup - August:&nbsp;<span class=\"date\">27 August 2014 06:30PM</span></a></li>\n<li><a href=\"/meetups/130\">[Utrecht] Cognitive Biases:&nbsp;<span class=\"date\">23 August 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13a\">[Utrecht] Topic to be determinined:&nbsp;<span class=\"date\">06 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13b\">[Utrecht] Debiasing techniques:&nbsp;<span class=\"date\">20 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13c\">Vienna:&nbsp;<span class=\"date\">23 August 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/13i\">Washington, D.C.: Museums Meetup:&nbsp;<span class=\"date\">24 August 2014 03:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Buffalo.2C_NY\">Buffalo</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Moscow.2C_Russia\">Moscow</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong>&nbsp;</strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Sydney\">Sydney</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> </strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tel_Aviv.2C_Israel\">Tel Aviv</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Warsaw.2C_Poland\">Warsaw</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aDeaxfLsdK8iwyNdQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.964078010925437e-06, "legacy": true, "legacyId": "26993", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ToSmEvJDYJRZ8hiB2", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-29T16:17:56.740Z", "modifiedAt": null, "url": null, "title": "The Great Filter is early, or AI is hard", "slug": "the-great-filter-is-early-or-ai-is-hard", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:35.876Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/69tRTQX4Ce5jkSdqk/the-great-filter-is-early-or-ai-is-hard", "pageUrlRelative": "/posts/69tRTQX4Ce5jkSdqk/the-great-filter-is-early-or-ai-is-hard", "linkUrl": "https://www.lesswrong.com/posts/69tRTQX4Ce5jkSdqk/the-great-filter-is-early-or-ai-is-hard", "postedAtFormatted": "Friday, August 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Great%20Filter%20is%20early%2C%20or%20AI%20is%20hard&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Great%20Filter%20is%20early%2C%20or%20AI%20is%20hard%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F69tRTQX4Ce5jkSdqk%2Fthe-great-filter-is-early-or-ai-is-hard%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Great%20Filter%20is%20early%2C%20or%20AI%20is%20hard%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F69tRTQX4Ce5jkSdqk%2Fthe-great-filter-is-early-or-ai-is-hard", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F69tRTQX4Ce5jkSdqk%2Fthe-great-filter-is-early-or-ai-is-hard", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p>Attempt at the briefest content-full Less Wrong post:</p>\n<p>Once AI is developed, it could \"<a href=\"/lw/hll/to_reduce_astronomical_waste_take_your_time_then/\">easily</a>\" <a href=\"/lw/g1s/ufai_cannot_be_the_great_filter/\">colonise</a> the universe. So the <a href=\"http://en.wikipedia.org/wiki/Great_Filter\">Great Filter</a> (preventing the emergence of star-spanning civilizations) must strike before AI could be developed. If AI is easy, we could conceivably have built it already, or we could be on the cusp of building it. So the Great Filter must predate us, unless AI is hard.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"25oxqHiadqM6Hf7Gn": 2, "sYm3HiWcfZvrGu3ui": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "69tRTQX4Ce5jkSdqk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 32, "extendedScore": null, "score": 1.964136995197545e-06, "legacy": true, "legacyId": "27058", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 76, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["S4PGuNHgjNpnnnQvT", "BQ4KLnmB7tcAZLNfm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-30T00:36:55.622Z", "modifiedAt": null, "url": null, "title": "[question] Recommendations for fasting", "slug": "question-recommendations-for-fasting", "viewCount": null, "lastCommentedAt": "2017-07-15T22:42:09.575Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3i6XqtnzfAi2GACej/question-recommendations-for-fasting", "pageUrlRelative": "/posts/3i6XqtnzfAi2GACej/question-recommendations-for-fasting", "linkUrl": "https://www.lesswrong.com/posts/3i6XqtnzfAi2GACej/question-recommendations-for-fasting", "postedAtFormatted": "Saturday, August 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bquestion%5D%20Recommendations%20for%20fasting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bquestion%5D%20Recommendations%20for%20fasting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3i6XqtnzfAi2GACej%2Fquestion-recommendations-for-fasting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bquestion%5D%20Recommendations%20for%20fasting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3i6XqtnzfAi2GACej%2Fquestion-recommendations-for-fasting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3i6XqtnzfAi2GACej%2Fquestion-recommendations-for-fasting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 367, "htmlBody": "<p>I consider fasting for two weeks in October, but I'm unclear about it being beneficial in general or for what kind of fasting it might be beneficial and healthy. Thus this is a kind of request for rational discussion of this topic.</p>\n<p>I looked for relevant LW posts but couldn't find clear evidence. I think this is an underrepresented and possibly underutilized lifestyle intervention.</p>\n<p><a id=\"more\"></a></p>\n<p>As a starter you might look at</p>\n<p>Wikipedia has&nbsp;<a href=\"http://en.wikipedia.org/wiki/Fasting\">fasting</a>&nbsp;and <a href=\"http://en.wikipedia.org/wiki/Intermittent_fasting\">intermittent fasting</a>. The latter shows</p>\n<blockquote>\n<p>health benefits include stress resistance, increased insulin sensitivity, reduced morbidity, and increased life span.</p>\n</blockquote>\n<p>But as I'm healthy and lean the benefits of intermittent fasting are likely small for me. What other benefits could there be?</p>\n<p>I understand that <a href=\"http://en.wikipedia.org/wiki/Body_cleansing\">body cleansing</a> is not a real thing, but I wonder about the effects of fasting on the <a href=\"http://en.wikipedia.org/wiki/Gut_flora\">gut flora</a>. Could it be that fastig has a beneficial effect on your gut bacteria? Just because your body doesn't get rid of any poison this way as the greeks believed doesn't mean that fasting has no cleansing effects at all. It could be that you get rid of some harmful gut bacteria or other parasites (not that these are frequent these days).</p>\n<p>Another thing is that fasting might activate and kind of train metabolism cycles which the body may loose over time otherwise. For me it might be too late (I'm 41) for that (mouse experiments show fasting tolerance is plastic with age), but maybe not. On the other hand I'm not very likely to ever need the ability to deal with lack of food (except possibly in case of severe illness of injury).</p>\n<p>Links on LW:&nbsp;<a href=\"/lw/c7z/low_hanging_fruit_analyzing_your_nutrition/\">Low hanging fruit: analyzing your nutrition</a>&nbsp;and <a href=\"/lw/byy/if_calorie_restriction_works_in_humans_should_we/\">If calorie restriction works in humans, should we have observed it already?</a>&nbsp;both mention intermittent fasting but I gain little insight from these.</p>\n<p>Also related is&nbsp;<a href=\"/lw/jrt/lifestyle_interventions_to_increase_longevity/\">Lifestyle interventions to increase longevity</a>. (Intermittent) fasting is also mentioned on&nbsp;<a href=\"/lw/831/mental_rebooting_your_brain_on_porn/\">Mental rebooting your brain</a>.</p>\n<p>My current plan is to use <a href=\"http://en.wikipedia.org/wiki/Otto_Buchinger\">Buchinger</a> style fasting with fruit juice, thin vegetable broth and protein additions (which kind of protein I'm still unclear). I will reduce exercise to balance and walking level types.</p>\n<p>I'm also unclear how to measure and track the effect of this diet. Sure I will track weight. But should I track satisaction somehow?</p>\n<p>What do you think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3i6XqtnzfAi2GACej", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 12, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "27061", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MXwyMPM4BHQnZAReN", "vjFtitfWa9XRgJF4Q", "PhXENjdXiHhsWGfQo", "myAvQcAGG7YBu5spt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-30T00:42:13.492Z", "modifiedAt": null, "url": null, "title": "Funding cannibalism motivates concern for overheads", "slug": "funding-cannibalism-motivates-concern-for-overheads", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:09.302Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thrasymachus", "createdAt": "2011-08-26T20:45:38.594Z", "isAdmin": false, "displayName": "Thrasymachus"}, "userId": "FkqMcEbDiSbJPBMtq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7sR3fjJojD7hjTC6p/funding-cannibalism-motivates-concern-for-overheads", "pageUrlRelative": "/posts/7sR3fjJojD7hjTC6p/funding-cannibalism-motivates-concern-for-overheads", "linkUrl": "https://www.lesswrong.com/posts/7sR3fjJojD7hjTC6p/funding-cannibalism-motivates-concern-for-overheads", "postedAtFormatted": "Saturday, August 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Funding%20cannibalism%20motivates%20concern%20for%20overheads&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFunding%20cannibalism%20motivates%20concern%20for%20overheads%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7sR3fjJojD7hjTC6p%2Ffunding-cannibalism-motivates-concern-for-overheads%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Funding%20cannibalism%20motivates%20concern%20for%20overheads%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7sR3fjJojD7hjTC6p%2Ffunding-cannibalism-motivates-concern-for-overheads", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7sR3fjJojD7hjTC6p%2Ffunding-cannibalism-motivates-concern-for-overheads", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 993, "htmlBody": "<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong>Summary:&nbsp;</strong>Overhead expenses' (CEO salary, percentage spent on fundraising) are often deemed a poor measure of charity effectiveness by Effective Altruists, and so they disprefer means of charity evaluation which rely on these. However, 'funding cannibalism' suggests that these metrics (and the norms that engender them) have value: if fundraising is broadly a zero-sum game between charities, then there's a commons problem where all charities could spend less money on fundraising and all do more good, but each is locally incentivized to spend more. Donor norms against increasing spending on zero-sum 'overheads' might be a good way of combating this. This valuable collective action of donors may explain the apparent underutilization of fundraising by charities, and perhaps should make us cautious in undermining it.</p>\n<h1 style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 19px;\">The EA critique of charity evaluation</h1>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Pre-Givewell, the common means of evaluating charities (<a href=\"http://www.guidestar.org/\">Guidestar</a>,&nbsp;<a href=\"http://www.charitynavigator.org/\">Charity Navigator</a>) used a mixture of governance checklists 'overhead indicators'. Charities would gain points both for having features associated with good governance (being transparent in the right ways, balancing budgets, the right sorts of corporate structure), but also in spending its money on programs and avoiding 'overhead expenses' like administration and (especially) fundraising. For shorthand, call this 'common sense' evaluation.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">The standard EA critique is that common sense evaluation doesn't capture what is really important: outcomes. It is easy to imagine charities that look really good to common sense evaluation yet have negligible (or negative) outcomes. &nbsp;In the case of overheads, it becomes unclear whether these are even&nbsp;<em>proxy</em>&nbsp;measures of efficacy. Any fundraising that still 'turns a profit' looks like a good deal, whether it comprises five percent of a charity's spending or fifty.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">A summary of the EA critique of common sense evaluation that its myopic focus on these metrics gives&nbsp;<em>pathological incentives</em>, as these metrics frequently lie anti-parallel to maximizing efficacy. To score well on these evaluations, charities may be encouraged to raise less money, hire less able staff, and cut corners in their own management, even if doing these things would be false economies.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">&nbsp;</p>\n<h1 style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 19px;\">Funding cannibalism and commons tragedies</h1>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">In the wake of the ALS 'Ice bucket challenge',&nbsp;<a href=\"http://qz.com/249649/the-cold-hard-truth-about-the-ice-bucket-challenge/\">Will MacAskill suggested</a>&nbsp;there is considerable of 'funding cannabilism' in the non-profit sector. Instead of the Ice bucket challenge 'raising' money for ALS, it has taken money that would have been donated to other causes instead - cannibalizing other causes. Rather than each charity raising funds independently of one another, they compete for a fairly fixed pie of aggregate charitable giving.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">The 'cannabilism' thesis is controversial, but looks plausible to me, especially when looking at 'macro' indicators: proportion of household charitable spending looks pretty fixed whilst fundraising has increased dramatically, for example.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">If true, cannibalism is important. As MacAskill points out, the money tens of millions of dollars raised for ALS is no longer an untrammelled good, alloyed as it is with the opportunity cost of whatever other causes it has cannibalized (<a href=\"http://80000hours.org/blog/93-why-most-charity-fundraisers-cause-harm\">q.v.</a>). There's also a more general consideration: if there is a fixed pot of charitable giving insensitive to aggregate fundraising, then fundraising becomes a commons problem. If all charities could spend less on their fundraising, none would lose out, so all could spend more of their funds on their programs. However, for any alone to spend less on fundraising allows the others to cannibalize it.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">&nbsp;</p>\n<h1 style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 19px;\">Civilizing Charitable Cannibals, and Metric Meta-Myopia</h1>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Coordination among charities to avoid this commons tragedy is far fetched. Yet coordination of &nbsp;<em>donors&nbsp;</em>on shared&nbsp;norms about 'overhead ratio' can help. By penalizing a charity for spending too much on zero-sum games with other charities like fundraising, donors can stop a race to the bottom fundraising free for all and burning of the charitable commons that implies.&nbsp;<a href=\"http://80000hours.org/blog/92-why-don-t-charities-spend-more-on-fundraising\">The apparently-high marginal return to fundraising</a>&nbsp;might suggest this is already in effect (and effective!)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">The contrarian take would be that it is the<em>&nbsp;EA critique&nbsp;</em>of charity evaluation which is myopic, not the charity evaluation itself - by looking at the apparent benefit&nbsp;<em>for a single charity</em>&nbsp;of more overhead, the EA critique ignores the broader picture of the non-profit ecosystem, and their attack undermines a key environmental protection of an important commons - further, one which the right tail of most effective charities benefit from just as much as the crowd of 'great unwashed' other causes. (Fundraising ability and efficacy look like they should be pretty orthogonal. Besides, if they correlate well enough that you'd expect the most efficacious charities would win the zero-sum fundraising game, couldn't you dispense with Givewell and give to the best fundraisers?)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">The contrarian view probably goes too far. Although&nbsp;there's a case for communally caring about&nbsp;<em>fundraising</em>&nbsp;overheads, as cannibalism leads us to guess it is zero sum, parallel reasoning is hard to apply to&nbsp;<em>administration&nbsp;</em>overhead: charity X doesn't lose out if charity Y spends more on management, but charity Y is still penalized by common sense evaluation even if its overall efficacy increases. I'd guess that features like executive pay lie somewhere in the middle: non-profit executives could be poached by for-profit industries, so it is not as simple as donors prodding charities to coordinate to lower executive pay; but donors&nbsp;<em>can</em>&nbsp;prod charities not to throw away whatever 'non-profit premium' they do have in competing with one another for top talent (<a href=\"http://www.kenscommentary.org/2014/08/how-much-should-we-pay-charity-ceos.html\">c.f.</a>).&nbsp;If so, we should castigate people less for caring about overhead, even if we still want to encourage them to care about efficacy too.</p>\n<h1 style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 19px;\">The invisible hand of charitable pan-handling</h1>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">If true, it is unclear whether the story that should be told is 'common sense was right all along and the EA movement overconfidently criticised' or 'A stopped clock is right twice a day, and the generally wrong-headed common sense had an unintended feature amongst the bugs'. I'd lean towards the latter, simply the advocates of the common sense approach have&nbsp;<a href=\"http://www.ssireview.org/blog/entry/the_elitist_philanthropy_of_so_called_effective_altruism\">not</a>&nbsp;(to my knowledge) articulated these considerations themselves.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">However, many of us believe the implicit machinery of the market can turn without many of the actors within it having any explicit understanding of it. Perhaps the same applies here. If so, we should be less confident in claiming the status quo is pathological and we can do better: there may be a rationale eluding both us&nbsp;<em>and</em>&nbsp;its defenders.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 1, "txkDg4aLmiRq8wsSu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7sR3fjJojD7hjTC6p", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 40, "extendedScore": null, "score": 1.9650118542039256e-06, "legacy": true, "legacyId": "27062", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong>Summary:&nbsp;</strong>Overhead expenses' (CEO salary, percentage spent on fundraising) are often deemed a poor measure of charity effectiveness by Effective Altruists, and so they disprefer means of charity evaluation which rely on these. However, 'funding cannibalism' suggests that these metrics (and the norms that engender them) have value: if fundraising is broadly a zero-sum game between charities, then there's a commons problem where all charities could spend less money on fundraising and all do more good, but each is locally incentivized to spend more. Donor norms against increasing spending on zero-sum 'overheads' might be a good way of combating this. This valuable collective action of donors may explain the apparent underutilization of fundraising by charities, and perhaps should make us cautious in undermining it.</p>\n<h1 style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 19px;\" id=\"The_EA_critique_of_charity_evaluation\">The EA critique of charity evaluation</h1>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Pre-Givewell, the common means of evaluating charities (<a href=\"http://www.guidestar.org/\">Guidestar</a>,&nbsp;<a href=\"http://www.charitynavigator.org/\">Charity Navigator</a>) used a mixture of governance checklists 'overhead indicators'. Charities would gain points both for having features associated with good governance (being transparent in the right ways, balancing budgets, the right sorts of corporate structure), but also in spending its money on programs and avoiding 'overhead expenses' like administration and (especially) fundraising. For shorthand, call this 'common sense' evaluation.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">The standard EA critique is that common sense evaluation doesn't capture what is really important: outcomes. It is easy to imagine charities that look really good to common sense evaluation yet have negligible (or negative) outcomes. &nbsp;In the case of overheads, it becomes unclear whether these are even&nbsp;<em>proxy</em>&nbsp;measures of efficacy. Any fundraising that still 'turns a profit' looks like a good deal, whether it comprises five percent of a charity's spending or fifty.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">A summary of the EA critique of common sense evaluation that its myopic focus on these metrics gives&nbsp;<em>pathological incentives</em>, as these metrics frequently lie anti-parallel to maximizing efficacy. To score well on these evaluations, charities may be encouraged to raise less money, hire less able staff, and cut corners in their own management, even if doing these things would be false economies.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">&nbsp;</p>\n<h1 style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 19px;\" id=\"Funding_cannibalism_and_commons_tragedies\">Funding cannibalism and commons tragedies</h1>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">In the wake of the ALS 'Ice bucket challenge',&nbsp;<a href=\"http://qz.com/249649/the-cold-hard-truth-about-the-ice-bucket-challenge/\">Will MacAskill suggested</a>&nbsp;there is considerable of 'funding cannabilism' in the non-profit sector. Instead of the Ice bucket challenge 'raising' money for ALS, it has taken money that would have been donated to other causes instead - cannibalizing other causes. Rather than each charity raising funds independently of one another, they compete for a fairly fixed pie of aggregate charitable giving.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">The 'cannabilism' thesis is controversial, but looks plausible to me, especially when looking at 'macro' indicators: proportion of household charitable spending looks pretty fixed whilst fundraising has increased dramatically, for example.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">If true, cannibalism is important. As MacAskill points out, the money tens of millions of dollars raised for ALS is no longer an untrammelled good, alloyed as it is with the opportunity cost of whatever other causes it has cannibalized (<a href=\"http://80000hours.org/blog/93-why-most-charity-fundraisers-cause-harm\">q.v.</a>). There's also a more general consideration: if there is a fixed pot of charitable giving insensitive to aggregate fundraising, then fundraising becomes a commons problem. If all charities could spend less on their fundraising, none would lose out, so all could spend more of their funds on their programs. However, for any alone to spend less on fundraising allows the others to cannibalize it.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">&nbsp;</p>\n<h1 style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 19px;\" id=\"Civilizing_Charitable_Cannibals__and_Metric_Meta_Myopia\">Civilizing Charitable Cannibals, and Metric Meta-Myopia</h1>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Coordination among charities to avoid this commons tragedy is far fetched. Yet coordination of &nbsp;<em>donors&nbsp;</em>on shared&nbsp;norms about 'overhead ratio' can help. By penalizing a charity for spending too much on zero-sum games with other charities like fundraising, donors can stop a race to the bottom fundraising free for all and burning of the charitable commons that implies.&nbsp;<a href=\"http://80000hours.org/blog/92-why-don-t-charities-spend-more-on-fundraising\">The apparently-high marginal return to fundraising</a>&nbsp;might suggest this is already in effect (and effective!)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">The contrarian take would be that it is the<em>&nbsp;EA critique&nbsp;</em>of charity evaluation which is myopic, not the charity evaluation itself - by looking at the apparent benefit&nbsp;<em>for a single charity</em>&nbsp;of more overhead, the EA critique ignores the broader picture of the non-profit ecosystem, and their attack undermines a key environmental protection of an important commons - further, one which the right tail of most effective charities benefit from just as much as the crowd of 'great unwashed' other causes. (Fundraising ability and efficacy look like they should be pretty orthogonal. Besides, if they correlate well enough that you'd expect the most efficacious charities would win the zero-sum fundraising game, couldn't you dispense with Givewell and give to the best fundraisers?)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">The contrarian view probably goes too far. Although&nbsp;there's a case for communally caring about&nbsp;<em>fundraising</em>&nbsp;overheads, as cannibalism leads us to guess it is zero sum, parallel reasoning is hard to apply to&nbsp;<em>administration&nbsp;</em>overhead: charity X doesn't lose out if charity Y spends more on management, but charity Y is still penalized by common sense evaluation even if its overall efficacy increases. I'd guess that features like executive pay lie somewhere in the middle: non-profit executives could be poached by for-profit industries, so it is not as simple as donors prodding charities to coordinate to lower executive pay; but donors&nbsp;<em>can</em>&nbsp;prod charities not to throw away whatever 'non-profit premium' they do have in competing with one another for top talent (<a href=\"http://www.kenscommentary.org/2014/08/how-much-should-we-pay-charity-ceos.html\">c.f.</a>).&nbsp;If so, we should castigate people less for caring about overhead, even if we still want to encourage them to care about efficacy too.</p>\n<h1 style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 19px;\" id=\"The_invisible_hand_of_charitable_pan_handling\">The invisible hand of charitable pan-handling</h1>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">If true, it is unclear whether the story that should be told is 'common sense was right all along and the EA movement overconfidently criticised' or 'A stopped clock is right twice a day, and the generally wrong-headed common sense had an unintended feature amongst the bugs'. I'd lean towards the latter, simply the advocates of the common sense approach have&nbsp;<a href=\"http://www.ssireview.org/blog/entry/the_elitist_philanthropy_of_so_called_effective_altruism\">not</a>&nbsp;(to my knowledge) articulated these considerations themselves.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">However, many of us believe the implicit machinery of the market can turn without many of the actors within it having any explicit understanding of it. Perhaps the same applies here. If so, we should be less confident in claiming the status quo is pathological and we can do better: there may be a rationale eluding both us&nbsp;<em>and</em>&nbsp;its defenders.</p>", "sections": [{"title": "The EA critique of charity evaluation", "anchor": "The_EA_critique_of_charity_evaluation", "level": 1}, {"title": "Funding cannibalism and commons tragedies", "anchor": "Funding_cannibalism_and_commons_tragedies", "level": 1}, {"title": "Civilizing Charitable Cannibals, and Metric Meta-Myopia", "anchor": "Civilizing_Charitable_Cannibals__and_Metric_Meta_Myopia", "level": 1}, {"title": "The invisible hand of charitable pan-handling", "anchor": "The_invisible_hand_of_charitable_pan_handling", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-30T05:09:16.032Z", "modifiedAt": null, "url": null, "title": "Meetup : Michigan Meetup", "slug": "meetup-michigan-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:06.993Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3cwvq3vdR5kYoGXEo/meetup-michigan-meetup", "pageUrlRelative": "/posts/3cwvq3vdR5kYoGXEo/meetup-michigan-meetup", "linkUrl": "https://www.lesswrong.com/posts/3cwvq3vdR5kYoGXEo/meetup-michigan-meetup", "postedAtFormatted": "Saturday, August 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Michigan%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Michigan%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3cwvq3vdR5kYoGXEo%2Fmeetup-michigan-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Michigan%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3cwvq3vdR5kYoGXEo%2Fmeetup-michigan-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3cwvq3vdR5kYoGXEo%2Fmeetup-michigan-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13s'>Michigan Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 September 2014 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">19334 Angling Street, Livonia, MI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our more-or-less trimonthly Ann Arbor + Detroit area meetup. Myself and Ozy will be hosting, and <a href=\"http://lesswrong.com/user/Swimmer963/\">Swimmer963</a> will be visiting from Canada.</p>\n\n<p>No particular topic, but bring games and discussion topics as they interest you.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13s'>Michigan Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3cwvq3vdR5kYoGXEo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "27064", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Michigan_Meetup\">Discussion article for the meetup : <a href=\"/meetups/13s\">Michigan Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 September 2014 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">19334 Angling Street, Livonia, MI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our more-or-less trimonthly Ann Arbor + Detroit area meetup. Myself and Ozy will be hosting, and <a href=\"http://lesswrong.com/user/Swimmer963/\">Swimmer963</a> will be visiting from Canada.</p>\n\n<p>No particular topic, but bring games and discussion topics as they interest you.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Michigan_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/13s\">Michigan Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Michigan Meetup", "anchor": "Discussion_article_for_the_meetup___Michigan_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Michigan Meetup", "anchor": "Discussion_article_for_the_meetup___Michigan_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-30T09:40:23.009Z", "modifiedAt": null, "url": null, "title": "LessWrong Hamburg Meetup Notes - Diet", "slug": "lesswrong-hamburg-meetup-notes-diet", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:05.581Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PDStPEfcpJs8LcJ6k/lesswrong-hamburg-meetup-notes-diet", "pageUrlRelative": "/posts/PDStPEfcpJs8LcJ6k/lesswrong-hamburg-meetup-notes-diet", "linkUrl": "https://www.lesswrong.com/posts/PDStPEfcpJs8LcJ6k/lesswrong-hamburg-meetup-notes-diet", "postedAtFormatted": "Saturday, August 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20Hamburg%20Meetup%20Notes%20-%20Diet&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20Hamburg%20Meetup%20Notes%20-%20Diet%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPDStPEfcpJs8LcJ6k%2Flesswrong-hamburg-meetup-notes-diet%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20Hamburg%20Meetup%20Notes%20-%20Diet%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPDStPEfcpJs8LcJ6k%2Flesswrong-hamburg-meetup-notes-diet", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPDStPEfcpJs8LcJ6k%2Flesswrong-hamburg-meetup-notes-diet", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<p><strong style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">Review of our <a href=\"/lw/krl/meetup_lesswrong_hamburg_diet/\">LessWrong Hamburg Meetup - Diet</a></strong></p>\n<p><strong style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-weight: normal; line-height: normal; text-align: start;\">After I was approched a few times about another meetup I scheduled it on short notice and six of us met yesterday evening at my place.</span></strong></p>\n<h2>Summary</h2>\n<p>It was an mostly unstructured talk where we discussed diet from different angles and a few other tangential topics. I also reported from my participation in the LW Berlin Meetup a few weeks ago (which led to a side-track about polyphasic sleep).</p>\n<p>We discussed the benefits and risks of misc. dietary recommendations and seemed to agree on most points, most of which coincide with those discussed on LW before:</p>\n<div>\n<ul>\n<li>http://lesswrong.com/lw/jrt/lifestyle_interventions_to_increase_longevity</li>\n<li>http://circ.ahajournals.org/content/121/21/2271.short</li>\n</ul>\n</div>\n<p>Links about polyphasic sleep:</p>\n<div>\n<ul>\n<li>http://en.wikipedia.org/wiki/Polyphasic_sleep</li>\n<li>http://www.polyphasicsociety.com/polyphasic-sleep/overviews/segmented-sleep/</li>\n<li>http://lesswrong.com/lw/ip6/polyphasic_sleep_seed_study_reprise/</li>\n</ul>\n<div>Other links for topics discussed during the meetup (for reference):</div>\n<div>\n<ul>\n<li>http://www.wired.com/2010/12/kids-study-bees/</li>\n<li>http://rsbl.royalsocietypublishing.org/content/early/2010/12/18/rsbl.2010.1056</li>\n<li>http://littlebits.cc/</li>\n<li>http://code.org/learn</li>\n</ul>\n<div>There will be further meetups which will be scheduled via doodle.de (and then announced on the mailing-list).</div>\n</div>\n</div>\n<p><span style=\"font-size: 16px;\">Other LW Hamburg Meetup reviews</span></p>\n<div>\n<div>\n<ul>\n<li><a href=\"https://www.google.com/url?q=http://lesswrong.com/lw/jtj/lesswrong_hamburg_third_meetup_notes_small_steps/&amp;sa=U&amp;ei=eBABVOHXGeGc0AWEmYCABg&amp;ved=0CAgQFjAB&amp;client=internal-uds-cse&amp;usg=AFQjCNF5HWQh58euEsGCNgKBdnvJlVK90g\">Third Meetup Notes: Small Steps Forward</a></li>\n<li><a href=\"/r/discussion/lw/jqr/lesswrong_hamburg_second_meetup_notes_in_need_of/\">Second Meetup Notes: In need of Structure</a></li>\n<li><a href=\"/lw/jn3/lesswrong_hamburg_first_meetup_notes_starting/\">First Meetup Notes: Starting small</a></li>\n</ul>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PDStPEfcpJs8LcJ6k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 1.9659462504728345e-06, "legacy": true, "legacyId": "27060", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" id=\"Review_of_our_LessWrong_Hamburg_Meetup___Diet\">Review of our <a href=\"/lw/krl/meetup_lesswrong_hamburg_diet/\">LessWrong Hamburg Meetup - Diet</a></strong></p>\n<p><strong style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" id=\"After_I_was_approched_a_few_times_about_another_meetup_I_scheduled_it_on_short_notice_and_six_of_us_met_yesterday_evening_at_my_place_\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-weight: normal; line-height: normal; text-align: start;\">After I was approched a few times about another meetup I scheduled it on short notice and six of us met yesterday evening at my place.</span></strong></p>\n<h2 id=\"Summary\">Summary</h2>\n<p>It was an mostly unstructured talk where we discussed diet from different angles and a few other tangential topics. I also reported from my participation in the LW Berlin Meetup a few weeks ago (which led to a side-track about polyphasic sleep).</p>\n<p>We discussed the benefits and risks of misc. dietary recommendations and seemed to agree on most points, most of which coincide with those discussed on LW before:</p>\n<div>\n<ul>\n<li>http://lesswrong.com/lw/jrt/lifestyle_interventions_to_increase_longevity</li>\n<li>http://circ.ahajournals.org/content/121/21/2271.short</li>\n</ul>\n</div>\n<p>Links about polyphasic sleep:</p>\n<div>\n<ul>\n<li>http://en.wikipedia.org/wiki/Polyphasic_sleep</li>\n<li>http://www.polyphasicsociety.com/polyphasic-sleep/overviews/segmented-sleep/</li>\n<li>http://lesswrong.com/lw/ip6/polyphasic_sleep_seed_study_reprise/</li>\n</ul>\n<div>Other links for topics discussed during the meetup (for reference):</div>\n<div>\n<ul>\n<li>http://www.wired.com/2010/12/kids-study-bees/</li>\n<li>http://rsbl.royalsocietypublishing.org/content/early/2010/12/18/rsbl.2010.1056</li>\n<li>http://littlebits.cc/</li>\n<li>http://code.org/learn</li>\n</ul>\n<div>There will be further meetups which will be scheduled via doodle.de (and then announced on the mailing-list).</div>\n</div>\n</div>\n<p><span style=\"font-size: 16px;\">Other LW Hamburg Meetup reviews</span></p>\n<div>\n<div>\n<ul>\n<li><a href=\"https://www.google.com/url?q=http://lesswrong.com/lw/jtj/lesswrong_hamburg_third_meetup_notes_small_steps/&amp;sa=U&amp;ei=eBABVOHXGeGc0AWEmYCABg&amp;ved=0CAgQFjAB&amp;client=internal-uds-cse&amp;usg=AFQjCNF5HWQh58euEsGCNgKBdnvJlVK90g\">Third Meetup Notes: Small Steps Forward</a></li>\n<li><a href=\"/r/discussion/lw/jqr/lesswrong_hamburg_second_meetup_notes_in_need_of/\">Second Meetup Notes: In need of Structure</a></li>\n<li><a href=\"/lw/jn3/lesswrong_hamburg_first_meetup_notes_starting/\">First Meetup Notes: Starting small</a></li>\n</ul>\n</div>\n</div>", "sections": [{"title": "Review of our LessWrong Hamburg Meetup - Diet", "anchor": "Review_of_our_LessWrong_Hamburg_Meetup___Diet", "level": 2}, {"title": "After I was approched a few times about another meetup I scheduled it on short notice and six of us met yesterday evening at my place.", "anchor": "After_I_was_approched_a_few_times_about_another_meetup_I_scheduled_it_on_short_notice_and_six_of_us_met_yesterday_evening_at_my_place_", "level": 2}, {"title": "Summary", "anchor": "Summary", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sFJpFbbF8h25fM6gK", "d2f6Eggpj3edzKefe", "guYSP8Km3hChHD4ja"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-30T12:32:13.625Z", "modifiedAt": null, "url": null, "title": "Meetup Report Thread: September 2014", "slug": "meetup-report-thread-september-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:38.859Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i3RDeos7z6o3JwKzL/meetup-report-thread-september-2014", "pageUrlRelative": "/posts/i3RDeos7z6o3JwKzL/meetup-report-thread-september-2014", "linkUrl": "https://www.lesswrong.com/posts/i3RDeos7z6o3JwKzL/meetup-report-thread-september-2014", "postedAtFormatted": "Saturday, August 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20Report%20Thread%3A%20September%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20Report%20Thread%3A%20September%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi3RDeos7z6o3JwKzL%2Fmeetup-report-thread-september-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20Report%20Thread%3A%20September%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi3RDeos7z6o3JwKzL%2Fmeetup-report-thread-september-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi3RDeos7z6o3JwKzL%2Fmeetup-report-thread-september-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">If you had an interesting Less Wrong meetup recently, but don't have the time to write up a big report to post to Discussion, feel free to write a comment here.&nbsp; Even if it's just a couple lines about what you did and how people felt about it, it might encourage some people to attend meetups or start meetups in their area.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">If you have the time, you can also describe what types of exercises you did, what worked and what didn't.&nbsp; This could help inspire meetups to try new things and improve themselves in various ways.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">If you're inspired by what's posted below and want to organize a meetup, check out&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">this page</a>&nbsp;for some resources to get started!&nbsp; You can also check FrankAdamek's weekly post on <a href=\"/r/discussion/tag/meetups-announcement/\">meetups for the week</a>.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">Previous Meetup Report Thread: <a href=\"/lw/jp1/meetup_report_thread_february_2014/\">February 2014</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">Guidelines:&nbsp; Please post the meetup reports as top-level comments, and debate the specific meetup below its comment.&nbsp; Anything else goes under the \"Meta\" top-level comment.&nbsp; The title of this thread should be interpreted as \"up to and including September 2014\", which means feel free to post reports of meetups that happened in August, July, June, etc.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i3RDeos7z6o3JwKzL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 1.9662447853513663e-06, "legacy": true, "legacyId": "27065", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WHPnsAQCEiruKN5QN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-30T14:04:05.959Z", "modifiedAt": null, "url": null, "title": "[LINK] Article in the Guardian about CSER, mentions MIRI and paperclip AI", "slug": "link-article-in-the-guardian-about-cser-mentions-miri-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:05.761Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sarokrae", "createdAt": "2011-09-24T17:07:52.022Z", "isAdmin": false, "displayName": "Sarokrae"}, "userId": "qFQ84ipNbi3wDmn6Q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/szaqnDaMfboWreTfy/link-article-in-the-guardian-about-cser-mentions-miri-and", "pageUrlRelative": "/posts/szaqnDaMfboWreTfy/link-article-in-the-guardian-about-cser-mentions-miri-and", "linkUrl": "https://www.lesswrong.com/posts/szaqnDaMfboWreTfy/link-article-in-the-guardian-about-cser-mentions-miri-and", "postedAtFormatted": "Saturday, August 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Article%20in%20the%20Guardian%20about%20CSER%2C%20mentions%20MIRI%20and%20paperclip%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Article%20in%20the%20Guardian%20about%20CSER%2C%20mentions%20MIRI%20and%20paperclip%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FszaqnDaMfboWreTfy%2Flink-article-in-the-guardian-about-cser-mentions-miri-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Article%20in%20the%20Guardian%20about%20CSER%2C%20mentions%20MIRI%20and%20paperclip%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FszaqnDaMfboWreTfy%2Flink-article-in-the-guardian-about-cser-mentions-miri-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FszaqnDaMfboWreTfy%2Flink-article-in-the-guardian-about-cser-mentions-miri-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 196, "htmlBody": "<p><a title=\"The scientific A-Team saving the world from killer viruses, rogue AI and the paperclip apocalypse\" href=\"http://www.theguardian.com/technology/2014/aug/30/saviours-universe-four-unlikely-men-save-world\" target=\"_blank\">http://www.theguardian.com/technology/2014/aug/30/saviours-universe-four-unlikely-men-save-world</a></p>\n<p>The article is titled \"The scientific A-Team saving the world from killer viruses, rogue AI and the paperclip apocalypse\", and features interviews with Martin Rees, Huw Price, Jaan Tallinn and Partha Dasgupta. The author takes a rather positive tone about CSER and MIRI's endeavours, and mentions x-risks other than AI (bioengineered pandemic, global warming with human interference, distributed manufacturing).</p>\n<p>I find it interesting that the inferential distance for the layman to the concept of paperclipping AI is much reduced by talking about paperclipping <em>America,</em> rather than the entire universe: though the author admits still struggling with the concept. Unusually for an journalist who starts off unfamiliar with these concepts, he writes in a tone that suggests that he takes the ideas seriously, without the sort of \"this is very far-fetched and thus I will not lower myself to seriously considering it\" countersignalling usually seen with x-risk coverage. There is currently the usual degree of incredulity in the comments section though.</p>\n<p>For those unfamiliar with The Guardian, it is a British left-leaning newspaper with a heavy focus on social justice and left-wing political issues.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2, "QH4LhvnyR4QkW9MG8": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "szaqnDaMfboWreTfy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 27, "extendedScore": null, "score": 1.9664044230148455e-06, "legacy": true, "legacyId": "27066", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-30T17:51:17.058Z", "modifiedAt": null, "url": null, "title": "Solstice 2014 / Rational Ritual Retreat - A Call to Arms", "slug": "solstice-2014-rational-ritual-retreat-a-call-to-arms", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:06.342Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w74rih378faqmNtDs/solstice-2014-rational-ritual-retreat-a-call-to-arms", "pageUrlRelative": "/posts/w74rih378faqmNtDs/solstice-2014-rational-ritual-retreat-a-call-to-arms", "linkUrl": "https://www.lesswrong.com/posts/w74rih378faqmNtDs/solstice-2014-rational-ritual-retreat-a-call-to-arms", "postedAtFormatted": "Saturday, August 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Solstice%202014%20%2F%20Rational%20Ritual%20Retreat%20-%20A%20Call%20to%20Arms&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASolstice%202014%20%2F%20Rational%20Ritual%20Retreat%20-%20A%20Call%20to%20Arms%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw74rih378faqmNtDs%2Fsolstice-2014-rational-ritual-retreat-a-call-to-arms%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Solstice%202014%20%2F%20Rational%20Ritual%20Retreat%20-%20A%20Call%20to%20Arms%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw74rih378faqmNtDs%2Fsolstice-2014-rational-ritual-retreat-a-call-to-arms", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw74rih378faqmNtDs%2Fsolstice-2014-rational-ritual-retreat-a-call-to-arms", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1091, "htmlBody": "<p><a href=\"http://humanistculture.com\"><img src=\"http://humanistculture.files.wordpress.com/2013/08/splash5.jpg?w=1900&amp;h=768\" alt=\"\" width=\"512\" height=\"212\" /></a></p>\n<p><br />Summary:</p>\n<p><em>&nbsp;&bull; &nbsp;I'm beginning work on the 2014 Winter Solstice. There are a lot of jobs to be done, and the more people who can dedicate serious time to it, the better the end result will be and the more locations it can take place. A few people have volunteered serious time, and I wanted to issue a general call, to anyone who's wanted to be part of this but wasn't sure how. Send me an e-mail at raemon777@gmail.com if you'd like to help with any of the tasks listed below (or others I haven't thought of).<br /><br />&nbsp;&bull; &nbsp;More generally, I think people working on rational ritual, in any form, should be sharing notes and collaborating more. There's a fair number of us, but we're scattered across the country and haven't really felt like part of the same team. And it seems a bit silly for people working on ritual, to be scattered and unified. So I am hosting the first Rational Ritual Retreat at the end of September. The exact date and location have yet to be determined. You can apply at humanistculture.com, noting your availability, and I will determine</em></p>\n<p><em><br /></em></p>\n<hr />\n<h2>The Rational Ritual Retreat</h2>\n<p>For the past three years, I've been running a winter solstice holiday, celebrating science and human achievement. Several people have come up to me and told me it was one of the most unique, profound experiences they've participated in, inspiring them to work harder to make sure humanity has a bright future.&nbsp;</p>\n<p>I've also had a number of people concerned that I'm messing with dangerous aspects of human psychology, fearing what will happen to a rationality community that gets involved with ritual.<br /><br />Both of these thoughts are incredibly important. I've written a lot on the <a href=\"/lw/93l/the_value_and_danger_of_ritual/\">value and danger of ritua</a>l. [1]<br /><br />Ritual is central to the human experience. We've used it for thousands of years to&nbsp;<a href=\"/lw/by9/to_like_each_other_sing_and_dance_in_synchrony/\">bind groups together</a>. It helps us internalize complex ideas. A winning version of rationality needs *some* way of taking complex ideas and getting System 1 to care about them, and I think ritual is at least one tool we should consider.</p>\n<p>In the past couple weeks, a few thoughts occurred to me at once:</p>\n<p style=\"padding-left: 30px;\">1) Figuring out a rational approach to ritual that has a meaningful, useful effect on the world will require a lot of coordination among many skilled people.</p>\n<p style=\"padding-left: 30px;\">2) If this project *were* to go badly somehow, I think the most likely reason would be someone copying parts of what I'm working on without understanding all the considerations that went into it, and creating a toxic (or hollow) variant that spirals out of control.</p>\n<p style=\"padding-left: 30px;\">3) Many other people have approached the concept of rational ritual. But we've generally done so independently, often duplicating a lot of the same work and rarely moving on to more interesting and valuable experimentation. When we do experiment, we rarely share notes.</p>\n<p style=\"padding-left: 30px;\">This all prompted a fourth realization:<br /><br />4) If ritual designers are isolated and poorly coordinated... if we're duplicating a lot of the same early work and not sharing concerns about potential dangers, then one obvious (in retrospect) solution is to have a ritual about ritual creation.</p>\n<p>So, the <a href=\"http://humanistculture.com/\">Rational Ritual Retreat</a>. We'll hike out into a dark sky reserve, when there's no light pollution and the Milky Way looms large and beautiful above us. We'll share our stories, our ideas for a culture grounded in rationality yet tapped into our primal human desires. Over the course of an evening we'll create a ceremony or two together, through group consensus and collaboration. We'll experiment with new ideas, aware that some may work well, and some may not - that's how progress is made.<br /><br />This is my experiment, attempting to answer the question Eliezer raised in \"Bayesians vs Barbarians.\" It just seems really exceptionally <em>silly</em> to me that people motivated by rationality AND ritual should be so uncoordinated.&nbsp;<br /><br />Whether you're interested directly creating ritual, or helping to facilitate its creation in one way or another (helping with art, marketing, logistics or funding of future projects), you are invited to attend. The location is currently undecided - there are reasons to consider the West Coast, East Coast or (if there's enough interest in both locations) both.&nbsp;<br /><br /><a href=\"https://docs.google.com/forms/d/1KvvVF5c7BVgGJznw_np1c-95TEUjqbigIgPh19k8_mg/viewform\">Send in a brief application</a> so I can make decisions about where and when to host it. I'll make the final decisions this upcoming Friday.</p>\n<p>&nbsp;</p>\n<hr />\n<h2>The Winter Solstice</h2>\n<p>The Retreat is part of a long-term vision, of many people coming together to produce a culture (undoubtably, with numerous subcultures focusing on different aesthetics). Tentatively, I'd expect a successful rational-ritual culture to look sort of Open Source ish. (Or, more appropriately - I'd expect it to look like Burning Man. To be clear, Burning Man and variations already exist, my goal is not to duplicate that effort. It's to create something that's a) easier to integrate into people's lives, and b) specifically focuses on rationality and human progress)<br /><br />The Winter Solstice project as (at least for now) an important piece of that, partly because of the particular ideas it celebrates, but also because it's a demonstration of how you create *any* cultural holiday from scratch that celebrates serious ideas in a non-ironic fashion.<br /><br />My minimum goal this year is to finish the Hymnal, put more material online to help people create their own private events, and run another largish event in NYC. My stretch goals are to have a high quality public event in Boston and San Francisco. (Potentially other places if a lot of local people are interested and are willing to do the legwork).&nbsp;<br /><br />My hope, to make those stretch goals possible, is to find collaborators willing to put in a fair amount of work. I'm specifically looking for people who can:</p>\n<ul>\n<li>Creative Collaboration. Want to perform, create music, visual art, or host an event in your city?</li>\n<li>Help with logistics, especially in different cities. (Finding venues, arranging catering, etc)</li>\n<li>Marketing, reaching out to bloggers, or creating images or videos for the social media campaign.</li>\n<li>Helping with technical aspects of production for the Hymnal (editing, figuring out best places</li>\n</ul>\n<p>Each of these are things I'm able to do, but I have limited time, and the more time I can focus on creating<br /><br />If you're interested in collaborating, volunteering, or running a local event, either reply here or send me an e-mail at raemon777@gmail.com&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vtozKm5BZ8gf6zd45": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w74rih378faqmNtDs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 23, "extendedScore": null, "score": 9.7e-05, "legacy": true, "legacyId": "26981", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://humanistculture.com\"><img src=\"http://humanistculture.files.wordpress.com/2013/08/splash5.jpg?w=1900&amp;h=768\" alt=\"\" width=\"512\" height=\"212\"></a></p>\n<p><br>Summary:</p>\n<p><em>&nbsp;\u2022 &nbsp;I'm beginning work on the 2014 Winter Solstice. There are a lot of jobs to be done, and the more people who can dedicate serious time to it, the better the end result will be and the more locations it can take place. A few people have volunteered serious time, and I wanted to issue a general call, to anyone who's wanted to be part of this but wasn't sure how. Send me an e-mail at raemon777@gmail.com if you'd like to help with any of the tasks listed below (or others I haven't thought of).<br><br>&nbsp;\u2022 &nbsp;More generally, I think people working on rational ritual, in any form, should be sharing notes and collaborating more. There's a fair number of us, but we're scattered across the country and haven't really felt like part of the same team. And it seems a bit silly for people working on ritual, to be scattered and unified. So I am hosting the first Rational Ritual Retreat at the end of September. The exact date and location have yet to be determined. You can apply at humanistculture.com, noting your availability, and I will determine</em></p>\n<p><em><br></em></p>\n<hr>\n<h2 id=\"The_Rational_Ritual_Retreat\">The Rational Ritual Retreat</h2>\n<p>For the past three years, I've been running a winter solstice holiday, celebrating science and human achievement. Several people have come up to me and told me it was one of the most unique, profound experiences they've participated in, inspiring them to work harder to make sure humanity has a bright future.&nbsp;</p>\n<p>I've also had a number of people concerned that I'm messing with dangerous aspects of human psychology, fearing what will happen to a rationality community that gets involved with ritual.<br><br>Both of these thoughts are incredibly important. I've written a lot on the <a href=\"/lw/93l/the_value_and_danger_of_ritual/\">value and danger of ritua</a>l. [1]<br><br>Ritual is central to the human experience. We've used it for thousands of years to&nbsp;<a href=\"/lw/by9/to_like_each_other_sing_and_dance_in_synchrony/\">bind groups together</a>. It helps us internalize complex ideas. A winning version of rationality needs *some* way of taking complex ideas and getting System 1 to care about them, and I think ritual is at least one tool we should consider.</p>\n<p>In the past couple weeks, a few thoughts occurred to me at once:</p>\n<p style=\"padding-left: 30px;\">1) Figuring out a rational approach to ritual that has a meaningful, useful effect on the world will require a lot of coordination among many skilled people.</p>\n<p style=\"padding-left: 30px;\">2) If this project *were* to go badly somehow, I think the most likely reason would be someone copying parts of what I'm working on without understanding all the considerations that went into it, and creating a toxic (or hollow) variant that spirals out of control.</p>\n<p style=\"padding-left: 30px;\">3) Many other people have approached the concept of rational ritual. But we've generally done so independently, often duplicating a lot of the same work and rarely moving on to more interesting and valuable experimentation. When we do experiment, we rarely share notes.</p>\n<p style=\"padding-left: 30px;\">This all prompted a fourth realization:<br><br>4) If ritual designers are isolated and poorly coordinated... if we're duplicating a lot of the same early work and not sharing concerns about potential dangers, then one obvious (in retrospect) solution is to have a ritual about ritual creation.</p>\n<p>So, the <a href=\"http://humanistculture.com/\">Rational Ritual Retreat</a>. We'll hike out into a dark sky reserve, when there's no light pollution and the Milky Way looms large and beautiful above us. We'll share our stories, our ideas for a culture grounded in rationality yet tapped into our primal human desires. Over the course of an evening we'll create a ceremony or two together, through group consensus and collaboration. We'll experiment with new ideas, aware that some may work well, and some may not - that's how progress is made.<br><br>This is my experiment, attempting to answer the question Eliezer raised in \"Bayesians vs Barbarians.\" It just seems really exceptionally <em>silly</em> to me that people motivated by rationality AND ritual should be so uncoordinated.&nbsp;<br><br>Whether you're interested directly creating ritual, or helping to facilitate its creation in one way or another (helping with art, marketing, logistics or funding of future projects), you are invited to attend. The location is currently undecided - there are reasons to consider the West Coast, East Coast or (if there's enough interest in both locations) both.&nbsp;<br><br><a href=\"https://docs.google.com/forms/d/1KvvVF5c7BVgGJznw_np1c-95TEUjqbigIgPh19k8_mg/viewform\">Send in a brief application</a> so I can make decisions about where and when to host it. I'll make the final decisions this upcoming Friday.</p>\n<p>&nbsp;</p>\n<hr>\n<h2 id=\"The_Winter_Solstice\">The Winter Solstice</h2>\n<p>The Retreat is part of a long-term vision, of many people coming together to produce a culture (undoubtably, with numerous subcultures focusing on different aesthetics). Tentatively, I'd expect a successful rational-ritual culture to look sort of Open Source ish. (Or, more appropriately - I'd expect it to look like Burning Man. To be clear, Burning Man and variations already exist, my goal is not to duplicate that effort. It's to create something that's a) easier to integrate into people's lives, and b) specifically focuses on rationality and human progress)<br><br>The Winter Solstice project as (at least for now) an important piece of that, partly because of the particular ideas it celebrates, but also because it's a demonstration of how you create *any* cultural holiday from scratch that celebrates serious ideas in a non-ironic fashion.<br><br>My minimum goal this year is to finish the Hymnal, put more material online to help people create their own private events, and run another largish event in NYC. My stretch goals are to have a high quality public event in Boston and San Francisco. (Potentially other places if a lot of local people are interested and are willing to do the legwork).&nbsp;<br><br>My hope, to make those stretch goals possible, is to find collaborators willing to put in a fair amount of work. I'm specifically looking for people who can:</p>\n<ul>\n<li>Creative Collaboration. Want to perform, create music, visual art, or host an event in your city?</li>\n<li>Help with logistics, especially in different cities. (Finding venues, arranging catering, etc)</li>\n<li>Marketing, reaching out to bloggers, or creating images or videos for the social media campaign.</li>\n<li>Helping with technical aspects of production for the Hymnal (editing, figuring out best places</li>\n</ul>\n<p>Each of these are things I'm able to do, but I have limited time, and the more time I can focus on creating<br><br>If you're interested in collaborating, volunteering, or running a local event, either reply here or send me an e-mail at raemon777@gmail.com&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "The Rational Ritual Retreat", "anchor": "The_Rational_Ritual_Retreat", "level": 1}, {"title": "The Winter Solstice", "anchor": "The_Winter_Solstice", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "22 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rijoxTpkSPXcTXRbN", "GahkhPWinnAvri8Td"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-31T00:36:41.584Z", "modifiedAt": null, "url": null, "title": "Meetup : Sydney Rationality Dojo - Habits", "slug": "meetup-sydney-rationality-dojo-habits", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "luminosity", "createdAt": "2010-05-31T03:00:24.334Z", "isAdmin": false, "displayName": "luminosity"}, "userId": "4SuPdAqJpj7TzsaqG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vLJgwGMwcFYncBQse/meetup-sydney-rationality-dojo-habits", "pageUrlRelative": "/posts/vLJgwGMwcFYncBQse/meetup-sydney-rationality-dojo-habits", "linkUrl": "https://www.lesswrong.com/posts/vLJgwGMwcFYncBQse/meetup-sydney-rationality-dojo-habits", "postedAtFormatted": "Sunday, August 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sydney%20Rationality%20Dojo%20-%20Habits&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sydney%20Rationality%20Dojo%20-%20Habits%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvLJgwGMwcFYncBQse%2Fmeetup-sydney-rationality-dojo-habits%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sydney%20Rationality%20Dojo%20-%20Habits%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvLJgwGMwcFYncBQse%2Fmeetup-sydney-rationality-dojo-habits", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvLJgwGMwcFYncBQse%2Fmeetup-sydney-rationality-dojo-habits", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13t'>Sydney Rationality Dojo - Habits</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 September 2014 04:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Humanist House, 10 Shepherd St Chippendale</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This time around we will be looking at how to establish or replace habits. I personally think the ability to establish habits deliberately is crucially useful in crafting the life you want.</p>\n\n<p>As per usual, we will be having an optional dinner after the main event.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13t'>Sydney Rationality Dojo - Habits</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vLJgwGMwcFYncBQse", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.9675042444338985e-06, "legacy": true, "legacyId": "27068", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sydney_Rationality_Dojo___Habits\">Discussion article for the meetup : <a href=\"/meetups/13t\">Sydney Rationality Dojo - Habits</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 September 2014 04:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Humanist House, 10 Shepherd St Chippendale</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This time around we will be looking at how to establish or replace habits. I personally think the ability to establish habits deliberately is crucially useful in crafting the life you want.</p>\n\n<p>As per usual, we will be having an optional dinner after the main event.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sydney_Rationality_Dojo___Habits1\">Discussion article for the meetup : <a href=\"/meetups/13t\">Sydney Rationality Dojo - Habits</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sydney Rationality Dojo - Habits", "anchor": "Discussion_article_for_the_meetup___Sydney_Rationality_Dojo___Habits", "level": 1}, {"title": "Discussion article for the meetup : Sydney Rationality Dojo - Habits", "anchor": "Discussion_article_for_the_meetup___Sydney_Rationality_Dojo___Habits1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-31T10:36:30.117Z", "modifiedAt": null, "url": null, "title": "Why appearance matters or \"to behave as if\"", "slug": "why-appearance-matters-or-to-behave-as-if-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaLeptikon", "createdAt": "2014-03-29T17:32:19.520Z", "isAdmin": false, "displayName": "AnnaLeptikon"}, "userId": "FyZibA2dPTe9zcmND", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PgbenEuN3QnRTTmST/why-appearance-matters-or-to-behave-as-if-0", "pageUrlRelative": "/posts/PgbenEuN3QnRTTmST/why-appearance-matters-or-to-behave-as-if-0", "linkUrl": "https://www.lesswrong.com/posts/PgbenEuN3QnRTTmST/why-appearance-matters-or-to-behave-as-if-0", "postedAtFormatted": "Sunday, August 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20appearance%20matters%20or%20%22to%20behave%20as%20if%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20appearance%20matters%20or%20%22to%20behave%20as%20if%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPgbenEuN3QnRTTmST%2Fwhy-appearance-matters-or-to-behave-as-if-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20appearance%20matters%20or%20%22to%20behave%20as%20if%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPgbenEuN3QnRTTmST%2Fwhy-appearance-matters-or-to-behave-as-if-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPgbenEuN3QnRTTmST%2Fwhy-appearance-matters-or-to-behave-as-if-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 516, "htmlBody": "<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><em>\"*checking the name of the writer* Ooookay, this article about appearance is written by a woman. As was expected. It's probably not worth to read it...\"</em></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">If you thought something like this you confirmed how&nbsp;prejudices dominate our mind. And even if you didn't think something like that, you can't argue its importance away.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><strong>prejudices and stereotypes</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><span style=\"color: #800080;\"><a href=\"http://en.wikipedia.org/wiki/Prejudice\"><span style=\"color: #800080;\">Prejudice is prejudgment, or forming an opinion before becoming aware of the relevant facts of a case.</span></a></span></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><span style=\"color: #800080;\"><a href=\"http://en.wikipedia.org/wiki/Stereotype#Functions\"><span style=\"color: #800080;\">The cognitive function of stereotypes is to&nbsp;help make sense of the world. They are a form of categorization that helps to simplify and systematize information. Thus, information is more easily identified, recalled, predicted, and reacted to.</span></a></span></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">Prejudices and stereotypes might be useful and also harmful in some situations, but they definitely exist with all their advantages and disadvantages.&nbsp;They are based on the fastest&nbsp;available information. General assumptions about latent variables (such as intelligence and character) are made \u200b\u200bon external&nbsp; factors such as behavior and appearance.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><strong style=\"color: #000000;\">Pygmalion effect/self-fulfilling prophecy</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><a href=\"http://en.wikipedia.org/wiki/Pygmalion_effect\">The Pygmalion effect, or Rosenthal effect, is the phenomenon whereby the greater the expectation placed upon people, the better they perform.[1]&nbsp;</a>(Or the observer thinks it would be so!)&nbsp;<a href=\"http://en.wikipedia.org/wiki/Pygmalion_effect\">A corollary of the Pygmalion effect is the golem effect, in which low expectations lead to a decrease in performance.[1]</a></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">So what others (and we) expect from us influences how they and we behave and therefore influences our future and what we become!</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><strong>Confirmation Bias</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><a href=\"http://en.wikipedia.org/wiki/Confirmation_bias\">Confirmation bias, also called myside bias, is the tendency to search for or interpret information in a way that confirms one's beliefs or hypotheses.</a></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">Because it is easier to confirm people in their presumption than to convince them otherwise,&nbsp;it's a good decision&nbsp;to look like you want people to think you are as a person.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><strong>Minority influence/innovation</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><a href=\"http://en.wikipedia.org/wiki/Minority_influence\"><span style=\"color: #800080;\">Majority influence refers to the majority trying to produce conformity on the minority, while minority influence is converting the majority to adopt the thinking of the minority group.[1] Unlike other forms of influence, minority influence usually involves a personal shift in private opinion. Minority influence is also a central component of identity politics.&nbsp;</span></a></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">Minorities have a bigger impact when they: are consistent, are part of the ingroup and differ just in this one point (Idiosynkrasiekredit). As an example, your chances are higher to convince others to legalize cannabis if you wear suits instead of dreadlocks and hippie clothes.&nbsp;Your influence is therefore likely to be bigger if you behave and look like a adapted or even successful person.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><span style=\"color: #800080;\"><a href=\"http://en.wikipedia.org/wiki/Self-evaluation_motives\"><span style=\"color: #800080;\"><strong>Self-evaluation</strong></span></a></span></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">Since what others think of you will modify your self-evaluation your appearance will influence your self-evaluation, too. Also by direct feedback when looking in the mirror.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><strong>My personal background</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">I tried lots of different styles (bold, dreadlocks, gothic, sporty, well-dressed ... &nbsp;) and experienced big effects on how people behaved&nbsp;towards me.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">Pictures: http://annaleptikon.com/wp-content/uploads/2014/08/10015097_677981302243907_1187239476_o.jpg</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><strong>further thoughts/questions:</strong></p>\n<ul style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">\n<li>probably only people who thought about this already will be attracted by the topic and read this article ^^</li>\n<li>in the opposite way: being dressed to well might make you look stupid (imagine a \"Barbie\" talking about AI)</li>\n<li>To which extend is it useful to \"behave as if\"?</li>\n<li>Do you think the effect is bigger for women?</li>\n<li>What do you think about this in general?</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PgbenEuN3QnRTTmST", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "27069", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><em>\"*checking the name of the writer* Ooookay, this article about appearance is written by a woman. As was expected. It's probably not worth to read it...\"</em></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">If you thought something like this you confirmed how&nbsp;prejudices dominate our mind. And even if you didn't think something like that, you can't argue its importance away.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><strong id=\"prejudices_and_stereotypes\">prejudices and stereotypes</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><span style=\"color: #800080;\"><a href=\"http://en.wikipedia.org/wiki/Prejudice\"><span style=\"color: #800080;\">Prejudice is prejudgment, or forming an opinion before becoming aware of the relevant facts of a case.</span></a></span></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><span style=\"color: #800080;\"><a href=\"http://en.wikipedia.org/wiki/Stereotype#Functions\"><span style=\"color: #800080;\">The cognitive function of stereotypes is to&nbsp;help make sense of the world. They are a form of categorization that helps to simplify and systematize information. Thus, information is more easily identified, recalled, predicted, and reacted to.</span></a></span></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">Prejudices and stereotypes might be useful and also harmful in some situations, but they definitely exist with all their advantages and disadvantages.&nbsp;They are based on the fastest&nbsp;available information. General assumptions about latent variables (such as intelligence and character) are made \u200b\u200bon external&nbsp; factors such as behavior and appearance.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><strong style=\"color: #000000;\" id=\"Pygmalion_effect_self_fulfilling_prophecy\">Pygmalion effect/self-fulfilling prophecy</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><a href=\"http://en.wikipedia.org/wiki/Pygmalion_effect\">The Pygmalion effect, or Rosenthal effect, is the phenomenon whereby the greater the expectation placed upon people, the better they perform.[1]&nbsp;</a>(Or the observer thinks it would be so!)&nbsp;<a href=\"http://en.wikipedia.org/wiki/Pygmalion_effect\">A corollary of the Pygmalion effect is the golem effect, in which low expectations lead to a decrease in performance.[1]</a></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">So what others (and we) expect from us influences how they and we behave and therefore influences our future and what we become!</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><strong id=\"Confirmation_Bias\">Confirmation Bias</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><a href=\"http://en.wikipedia.org/wiki/Confirmation_bias\">Confirmation bias, also called myside bias, is the tendency to search for or interpret information in a way that confirms one's beliefs or hypotheses.</a></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">Because it is easier to confirm people in their presumption than to convince them otherwise,&nbsp;it's a good decision&nbsp;to look like you want people to think you are as a person.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><strong id=\"Minority_influence_innovation\">Minority influence/innovation</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><a href=\"http://en.wikipedia.org/wiki/Minority_influence\"><span style=\"color: #800080;\">Majority influence refers to the majority trying to produce conformity on the minority, while minority influence is converting the majority to adopt the thinking of the minority group.[1] Unlike other forms of influence, minority influence usually involves a personal shift in private opinion. Minority influence is also a central component of identity politics.&nbsp;</span></a></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">Minorities have a bigger impact when they: are consistent, are part of the ingroup and differ just in this one point (Idiosynkrasiekredit). As an example, your chances are higher to convince others to legalize cannabis if you wear suits instead of dreadlocks and hippie clothes.&nbsp;Your influence is therefore likely to be bigger if you behave and look like a adapted or even successful person.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><span style=\"color: #800080;\"><a href=\"http://en.wikipedia.org/wiki/Self-evaluation_motives\"><span style=\"color: #800080;\"><strong>Self-evaluation</strong></span></a></span></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">Since what others think of you will modify your self-evaluation your appearance will influence your self-evaluation, too. Also by direct feedback when looking in the mirror.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><strong id=\"My_personal_background\">My personal background</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">I tried lots of different styles (bold, dreadlocks, gothic, sporty, well-dressed ... &nbsp;) and experienced big effects on how people behaved&nbsp;towards me.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\">Pictures: http://annaleptikon.com/wp-content/uploads/2014/08/10015097_677981302243907_1187239476_o.jpg</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px; text-align: justify;\"><strong id=\"further_thoughts_questions_\">further thoughts/questions:</strong></p>\n<ul style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">\n<li>probably only people who thought about this already will be attracted by the topic and read this article ^^</li>\n<li>in the opposite way: being dressed to well might make you look stupid (imagine a \"Barbie\" talking about AI)</li>\n<li>To which extend is it useful to \"behave as if\"?</li>\n<li>Do you think the effect is bigger for women?</li>\n<li>What do you think about this in general?</li>\n</ul>", "sections": [{"title": "prejudices and stereotypes", "anchor": "prejudices_and_stereotypes", "level": 1}, {"title": "Pygmalion effect/self-fulfilling prophecy", "anchor": "Pygmalion_effect_self_fulfilling_prophecy", "level": 1}, {"title": "Confirmation Bias", "anchor": "Confirmation_Bias", "level": 1}, {"title": "Minority influence/innovation", "anchor": "Minority_influence_innovation", "level": 1}, {"title": "My personal background", "anchor": "My_personal_background", "level": 1}, {"title": "further thoughts/questions:", "anchor": "further_thoughts_questions_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-31T13:26:24.555Z", "modifiedAt": null, "url": null, "title": "Robin Hanson's \"Overcoming Bias\"  posts as an e-book.", "slug": "robin-hanson-s-overcoming-bias-posts-as-an-e-book", "viewCount": null, "lastCommentedAt": "2014-09-11T22:18:21.616Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tk6hKNyCSfDP7ayu7/robin-hanson-s-overcoming-bias-posts-as-an-e-book", "pageUrlRelative": "/posts/tk6hKNyCSfDP7ayu7/robin-hanson-s-overcoming-bias-posts-as-an-e-book", "linkUrl": "https://www.lesswrong.com/posts/tk6hKNyCSfDP7ayu7/robin-hanson-s-overcoming-bias-posts-as-an-e-book", "postedAtFormatted": "Sunday, August 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Robin%20Hanson's%20%22Overcoming%20Bias%22%20%20posts%20as%20an%20e-book.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARobin%20Hanson's%20%22Overcoming%20Bias%22%20%20posts%20as%20an%20e-book.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftk6hKNyCSfDP7ayu7%2Frobin-hanson-s-overcoming-bias-posts-as-an-e-book%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Robin%20Hanson's%20%22Overcoming%20Bias%22%20%20posts%20as%20an%20e-book.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftk6hKNyCSfDP7ayu7%2Frobin-hanson-s-overcoming-bias-posts-as-an-e-book", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftk6hKNyCSfDP7ayu7%2Frobin-hanson-s-overcoming-bias-posts-as-an-e-book", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 49, "htmlBody": "<p>At Luke Muehlhauser's request, I wrote a script to scrape all of Robin Hanson's posts to <a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a> into an e-book; here's a first beta release. Please comment here with any problems&mdash;posts in the wrong order, broken links, bad formatting, missing posts. Thanks!</p>\n<p>&nbsp;</p>\n<ul>\n<li><a href=\"https://drive.google.com/file/d/0B058DTBqtxVLUjVfU0hFRUx4NG8/edit?usp=sharing\">robinhansonblogs-beta-2014-08-31.epub</a></li>\n<li><a href=\"https://drive.google.com/file/d/0B058DTBqtxVLdXRRYVpGMnpEUWs/edit?usp=sharing\">robinhansonblogs-beta-2014-08-31.mobi</a></li>\n</ul>\n<div><br /></div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tk6hKNyCSfDP7ayu7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 30, "extendedScore": null, "score": 1.968843934245498e-06, "legacy": true, "legacyId": "27071", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-31T13:40:15.659Z", "modifiedAt": null, "url": null, "title": "Meetup : Utrecht: Effective Altruism and Politics", "slug": "meetup-utrecht-effective-altruism-and-politics", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "immasix", "createdAt": "2014-04-06T17:36:45.072Z", "isAdmin": false, "displayName": "immasix"}, "userId": "44GtXZbQH2bdkvJ64", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/juD9DmzGzJyJYcngk/meetup-utrecht-effective-altruism-and-politics", "pageUrlRelative": "/posts/juD9DmzGzJyJYcngk/meetup-utrecht-effective-altruism-and-politics", "linkUrl": "https://www.lesswrong.com/posts/juD9DmzGzJyJYcngk/meetup-utrecht-effective-altruism-and-politics", "postedAtFormatted": "Sunday, August 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Utrecht%3A%20Effective%20Altruism%20and%20Politics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Utrecht%3A%20Effective%20Altruism%20and%20Politics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjuD9DmzGzJyJYcngk%2Fmeetup-utrecht-effective-altruism-and-politics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Utrecht%3A%20Effective%20Altruism%20and%20Politics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjuD9DmzGzJyJYcngk%2Fmeetup-utrecht-effective-altruism-and-politics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjuD9DmzGzJyJYcngk%2Fmeetup-utrecht-effective-altruism-and-politics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13u'>Utrecht: Effective Altruism and Politics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 October 2014 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Film Caf\u00e9 Oskar, Slachtstraat 5, Utrecht</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We have biweekly meetups in a pub in Utrecht, near Central Station. For details, please look on meetup.com which is supposed to be up to date.\n<a href=\"http://www.meetup.com/LWEANL/events/202722652/\" rel=\"nofollow\">http://www.meetup.com/LWEANL/events/202722652/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13u'>Utrecht: Effective Altruism and Politics</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "juD9DmzGzJyJYcngk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.9688680579202136e-06, "legacy": true, "legacyId": "27072", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Utrecht__Effective_Altruism_and_Politics\">Discussion article for the meetup : <a href=\"/meetups/13u\">Utrecht: Effective Altruism and Politics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 October 2014 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Film Caf\u00e9 Oskar, Slachtstraat 5, Utrecht</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We have biweekly meetups in a pub in Utrecht, near Central Station. For details, please look on meetup.com which is supposed to be up to date.\n<a href=\"http://www.meetup.com/LWEANL/events/202722652/\" rel=\"nofollow\">http://www.meetup.com/LWEANL/events/202722652/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Utrecht__Effective_Altruism_and_Politics1\">Discussion article for the meetup : <a href=\"/meetups/13u\">Utrecht: Effective Altruism and Politics</a></h2>", "sections": [{"title": "Discussion article for the meetup : Utrecht: Effective Altruism and Politics", "anchor": "Discussion_article_for_the_meetup___Utrecht__Effective_Altruism_and_Politics", "level": 1}, {"title": "Discussion article for the meetup : Utrecht: Effective Altruism and Politics", "anchor": "Discussion_article_for_the_meetup___Utrecht__Effective_Altruism_and_Politics1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-31T13:43:18.603Z", "modifiedAt": null, "url": null, "title": "Meetup : Utrecht: Artificial Intelligence", "slug": "meetup-utrecht-artificial-intelligence", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "immasix", "createdAt": "2014-04-06T17:36:45.072Z", "isAdmin": false, "displayName": "immasix"}, "userId": "44GtXZbQH2bdkvJ64", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TrWNQ4Wp7BfMNZRRZ/meetup-utrecht-artificial-intelligence", "pageUrlRelative": "/posts/TrWNQ4Wp7BfMNZRRZ/meetup-utrecht-artificial-intelligence", "linkUrl": "https://www.lesswrong.com/posts/TrWNQ4Wp7BfMNZRRZ/meetup-utrecht-artificial-intelligence", "postedAtFormatted": "Sunday, August 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Utrecht%3A%20Artificial%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Utrecht%3A%20Artificial%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTrWNQ4Wp7BfMNZRRZ%2Fmeetup-utrecht-artificial-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Utrecht%3A%20Artificial%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTrWNQ4Wp7BfMNZRRZ%2Fmeetup-utrecht-artificial-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTrWNQ4Wp7BfMNZRRZ%2Fmeetup-utrecht-artificial-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13v'>Utrecht: Artificial Intelligence</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 October 2014 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Film Caf\u00e9 Oskar, Slachtstraat 5,  Utrecht</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We have biweekly meetups in a pub in Utrecht, near Central Station. For details, please look on meetup.com which is supposed to be up to date.\n<a href=\"http://www.meetup.com/LWEANL/events/202722812/\" rel=\"nofollow\">http://www.meetup.com/LWEANL/events/202722812/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13v'>Utrecht: Artificial Intelligence</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TrWNQ4Wp7BfMNZRRZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.9688733680533015e-06, "legacy": true, "legacyId": "27073", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Utrecht__Artificial_Intelligence\">Discussion article for the meetup : <a href=\"/meetups/13v\">Utrecht: Artificial Intelligence</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 October 2014 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Film Caf\u00e9 Oskar, Slachtstraat 5,  Utrecht</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We have biweekly meetups in a pub in Utrecht, near Central Station. For details, please look on meetup.com which is supposed to be up to date.\n<a href=\"http://www.meetup.com/LWEANL/events/202722812/\" rel=\"nofollow\">http://www.meetup.com/LWEANL/events/202722812/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Utrecht__Artificial_Intelligence1\">Discussion article for the meetup : <a href=\"/meetups/13v\">Utrecht: Artificial Intelligence</a></h2>", "sections": [{"title": "Discussion article for the meetup : Utrecht: Artificial Intelligence", "anchor": "Discussion_article_for_the_meetup___Utrecht__Artificial_Intelligence", "level": 1}, {"title": "Discussion article for the meetup : Utrecht: Artificial Intelligence", "anchor": "Discussion_article_for_the_meetup___Utrecht__Artificial_Intelligence1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-31T13:52:20.078Z", "modifiedAt": null, "url": null, "title": "Meetup : Utrecht: Climate Change", "slug": "meetup-utrecht-climate-change", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "immasix", "createdAt": "2014-04-06T17:36:45.072Z", "isAdmin": false, "displayName": "immasix"}, "userId": "44GtXZbQH2bdkvJ64", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Sd7NcR9eciifGjpjr/meetup-utrecht-climate-change", "pageUrlRelative": "/posts/Sd7NcR9eciifGjpjr/meetup-utrecht-climate-change", "linkUrl": "https://www.lesswrong.com/posts/Sd7NcR9eciifGjpjr/meetup-utrecht-climate-change", "postedAtFormatted": "Sunday, August 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Utrecht%3A%20Climate%20Change&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Utrecht%3A%20Climate%20Change%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSd7NcR9eciifGjpjr%2Fmeetup-utrecht-climate-change%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Utrecht%3A%20Climate%20Change%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSd7NcR9eciifGjpjr%2Fmeetup-utrecht-climate-change", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSd7NcR9eciifGjpjr%2Fmeetup-utrecht-climate-change", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13w'>Utrecht: Climate Change</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 November 2014 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Film Caf\u00e9 Oskar, Slachtstraat 5,  Utrecht</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We have biweekly meetups in a pub in Utrecht, near Central Station. For details, please look on meetup.com which is supposed to be up to date.\n<a href=\"http://www.meetup.com/LWEANL/events/202723022/\" rel=\"nofollow\">http://www.meetup.com/LWEANL/events/202723022/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13w'>Utrecht: Climate Change</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Sd7NcR9eciifGjpjr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.9688890852854616e-06, "legacy": true, "legacyId": "27074", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Utrecht__Climate_Change\">Discussion article for the meetup : <a href=\"/meetups/13w\">Utrecht: Climate Change</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 November 2014 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Film Caf\u00e9 Oskar, Slachtstraat 5,  Utrecht</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We have biweekly meetups in a pub in Utrecht, near Central Station. For details, please look on meetup.com which is supposed to be up to date.\n<a href=\"http://www.meetup.com/LWEANL/events/202723022/\" rel=\"nofollow\">http://www.meetup.com/LWEANL/events/202723022/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Utrecht__Climate_Change1\">Discussion article for the meetup : <a href=\"/meetups/13w\">Utrecht: Climate Change</a></h2>", "sections": [{"title": "Discussion article for the meetup : Utrecht: Climate Change", "anchor": "Discussion_article_for_the_meetup___Utrecht__Climate_Change", "level": 1}, {"title": "Discussion article for the meetup : Utrecht: Climate Change", "anchor": "Discussion_article_for_the_meetup___Utrecht__Climate_Change1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-31T14:59:31.480Z", "modifiedAt": null, "url": null, "title": "Superintelligence reading group", "slug": "superintelligence-reading-group", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:39.976Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QDmzDZ9CEHrKQdvcn/superintelligence-reading-group", "pageUrlRelative": "/posts/QDmzDZ9CEHrKQdvcn/superintelligence-reading-group", "linkUrl": "https://www.lesswrong.com/posts/QDmzDZ9CEHrKQdvcn/superintelligence-reading-group", "postedAtFormatted": "Sunday, August 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Superintelligence%20reading%20group&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuperintelligence%20reading%20group%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQDmzDZ9CEHrKQdvcn%2Fsuperintelligence-reading-group%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Superintelligence%20reading%20group%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQDmzDZ9CEHrKQdvcn%2Fsuperintelligence-reading-group", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQDmzDZ9CEHrKQdvcn%2Fsuperintelligence-reading-group", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 591, "htmlBody": "<p>In just over two weeks I will be running an online reading group on Nick Bostrom's <em><a href=\"http://en.wikipedia.org/wiki/Superintelligence\">Superintelligence</a></em>, on behalf of <a href=\"http://intelligence.org/\">MIRI</a>. It will be here on LessWrong. This is an advance warning, so you can <a href=\"http://api.viglink.com/api/click?format=go&amp;jsonp=vglnk_jsonp_14094970392396&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;libId=eba361d5-f623-4b32-ba74-e1dc943c603d&amp;loc=http%3A%2F%2Flesswrong.com%2Fuser%2FKatjaGrace%2Foverview%2F&amp;v=1&amp;out=http%3A%2F%2Fwww.amazon.com%2FSuperintelligence-Dangers-Strategies-Nick-Bostrom%2Fdp%2F0199678111&amp;ref=http%3A%2F%2Flesswrong.com%2F&amp;title=Overview%20for%20KatjaGrace%20-%20Less%20Wrong&amp;txt=Superintelligence\">get a copy</a> and get ready for some stimulating discussion. MIRI's <a href=\"http://intelligence.org/2014/08/31/superintelligence-reading-group/\">post</a>, appended below, gives the details.</p>\n<p><strong>Added:</strong>&nbsp;At the bottom of this post is a list of the discussion posts so far.</p>\n<hr />\n<p><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_kw4_0.png\" alt=\"\" width=\"489\" height=\"157\" /></p>\n<p>Nick Bostrom&rsquo;s eagerly awaited&nbsp;<a style=\"color: #2863a2; text-decoration: none;\" href=\"http://smile.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111/\"><em>Superintelligence</em></a>&nbsp;comes out in the US this&nbsp;week. To help you get the most out of it, MIRI is running&nbsp;an online reading group where you can join with others to ask questions, discuss ideas, and probe the arguments more deeply.</p>\n<p>The reading group will &ldquo;meet&rdquo; on&nbsp;a weekly post on the&nbsp;<a style=\"color: #2863a2; text-decoration: none;\" href=\"/r/discussion/new/\">LessWrong discussion forum</a>. For each &lsquo;meeting&rsquo;, we will read about half a chapter of&nbsp;<em>Superintelligence</em>, then come together virtually to discuss. I&rsquo;ll summarize the chapter, and offer a few relevant notes, thoughts, and ideas for further investigation.&nbsp;(My notes will also be used as the source material for the&nbsp;final reading guide for the book.)</p>\n<p>Discussion will take place in the comments. I&rsquo;ll offer some questions, and invite you to bring your own, as well as thoughts, criticisms and suggestions for interesting related material. Your contributions to the reading group might&nbsp;also (with permission) be used in our final&nbsp;reading guide for the book.</p>\n<p>We welcome both newcomers and veterans on&nbsp;the topic. Content will aim to be intelligible to a wide audience, and topics will range from novice to expert level. All levels of time commitment are&nbsp;welcome.</p>\n<p>We will follow&nbsp;<a style=\"color: #2863a2; text-decoration: none;\" href=\"https://intelligence.org/wp-content/uploads/2014/08/Superintelligence-Readers-Guide-early-version.pdf\"><strong>this preliminary&nbsp;reading guide</strong></a>, produced by MIRI, reading one section per week.</p>\n<p>If you have already read the book, don&rsquo;t worry! To the extent you remember what it says, your superior expertise will only be a bonus. To the extent you don&rsquo;t remember what it says, now&nbsp;is a good time for a review! If you don&rsquo;t have time to read the book, but still want to participate, you are also welcome to join in. I will provide summaries, and many things will have page numbers, in case you want to skip to the relevant parts.</p>\n<p>If this sounds good to you, first grab a&nbsp;copy of&nbsp;<em><a style=\"color: #2863a2; text-decoration: none;\" href=\"http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111\">Superintelligence</a></em>. You may also want to&nbsp;<a style=\"color: #2863a2; text-decoration: none;\" href=\"http://eepurl.com/1-S41\"><strong>sign up&nbsp;here</strong></a>&nbsp;to be emailed when the discussion begins each week. The first virtual meeting (forum post) will go live at 6pm Pacific on&nbsp;<strong>Monday, September 15th</strong>. Following meetings will start at 6pm every Monday, so if you&rsquo;d like to coordinate for quick fire discussion with others, put that into your calendar. If you prefer flexibility, come by any time!&nbsp;And remember that if there are any people you would especially enjoy discussing&nbsp;<em>Superintelligence</em>&nbsp;with, link them to this post!</p>\n<p>Topics for the first week will include impressive displays of artificial intelligence, why computers play board games so well, and what a reasonable person should infer from the agricultural and industrial&nbsp;revolutions.</p>\n<hr />\n<h1>Posts in this sequence</h1>\n<p>Week 1: <a href=\"/lw/ku6/superintelligence_reading_group_section_1_past/\">Past developments and present capabilities</a></p>\n<p>Week 2: <a href=\"/r/discussion/lw/l0o/superintelligence_reading_group_2_forecasting_ai/\">Forecasting AI</a></p>\n<p>Week 3: <a href=\"/r/discussion/lw/l10/superintelligence_reading_group_3_ai_and_uploads/\">AI and uploads</a></p>\n<p>Week 4: <a href=\"/r/discussion/lw/l2n/srg_4_biological_cognition_bcis_organizations/\">Biological cognition, BCIs, organizations</a></p>\n<p>Week 5: <a href=\"/lw/l40/superintelligence_5_forms_of_superintelligence/\">Forms of superintelligence</a></p>\n<p>Week 6: <a href=\"/r/discussion/lw/l4e/superintelligence_6_intelligence_explosion/\">Intelligence explosion kinetics</a></p>\n<p>Week 7: <a href=\"/lw/l4i/superintelligence_7_decisive_strategic_advantage/\">Decisive strategic advantage</a></p>\n<p>Week 8: <a href=\"/r/discussion/lw/l4h/superintelligence_8_cognitive_superpowers/\">Cognitive superpowers</a></p>\n<p>Week 9: <a href=\"/r/discussion/lw/l4g/superintelligence_9_the_orthogonality_of/\">The orthogonality of intelligence and goals</a></p>\n<p>Week 10: <a href=\"/r/discussion/lw/l4f/superintelligence_10_instrumentally_convergent/\">Instrumentally convergent goals</a></p>\n<p>Week 11: <a href=\"/r/discussion/lw/l9u/superintelligence_11_the_treacherous_turn/\">The treacherous turn</a></p>\n<p>Week 12: <a href=\"/r/discussion/lw/l9t/superintelligence_12_malignant_failure_modes/\">Malignant failure modes</a></p>\n<p>Week 13: <a href=\"/r/discussion/lw/l9s/superintelligence_13_capability_control_methods/\">Capability control methods</a></p>\n<p>Week 14: <a href=\"/lw/l9r/superintelligence_14_motivation_selection_methods/\">Motivation selection methods</a></p>\n<p>Week 15: <a href=\"/lw/l9q/superintelligence_15_oracles_genies_and_sovereigns/\">Oracles, genies and sovereigns</a></p>\n<p>Week 16: <a href=\"/r/discussion/lw/l9p/superintelligence_16_tool_ais/\">Tool AIs</a></p>\n<p>Week 17: <a href=\"/r/discussion/lw/l9o/superintelligence_17_multipolar_scenarios/\">Multipolar scenarios</a></p>\n<p>Week 18: <a href=\"/r/discussion/lw/l9n/superintelligence_18_life_in_an_algorithmic/\">Life in an algorithmic economy</a></p>\n<p>Week 19: <a href=\"/r/discussion/lw/l9m/superintelligence_19_posttransition_formation_of/\">Post-transition formation of a singleton</a></p>\n<p>Week 20: <a href=\"/r/discussion/lw/llr/superintelligence_20_the_valueloading_problem/\">The value-loading problem</a></p>\n<p>Week 21: <a href=\"/lw/llg/superintelligence_21_value_learning/\">Value learning</a></p>\n<p>Week 22: <a href=\"/r/discussion/lw/llh/superintelligence_22_emulation_modulation_and/\">Emulation modulation and institution design</a></p>\n<p>Week 23: <a href=\"/r/discussion/lw/llq/superintelligence_23_coherent_extrapolated/\">Coherent extrapolated volition</a></p>\n<p>Week 24: <a href=\"/r/discussion/lw/llp/superintelligence_24_morality_models_and_do_what/\">Morality models and \"do what I mean\"</a></p>\n<p>Week 25: <a href=\"/r/discussion/lw/llo/superintelligence_25_components_list_for/\">Components list for acquiring values</a></p>\n<p>Week 26: <a href=\"/r/discussion/lw/lln/superintelligence_26_science_and_technology/\">Science and technology strategy</a></p>\n<p>Week 27: <a href=\"/r/discussion/lw/llm/superintelligence_27_pathways_and_enablers/\">Pathways and enablers</a></p>\n<p>Week 28: <a href=\"/r/discussion/lw/lll/superintelligence_28_collaboration/\">Collaboration</a></p>\n<p>Week 29: <a href=\"/r/discussion/lw/llk/superintelligence_29_crunch_time/\">Crunch time</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"tdt83ChxnEgwwKxi6": 1, "sYm3HiWcfZvrGu3ui": 1, "5f5c37ee1b5cdee568cfb297": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QDmzDZ9CEHrKQdvcn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 30, "extendedScore": null, "score": 1.969006110688378e-06, "legacy": true, "legacyId": "27076", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mmZ2PaRo86pDXu8ii", "56b8n8FT6fksnDZwY", "hopzyM5ckzMNHwcQR", "SEtkbLgusgtQ9dAzX", "semvkn56ZFcXBNc2d", "GT8uvxBjidrmM3MCv", "vkjWGJrFWBnzHtxrw", "C9KJ8BrLqfsBuHgpB", "FtAJZWCMps7FWKTT3", "BD6G9wzRRt3fxckNC", "B39GNTsN3HocW8KFo", "BqoE5vhPNCB7X6Say", "398Swu6jmczzSRvHy", "FBEaheqfmDgL6gB5x", "yTy2Fp8Wm7m8rHHz5", "sL8hCYecDwcrRhfCT", "8QgNrNPaoyZeEY4ZD", "iZNcMkS6ghqBQA24E", "BoGWagWmM9XMnS9DH", "FP8T6rdZ3ohXxJRto", "bFQfgwm72Zz9ZjTh4", "NFTe38cwu7LqT2oTy", "EQFfj5eC5mqBMxF2s", "NK5bpqBcJvtBxrorh", "MFgj8hcTB9gjjL9rE", "kADkXCAq6aBBxSyqE", "sfGBkyDyu96eePZZ6", "uBzeBhySrQaoZkNCD", "H7kzai8uwPj9mQz9M"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-31T19:00:38.209Z", "modifiedAt": null, "url": null, "title": "Why appearance matters or \u201cto behave as if\u201d", "slug": "why-appearance-matters-or-to-behave-as-if-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:05.991Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaLeptikon", "createdAt": "2014-03-29T17:32:19.520Z", "isAdmin": false, "displayName": "AnnaLeptikon"}, "userId": "FyZibA2dPTe9zcmND", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Hpcy6QLiY22uLxdnQ/why-appearance-matters-or-to-behave-as-if-1", "pageUrlRelative": "/posts/Hpcy6QLiY22uLxdnQ/why-appearance-matters-or-to-behave-as-if-1", "linkUrl": "https://www.lesswrong.com/posts/Hpcy6QLiY22uLxdnQ/why-appearance-matters-or-to-behave-as-if-1", "postedAtFormatted": "Sunday, August 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20appearance%20matters%20or%20%E2%80%9Cto%20behave%20as%20if%E2%80%9D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20appearance%20matters%20or%20%E2%80%9Cto%20behave%20as%20if%E2%80%9D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHpcy6QLiY22uLxdnQ%2Fwhy-appearance-matters-or-to-behave-as-if-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20appearance%20matters%20or%20%E2%80%9Cto%20behave%20as%20if%E2%80%9D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHpcy6QLiY22uLxdnQ%2Fwhy-appearance-matters-or-to-behave-as-if-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHpcy6QLiY22uLxdnQ%2Fwhy-appearance-matters-or-to-behave-as-if-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 554, "htmlBody": "<div style=\"font-family: Helvetica; font-size: 13px;\">\"*checking the name of the writer* Ooookay, this article about appearance is written by a woman. As was expected. It's probably not worth to read it...\"</div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\">If you thought something like this you confirmed how prejudices dominate our mind. And even if you didn't think something like that, you can't argue its importance away.</div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><strong>prejudices and stereotypes</strong></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><em>Prejudice is prejudgment, or forming an opinion before becoming aware of the relevant facts of a case. (wikipedia)</em></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><em><br /></em></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><em>The cognitive function of stereotypes is to help make sense of the world. They are a form of categorization that helps to simplify and systematize information. Thus, information is more easily identified, recalled, predicted, and reacted to. (wikipedia)</em></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\">Prejudices and stereotypes might be useful and also harmful in some situations, but they definitely exist with all their advantages and disadvantages. They are based on the fastest available information. General assumptions about latent variables (such as intelligence and character) are made \u200b\u200bon external &nbsp;factors such as behavior and appearance.</div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><strong>Pygmalion effect/self-fulfilling prophecy</strong></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><em>The Pygmalion effect, or Rosenthal effect, is the phenomenon whereby the greater the expectation placed upon people, the better they perform.[1] </em>(Or the observer thinks it would be so!)<em>&nbsp;A corollary of the Pygmalion effect is the golem effect, in which low expectations lead to a decrease in performance. (wikipedia)</em></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\">So what others (and we) expect from us influences how they and we behave and therefore influences our future and what we become!</div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><strong>Confirmation Bias</strong></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><em>Confirmation bias, also called myside bias, is the tendency to search for or interpret information in a way that confirms one's beliefs or hypotheses. (wikipedia)</em></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\">Because it is easier to confirm people in their presumption than to convince them otherwise, it's a good decision to look like you want people to think you are as a person.</div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><strong>Minority influence/innovation</strong></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><strong><br /></strong></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><em>Majority influence refers to the majority trying to produce conformity on the minority, while minority influence is converting the majority to adopt the thinking of the minority group.[1] Unlike other forms of influence, minority influence usually involves a personal shift in private opinion. Minority influence is also a central component of identity politics.(wikipedia)</em></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\">Minorities have a bigger impact when they: are consistent, are part of the ingroup and differ just in this one point (Idiosynkrasiekredit). As an example, your chances are higher to convince others to legalize cannabis if you wear suits instead of dreadlocks and hippie clothes. Your influence is therefore likely to be bigger if you behave and look like a adapted or even successful person.</div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><strong>Self-evaluation</strong></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><strong><br /></strong></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\">Since what others think of you will modify your self-evaluation, your appearance will influence your self-evaluation, too. Also by direct feedback when looking in the mirror.</div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><br /></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\"><strong>further thoughts/questions:</strong></div>\n<div style=\"font-family: Helvetica; font-size: 13px;\">\n<ul>\n<li>probably only people who thought about this already will be attracted by the topic and read this article ^^</li>\n<li>in the opposite way: being dressed to well might make you look stupid (imagine a \"Barbie\" talking about AI)</li>\n<li>To which extend is it useful to \"behave as if\"?</li>\n<li>What do you think about this thoughts in general?</li>\n</ul>\n<div><br /></div>\n<div>My personal background: I tried lots of different styles (bold, dreadlocks, gothic, sporty, well-dressed ... ) and experienced big effects on how people behaved towards me.(<a href=\" http://annaleptikon.com/wp-content/uploads/2014/08/10015097_677981302243907_1187239476_o-1024x386.jpg\">pictures</a>)</div>\n<strong></strong></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Hpcy6QLiY22uLxdnQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -5, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "27077", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-31T22:38:38.332Z", "modifiedAt": null, "url": null, "title": "Tips for writing philosophical texts", "slug": "tips-for-writing-philosophical-texts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:06.295Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jan_Rzymkowski", "createdAt": "2014-05-09T22:34:32.079Z", "isAdmin": false, "displayName": "Jan_Rzymkowski"}, "userId": "kTxmHkNaDGDPpFwci", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/D7GdiSMCzErAqNo6w/tips-for-writing-philosophical-texts", "pageUrlRelative": "/posts/D7GdiSMCzErAqNo6w/tips-for-writing-philosophical-texts", "linkUrl": "https://www.lesswrong.com/posts/D7GdiSMCzErAqNo6w/tips-for-writing-philosophical-texts", "postedAtFormatted": "Sunday, August 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tips%20for%20writing%20philosophical%20texts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATips%20for%20writing%20philosophical%20texts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD7GdiSMCzErAqNo6w%2Ftips-for-writing-philosophical-texts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tips%20for%20writing%20philosophical%20texts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD7GdiSMCzErAqNo6w%2Ftips-for-writing-philosophical-texts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD7GdiSMCzErAqNo6w%2Ftips-for-writing-philosophical-texts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>For about four years I am struggling to write a series of articles presenting few of my ideas. While this \"philosophy\" (I'd rather avoid being too pompous about it) is still developing, there is a bunch of stuff of which I have a clear image in my mind. It is a framework for model building, with some possible applications for AI developement, paradox resolving, semantics. Not any serious impact, but I do believe it would prove useful.</p>\n<p>I tried making notes or plans for articles several times, but every time I was discouraged by those problems:</p>\n<ul>\n<li>presented concept is too obvious</li>\n<li>presented concept is superflous</li>\n<li>presented concept needs more basic ideas to be introduced beforehand</li>\n</ul>\n<p>So the core problem is that to show applications of the theory (or generally more interesing results), more basic concepts must be introduced first. Yet presenting the basics seems boring and uninsightful without the application side. This seems to characterise many complex ideas.</p>\n<p>Can you provide me with any practical tips as how to tackle this problem?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "D7GdiSMCzErAqNo6w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.969806083414162e-06, "legacy": true, "legacyId": "27078", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-08-31T22:43:27.697Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Practical Rationality", "slug": "meetup-urbana-champaign-practical-rationality", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jHb8F7nY8kMPFTYX7/meetup-urbana-champaign-practical-rationality", "pageUrlRelative": "/posts/jHb8F7nY8kMPFTYX7/meetup-urbana-champaign-practical-rationality", "linkUrl": "https://www.lesswrong.com/posts/jHb8F7nY8kMPFTYX7/meetup-urbana-champaign-practical-rationality", "postedAtFormatted": "Sunday, August 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Practical%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Practical%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjHb8F7nY8kMPFTYX7%2Fmeetup-urbana-champaign-practical-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Practical%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjHb8F7nY8kMPFTYX7%2Fmeetup-urbana-champaign-practical-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjHb8F7nY8kMPFTYX7%2Fmeetup-urbana-champaign-practical-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 68, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13x'>Urbana-Champaign: Practical Rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 September 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">206 S. Cedar St, 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>What techniques do you use for acting effectively given that you are a squishy irrational meatbag? Jack will contribute some ideas which he has smuggled to us from CFAR.</p>\n\n<p>Also, we will continue refining the rules of Wits and Wagers.</p>\n\n<p><a href=\"https://groups.google.com/forum/#!forum/lesswrong-urbana-champaign\">Google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13x'>Urbana-Champaign: Practical Rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jHb8F7nY8kMPFTYX7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.9698144896669013e-06, "legacy": true, "legacyId": "27079", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Practical_Rationality\">Discussion article for the meetup : <a href=\"/meetups/13x\">Urbana-Champaign: Practical Rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 September 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">206 S. Cedar St, 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>What techniques do you use for acting effectively given that you are a squishy irrational meatbag? Jack will contribute some ideas which he has smuggled to us from CFAR.</p>\n\n<p>Also, we will continue refining the rules of Wits and Wagers.</p>\n\n<p><a href=\"https://groups.google.com/forum/#!forum/lesswrong-urbana-champaign\">Google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Practical_Rationality1\">Discussion article for the meetup : <a href=\"/meetups/13x\">Urbana-Champaign: Practical Rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Practical Rationality", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Practical_Rationality", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Practical Rationality", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Practical_Rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-01T03:13:41.996Z", "modifiedAt": null, "url": null, "title": "Today's Extremist \"Radical\" Professors vs the Old \"Red Intellectuals\"", "slug": "today-s-extremist-radical-professors-vs-the-old-red", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:06.006Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HalMorris", "createdAt": "2012-12-08T02:54:12.946Z", "isAdmin": false, "displayName": "HalMorris"}, "userId": "8cZxp4PS87vNbhmCf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6bXtjSH6nCvo4Npzc/today-s-extremist-radical-professors-vs-the-old-red", "pageUrlRelative": "/posts/6bXtjSH6nCvo4Npzc/today-s-extremist-radical-professors-vs-the-old-red", "linkUrl": "https://www.lesswrong.com/posts/6bXtjSH6nCvo4Npzc/today-s-extremist-radical-professors-vs-the-old-red", "postedAtFormatted": "Monday, September 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Today's%20Extremist%20%22Radical%22%20Professors%20vs%20the%20Old%20%22Red%20Intellectuals%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AToday's%20Extremist%20%22Radical%22%20Professors%20vs%20the%20Old%20%22Red%20Intellectuals%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bXtjSH6nCvo4Npzc%2Ftoday-s-extremist-radical-professors-vs-the-old-red%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Today's%20Extremist%20%22Radical%22%20Professors%20vs%20the%20Old%20%22Red%20Intellectuals%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bXtjSH6nCvo4Npzc%2Ftoday-s-extremist-radical-professors-vs-the-old-red", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bXtjSH6nCvo4Npzc%2Ftoday-s-extremist-radical-professors-vs-the-old-red", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 315, "htmlBody": "<p>The following is how it looks to me from a distance.&nbsp; Since many readers are fairly recent college graduates, I'd be interested in other views.&nbsp; I've been thinking of posing a question about \"Anti-rationalism on Campus\".&nbsp; I suspect there is a small cadre who may say more extreme sounding things than have been said in the past, but they are incoherent, and reports of Universities as left wing robot factories are highly exagerated.</p>\n<p>&gt;Today's \"left wing\" intellectuals are blatherers. Postmodernism is  anti-Enlightenment and views Marxism as an unfortunate result of the  Enlightenment the same as capitalism. Noam Chomsky calls himself an  anarchist. They tend to be anti-everything when it comes to actually  doing something.</p>\n<p>&gt;There  is no international Communist movement, and there's been virtually none  since Brezhnev, though the USSR ran around trying to buy a lot of  countries, and certainly made a lot of trouble. If you want a clear picture of the era of \"Red  Intellectuals\", read <em>Witness</em> by Whittaker Chambers, and then I suggest <em>Reds: McCarthyism in Twentieth-Century America</em> by Ted Morgan (despite the subtitle, McCarthyism is less than half of  what the book covers). Chambers was the star witness for Nixon's  \"pumpkin papers\" trial. Both cover a lot of just how deep the  international Communist movement got into America, and Chambers writes  beautifully and helps you to see why that was. He also speaks for the  many who became deeply disillusioned by the Hitler-Stalin pact. I used  to think that was odd because in my view it was a very natural reaction  to Chamberlain's Munich, but the Communists really did put up a very  good show of defining and opposing the Fascists (I say \"a good show\" for  a reason but it's too complicated to say more), and for as long as that  was true, a lot of people put a halo on them for that, then many of  them because naively heartbroken.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6bXtjSH6nCvo4Npzc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -14, "extendedScore": null, "score": -4.9e-05, "legacy": true, "legacyId": "27080", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-01T10:03:05.770Z", "modifiedAt": null, "url": null, "title": "[link] Large Social Networks can be Targeted for Viral Marketing with Small Seed Sets", "slug": "link-large-social-networks-can-be-targeted-for-viral", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:08.037Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yyCmfdc2jXmAQt82L/link-large-social-networks-can-be-targeted-for-viral", "pageUrlRelative": "/posts/yyCmfdc2jXmAQt82L/link-large-social-networks-can-be-targeted-for-viral", "linkUrl": "https://www.lesswrong.com/posts/yyCmfdc2jXmAQt82L/link-large-social-networks-can-be-targeted-for-viral", "postedAtFormatted": "Monday, September 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Large%20Social%20Networks%20can%20be%20Targeted%20for%20Viral%20Marketing%20with%20Small%20Seed%20Sets&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Large%20Social%20Networks%20can%20be%20Targeted%20for%20Viral%20Marketing%20with%20Small%20Seed%20Sets%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyyCmfdc2jXmAQt82L%2Flink-large-social-networks-can-be-targeted-for-viral%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Large%20Social%20Networks%20can%20be%20Targeted%20for%20Viral%20Marketing%20with%20Small%20Seed%20Sets%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyyCmfdc2jXmAQt82L%2Flink-large-social-networks-can-be-targeted-for-viral", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyyCmfdc2jXmAQt82L%2Flink-large-social-networks-can-be-targeted-for-viral", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 200, "htmlBody": "<p><a href=\"http://arxiv.org/abs/1205.4431\">Large Social Networks can be Targeted for Viral Marketing with Small Seed Sets</a></p>\n<p>It shows how easy a population can be influenced if control over a small sub-set exists. &nbsp;</p>\n<blockquote>\n<p><span style=\"font-family: 'Lucida Grande', helvetica, arial, verdana, sans-serif; font-size: 14px; line-height: 20.15999984741211px;\">A key problem for viral marketers is to determine an initial \"seed\" set&nbsp;</span><span style=\"font-family: 'Lucida Grande', helvetica, arial, verdana, sans-serif; font-size: 14.399999618530273px; line-height: 20.15999984741211px;\">[&lt;1% of total size]</span><span style=\"font-family: 'Lucida Grande', helvetica, arial, verdana, sans-serif; font-size: 14.399999618530273px; line-height: 20.15999984741211px;\">&nbsp;</span><span style=\"font-family: 'Lucida Grande', helvetica, arial, verdana, sans-serif; font-size: 14px; line-height: 20.15999984741211px;\">in a network such that if given a property then the entire network adopts the behavior. Here we introduce a method for quickly finding seed sets that scales to very large networks. Our approach finds a set of nodes that guarantees spreading to the entire network under the tipping model. After experimentally evaluating 31 real-world networks, we found that our approach often finds such sets that are several orders of magnitude smaller than the population size. Our approach also scales well - on a Friendster social network consisting of 5.6 million nodes and 28 million edges we found a seed sets in under 3.6 hours. We also find that highly clustered local neighborhoods and dense network-wide community structure together suppress the ability of a trend to spread under the tipping model.</span></p>\n</blockquote>\n<p>This is relevant for LW because</p>\n<p>a) Rational agents should hedge against this.</p>\n<p>b) An UFAI could exploit this.</p>\n<p>c) It gives hints to proof systems against this 'exploit'.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yyCmfdc2jXmAQt82L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 1.9709997680303092e-06, "legacy": true, "legacyId": "27083", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-01T12:18:56.648Z", "modifiedAt": null, "url": null, "title": "Open thread, Sept. 1-7, 2014", "slug": "open-thread-sept-1-7-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:39.249Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vhBkoGYy6yh98bxCS/open-thread-sept-1-7-2014", "pageUrlRelative": "/posts/vhBkoGYy6yh98bxCS/open-thread-sept-1-7-2014", "linkUrl": "https://www.lesswrong.com/posts/vhBkoGYy6yh98bxCS/open-thread-sept-1-7-2014", "postedAtFormatted": "Monday, September 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20Sept.%201-7%2C%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20Sept.%201-7%2C%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvhBkoGYy6yh98bxCS%2Fopen-thread-sept-1-7-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20Sept.%201-7%2C%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvhBkoGYy6yh98bxCS%2Fopen-thread-sept-1-7-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvhBkoGYy6yh98bxCS%2Fopen-thread-sept-1-7-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px; font-weight: bold;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px; font-weight: bold;\"><br /></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">3.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should be posted in Discussion, and not Main.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">4.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should start on Monday, and end on Sunday.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vhBkoGYy6yh98bxCS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.97123683728156e-06, "legacy": true, "legacyId": "27084", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 162, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-01T17:05:53.382Z", "modifiedAt": null, "url": null, "title": "September 2014 Media Thread", "slug": "september-2014-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:39.419Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ArisKatsaris", "createdAt": "2010-10-07T10:24:25.721Z", "isAdmin": false, "displayName": "ArisKatsaris"}, "userId": "fLbksBTnFsbwYmzsT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ag7s4KBvoZ2S6m7Xm/september-2014-media-thread", "pageUrlRelative": "/posts/Ag7s4KBvoZ2S6m7Xm/september-2014-media-thread", "linkUrl": "https://www.lesswrong.com/posts/Ag7s4KBvoZ2S6m7Xm/september-2014-media-thread", "postedAtFormatted": "Monday, September 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20September%202014%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeptember%202014%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAg7s4KBvoZ2S6m7Xm%2Fseptember-2014-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=September%202014%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAg7s4KBvoZ2S6m7Xm%2Fseptember-2014-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAg7s4KBvoZ2S6m7Xm%2Fseptember-2014-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 219, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the <a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please post only under one of the already created subthreads, and never directly under the parent media thread.</li>\n<li>Use the \"Other Media\" thread if you believe the piece of media you want to discuss doesn't fit under any of the established categories.</li>\n<li>Use the \"Meta\" thread if you want to discuss about the monthly media thread itself (e.g. to propose adding/removing/splitting/merging subthreads, or to discuss the type of content properly belonging to each subthread) or for any other question or issue you may have about the thread or the rules.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ag7s4KBvoZ2S6m7Xm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.9717377539798568e-06, "legacy": true, "legacyId": "27086", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-02T02:05:26.189Z", "modifiedAt": null, "url": null, "title": "Truth and the Liar Paradox", "slug": "truth-and-the-liar-paradox", "viewCount": null, "lastCommentedAt": "2019-12-09T04:29:44.477Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "casebash", "createdAt": "2012-11-04T06:10:08.650Z", "isAdmin": false, "displayName": "casebash"}, "userId": "MrwJ5w7siWBQ4bMiE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZBjG6gEXCYWwdbZeq/truth-and-the-liar-paradox", "pageUrlRelative": "/posts/ZBjG6gEXCYWwdbZeq/truth-and-the-liar-paradox", "linkUrl": "https://www.lesswrong.com/posts/ZBjG6gEXCYWwdbZeq/truth-and-the-liar-paradox", "postedAtFormatted": "Tuesday, September 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Truth%20and%20the%20Liar%20Paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATruth%20and%20the%20Liar%20Paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZBjG6gEXCYWwdbZeq%2Ftruth-and-the-liar-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Truth%20and%20the%20Liar%20Paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZBjG6gEXCYWwdbZeq%2Ftruth-and-the-liar-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZBjG6gEXCYWwdbZeq%2Ftruth-and-the-liar-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1288, "htmlBody": "<p>Related: <a href=\"http://wiki.lesswrong.com/wiki/The_map_is_not_the_territory\">The map is not the territory</a>, <a href=\"/lw/36v/unsolved_problems_in_philosophy_part_1_the_liars/\">Unresolved questions in philosophy part 1: The Liar paradox</a></p>\n<p>A well-known brainteaser asks about the truth of the statement \"this statement is false\". If the statement is true, then the sentence must be false, but if it false then the sentence must be true. This paradox, far from being just a game, illustrates a question fundamental to understanding the nature of truth itself.</p>\n<p>A number of different solutions have been proposed to this paradox (and the closely related&nbsp;<a href=\"http://en.wikipedia.org/wiki/Epimenides_paradox\">Epimenides paradox</a>,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Pinocchio_paradox\">Pinocchio paradox</a>). One approach is to reject the principal of bivalence - that every proposition must be true or false - and argue that this statement is neither true nor false. Unfortunately, this approach fails to resolve the truth of \"this statement is not true\". A second approach called Dialetheism is to argue that it should be both true and false, but this fails on \"this statement is only false\".</p>\n<p>Arthur Prior's resolution it to claim that each statement implicitly asserts its own truth, so that \"this statement is false\" becomes \"this statement is false and this statement is true\". This later statement is clearly false. There do appear to be some advantages to constructing a system where each statement asserts its own truth, but the normative claim that truth should always be constructed in this manner seems to be hard to justify.</p>\n<p>Another solution (non-cognitivism) is to deny that these statement have any truth content at all, similar to meaningless statements (\"Are you a?\") or non-propositional statements like commands (\"Get me some milk?\"). If we take this approach, then a natural question is \"Which statements are meaningless?\" One answer is to exclude all statements that are self referential. However, there are a few paradoxes that complicate this. One is the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Card_paradox\">Card paradox</a>&nbsp;where the front says that the sentence on the back is true and the back says that the sentence on the front is false. Another is&nbsp;<a href=\"http://en.wikipedia.org/wiki/Quine's_paradox\">Quine's paradox</a>&nbsp;- \"<span style=\"color: #252525; font-family: sans-serif; font-size: 14px; line-height: 22.399999618530273px;\">\"Yields falsehood when preceded by its quotation\" yields falsehood when preceded by its quotation\". One other common example is:&nbsp;</span>\"The statement on the blackboard in Carslaw Room 201 is false\". The Card paradox and blackboard paradox are interesting in that if we declare the Liar paradox to be meaningless, these paradoxes are meaningless or meaningful depending on the state of the world.</p>\n<p>This problem has been previously discussed on <a href=\"/lw/36v/unsolved_problems_in_philosophy_part_1_the_liars/\">Less Wrong</a>, but I think that there is more that is worth being said on this topic. <a href=\"/lw/36v/unsolved_problems_in_philosophy_part_1_the_liars/3136\">Cousin_it</a> noted that the formalist school of philosophy (in maths) believes that \"meaningful questions have to be phrased in terms of finite computational processes\". <a href=\"/lw/36v/unsolved_problems_in_philosophy_part_1_the_liars/32k4\">Yvain</a>&nbsp;took a similar approach arguing that \"you can't use a truth-function to evaluate the truth of a noun until you unpack the noun into a sentence\" and that it would require infinite unpacking to evaluate, while&nbsp;\"This sentence is in English\" would only require a single unpacking.</p>\n<p>I'll take a similar approach, but I'll be exploring the notion of truth as a constructed concept. First I'll note that there are at least two different kinds of truth - truth of statements about the world and truth of mathematical concepts. These two kinds of truth are about completely different kinds of objects. The first are true if part of world is in a particular configuration and satisfy bivalence because the world is either in that configuration or not in that configuration.</p>\n<p>The second is a constructed system where certain basic axioms start off in the class of true formulas and we have rules of deduction to allow us to add more formulas into this class or to determine that formulas aren't in the class. One particularly interesting class of axiomatic systems has the following deductive rules:</p>\n<p>if x is in the true class, then not x is in the false class<br />if x is in the false class, then not x is in the true class<br />if not x is in the true class, then x is in the false class<br />if not x is in the false class, then x is in the true class</p>\n<p>If we start with certain primitive propositions defined as true or false and start adding operations like \"AND\", \"OR\", \"NOT\", ect. then we get propositional logic. If we define variables and predicates (functions from variables to boolean values) and \"FOR EACH\", \"THERE EXISTS\", ect, then we get first-order predicate logic and later higher order predicate logics. These logics work with the two given deductive rules and avoid a situation where both x and not x are in the true class which would for any non-trivial classical logic lead to all formulas being in the true class, which would not be a useful system.</p>\n<p>The system has a binary notion of truth which satisfies the law of excluded model because it was constructed in this manner. Mathematical truth does not exist in its own right, in only exists within a system of logic. Geometry, arithmetic and set theory can all be modelled within the same set-theoretic logic which has the same rules related to truth. But this doesn't mean that truth is a set-theoretic concept - set-theory is only one possible way of modelling these systems which then lets us combine objects from these different domains into the one proposition. Set-theory simply shows us being within the true or false class has similar effects across multiple systems. This explains why we believe that mathematical truth exists - leaving us with no reason to suppose that this kind of \"truth\" has an inherent meaning. These aren't models of the truth, \"truth\" is really just a set of useful models with similar properties.</p>\n<p>Once we realise this, these paradoxes completely dissolve. What is the truth value of \"This statement is false\"? Is it Arthur Prior's solution where he infers that the statement asserts its own truth? Is it invalid because of infinite recursion? Is it both true and false? These questions all miss the point. We define a system that puts statements into the true class, false class or whatever other classes that we want. There is no reason to assume that there is one necessarily best way of determining the truth of the statement. The value of this solution is that this dissolves the paradox without philosophically committing ourselves to formalism or Arthur Prior's notion of truth or Dialetheism&nbsp;or any other such system that would be difficult to justify as being \"the true solution\". Instead we simply have a choice of which system we wish to construct.&nbsp;</p>\n<p>I have also seen a few mentions of Tarski's type hierarchies and Kripke's fixed point theory of truth as resolving the paradox. I can't comment too much because I haven't had time to learn these yet. However, the point of this post is to resolve the paradox without committing us to a specific model of truth, as opposed to the general notion of truth as a construct.</p>\n<p><strong>Edit:&nbsp;</strong>I removed the discussion of \"This statement is true\" as it was incorrect (thanks to Manfred). The proper example was, \"This statement is either true or false\". If it is true, then that works. If it is false, then there is a contradiction. So is it true or is it meaningless given that it doesn't seem to refer to anything? This depends on how we define truth. We can either define truth only for statements that can be unpacked or we can define it for statements that have a single stable value allocation. Either version of truth could work.</p>\n<div><strong>Update</strong>: Perhaps I should have just argued that truth is constructed at least to some extent. Maybe there is actually a basic fundamental notion of truth for simple statements, but when we get to anything complicated, such as \"this statement is false\" any system for assigning truth values is simply a construction.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LnEEs8xGooYmQ8iLA": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZBjG6gEXCYWwdbZeq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 5, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "27081", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2PdR5oXCAoNjpnSSd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-02T02:47:42.730Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, September 1-15", "slug": "group-rationality-diary-september-1-15", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:19.035Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QtTp8eFW6wzQ3FK3r/group-rationality-diary-september-1-15", "pageUrlRelative": "/posts/QtTp8eFW6wzQ3FK3r/group-rationality-diary-september-1-15", "linkUrl": "https://www.lesswrong.com/posts/QtTp8eFW6wzQ3FK3r/group-rationality-diary-september-1-15", "postedAtFormatted": "Tuesday, September 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20September%201-15&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20September%201-15%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQtTp8eFW6wzQ3FK3r%2Fgroup-rationality-diary-september-1-15%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20September%201-15%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQtTp8eFW6wzQ3FK3r%2Fgroup-rationality-diary-september-1-15", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQtTp8eFW6wzQ3FK3r%2Fgroup-rationality-diary-september-1-15", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.2727279663086px;\">This is the public group instrumental rationality diary for September 1-15.</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.2727279663086px;\">\n<p style=\"margin: 0px 0px 1em;\">It's a place to record and chat about it if you have done, or are actively doing, things like:&nbsp;</p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.2727279663086px;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.2727279663086px;\">Next diary: <a href=\"/r/discussion/lw/kzv/group_rationality_diary_september_1630/\">September 16-30</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.2727279663086px;\">Previous diary:&nbsp;<span style=\"text-decoration: underline;\"><a href=\"/lw/ks1/group_rationality_diary_august_1631/\"><span style=\"color: #8a8a8b;\">August 1</span>6-31</a></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.2727279663086px;\"><a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QtTp8eFW6wzQ3FK3r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "27090", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hMReCnYz39fRupJAa", "LiRcfMM9gTBcd9igf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-02T04:04:14.656Z", "modifiedAt": null, "url": null, "title": "Overly convenient clusters, or: Beware sour grapes", "slug": "overly-convenient-clusters-or-beware-sour-grapes", "viewCount": null, "lastCommentedAt": "2020-10-19T17:54:04.034Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KnaveOfAllTrades", "createdAt": "2012-07-20T02:08:23.538Z", "isAdmin": false, "displayName": "KnaveOfAllTrades"}, "userId": "FuACexYrBpyrMmz5C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DP8HCLcSPSywDz76e/overly-convenient-clusters-or-beware-sour-grapes", "pageUrlRelative": "/posts/DP8HCLcSPSywDz76e/overly-convenient-clusters-or-beware-sour-grapes", "linkUrl": "https://www.lesswrong.com/posts/DP8HCLcSPSywDz76e/overly-convenient-clusters-or-beware-sour-grapes", "postedAtFormatted": "Tuesday, September 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Overly%20convenient%20clusters%2C%20or%3A%20Beware%20sour%20grapes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOverly%20convenient%20clusters%2C%20or%3A%20Beware%20sour%20grapes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDP8HCLcSPSywDz76e%2Foverly-convenient-clusters-or-beware-sour-grapes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Overly%20convenient%20clusters%2C%20or%3A%20Beware%20sour%20grapes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDP8HCLcSPSywDz76e%2Foverly-convenient-clusters-or-beware-sour-grapes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDP8HCLcSPSywDz76e%2Foverly-convenient-clusters-or-beware-sour-grapes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1105, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:DoNotOptimizeForBrowser /> </w:WordDocument> </xml><![endif]--></p>\n<p class=\"MsoNormal\"><strong>Related to</strong>: <a href=\"/lw/gz/policy_debates_should_not_appear_onesided/\">Policy Debates Should Not Appear One-Sided</a></p>\n<p class=\"MsoNormal\">There is <a href=\"https://en.wikipedia.org/wiki/The%20Fox%20and%20the%20Grapes\">a well-known fable</a> which runs thus:</p>\n<p class=\"MsoBodyText\"><em>&ldquo;Driven by hunger, a fox tried to reach some grapes hanging high on the vine but was unable to, although he leaped with all his strength. As he went away, the fox remarked 'Oh, you aren't even ripe yet! I don't need any sour grapes.' People who speak disparagingly of things that they cannot attain would do well to apply this story to themselves.&rdquo;</em></p>\n<p class=\"MsoNormal\">This gives rise to the common expression &lsquo;sour grapes&rsquo;, referring to a situation in which one incorrectly claims to not care about something to save face or feel better after being unable to get it.</p>\n<p class=\"MsoNormal\">This seems to be related to a general phenomenon, in which <a href=\"http://wiki.lesswrong.com/wiki/Motivated_cognition\">motivated cognition</a> leads one to flinch away from the prospect of an action that is inconvenient or painful in the short term by concluding that a less-painful option <a href=\"https://en.wikipedia.org/w/index.php?title=Strategic_dominance&amp;oldid=586287995#Terminology\">strictly dominates</a> the more-painful one.</p>\n<p class=\"MsoNormal\">In the fox&rsquo;s case, the allegedly-dominating option is believing (or professing) that he did not want the grapes. This spares him the pain of feeling impotent in face of his initial failure, or the embarrassment of others thinking him to have failed. If he can&rsquo;t get the grapes anyway, then he might as well erase the fact that he ever wanted them, right? The problem is that considering this line of reasoning will make it more tempting to conclude that the option really was dominating&mdash;that he really couldn&rsquo;t have gotten the grapes. But maybe he could&rsquo;ve gotten the grapes with a bit more work&mdash;by getting a ladder, or making a hook, or Doing More Squats in order to Improve His Vert.</p>\n<p class=\"MsoNormal\">The fable of the fox and the grapes doesn&rsquo;t feel like a perfect fit, though, because the fox doesn&rsquo;t engage in any conscious deliberation before giving up on sour grapes; the whole thing takes place subconsciously. Here are some other examples that more closely illustrate the idea of conscious rationalization by use of overly convenient partitions:</p>\n<p class=\"MsoNormal\"><a href=\"http://en.wikiquote.org/w/index.php?title=Dr._Seuss&amp;oldid=1773894#Misattributed\"><strong>The Seating Fallacy</strong>:</a></p>\n<p class=\"MsoBodyText\"><em>&ldquo;Be who you are and say what you feel, because those who mind don't matter and those who matter don't mind.&rdquo;</em></p>\n<p class=\"MsoNormal\">This advice is <a href=\"http://slatestarcodex.com/2013/06/09/all-debates-are-bravery-debates/\">neither good in full generality nor bad in full generality</a>. Clearly there are some situations where some person is worrying too much about other people judging them, or is anxious about inconveniencing others without taking their own preferences into account. But there are also clearly situations (like dealing with an unpleasant, incompetent boss) where fully exposing oneself or saying whatever comes into one&rsquo;s head is not strategic and outright disastrous. Without taking into account the specifics of the situation of the recipient of the advice, it is of limited use.</p>\n<p class=\"MsoNormal\">It is convenient to absolve oneself of blame by writing off anybody who challenges our first impulse as someone who &lsquo;doesn&rsquo;t matter&rsquo;; it means that if something goes wrong, one can avoid the painful task of analysing and modifying one&rsquo;s behaviour.</p>\n<p class=\"MsoNormal\">In particular, we have the following corollary:</p>\n<p class=\"MsoNormal\"><strong>The Fundamental Fallacy of Dating</strong>:</p>\n<p class=\"MsoBodyText\"><em>&ldquo;Be yourself and don&rsquo;t hide who you are. Be up-front about what you want. If it puts your date off, then they wouldn&rsquo;t have been good for you anyway, and you&rsquo;ve dodged a bullet!&rdquo;</em></p>\n<p class=\"MsoNormal\">In the short-term it is convenient to not have to filter or reflect on what one says (face-to-face) or writes (online dating). In the longer term, having no filter is not a smart way to approach dating. As the biases and heuristics program has shown, people are often mistaken about what they would prefer under reflection, and are often inefficient and irrational in pursuing what they want. There are complicated courtship conventions governing timelines for revealing information about oneself and negotiating preferences, that have evolved to work around these irrationalities, to the benefit of both parties. In particular, people are <a href=\"http://en.wikipedia.org/wiki/Dynamic_inconsistency\">dynamically inconsistent</a>, and willing to compromise a lot more later on in a courtship than they thought they would earlier on; it is often a favour to both of you to respect established boundaries regarding revealing information and getting ahead of the current stage of the relationship.</p>\n<p class=\"MsoNormal\">For those who have not much practised the skill of avoiding triggering Too Much Information reactions, it can feel painful and disingenuous to even try changing their behaviour, and they rationalise it via the Fundamental Fallacy. At any given moment, changing this behaviour is painful and causes a flinch reaction, even though the <a href=\"http://en.wikipedia.org/wiki/Value_of_information\">value of information</a> of trying a different approach might be very high, and might cause less pain (e.g. through reduced loneliness) in the long term.</p>\n<p class=\"MsoNormal\">We also have:</p>\n<p class=\"MsoNormal\"><strong>PR rationalization and incrimination</strong>:</p>\n<p class=\"MsoBodyText\"><em>&ldquo;There&rsquo;s already enough ammunition out there if anybody wants to assassinate my character, launch a smear campaign, or perform a hatchet job. Nothing I say at this point could make it worse, so there&rsquo;s no reason to censor myself.&rdquo;</em></p>\n<p class=\"MsoNormal\">This is an overly convenient excuse. It does not take into account, for example, that new statements provide a <em>new</em> opportunity for one to come to the attention of <a href=\"http://www.urbandictionary.com/define.php?term=quote%20mining&amp;defid=3278007\">quote miners</a> in the first place, or that different statements might be more or less easy to seed a smear campaign; ammunition can vary in type and accessibility, so that adding more can increase the convenience of a hatchet job. It might turn out, after weighing the costs and benefits, that speaking honestly is the right decision. But one can&rsquo;t know that on the strength of a convenient deontological argument that doesn&rsquo;t consider those costs. Similarly:</p>\n<p class=\"MsoBodyText\"><em>&ldquo;I&rsquo;ve already pirated so much stuff I&rsquo;d be screwed if I got caught. Maybe it was unwise and impulsive at first, but by now I&rsquo;m past the point of no return.&rdquo;</em></p>\n<p class=\"MsoBodyText\"><span style=\"font-style: normal;\">&nbsp;This again fails to take into account the increased risk of one&rsquo;s deeds coming to attention; if most prosecutions are caused by (even if not purely about) offences shortly before the prosecution, and you expect to pirate long into the future, then your position now is the same as when you first pirated; if it was unwise then, then it&rsquo;s unwise now.</span></p>\n<p class=\"MsoBodyText\"><span style=\"font-style: normal;\">~~~~</span></p>\n<p class=\"MsoBodyText\"><span style=\"font-style: normal;\">The common fallacy in all these cases is that one looks at only the extreme possibilities, and throws out the inconvenient, ambiguous cases. This results in a disconnected space of possibilities that is engineered to allow one to prove a convenient conclusion. For example, the Seating Fallacy throws out the possibility that there are people who mind but also matter; the Fundamental Fallacy of Dating prematurely rules out people who are dynamically inconsistent or are imperfect introspectors, or who have uncertainty over preferences; PR rationalization fails to consider marginal effects and quantify risks in favour of a lossy binary approach.</span></p>\n<p class=\"MsoBodyText\"><span style=\"font-style: normal;\">What are other examples of situations where people (or Less Wrongers specifically) might fall prey to this failure mode?</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LDTSbmXtokYAsEq8e": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DP8HCLcSPSywDz76e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 31, "extendedScore": null, "score": 0.000134, "legacy": true, "legacyId": "27092", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PeSzc9JTBxhaYRp9b"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-02T14:09:43.925Z", "modifiedAt": null, "url": null, "title": "Meetup : Nick Bostrom Talk on Superintelligence", "slug": "meetup-nick-bostrom-talk-on-superintelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:07.091Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anders_H", "createdAt": "2013-07-28T20:46:58.747Z", "isAdmin": false, "displayName": "Anders_H"}, "userId": "jfdosp4Hn7tFNbf9k", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/siMQxNzt35TkSqNNn/meetup-nick-bostrom-talk-on-superintelligence", "pageUrlRelative": "/posts/siMQxNzt35TkSqNNn/meetup-nick-bostrom-talk-on-superintelligence", "linkUrl": "https://www.lesswrong.com/posts/siMQxNzt35TkSqNNn/meetup-nick-bostrom-talk-on-superintelligence", "postedAtFormatted": "Tuesday, September 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Nick%20Bostrom%20Talk%20on%20Superintelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Nick%20Bostrom%20Talk%20on%20Superintelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsiMQxNzt35TkSqNNn%2Fmeetup-nick-bostrom-talk-on-superintelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Nick%20Bostrom%20Talk%20on%20Superintelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsiMQxNzt35TkSqNNn%2Fmeetup-nick-bostrom-talk-on-superintelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsiMQxNzt35TkSqNNn%2Fmeetup-nick-bostrom-talk-on-superintelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13y'>Nick Bostrom Talk on Superintelligence</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 September 2014 08:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Emerson 105, Harvard University, Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>What happens when machines surpass humans in general intelligence? Will artificial agents save or destroy us? In his new book - Superintelligence: Paths, Dangers, Strategies - Professor Bostrom explores these questions, laying the foundation for understanding the future of humanity and intelligent life. Q&amp;A will follow the talk.</p>\n\n<p><a href=\"http://harvardea.org/event/2014/09/04/bostrom/\" rel=\"nofollow\">http://harvardea.org/event/2014/09/04/bostrom/</a></p>\n\n<p>(This event is organized by Harvard Effective Altruism. It is not technically a Less Wrong Meetup, but the topic is highly relevant and most of the Boston area rationalist community will be there)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13y'>Nick Bostrom Talk on Superintelligence</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "siMQxNzt35TkSqNNn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.9739467076453544e-06, "legacy": true, "legacyId": "27095", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Nick_Bostrom_Talk_on_Superintelligence\">Discussion article for the meetup : <a href=\"/meetups/13y\">Nick Bostrom Talk on Superintelligence</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 September 2014 08:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Emerson 105, Harvard University, Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>What happens when machines surpass humans in general intelligence? Will artificial agents save or destroy us? In his new book - Superintelligence: Paths, Dangers, Strategies - Professor Bostrom explores these questions, laying the foundation for understanding the future of humanity and intelligent life. Q&amp;A will follow the talk.</p>\n\n<p><a href=\"http://harvardea.org/event/2014/09/04/bostrom/\" rel=\"nofollow\">http://harvardea.org/event/2014/09/04/bostrom/</a></p>\n\n<p>(This event is organized by Harvard Effective Altruism. It is not technically a Less Wrong Meetup, but the topic is highly relevant and most of the Boston area rationalist community will be there)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Nick_Bostrom_Talk_on_Superintelligence1\">Discussion article for the meetup : <a href=\"/meetups/13y\">Nick Bostrom Talk on Superintelligence</a></h2>", "sections": [{"title": "Discussion article for the meetup : Nick Bostrom Talk on Superintelligence", "anchor": "Discussion_article_for_the_meetup___Nick_Bostrom_Talk_on_Superintelligence", "level": 1}, {"title": "Discussion article for the meetup : Nick Bostrom Talk on Superintelligence", "anchor": "Discussion_article_for_the_meetup___Nick_Bostrom_Talk_on_Superintelligence1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-02T14:13:33.018Z", "modifiedAt": null, "url": null, "title": "Meetup : Prediction Markets and Futarchy", "slug": "meetup-prediction-markets-and-futarchy", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anders_H", "createdAt": "2013-07-28T20:46:58.747Z", "isAdmin": false, "displayName": "Anders_H"}, "userId": "jfdosp4Hn7tFNbf9k", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uryBPekq8SLLgeEf4/meetup-prediction-markets-and-futarchy", "pageUrlRelative": "/posts/uryBPekq8SLLgeEf4/meetup-prediction-markets-and-futarchy", "linkUrl": "https://www.lesswrong.com/posts/uryBPekq8SLLgeEf4/meetup-prediction-markets-and-futarchy", "postedAtFormatted": "Tuesday, September 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Prediction%20Markets%20and%20Futarchy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Prediction%20Markets%20and%20Futarchy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuryBPekq8SLLgeEf4%2Fmeetup-prediction-markets-and-futarchy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Prediction%20Markets%20and%20Futarchy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuryBPekq8SLLgeEf4%2Fmeetup-prediction-markets-and-futarchy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuryBPekq8SLLgeEf4%2Fmeetup-prediction-markets-and-futarchy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 177, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/13z'>Prediction Markets and Futarchy</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 September 2014 03:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">98 Elm St Apt 1, Somerville, MA </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I will give a talk about prediction markets and futarchy.  The talk is intended as a basic introduction for people who are new to the concept. After my slides, I hope to have a discussion about whether futarchy is feasible.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 3:30pm, and have an alternating location:</p>\n\n<p>1st Sunday meetups are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p>\n\n<p>3rd Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p>\n\n<p>(We also have last Wednesday meetups at Citadel at 7pm.)</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 4pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/13z'>Prediction Markets and Futarchy</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uryBPekq8SLLgeEf4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.9739533877318005e-06, "legacy": true, "legacyId": "27096", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Prediction_Markets_and_Futarchy\">Discussion article for the meetup : <a href=\"/meetups/13z\">Prediction Markets and Futarchy</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 September 2014 03:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">98 Elm St Apt 1, Somerville, MA </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I will give a talk about prediction markets and futarchy.  The talk is intended as a basic introduction for people who are new to the concept. After my slides, I hope to have a discussion about whether futarchy is feasible.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 3:30pm, and have an alternating location:</p>\n\n<p>1st Sunday meetups are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p>\n\n<p>3rd Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p>\n\n<p>(We also have last Wednesday meetups at Citadel at 7pm.)</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 4pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Prediction_Markets_and_Futarchy1\">Discussion article for the meetup : <a href=\"/meetups/13z\">Prediction Markets and Futarchy</a></h2>", "sections": [{"title": "Discussion article for the meetup : Prediction Markets and Futarchy", "anchor": "Discussion_article_for_the_meetup___Prediction_Markets_and_Futarchy", "level": 1}, {"title": "Discussion article for the meetup : Prediction Markets and Futarchy", "anchor": "Discussion_article_for_the_meetup___Prediction_Markets_and_Futarchy1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-02T18:21:14.923Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow Meetup: Codename Felix", "slug": "meetup-moscow-meetup-codename-felix", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexander230", "createdAt": "2014-08-27T08:55:16.153Z", "isAdmin": false, "displayName": "Alexander230"}, "userId": "xqoKSJayCCtP5juLh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HJZHgudbnB3fbwmRt/meetup-moscow-meetup-codename-felix", "pageUrlRelative": "/posts/HJZHgudbnB3fbwmRt/meetup-moscow-meetup-codename-felix", "linkUrl": "https://www.lesswrong.com/posts/HJZHgudbnB3fbwmRt/meetup-moscow-meetup-codename-felix", "postedAtFormatted": "Tuesday, September 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%20Meetup%3A%20Codename%20Felix&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%20Meetup%3A%20Codename%20Felix%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHJZHgudbnB3fbwmRt%2Fmeetup-moscow-meetup-codename-felix%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%20Meetup%3A%20Codename%20Felix%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHJZHgudbnB3fbwmRt%2Fmeetup-moscow-meetup-codename-felix", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHJZHgudbnB3fbwmRt%2Fmeetup-moscow-meetup-codename-felix", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/140'>Moscow Meetup: Codename Felix</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 September 2014 02:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Here's our plan:</p>\n\n<ul>\n<li>A talk about Gollwitzer, Implementation intentions and Rubicon model.</li>\n<li>Calibration excercise.</li>\n<li>Debates on Popper's protocol.</li>\n<li>\"Fallacymania\" game on afterparty.</li>\n<li>\"Contact\" game on afterparty.</li>\n</ul>\n\n<p>Details and schedule:</p>\n\n<p><a href=\"https://lesswrong-ru.hackpad.com/-14--kxLrEaIY6wr\" rel=\"nofollow\">https://lesswrong-ru.hackpad.com/-14--kxLrEaIY6wr</a></p>\n\n<p>Yudcoins, positive reinforcement and pizza will all be present. If you've been to our meetups, you know what I'm talking about, and if you didn't, the best way to find out is to come and see for yourself.</p>\n\n<p>Info for newcomers: We gather in the Yandex office, you need the first revolving door under the archway. Here is a guide how to get there:</p>\n\n<p><a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">http://company.yandex.ru/contacts/redrose/</a></p>\n\n<p>Call Slava at +7(926)313-96-42 if you're late. We start at 14:00 and stay until at least 19-20. Please pay attention that we only gather near the entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/140'>Moscow Meetup: Codename Felix</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HJZHgudbnB3fbwmRt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.9743868328331367e-06, "legacy": true, "legacyId": "27097", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow_Meetup__Codename_Felix\">Discussion article for the meetup : <a href=\"/meetups/140\">Moscow Meetup: Codename Felix</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 September 2014 02:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Here's our plan:</p>\n\n<ul>\n<li>A talk about Gollwitzer, Implementation intentions and Rubicon model.</li>\n<li>Calibration excercise.</li>\n<li>Debates on Popper's protocol.</li>\n<li>\"Fallacymania\" game on afterparty.</li>\n<li>\"Contact\" game on afterparty.</li>\n</ul>\n\n<p>Details and schedule:</p>\n\n<p><a href=\"https://lesswrong-ru.hackpad.com/-14--kxLrEaIY6wr\" rel=\"nofollow\">https://lesswrong-ru.hackpad.com/-14--kxLrEaIY6wr</a></p>\n\n<p>Yudcoins, positive reinforcement and pizza will all be present. If you've been to our meetups, you know what I'm talking about, and if you didn't, the best way to find out is to come and see for yourself.</p>\n\n<p>Info for newcomers: We gather in the Yandex office, you need the first revolving door under the archway. Here is a guide how to get there:</p>\n\n<p><a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">http://company.yandex.ru/contacts/redrose/</a></p>\n\n<p>Call Slava at +7(926)313-96-42 if you're late. We start at 14:00 and stay until at least 19-20. Please pay attention that we only gather near the entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow_Meetup__Codename_Felix1\">Discussion article for the meetup : <a href=\"/meetups/140\">Moscow Meetup: Codename Felix</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow Meetup: Codename Felix", "anchor": "Discussion_article_for_the_meetup___Moscow_Meetup__Codename_Felix", "level": 1}, {"title": "Discussion article for the meetup : Moscow Meetup: Codename Felix", "anchor": "Discussion_article_for_the_meetup___Moscow_Meetup__Codename_Felix1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-02T21:44:54.965Z", "modifiedAt": null, "url": null, "title": "Bayesianism for humans: \"probable enough\"", "slug": "bayesianism-for-humans-probable-enough", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:38.316Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BT_Uytya", "createdAt": "2011-12-03T16:41:14.863Z", "isAdmin": false, "displayName": "BT_Uytya"}, "userId": "Enh7Ap3zRTQDR4gMH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LCNdGLGpq89oRQBih/bayesianism-for-humans-probable-enough", "pageUrlRelative": "/posts/LCNdGLGpq89oRQBih/bayesianism-for-humans-probable-enough", "linkUrl": "https://www.lesswrong.com/posts/LCNdGLGpq89oRQBih/bayesianism-for-humans-probable-enough", "postedAtFormatted": "Tuesday, September 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayesianism%20for%20humans%3A%20%22probable%20enough%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayesianism%20for%20humans%3A%20%22probable%20enough%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLCNdGLGpq89oRQBih%2Fbayesianism-for-humans-probable-enough%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayesianism%20for%20humans%3A%20%22probable%20enough%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLCNdGLGpq89oRQBih%2Fbayesianism-for-humans-probable-enough", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLCNdGLGpq89oRQBih%2Fbayesianism-for-humans-probable-enough", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1154, "htmlBody": "<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">(followup to <a href=\"/lw/iat/what_bayesianism_taught_me/\">What Bayesianism has taught me?</a> and <a href=\"/lw/iwb/bayesianism_for_humans/\">Bayesianism for Humans</a></span><span class=\"author-p-64120\">)</span></div>\n<div id=\"magicdomid12\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<p>There are two insights from Bayesianism which occurred to me and which I hadn't seen anywhere else before. <br />I like lists in the two posts linked above, so for the sake of completeness, I'm going to add my two cents to a public domain. Second penny is <a href=\"/lw/ku4/bayesianism_for_humans_prosaic_priors/\">here</a>.</p>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><br /></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><br /></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><strong>\"Probable enough\"</strong><br /></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><br /></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\" style=\"padding-left: 120px;\"><span class=\"author-p-64120 i\"><em>When you have eliminated the impossible, whatever&nbsp; remains is often more improbable than your having made a mistake in one&nbsp; of your impossibility proofs.</em></span><span class=\"author-p-64120 attrlink url\"><a class=\"attrlink\" href=\"/lw/3m/rationalist_fiction/2p6\"></a></span></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\" style=\"padding-left: 120px;\"><span class=\"author-p-64120 attrlink url\"><a class=\"attrlink\" href=\"/lw/3m/rationalist_fiction/2p6\">Steven Kaas</a></span></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><br /></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><br />\n<div id=\"magicdomid21\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">Bayesian way of thinking introduced me to the idea of \"hypothesis which is probably isn't true, but <a href=\"http://wiki.lesswrong.com/wiki/Locate_the_hypothesis\">probable enough to rise to the level of conscious attention\"</a> &mdash; in other words, to the situation when P(H) is </span><span class=\"author-p-64120 b\">n</span><span class=\"author-p-64120\">otable but less than 50%.</span></div>\n<div id=\"magicdomid22\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid23\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">Looking back, I think that the notion of taking seriously something which you don't think is true was alien to me. Hence, everything was either probably true or probably false; things from the former category were over-confidently certain, and things from the latter category were barely worth thinking about.</span></div>\n<div id=\"magicdomid24\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid25\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">This model was correct, but only in a formal sense.</span></div>\n<div id=\"magicdomid26\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid27\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">Suppose you are living in Gotham, the city famous because of it's crime rate and it's masked (and well-funded) vigilante, Batman. Recently you had read </span><span class=\"author-p-64120 i\"><em>The Better Angels of Our Nature: Why Violence Has Declined </em></span><span class=\"author-p-64120\">by Steven Pinker, and according to some theories described here, Batman isn't good for Gotham at all.</span></div>\n<div id=\"magicdomid28\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">Now you know, for example, the theory of Donald Black that <em>\"crime is, from the point of view of the perpetrator, the pursuit of justice\"</em>. You know about idea that in order for crime rate to drop, people should perceive their law system as legitimate. You suspect that criminals beaten by Bats don't perceive the act as a fair and regular punishment for something bad, or an attempt to defend them from injustice; instead the act is perceived as a round of bad luck. So, the criminals are busy plotting their revenge, not internalizing civil norms.</span></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><br />\n<div id=\"magicdomid35\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">You believe that if you send your copy of book (with key passages highlighted) to the person connected to Batman, Batman will change his ways and Gotham will become much more nice in terms of homicide rate.&nbsp;</span></div>\n<div id=\"magicdomid36\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid37\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">So you are trying to find out Batman's secret identity, and there are 17 possible suspects. Derek Powers looks like a good candidate: he is wealthy, and has a long history of secretly delegating illegal-violence-including tasks to his henchmen; however, his motivation is far from obvious. You estimate P(Derek Powers employs Batman) as 20%. You have very little information about other candidates, like Ferris Boyle, Bruce Wayne, Roland Daggett, Lucius Fox or Matches Malone, so you assign an equal 5% to everyone else.</span></div>\n</div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><br /></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\">\n<div id=\"magicdomid41\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">In this case you should pick Derek Powers as your best guess when forced to name only one candidate (for example, if you forced to send the book to someone today), but also you should be aware that your guess is 80% likely to be wrong. When making expected utility calculations, you should take Derek Powers more seriously than Lucius Fox, but only by 15% more seriously.</span></div>\n<div id=\"magicdomid42\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid43\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">In other words, you should take </span><span class=\"author-p-64120 i\"><em>maximum a posteriori probability</em></span><span class=\"author-p-64120\"> hypothesis into account while not deluding yourself into thinking that now you understand everything or nothing at all. Derek Powers hypothesis probably isn't </span><span class=\"author-p-64120 i\"><em>true; </em></span><span class=\"author-p-64120\">but it is </span><span class=\"author-p-64120 i\"><em>useful</em></span><span class=\"author-p-64120\">.</span></div>\n<div id=\"magicdomid44\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid45\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">Sometimes I find it easier to reframe question from \"what hypothesis is true?\" to \"what hypothesis is probable enough?\". Now it's totally okay that your pet theory isn't probable but still probable enough, so doubt becomes easier. Also, you are aware that your pet theory is likely to be wrong (and this is nothing to be sad about), so the alternatives come to mind more naturally.</span></div>\n<div id=\"magicdomid46\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid47\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">These \"probable enough\" hypothesis can serve as a very concise summaries of state of your knowledge when you simultaneously outline the general sort of evidence you've observed, and stress that you aren't really sure. I like to think about it like a rough, qualitative and more System1-friendly variant of <a href=\"http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html\">Likelihood ratio sharing</a>.</span></div>\n</div>\n<div id=\"magicdomid48\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div class=\"ace-line longKeep gutter-noauthor\">\n<div id=\"magicdomid51\" class=\"ace-line gutter-author-p-64120 emptyGutter toc-entry\"><span class=\"author-p-64120 b\"><strong>Planning Fallacy</strong></span></div>\n<div id=\"magicdomid52\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid53\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">The original explanation of planning fallacy (proposed by Kahneman and Tversky) is about people focusing on a most optimistic scenario when asked about typical one (instead of trying to do an <a href=\"http://wiki.lesswrong.com/wiki/Outside_view\">Outside VIew</a>). If you keep the distinction between \"probable\" and \"probable enough\" in mind, you can see this claim in a new light.</span></div>\n<div id=\"magicdomid54\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid55\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">Because the most optimistic scenario </span><span class=\"author-p-64120 i\"><em>is</em></span><span class=\"author-p-64120\"> the most probable </span><span class=\"author-p-64120 i\"><em>and</em></span><span class=\"author-p-64120\"> the most typical one, in a certain sense.</span></div>\n<div id=\"magicdomid56\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid57\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">The illustration, with numbers pulled out of thin air, goes like this: so, you want to visit a museum.</span></div>\n<div id=\"magicdomid58\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid59\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">The first thing you need to do is to get dressed and take your keys and stuff. Usually (with 80% probability) you do this very quick, but there is a weak possibility of your museum ticket having been devoured by an entropy monster living on your computer table.</span></div>\n<div id=\"magicdomid60\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid61\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">The second thing is to catch bus. Usually (p = 80%), bus is on schedule, but sometimes it can be too early or too late. After this, the bus could (20%) or could not (80%) get stuck in a traffic jam.</span></div>\n<div id=\"magicdomid62\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid63\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">Finally, you need to find a museum building. You've been there before once, so you sorta remember your route, yet still could be lost with 20% probability.</span></div>\n<div id=\"magicdomid64\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid65\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">And there you have it: P(everything is fine) = 40%, and probability of every other scenario is 10% or even less. \"Everything is fine\" is </span><span class=\"author-p-64120 i\"><em>probable enough, </em></span><span class=\"author-p-64120\">yet likely to be false. Supposedly, humans pick MAP hypothesis and then forget about every other scenario in order to save computations.</span></div>\n<div id=\"magicdomid66\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid67\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">Also, \"everything is fine\" is a good description of your plan. If your friend asks you, \"so how are you planning to get to the museum?\", and you answer \"well, I catch the bus, get stuck in a traffic jam for 30 agonizing minutes, and then just walk from here\", your friend is going&nbsp; to get a completely wrong idea about dangers of your journey. So, in a certain sense, \"everything is fine\" is a typical scenario.&nbsp;</span></div>\n<div id=\"magicdomid68\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid69\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">Maybe it isn't human inability to pick the most likely scenario which should be blamed. Maybe it is false assumption that \"most likely == likely to be correct\" which contributes to this ubiquitous error.</span></div>\n<div id=\"magicdomid70\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid71\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">In this case you would be better off having picked the \"something will go wrong, and I will be late\", instead of \"everything will be fine\".</span></div>\n<div id=\"magicdomid72\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid73\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">So, sometimes you are interested in the best specimen out of your hypothesis space, sometimes you are interested in a most likely thingy (and it doesn't matter how vague it would be), and sometimes there are no shortcuts, and you have to do an actual expected utility calculation.</span></div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 2, "bh7uxTTqmsQ8jZJdB": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LCNdGLGpq89oRQBih", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 52, "extendedScore": null, "score": 0.000179, "legacy": true, "legacyId": "26999", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JBnaLpsrYXLXjFocu", "XCwPQzoe9hmQikMiY", "LB4yT86wJnZFCBW3T"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-02T21:45:05.432Z", "modifiedAt": null, "url": null, "title": "Bayesianism for humans: prosaic priors", "slug": "bayesianism-for-humans-prosaic-priors", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:07.464Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BT_Uytya", "createdAt": "2011-12-03T16:41:14.863Z", "isAdmin": false, "displayName": "BT_Uytya"}, "userId": "Enh7Ap3zRTQDR4gMH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LB4yT86wJnZFCBW3T/bayesianism-for-humans-prosaic-priors", "pageUrlRelative": "/posts/LB4yT86wJnZFCBW3T/bayesianism-for-humans-prosaic-priors", "linkUrl": "https://www.lesswrong.com/posts/LB4yT86wJnZFCBW3T/bayesianism-for-humans-prosaic-priors", "postedAtFormatted": "Tuesday, September 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayesianism%20for%20humans%3A%20prosaic%20priors&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayesianism%20for%20humans%3A%20prosaic%20priors%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLB4yT86wJnZFCBW3T%2Fbayesianism-for-humans-prosaic-priors%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayesianism%20for%20humans%3A%20prosaic%20priors%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLB4yT86wJnZFCBW3T%2Fbayesianism-for-humans-prosaic-priors", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLB4yT86wJnZFCBW3T%2Fbayesianism-for-humans-prosaic-priors", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1241, "htmlBody": "<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">(followup to <a href=\"/lw/iat/what_bayesianism_taught_me/\">What Bayesianism has taught me?</a> and <a href=\"/lw/iwb/bayesianism_for_humans/\">Bayesianism for Humans</a></span><span class=\"author-p-64120\">, relevant to <a href=\"/lw/gx5/boring_advice_repository/\">Boring Advice Repository</a></span><span class=\"author-p-64120\"> )</span></div>\n<p>&nbsp;</p>\n<p>There are two insights from Bayesianism which occurred to me and which I hadn't seen anywhere else before. <br />I like lists in the two posts linked above, so for the sake of completeness, I'm going to add my two cents to a public domain. This post is about the second penny, the first one is <a href=\"/lw/ktz/bayesianism_for_humans_probable_enough/\">here</a>.</p>\n<div id=\"magicdomid79\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><br /></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\"><strong>Prosaic Priors</strong></span></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><br />\n<div id=\"magicdomid82\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">The second insight can be formulated as</span><em><span class=\"author-p-64120\"> &laquo;the dull explanations are more likely to be correct because they tend to have high prior probability.&raquo;</span></em></div>\n<div id=\"magicdomid87\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><br /></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">Why is that?&nbsp;</span></div>\n<div id=\"magicdomid88\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid89\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">1) Almost by definition! Some property X is 'banal' if X </span><span class=\"author-p-64120 i\"><em>applies to a lot of people</em></span><span class=\"author-p-64120\"> in an disappointingly mundane way, not having any redeeming features which would make it more rare (and, hence, interesting).</span></div>\n<div id=\"magicdomid90\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid91\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">In the other words, X is banal iff base rate of X is high. Or, you can say, prior probability of X is high.</span></div>\n<div id=\"magicdomid92\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid93\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">1.5) Because of <a href=\"/lw/jp/occams_razor/\">Occam's Razor</a> and <a href=\"http://wiki.lesswrong.com/wiki/Burdensome_details\">burdensome details</a>. One way to make something boring more exciting is to add interesting details: some special features which will make sure that this explanation is </span><span class=\"author-p-64120 i\"><em>about you</em></span><span class=\"author-p-64120\"> as opposed to 'about almost anybody'.</span></div>\n<div id=\"magicdomid94\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid95\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">This could work the other way around: sometimes the explanation feels unsatisfying exactly because it was shaved of any unnecessary and (ultimately) burdensome details.</span></div>\n<div id=\"magicdomid96\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid97\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">2) Often, the alternative of a mundane explanation is something unique and custom made to fit the case you are interested in. And anybody familiar with <a href=\"http://en.wikipedia.org/wiki/Overfitting\">overfitting</a> and <a href=\"http://wiki.lesswrong.com/wiki/Conjunction_fallacy\">conjunction fallacy</a> (and the fact that people tend to love coherent stories with blinding passion<sup>1</sup>) should be very suspicious about such things. So, there could be a strong bias against stale explanations, which should&nbsp; be countered.</span></div>\n<div id=\"magicdomid98\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n</div>\n<div class=\"ace-line longKeep gutter-noauthor\">* * *<br /></div>\n<div class=\"ace-line longKeep gutter-noauthor\">\n<div id=\"magicdomid101\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><br /></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">I fully grokked this when being in process of CBT-induced soul-searching; usage in this context still looks the most natural to me, but I believe that the area of application of this heuristic is wider.</span></div>\n</div>\n<div id=\"magicdomid102\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div class=\"ace-line longKeep gutter-noauthor\">\n<div id=\"magicdomid103\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><strong><span class=\"author-p-64120\">Examples</span></strong></div>\n<div id=\"magicdomid104\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid105\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">1) I'm fairly confident that I'm an introvert. Still, sometimes I can behave like an extrovert. I was interested in the causes of this \"extroversion activation\", as I called it<sup>2</sup>. I suspected that I really had two modes of functioning (with \"introversion\" being the default one), and some events &mdash; for example, mutual interest (when I am interested in a person I was talking to, and xe is interested in me) or feeling high-status &mdash; made me switch between them.</span></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><br />\n<div id=\"magicdomid110\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">Or, you know, it could be just reduction in a social anxiety, which makes people more communicative. Increased anxiety levels wasn't a new element to be postulated; I already knew I had it, yet I was tempted to make up new mental entities, and prosaic explanation about anxiety managed to avoid me for a while.</span></div>\n</div>\n<div id=\"magicdomid111\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div class=\"ace-line longKeep gutter-noauthor\">\n<div id=\"magicdomid114\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">2) I find it hard to do something I consider worthwhile while on a spring break, despite having lots of a free time. I tend to make grandiose plans &mdash; I should meet new people! I should be more involved in sports! I should start using Anki! I should learn Lojban! I should practice meditation! I should read these textbooks including doing most of exercises! &mdash; and then fail to do almost anything. Yet I manage to do some impressive stuff during academic term, despite having less time and more commitments.</span></div>\n<div id=\"magicdomid115\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid116\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">This paradoxical situation calls for explanation.</span></div>\n<div id=\"magicdomid117\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid118\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">The first hypothesis that came to my mind was about activation energy. It takes effort to go&nbsp; from \"procrastinating\" to \"doing something\"; speaking more generally, you can say that it takes effort to go from \"lazy day\" to \"productive day\". During the academic term, I am forced to make most of my days productive: I have to attend classes, do homework, etc. And, already having done something good, I can do something else as well. During spring break, I am deprived of that natural structure, and, hence I am on my own in terms of starting doing something I find worthwhile.</span></div>\n</div>\n</div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\">\n<div class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div class=\"ace-line longKeep gutter-noauthor\">\n<div id=\"magicdomid122\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">The alternative explanation: I was tired. Because, you know, vacation comes right after midterms, and I tend to go all out while preparing for midterms. I am exhausted, my energy and willpower are scarce, so it's no wonder I am having trouble utilizing it.</span></div>\n<div id=\"magicdomid123\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid124\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">(I don't really believe in the latter explanation (I think that my situation is caused by several factors, including two outlined above), so it is also an example of <a href=\"/r/discussion/lw/ktz/bayesianism_for_humans_probable_enough/\">descriptive \"probable enough\" hypothesis</a>)</span></div>\n<div id=\"magicdomid125\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid126\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">3) This example <a href=\"http://slatestarcodex.com/2014/05/19/nerds-can-be-bees-too/\">comes from Slate Star Codex</a></span><span class=\"author-p-64120\">. Nerds tend to find aversive many group bonding activities usual people supposedly enjoy, such as patriotism, prayer, team sports, and pep rallies. Supposedly, they should feel (with a tear-jerking passion of thousand exploding suns) the great unity with their fellow citizens, church-goers, teammates or pupils respectively, but instead they feel nothing.</span></div>\n<div id=\"magicdomid127\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid128\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">Might it be that nerds are unable to enjoy these activities because something is broken inside their brains? One could be tempted to construct an elaborate argument involving autism spectrum and a mild case of schizoid personality disorder. In other words, this calls for postulating a rare form of autism which affects only some types of social behaviour (perception of group activities), leaving other types unchanged.</span></div>\n</div>\n<div class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div class=\"ace-line longKeep gutter-noauthor\">\n<div id=\"magicdomid133\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">Or, you know, maybe nerds just don't like the group they are supposed to root for. Maybe nerds don't feel unity and relationship to The Great Whole because they don't feel like they truly belong here.</span></div>\n<div id=\"magicdomid134\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid135\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">As Scott put it, \"It&rsquo;s not that we lack the ability to lose ourselves in an in-group, it&rsquo;s that all the groups people expected us to lose ourselves in weren&rsquo;t ones we could imagine as our in-group by any stretch of the imagination\"<sup>3</sup>.</span></div>\n</div>\n</div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\"><br /></div>\n<div class=\"ace-line gutter-author-p-64120 emptyGutter\">\n<div id=\"magicdomid145\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">4) This example comes from <a href=\"http://www.prooffreader.com/2014/04/webcomic-4-sherlock-holmes-in-real-life.html\">this short comic</a> titled \"Sherlock Holmes in real life\".</span></div>\n<div id=\"magicdomid146\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div class=\"ace-line longKeep gutter-noauthor\">5) <a href=\"http://www.scottaaronson.com/blog/?p=1981\">Scott Aaronson</a> uses something similar to the <a href=\"http://en.wikipedia.org/wiki/Hanlon%27s_razor\">Hanlon's Razor</a> to explain that the lack of practical expertise of CS theorists aren't caused by arrogance or something like that:</div>\n<div class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div class=\"ace-line longKeep gutter-noauthor\">\"If theorists don&rsquo;t have as much experience building robots as they should have, don&rsquo;t know as much about large software projects as they should&nbsp; know, etc., then those are all defects to add to the long list of their other, unrelated defects.&nbsp; But it would be a mistake to assume that they failed to acquire this knowledge <em>because of disdain for practical people</em>, rather than for mundane reasons like busyness or laziness.\"<br /></div>\n<div id=\"magicdomid147\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div class=\"ace-line longKeep gutter-noauthor\">* * *<br /></div>\n<div class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div id=\"magicdomid148\" class=\"ace-line gutter-author-p-64120 emptyGutter\"><span class=\"author-p-64120\">...and after this the word \"prosaic\" quickly turned into an awesome compliment. Like, \"so, this hypothesis explains my behaviour well; </span><span class=\"author-p-64120 i\"><em>but is it boring enough</em></span><span class=\"author-p-64120\">?\", or \"your claim is refreshingly dull; I like it!\".</span></div>\n<div id=\"magicdomid149\" class=\"ace-line longKeep gutter-noauthor\"><br /></div>\n<div class=\"ace-line longKeep gutter-noauthor\">\n<hr />\n<div class=\"ace-line longKeep gutter-noauthor\">1. <span class=\"author-p-64120\">If you had read <em>Thinking: Fast and Slow</em>, you probably know what I mean. If you hadn't, you can look at <a href=\"http://wiki.lesswrong.com/wiki/Narrative_fallacy\">narrative fallacy</a> in order to get a general idea.</span></div>\n<div class=\"ace-line longKeep gutter-noauthor\"><span class=\"author-p-64120\">2. </span><span class=\"author-p-64120\">Which was, as I now realize, an excellent way to deceive myself via using word <a href=\"/lw/ng/words_as_hidden_inferences/\">with a lot of hidden assumptions</a></span><span class=\"author-p-64120\">. <a href=\"/lw/nu/taboo_your_words/\">Taboo your words</a></span><span class=\"author-p-64120\">, folks!</span>\n<div id=\"magicdomid113\" class=\"ace-line longKeep gutter-noauthor\">3. <span class=\"author-p-64120\">As a side note, my friend proposed an alternative explanation: the thing is, often nerds </span><span class=\"author-p-64120 i\"><em>are defined</em></span><span class=\"author-p-64120\"> as \"sort of people who dislike pep rallies\". So, naturally, we have \"usual people\" who like pep rallies and \"nerds\" who avoid them. And then \"nerds dislike pep rallies\" is tautology rather than something to be explained.</span></div>\n</div>\n<div class=\"ace-line longKeep gutter-noauthor\"><span class=\"author-p-64120\"><br /></span></div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LB4yT86wJnZFCBW3T", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 29, "extendedScore": null, "score": 1.9747436600259703e-06, "legacy": true, "legacyId": "27004", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JBnaLpsrYXLXjFocu", "XCwPQzoe9hmQikMiY", "HEn2qiMxk5BggN83J", "LCNdGLGpq89oRQBih", "f4txACqDWithRi7hs", "3nxs2WYDGzJbzcLMp", "WBdvyyHLdxZSAMmoz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-03T21:36:30.032Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes September 2014", "slug": "rationality-quotes-september-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:02.378Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jaime2000", "createdAt": "2013-07-29T01:47:35.714Z", "isAdmin": false, "displayName": "jaime2000"}, "userId": "z4AsqwchcTtf37Xj8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GosJZhy9LNBG5XDqr/rationality-quotes-september-2014", "pageUrlRelative": "/posts/GosJZhy9LNBG5XDqr/rationality-quotes-september-2014", "linkUrl": "https://www.lesswrong.com/posts/GosJZhy9LNBG5XDqr/rationality-quotes-september-2014", "postedAtFormatted": "Wednesday, September 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20September%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20September%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGosJZhy9LNBG5XDqr%2Frationality-quotes-september-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20September%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGosJZhy9LNBG5XDqr%2Frationality-quotes-september-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGosJZhy9LNBG5XDqr%2Frationality-quotes-september-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">\n<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson. If you'd like to revive an old quote from one of those sources, please do so&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/i6h/rationality_quotes_from_people_associated_with/\">here</a>.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n<li>Provide sufficient information (URL, title, date, page number, etc.) to enable a reader to find the place where you read the quote, or its original source if available. Do not quote with only a name.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GosJZhy9LNBG5XDqr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 4.9e-05, "legacy": true, "legacyId": "27085", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 379, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWTZj26MfR8e8b9nm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-03T21:37:49.834Z", "modifiedAt": null, "url": null, "title": "The Octopus, the Dolphin and Us: a Great Filter tale", "slug": "the-octopus-the-dolphin-and-us-a-great-filter-tale", "viewCount": null, "lastCommentedAt": "2020-10-21T14:09:18.970Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GMqZ2ofMnxwhoa7fD/the-octopus-the-dolphin-and-us-a-great-filter-tale", "pageUrlRelative": "/posts/GMqZ2ofMnxwhoa7fD/the-octopus-the-dolphin-and-us-a-great-filter-tale", "linkUrl": "https://www.lesswrong.com/posts/GMqZ2ofMnxwhoa7fD/the-octopus-the-dolphin-and-us-a-great-filter-tale", "postedAtFormatted": "Wednesday, September 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Octopus%2C%20the%20Dolphin%20and%20Us%3A%20a%20Great%20Filter%20tale&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Octopus%2C%20the%20Dolphin%20and%20Us%3A%20a%20Great%20Filter%20tale%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGMqZ2ofMnxwhoa7fD%2Fthe-octopus-the-dolphin-and-us-a-great-filter-tale%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Octopus%2C%20the%20Dolphin%20and%20Us%3A%20a%20Great%20Filter%20tale%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGMqZ2ofMnxwhoa7fD%2Fthe-octopus-the-dolphin-and-us-a-great-filter-tale", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGMqZ2ofMnxwhoa7fD%2Fthe-octopus-the-dolphin-and-us-a-great-filter-tale", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1013, "htmlBody": "<p>Is intelligence hard to evolve? Well, we're intelligent, so it must be easy... except that only an intelligent species would be able to ask that question, so we run straight into the problem of anthropics. Any being that asked that question would have to be intelligent, so this can't tell us anything about its difficulty (a similar mistake would be to ask \"is most of the universe hospitable to life?\", and then looking around and noting that everything seems pretty hospitable at first glance...).</p>\n<p>Instead, one could point at the great apes, note their high intelligence, see that intelligence arises separately, and hence that it can't be too hard to evolve.</p>\n<p>One could do that... but one would be wrong. The key test is not whether intelligence can arise <em>separately</em>, but whether it can arise <em>independently</em>. Chimpanzees, Bonobos and Gorillas and such are all \"on our line\": they are close to common ancestors of ours, which we would expect to be intelligent because we are intelligent. Intelligent species tend to have intelligent relatives. So they don't provide any extra information about the ease or difficulty of evolving intelligence.</p>\n<p>To get independent intelligence, we need to go far from our line. Enter the smart and cute icon on many student posters: the dolphin.</p>\n<p><img src=\"http://images.lesswrong.com/t3_kvk_0.png?v=a6329bff06969824d07e66a2351f20f0\" alt=\"\" width=\"300\" height=\"162\" /></p>\n<p><a id=\"more\"></a>Dolphins are certainly <a href=\"http://en.wikipedia.org/wiki/Cetacean_intelligence\">intelligent</a>. And they are certainly far from our line. It seems hard to find a definite answer, but it seems that the last common ancestor of humans and dolphins was <a href=\"http://www.nature.com/nature/journal/v446/n7135/full/nature05634.html\">a small mammal existing during the reign of the dinosaurs</a>. Humans and dolphins have been indicated by red rectangles, and their last common ancestor with a red circle.</p>\n<p><img src=\"http://images.lesswrong.com/t3_kvk_1.png?v=b933206bcd9bf79390d2d56f669496fe\" alt=\"\" width=\"640\" height=\"654\" /></p>\n<p>This red circle is well before the <a href=\"http://en.wikipedia.org/wiki/Cretaceous%E2%80%93Paleogene_boundary\">K-T boundary</a>&nbsp;(indicated by the dotted line), hence represents a mammal living in the literal shadow of the dinosaurs.</p>\n<p>We can apply a <a href=\"http://en.wikipedia.org/wiki/Convergent_evolution\">convergent evolution</a> argument to this common ancestor. Thus, assuming that subsequent evolution was somewhat independent, getting from that common ancestor to dolphin level of intelligence is something that can happen relatively easily.</p>\n<p>Can we go further? Well, what if we applied the argument twice? Let's bring in the most alien looking of the high-intelligence animals: the octopus.</p>\n<p><img src=\"http://images.lesswrong.com/t3_kvk_2.png?v=808d4e15d7925e86655bae6e4fd83849\" alt=\"\" width=\"384\" height=\"269\" /></p>\n<p>Let's make the further assumption that our common ancestor with dolphins was dumber than the modern octopus. This doesn't seem a stretch seeing how <a href=\"http://en.wikipedia.org/wiki/Cephalopod_intelligence\">intelligent the modern octopus can be</a>, how minor in terms of ecological role the common dolphin-human ancestor must have been, and seeing the stupidity of <a href=\"http://en.wikipedia.org/wiki/Lagomorpha\">many of the descendants</a> of that common ancestor.</p>\n<p>If we accept that assumption, we can then start looking for the common ancestor of humans and octopuses. Our two species are <a href=\"http://tolweb.org/Bilateria/2459\">really far apart</a>:</p>\n<p><img src=\"http://images.lesswrong.com/t3_kvk_3.png?v=9517ac5fb8a7e34ce18a11ef283a7e34\" alt=\"\" width=\"639\" height=\"649\" /></p>\n<p>We therefore have to go back to around the last common ancestor of the <a href=\"http://en.wikipedia.org/wiki/Bilateria\">Bilateria</a> (creatures with bilateral symmetry, i.e. they have a front and a back end, as well as an upside and downside, and therefore a left and a right). This is the (speculative)&nbsp;<a href=\"http://en.wikipedia.org/wiki/Urbilaterian\">urbilaterian</a>. There are no known examples or fossils of it, which means that it was likely less than 1 cm in length. To quote Wikipedia: \"The urbilaterian is often considered to have possessed a gut and internal organs, a segmented body and a centralised nervous system, as well as a biphasic life cycle (i.e. consisting of larvae and adults) and some features of embryonic development. However, this need not necessarily be the case.\" Very confusing, and with no information about intelligence level. However, since the organism was so small and since it was the ancestor of almost every animal alive today (including worms and <a href=\"http://en.wikipedia.org/wiki/Bryozoa\">Bryozoa</a>), our best estimate would be that it's pretty stupid, with the simplest possible \"brain\".</p>\n<p>Putting this all together, it seems evolutionarily easy to get from urbilatrian intelligence to Octopus intelligence, and from Octopus intelligence to dolphin intelligence - thus from urbilatrian to dolphin.</p>\n<p>Note that this argument assumes that intelligence can be put on something like a linear scale. One could argue that Octopuses have low social intelligence, for instance. But then one could repeat the argument with distant animals with high social intelligence such as certain <a href=\"http://www.sciencedirect.com/science/article/pii/S0960982205010419\">insects</a>. Especially if one believe in a more general form of intelligence, it seems that this family of arguments could be used effectively to demonstrate dolphin-level intelligence emerging easily from very low levels of intelligence.</p>\n<p>&nbsp;</p>\n<h2>Application to the Great Filter</h2>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Great_Filter\">Great Filter</a>&nbsp;(related to the&nbsp;Fermi Paradox) is the argument that since we don't see any evidence of complex technological life in the universe, something must be preventing its emergence. At some point on the trajectory, something is culling almost all species.</p>\n<p><img src=\"http://images.lesswrong.com/t3_kvk_5.png?v=5b75554c7f59311cc2beea57f5a0eec1\" alt=\"\" width=\"615\" height=\"437\" /></p>\n<p>An \"early\" great filter wouldn't affect us: that means that we got through the filter already, it's in our past, so the emptiness among the stars doesn't say anything negative for us. A \"late\" great filter is bad news: that implies that few civilizations make it from technological civilization to star-spanning civilization, with bad results for us. Note that AI is certainly not a great filter: an AI would likely expand through the universe itself</p>\n<p>The real filter could be a combination of an early one and a late one, of course. But, unless the factors are exquisitely well-balanced, its likely that there is one location in civilizational development where most of the filter lies (ie where the probability of getting to the next stage is the lowest). Some possible locations for this could be:</p>\n<ul>\n<li>Life itself is&nbsp;unlikely&nbsp;(very early great filter).</li>\n<li>Life with a central nervous system is unlikely.</li>\n<li><strong>A lot of different possible locations for the great filter in between urbilatiran and dolphin intelligence.</strong></li>\n<li>Getting from dolphin to human intelligence is unlikely.</li>\n<li>Getting from human intelligence to technological civilization is unlikely (latest early filter).</li>\n<li>Getting from technological civilization to star-spanning civilization is unlikely.</li>\n</ul>\n<p>These categories aren't of same size, of course - the first three are very diverse and large, for instance. Then what the evolutionary argument above says, is that the Great Filter in unlikely to be in the third, bolded category (which is in fact a multi-category).</p>\n<p>For what it's worth, my personal judgement is that the filter lies before the creation of a central nervous system.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nZCb9BSnmXZXSNA2u": 2, "25oxqHiadqM6Hf7Gn": 2, "ac84EpK6mZbPLzmqj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GMqZ2ofMnxwhoa7fD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 60, "baseScore": 76, "extendedScore": null, "score": 0.000243, "legacy": true, "legacyId": "27056", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 76, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 236, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-04T00:03:33.664Z", "modifiedAt": null, "url": null, "title": "What steep learning curve do you wish you'd climbed sooner?", "slug": "what-steep-learning-curve-do-you-wish-you-d-climbed-sooner", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:34.809Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stabilizer", "createdAt": "2011-12-02T09:36:56.841Z", "isAdmin": false, "displayName": "Stabilizer"}, "userId": "Qa3pLZx3o2TApyfgq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dXyBiizYzdxS26yAg/what-steep-learning-curve-do-you-wish-you-d-climbed-sooner", "pageUrlRelative": "/posts/dXyBiizYzdxS26yAg/what-steep-learning-curve-do-you-wish-you-d-climbed-sooner", "linkUrl": "https://www.lesswrong.com/posts/dXyBiizYzdxS26yAg/what-steep-learning-curve-do-you-wish-you-d-climbed-sooner", "postedAtFormatted": "Thursday, September 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20steep%20learning%20curve%20do%20you%20wish%20you'd%20climbed%20sooner%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20steep%20learning%20curve%20do%20you%20wish%20you'd%20climbed%20sooner%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdXyBiizYzdxS26yAg%2Fwhat-steep-learning-curve-do-you-wish-you-d-climbed-sooner%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20steep%20learning%20curve%20do%20you%20wish%20you'd%20climbed%20sooner%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdXyBiizYzdxS26yAg%2Fwhat-steep-learning-curve-do-you-wish-you-d-climbed-sooner", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdXyBiizYzdxS26yAg%2Fwhat-steep-learning-curve-do-you-wish-you-d-climbed-sooner", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<p>This is the question asked by <a href=\"http://www.johndcook.com/blog/2014/09/03/steep-learning-curves-you-wish-youd-climbed-sooner/\">John Cook</a>&nbsp;on <a href=\"https://twitter.com/JohnDCook\">Twitter</a>. He lists responses from different people:</p>\n<blockquote>\n<p>\n<ul>\n<li>R</li>\n<li>Version control</li>\n<li>Linear algebra</li>\n<li>Advanced math</li>\n<li>Bayesian statistics</li>\n<li>Category theory</li>\n<li>Foreign languages</li>\n<li>How to not waste time</li>\n<li>Women</li>\n</ul>\n</p>\n</blockquote>\n<p>Mine are: quantum mechanics, Python, cooking, the language of philosophy.</p>\n<p>What learning curve do <em>you</em> wish you'd climbed sooner? Give reasons and stories if you feel like it. Do you think other people should climb the same curves?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dXyBiizYzdxS26yAg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 1.9775106559696854e-06, "legacy": true, "legacyId": "27105", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-04T05:20:27.980Z", "modifiedAt": null, "url": null, "title": "LessWrong in Brisbane, Australia", "slug": "lesswrong-in-brisbane-australia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:32.950Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TRManderson", "createdAt": "2013-07-28T05:37:44.823Z", "isAdmin": false, "displayName": "TRManderson"}, "userId": "ZK65M6cmP2PGbdTgG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aibhXmSj7oNEM7BWe/lesswrong-in-brisbane-australia", "pageUrlRelative": "/posts/aibhXmSj7oNEM7BWe/lesswrong-in-brisbane-australia", "linkUrl": "https://www.lesswrong.com/posts/aibhXmSj7oNEM7BWe/lesswrong-in-brisbane-australia", "postedAtFormatted": "Thursday, September 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20in%20Brisbane%2C%20Australia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20in%20Brisbane%2C%20Australia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaibhXmSj7oNEM7BWe%2Flesswrong-in-brisbane-australia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20in%20Brisbane%2C%20Australia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaibhXmSj7oNEM7BWe%2Flesswrong-in-brisbane-australia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaibhXmSj7oNEM7BWe%2Flesswrong-in-brisbane-australia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<p>At present, the LessWrong presence in Brisbane is essentially non-existent. We have Brisbane Skeptics in the Pub, and that's the closest you can get. During the most recent Australia-wide LessWrong hangout, Nick Wolf of Melbourne and Eliot Redelman of Sydney persuaded me to create a Facebook group for LessWrong in Brisbane. This post is solely to announce that.</p>\n<p>The group can be <a title=\"LessWrong Brisbane - Facebook\" href=\"https://www.facebook.com/groups/1469995636588748/1477328472522131/\">found here</a>.</p>\n<p>Ideally a meetup will occur once more than the small handful currently on the group have joined.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aibhXmSj7oNEM7BWe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 1.9780670050292745e-06, "legacy": true, "legacyId": "27110", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-04T11:32:06.282Z", "modifiedAt": null, "url": null, "title": "Consistent extrapolated beliefs about math?", "slug": "consistent-extrapolated-beliefs-about-math", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:07.071Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bxNCr8jGDwXssb9Xb/consistent-extrapolated-beliefs-about-math", "pageUrlRelative": "/posts/bxNCr8jGDwXssb9Xb/consistent-extrapolated-beliefs-about-math", "linkUrl": "https://www.lesswrong.com/posts/bxNCr8jGDwXssb9Xb/consistent-extrapolated-beliefs-about-math", "postedAtFormatted": "Thursday, September 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Consistent%20extrapolated%20beliefs%20about%20math%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConsistent%20extrapolated%20beliefs%20about%20math%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbxNCr8jGDwXssb9Xb%2Fconsistent-extrapolated-beliefs-about-math%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Consistent%20extrapolated%20beliefs%20about%20math%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbxNCr8jGDwXssb9Xb%2Fconsistent-extrapolated-beliefs-about-math", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbxNCr8jGDwXssb9Xb%2Fconsistent-extrapolated-beliefs-about-math", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 358, "htmlBody": "<p>My beliefs about the integers are a little fuzzy. I believe the things that&nbsp;<a href=\"http://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory\">ZFC</a> can prove about the integers, but there seems to be more than that. In particular, I intuitively believe that \"my beliefs about the integers are consistent, because the integers exist\". That's an uncomfortable situation to be in, because we know that a consistent theory <a href=\"http://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems#Second_incompleteness_theorem\">can't assert its own consistency</a>.</p>\n<p>Should I conclude that my beliefs about the integers can't be covered by any single formal theory? That's a tempting line of thought, but it reminds me of all these people claiming that <a href=\"http://en.wikipedia.org/wiki/The_Emperor%27s_New_Mind\">the human mind is uncomputable</a>, or that humans will always be smarter than machines. It feels like being on the wrong side of history.</p>\n<p>It's also dangerous to believe that \"the integers exist\" due to my having clear intuitions about them, because humans sometimes make mistakes. Before Russell's paradox, someone could be forgiven for thinking that the objects of <a href=\"http://en.wikipedia.org/wiki/Naive_set_theory\">naive set theory</a> \"exist\" because they have clear intuitions about sets, but they would be wrong nonetheless.</p>\n<p>Let's explore the other direction instead. What if there was some way to <a href=\"http://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition\">extrapolate</a> my fuzzy beliefs about the integers? In full generality, the outcome of such a process should be a Turing machine that prints sentences about integers which I believe in. Such a machine would encode some effectively generated theory about the integers, which we know cannot assert its own consistency and be consistent at the same time.</p>\n<p>So it seems that in the process of extracting my \"consistent extrapolated beliefs\", something has to give. At some point, my belief in my own consistency has to go, if I want the final result to be consistent.</p>\n<p>But if I already know that much about the outcome, it might make sense for me to change my beliefs now, and end up with something like this: \"All my beliefs about the integers follow from some specific formal theory that I don't know yet. In particular, I don't believe that my beliefs about the integers are consistent.\"</p>\n<p>I'm not sure if there are gaps in the above reasoning, and I don't know if using <a href=\"/lw/h1k/reflection_in_probabilistic_logic/\">probabilistic reflection</a>&nbsp;changes the conclusions any. What do you think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bxNCr8jGDwXssb9Xb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 1.9787197948128795e-06, "legacy": true, "legacyId": "27115", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["duAkuSqJhGDcfMaTA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-04T13:17:23.800Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong Israel Meetup (Herzliya): Social and Board Games", "slug": "meetup-less-wrong-israel-meetup-herzliya-social-and-board", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanArmak", "createdAt": "2009-08-05T23:08:24.020Z", "isAdmin": false, "displayName": "DanArmak"}, "userId": "7KSbntzeQ2RNZq6Jw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Fo27Lpa7p8yzaATvP/meetup-less-wrong-israel-meetup-herzliya-social-and-board", "pageUrlRelative": "/posts/Fo27Lpa7p8yzaATvP/meetup-less-wrong-israel-meetup-herzliya-social-and-board", "linkUrl": "https://www.lesswrong.com/posts/Fo27Lpa7p8yzaATvP/meetup-less-wrong-israel-meetup-herzliya-social-and-board", "postedAtFormatted": "Thursday, September 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20Israel%20Meetup%20(Herzliya)%3A%20Social%20and%20Board%20Games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20Israel%20Meetup%20(Herzliya)%3A%20Social%20and%20Board%20Games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFo27Lpa7p8yzaATvP%2Fmeetup-less-wrong-israel-meetup-herzliya-social-and-board%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20Israel%20Meetup%20(Herzliya)%3A%20Social%20and%20Board%20Games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFo27Lpa7p8yzaATvP%2Fmeetup-less-wrong-israel-meetup-herzliya-social-and-board", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFo27Lpa7p8yzaATvP%2Fmeetup-less-wrong-israel-meetup-herzliya-social-and-board", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 251, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/141'>Less Wrong Israel Meetup (Herzliya): Social and Board Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 September 2014 07:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">HaMada 5, Herzliya Pituach, Israel, EMC offices, floor 4</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This time we're going to have a social meetup! We'll be socializing and playing games.</p>\n\n<p>We'll start the meetup at 19:00 and finish at 22:00 (we may move the discussion to a nearby cafe afterwards). Feel free to come a little bit later, as there is no agenda.</p>\n\n<p>We'll meet at the 4th floor of the building. Our gracious host is Yonatan Cale, and if you have trouble finding us, you can reach him at  052-5563141.</p>\n\n<p>Things that might happen: \n - You'll trade cool ideas with cool people from the Israel LW community. \n - You'll discover kindred spirits who agree with you about one/two boxing. \n - You'll kick someone's ass (and teach them how you did it) at some awesome boardgame \n - You'll discover how to build a friendly AGI running on cold fusion (well probably not) \n - You'll discuss interesting AI topics with new friends!</p>\n\n<p>Things that will happen for sure: \n - You'll get to hang out with awesome people and have fun!</p>\n\n<p>Parking: There's a parking place under the building, and several others around us too. I'll update about free parking (I don't have a car)</p>\n\n<p>Public transport: \nBusses: Namir, get off at \u05e6\u05d5\u05de\u05ea \u05d4\u05e8\u05e6\u05dc\u05d9\u05d4/\u05d0\u05e7\u05d3\u05d9\u05d4 (not \u05d4\u05e1\u05d9\u05e8\u05d4), it's 5 minutes slow-walking from there.\nHerzelia train: ~15 minutes walking\nUse your GPS!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/141'>Less Wrong Israel Meetup (Herzliya): Social and Board Games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Fo27Lpa7p8yzaATvP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.9789048113386107e-06, "legacy": true, "legacyId": "27117", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Israel_Meetup__Herzliya___Social_and_Board_Games\">Discussion article for the meetup : <a href=\"/meetups/141\">Less Wrong Israel Meetup (Herzliya): Social and Board Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 September 2014 07:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">HaMada 5, Herzliya Pituach, Israel, EMC offices, floor 4</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This time we're going to have a social meetup! We'll be socializing and playing games.</p>\n\n<p>We'll start the meetup at 19:00 and finish at 22:00 (we may move the discussion to a nearby cafe afterwards). Feel free to come a little bit later, as there is no agenda.</p>\n\n<p>We'll meet at the 4th floor of the building. Our gracious host is Yonatan Cale, and if you have trouble finding us, you can reach him at  052-5563141.</p>\n\n<p>Things that might happen: \n - You'll trade cool ideas with cool people from the Israel LW community. \n - You'll discover kindred spirits who agree with you about one/two boxing. \n - You'll kick someone's ass (and teach them how you did it) at some awesome boardgame \n - You'll discover how to build a friendly AGI running on cold fusion (well probably not) \n - You'll discuss interesting AI topics with new friends!</p>\n\n<p>Things that will happen for sure: \n - You'll get to hang out with awesome people and have fun!</p>\n\n<p>Parking: There's a parking place under the building, and several others around us too. I'll update about free parking (I don't have a car)</p>\n\n<p>Public transport: \nBusses: Namir, get off at \u05e6\u05d5\u05de\u05ea \u05d4\u05e8\u05e6\u05dc\u05d9\u05d4/\u05d0\u05e7\u05d3\u05d9\u05d4 (not \u05d4\u05e1\u05d9\u05e8\u05d4), it's 5 minutes slow-walking from there.\nHerzelia train: ~15 minutes walking\nUse your GPS!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Israel_Meetup__Herzliya___Social_and_Board_Games1\">Discussion article for the meetup : <a href=\"/meetups/141\">Less Wrong Israel Meetup (Herzliya): Social and Board Games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong Israel Meetup (Herzliya): Social and Board Games", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Israel_Meetup__Herzliya___Social_and_Board_Games", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong Israel Meetup (Herzliya): Social and Board Games", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Israel_Meetup__Herzliya___Social_and_Board_Games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-04T16:58:55.950Z", "modifiedAt": null, "url": null, "title": "\"NRx\" vs. \"Prog\" Assumptions: Locating the Sources of Disagreement Between Neoreactionaries and Progressives (Part 1)", "slug": "nrx-vs-prog-assumptions-locating-the-sources-of-disagreement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:31.197Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Matthew_Opitz", "createdAt": "2014-04-15T14:34:49.640Z", "isAdmin": false, "displayName": "Matthew_Opitz"}, "userId": "skrydvciYrrazNea9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6FzkEzpPQxrLyu8eH/nrx-vs-prog-assumptions-locating-the-sources-of-disagreement", "pageUrlRelative": "/posts/6FzkEzpPQxrLyu8eH/nrx-vs-prog-assumptions-locating-the-sources-of-disagreement", "linkUrl": "https://www.lesswrong.com/posts/6FzkEzpPQxrLyu8eH/nrx-vs-prog-assumptions-locating-the-sources-of-disagreement", "postedAtFormatted": "Thursday, September 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22NRx%22%20vs.%20%22Prog%22%20Assumptions%3A%20Locating%20the%20Sources%20of%20Disagreement%20Between%20Neoreactionaries%20and%20Progressives%20(Part%201)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22NRx%22%20vs.%20%22Prog%22%20Assumptions%3A%20Locating%20the%20Sources%20of%20Disagreement%20Between%20Neoreactionaries%20and%20Progressives%20(Part%201)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6FzkEzpPQxrLyu8eH%2Fnrx-vs-prog-assumptions-locating-the-sources-of-disagreement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22NRx%22%20vs.%20%22Prog%22%20Assumptions%3A%20Locating%20the%20Sources%20of%20Disagreement%20Between%20Neoreactionaries%20and%20Progressives%20(Part%201)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6FzkEzpPQxrLyu8eH%2Fnrx-vs-prog-assumptions-locating-the-sources-of-disagreement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6FzkEzpPQxrLyu8eH%2Fnrx-vs-prog-assumptions-locating-the-sources-of-disagreement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3319, "htmlBody": "<p style=\"margin-bottom: 0in;\">I know that many people on LessWrong want nothing to do with \"<a title=\"neoreaction\" href=\"http://iamlegionnaire.wordpress.com/core-works-of-neoreaction/#sthash.evlTGYwH.dpbs\" target=\"_blank\">neoreaction</a>.\"&nbsp; It does seem strange that a website commonly associated with techno-futurism, such as LessWrong, would end up with even the most tangential networked association with an intellectual current, such as neoreaction, that commonly includes nostalgia for <a title=\"absolute monarchies\" href=\"http://techcrunch.com/2013/11/22/geeks-for-monarchy/\" target=\"_blank\">absolute monarchies</a> and other avatistic obessions.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Perhaps blame it on Yvain, AKA Scott Alexander of slatestarcodex.com for attaching this strange intellectual node to LessWrong. ; ) That's at least how I found out about neoreaction, and I doubt that I am alone in this.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Certainly many on LessWrong would view any association with \"neoreaction\" as a Greek gift to be avoided. I understand the concept of keeping \"<a title=\"gardens\" href=\"/lw/c1/wellkept_gardens_die_by_pacifism/\" target=\"_blank\">well-kept gardens</a>\" and of politics being the \"<a title=\"politics mindkiller\" href=\"/lw/gw/politics_is_the_mindkiller/\" target=\"_blank\">mind-killer</a>,\" although some at LessWrong have argued that some of the most important questions humanity will face in the next decades will be questions that are unavoidably \"political\" in nature. Yes, \"<a title=\"politics hard mode\" href=\"/lw/kkp/politics_is_hard_mode/\" target=\"_blank\">politics is hard mode</a>,\" but so is life itself, and you don't get better at hard mode without practicing in hard mode.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">LessWrong proclaims itself as a community devoted to refining the art of rationality. One aspect of the art of rationality is locating the true sources of disagreement between two parties who want to communicate with each other, but who can't help but talk past each other in different languages due to having radically different pre-existing assumptions.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I believe that this is the problem that any discourse between neoreaction and progressivism currently faces.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Even if you have no interest at all in neoreaction or progressivism as ideologies, I invite you to read this analysis as a case study in locating sources of disagreement between ideologies that have different unspoken assumptions. I will try to steelman neoreaction as much as I can, despite the fact that I am more sympathetic to the progressivist point of view.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">In particular, I am interested in the following question:&nbsp; to what extent do neoreactionary and progressive disagreements stem from judgments that merely differ in degree?&nbsp; (For example, being slightly more or less pessimistic about X, Y, and Z propositions).&nbsp; Or to what extent do neoreactionary and progressive disagreements stem from assumptions that are <em>qualitatively </em>different?</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Normative vs. descriptive assumptions</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">\"Normative\" statements are \"ought\" statements, or judgments of value. \"Descriptive\" statements are \"is\" statements, or depictions of reality. While neoreaction and progressivism have a lot of differing descriptive assumptions, there is really only one fundamental normative disagreement, which I will address first.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Normative disagreement #1: Progressivism's subjective values vs. Neoreaction's objective[?] values</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">As I see it, <strong>Progressivism says</strong>, \"Our subjective values are worth pursuing in and of themselves just because it makes us feel good. It does not particularly matter where our values come from. Perhaps we are Cartesian dualists&mdash;unmoved movers with free will&mdash;who invent our values in an act of existential creation. Or perhaps our values are biological programming&mdash;spandrels manufactured by Nature, or as the neoreactionaries personify it, \"Gnon.\" It doesn't matter. In principle, if we could rewire our reward circuits to give us pleasure/fun/novelty/happiness/sadness/tragedy/suffering/whatever we desire* in response to whatever Nature had the automatic (or modified) disposition to offer us, then those good feelings would be just as worthwhile as anything else. (This is why neoreactionaries perceive progressive values as \"nihilistic.\")</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">According to this formulation, most LessWrongers, being averse to <a title=\"wireheading\" href=\"http://wiki.lesswrong.com/wiki/Wireheading\" target=\"_blank\">wireheading </a>in principle, are not full-fledged progressives at this most fundamental level.&nbsp; (Perhaps this explains some of the counter-intuitive overlap between the LessWrong and neoreactionary thoughtsphere....)&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">[Editorial:&nbsp; In my view, coming to terms with the obvious benefit of wireheading is the ultimate \"red pill\" to swallow. I am a progressive who would happily wirehead as long as I had concluded beforehand that I had adequately secured its completely automatic perpetuation even in the absence of any further input from me...although an optional override to shut it down and return me to the non-wireheaded state would not be unwelcome, just in case I had miscalculated and found that the system did not attend to my every wish as anticipated.]</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">*Note that I am aware that our subjective values are complex and that we are \"<a title=\"Godshatter\" href=\"/lw/l3/thou_art_godshatter/\" target=\"_blank\">Godshatter</a>.\" Nevertheless, this does not seem to me to be a fundamental impediment to wireheading. In principle, we should be able to dissect every last little bit of this \"Godshatter\" and figure out exactly what we want in all of its diversity...and then we can start designing a system of wireheading to give it to us. Is this not what Friendly AI is all about? Doesn't Friendly AI = Wireheading Done \"Right\"? Alternatively, we could re-wire ourselves to not be Godshatter, and to have a very simple list of things that would make us feel good. I am open to either one. LessWrongers, being neoreactionaries at heart (see below), would insist on maintaining our human complexity, our Godshatter values, and making our wireheading laboriously work around that. Okay, fine. I'll compromise...as long as I get my wireheading in some form. ; )</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Neoreaction says</strong>, \"There is objective value in the principle of \"perpetuating biological and/or civilizational complexity\" itself*; the best way to perpetuate biological and/or civilizational complexity is to \"serve Gnon\" (i.e. devote our efforts to fulfilling nature's pre-requisites for perpetuating our biologial and/or civilizational complexity); our subjective values are spandrels manufactured by natural selection/Gnon; insofar as our subjective values motivate us to serve Gnon and thereby ensure the perpetuation of biological and/or civilizational complexity, our subjective values are useful. (For example, natural selection makes sex a subjective value by making it pleasurable, which then motivates us to perpetuate our biological complexity). But, insofar as our subjective values mislead us from serving Gnon (such as by making non-procreative sex still feel good) and jeopardize our biological/civilizational perpetuation, we must sacrifice our subjective values for the objective good of perpetuating our biological/civilizational complexity\" (such as by buckling down and having procreative sex even if one would personally rather not enjoy raising kids).</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">*Note that different NRx thinkers might have different definitions about what counts as biological or civilizational \"complexity\" worthy of perpetuating...it could be \"Western Civilization,\" \"the White Race,\" \"Homo sapiens,\" \"one's own genetic material,\" \"intelligence, whether encoded in human brains or silicon AI,\" \"human complexity/Godshatter,\" etc. This has led to the so-called \"neoreactionary trichotomy\"&mdash;3 wings of the neoreactionary movement: Christian traditionalists, ethno-nationalists, and techno-commercialists.&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Most LessWrongers probably agree with neoreactionaries on this fundamental normative assumption, with the typical objective good of LessWrongers being \"human complexity/Godshatter,\" and thus the \"techno-commercialist\" wing of neoreaction being the one that typically finds the most interest among LessWrongers.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">[Editorial:&nbsp; pesumably, each neoreactionary is choosing his/her objective target of allegiance (such as \"Western Civilization\") because of the warm fuzzies that the idea elicits in him/herself. Has it ever occurred to neoreactionaries that humans' occasional predilection for being awed by a system bigger than themselves (such as \"Western Civilization\") and sacrificing for that system is itself a \"mere\" evolutionary spandrel?]</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Now, in an attempt to <a title=\"Steelmanning\" href=\"http://wiki.lesswrong.com/wiki/Steel_man\" target=\"_blank\">steelman</a> neoreaction's normative assumption, I would characterize it thus: \"In the most ultimate sense, neoreactionaries find the pursuit of subjective values just as worthwhile as progressives do. However, neoreactionaries are aware that human beings are short-sighted creatures with finite discount windows. If we tell ourselves that we should pursue our subjective values, we won't end up pursuing those subjective values in a farsighted way that involves, for example, maintaining a functioning civilization so that people continue to follow laws and don't rob or stab each other. Instead, we will invariably party it up and pursue short-term subjective values to the detriment of our long-term subjective values. So instead of admitting to ourselves that we are really interested in subjective value in the long run, we have to tell ourselves a noble lie that we are actually serving some higher objective purpose in order to motivate our primate brains to stick to what will happen to be good for subjective values in the long run.\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Indeed, I have found some neoreactionary writers muse on the problem of wanting to believe in God because it would serve as a unifying and motivating objective good, and lamenting the fact that they cannot bring themselves to do so.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Now, onto the descriptive disagreements....</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Descriptive assumption #1: Humanity can master nature (progressivism) vs. Nature will always end up mastering humanity (neoreaction).</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">Whereas progressives tend to have optimism that humankind can incrementally master the laws of nature (not change them, but master them, as in intelligently work around them, much like how we have worked around but not changed gravitation by inventing airplanes), neoreactionaries have a dour pessimism that humankind under-estimates the extent to which the laws of nature constantly pull our puppet strings.&nbsp; Far from being able to ever master nature, <a title=\"Gnon\" href=\"http://www.moreright.net/capturing-gnon/\" target=\"_blank\">humankind will always be mastered by nature</a>, by nature's command to \"race to the bottom\" in order to out-reproduce, out-compete one's rivals, even if that means having to sacrifice the nice things in life.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">For specific ways in which nature threatens to master humanity unless humanity somehow finds a way to exert tremendous efforts at collective coordination against nature, see Scott Alexander's \"<a title=\"Meditations on Moloch\" href=\"http://slatestarcodex.com/2014/07/30/meditations-on-moloch/\" target=\"_blank\">Meditations on Moloch</a>.\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Most progressives presumably hold out hope that we can collectively coordinate to overcome Moloch.&nbsp; If nature and its incentives threaten humanity with the strongest and most ruthless conquering the weak and charitable, perhaps we create a world government to prevent that.&nbsp; If nature and its incentives drive down wages to subsistence level, perhaps we create a global minimum wage.&nbsp; If humanity is threatened with dysgenic decline, perhaps a democratic world government organizes a eugenics program.&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Descriptive assumption #2:&nbsp; On average, people have, or can be trained to have, far-sighted <a title=\"Discount functions\" href=\"http://en.wikipedia.org/wiki/Discount_function\" target=\"_blank\">discount functions</a> (progressivism), vs. people typically have short-sighted discount functions (neoreaction).&nbsp; </strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">Part of the progressive assumption about humanity being able to master nature is that ordinary people are rational enough to see the big picture and submit to such controls if they are needed to avoid the disasters of Moloch.&nbsp; Part of the neoreactionary assumption about nature always mastering humanity is that, except for some bright outliers, most people are short-sighted primates who will insist on trading long-term well-being for short-term frills.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Descriptive assumption #3: Culture is a variable mostly dependent on material conditions (progressivism) vs. Culture is an independent variable with respect to material conditions (neoreaction).</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">Neoreactionaries often claim that life seems so much better in modern times in comparison to, say, 400 years ago, only because of our technological advancement since then has compensated for, and hidden, how our culture has rotted in the meantime. Neoreactionaries argue that, if one could combine our modern technology with, let's say, an absolute monarchy, then life would be so much better. This assumption of being able to mix &amp; match material conditions and political systems, or material conditions and culture, depends on an assumption that culture and social institutions are essentially independent variables. Perhaps with enough will, we can try to make any set of technologies work well with any set of cultural and social institutions.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Progressives, whether they realize it or not, are probably subtly influenced, instead, by the \"historical materialist\" (AKA Marxist) view of society which argues that certain material conditions and material incentives tend to automatically generate certain cultural and social responses.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">For example, to Marx, increased agricultural productivity in the late middle ages and Renaissance due to better agricultural technologies was a pre-requisite for the \"Acts of Enclosure\" in England, which booted the \"surplus\" farmers off of the farms and into the cities as propertyless proletarians who would be willing to work for a wage. Likewise, technologies like steam power were pre-requisites for providing an unprecedentedly profitable way of employing these proletarians to make a profit. (Otherwise, the proletarians might have just been left to rot on the street unemployed, with their numbers dwindling in Malthusian fashion). And because there were new avenues for making a profit, the people who stood to gain from chasing these new profit incentives produced new cultural habits and laws that would enable them to pursue these incentives more effectively. One of these new sets of laws was \"laissez-faire\" economics. Another was liberal democracy.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">To a progressive, the proposition that we could, even theoretically, run our modern technological society through an absolute monarchy would probably seem preposterous. It is not even an option. Our modern society is too complex, with too many conflicting interests to reconcile through any system that prohibits the peaceful discovery and negotiation of these varied interests through a democratic process involving \"voice.\" In reality, people are not content with being able only to exercise the \"right of exit\" from institutions or governments that they don't like. Perhaps the powerless have no choice but to immigrate. But elites have, historically, more often chosen to stand and fight rather than gracefully exit. Hence, feudalism, civil wars brought on by crises of royal succession, Masonic orders, factions, political parties, \"special interest groups,\" and so on.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Progressives would say, \"Do you honestly think that you can tame these beasts, when even a dictator like Hitler was just as much beholden to juggling interest groups and power blocs around him as he was the real dictator of events?\" Ah, but the neoreactionaries will say, \"Hitler's Nazism was still \"<a title=\"Neoreactionary glossary\" href=\"http://www.moreright.net/neoreactionary-glossary/\" target=\"_blank\">demotist</a>.\" It made the mistake of trying to justify itself to the public, if not through elections, then at least implicitly. We won't do that.\" To which progressives might say, \"You might not want to justify yourself to the rabble and to elite power blocs, but they will demand it&mdash;and not because they are all infected by some mysterious mental virus called the \"<a title=\"Neoreactionary glossary\" href=\"http://www.moreright.net/neoreactionary-glossary/\" target=\"_blank\">Cathedral</a>,\" but because they see a way to gain an advantage through politics, and in the modern era they have the means and coordination to effectively fight for it.\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">These are just examples. The take-away point is that, for progressives, culture appears to be more of a dependent variable, not a variable that is independent of material conditions. So, according to progressives, you can't say, \"Let's just combine today's technology with absolute monarchy, and voil&agrave;!\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Descriptive assumption #4: Western society is currently anabolic/ascendant (progressivism) vs. catabolic/decadent (neoreaction).</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">Neoreaction often gets <strong>caricatured </strong>as claiming that \"things are getting worse\" or \"have been getting worse for the past x number of years.\" This paints a weak straw-man of neoreaction because, on the surface, things seem so much \"obviously\" better now than ever. However, this isn't quite what neoreactionaries claim.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Neoreactionaries actually claim that Western society is decaying (note the subtle difference). Western society is gradually weakening its ability to reproduce itself. It is, to use a farming metaphor, eating up its seed-corn on present consumption, on insant gratification, which causes things to seem really swell on the surface...for now. However, according to neoreactionaries, conditions might not yet be getting worse on average (although they will point to inner city violence and other signs that conditions already have started to get worse in some places), but Western society's \"capital stock\" is getting worse, is already dwindling.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Envisioned more broadly, a society's \"capital\" is not just its money. It is its entire basket of tangible and intangible assets that help it reproduce and expand itself. So a society's \"capital\" would also include things like its citizens, its birth rates, its habits of harmonious gender relations, its education, its habits of civil propriety, its sustaining myths (such as patriotism or religion), its infrastructure, its environmental health [although NRxers tend to not focus on this], etc.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Another term for \"decadence\" might be \"catabolic collapse.\" A catabolic collapse is when an organism starts consuming its own muscles, its own seed-corn, if you will, in a last-ditch effort to stay alive. By contrast, an \"anabolic\" process is one that builds muscle&mdash;one that saves up capital, if you will. (Hence, \"anabolic\" steroids).</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Neoreactionaries believe that Western society is currently headed for a \"catabolic collapse.\"&nbsp; (See John Michael Greer, author of \"<a title=\"Catabolic Collapse\" href=\"http://www.ecoshock.org/transcripts/greer_on_collapse.pdf\" target=\"_blank\">How Civilizations Fall: A Theory of Catabolic Collapse</a>.\"&nbsp; Oddly enough, John Michael Greer started out 10 years ago as a trendy name in anarcho-primitivist intellectual circles.&nbsp; Now his ideas have been embraced by some neoreactionaries such as Nick Land, which makes me ponder whether anarcho-primitivism is really of the \"left\" or \"right\" to begin with...)</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">When it comes to progressives, most, I think, would argue that Western society is not currently catabolic/decadent. Granted, they would point to some problems with \"unsustainability,\" especially with regards to environmental pollution, resource depletion, and maybe public debt levels (especially worrisome to the libertarian-minded). But on the whole, progressives are still optimistic that these problems can be overcome without rolling back liberal democracy.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Now, let's look at some specific worries that neoreaction has about Western decadence....</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Descriptive Assumption #5: Our biggest population threat is overshoot and the attendant resource depletion, environmental pollution, and immiseration of living standards (progressivism) vs. Our biggest population threat is a demographic death spiral (neoreaction).</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">One thing I have noticed when looking at neoreactionary websites is that they are really obsessed with birth rates! They argue that countries with fertility below replacement level are on the road to annihilation. I found this interesting because my first impulse is to feel like this globe is getting too damn crowded.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Perhaps neoreactionaries envision the birth rates to stay below replacement level from here on out&mdash;that this is a permanent change. Perhaps they foresee world population following a sort of bell-shaped curve. My naive progressive assumption is that our population is already in a slight overshoot beyond what can be sustained at our current level of technology, and that any present declines in birth rates are probably just enough to bring us into the oscillating plateau of a typical <a title=\"S-shaped curve\" href=\"http://images.tutorvista.com/content/biotic-community/population-growth-forms.jpeg\" target=\"_blank\">S-shaped popoulation curve</a>, and that better economic prospects could easily reverse the trend. My naive progressive assumption is that raising kids will remain sufficiently fun and interesting to a large enough pool of adults that, given enough of a feeling of economic security, people will happily continue having kids in sufficient numbers to prevent a die-off of Homo sapiens. In other words, most progressives like myself would not see the need to roll back gender norms in Western society at the present time for the sake of popping out more babies.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Perhaps what worries neoreactionaries, though, is not so much the fear of a global planetary baby shortage, but rather a localized baby shortage among Westerners or Whites. Maybe they fear that all babies are not created equal....</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Descriptive assumption #6: \"Immigrants are OK\" (progressivism) vs. \"Immigrants will jeopardize Western Civilization/the White Race/intelligent human complexity/etc.\" (neoreaction)</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">Progressives say, \"It is not a big deal if Western society has to import some immigrants to keep its population topped off. Immigrant cultures will eventually blend with the \"nativist\" culture. Historically, this has turned out OK, despite xenophobic fears every time that it will end in disaster. The immigrants will mostly assimilate into the nativist culture. The nativist culture will pick up a few new habits from the immigrants (some of them helpful, some of them harmful, but on the balance nothing disastrous). Nor will the immigrants dirty the nativist gene pool with bad genes. As far as we can tell so far, no significant genetic differences in intelligence and/or physical vigor exist between immigrants and non-immigrants.\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Neoreactionaries say, \"It is a very big deal if Western society has to import some immigrants to keep its population topped off. Immigrant cultures will not assimilate with the nativist culture. Immigrant cultures will end up imparting a net influene of bad habits on the native culture. Civil decency will be eroded. Crime and societal dysfunction will increase. The native gene pool will also be dirtied with lower-intelligence immigrant genes. (And the only reason we can't see this is because the progressive Establishment AKA the \"Cathedral\" has systematically distorted <a title=\"IQ\" href=\"http://www.lagriffedulion.f2s.com/sft.htm\" target=\"_blank\">the research and discourse around IQ</a>). At worst, Western cities will act as \"<a title=\"IQ Shredders\" href=\"http://www.xenosystems.net/iq-shredders/\" target=\"_blank\">IQ Shredders</a>.\" Any intelligent immigrants who seize economic opportunities in wealthy Western cities will see their fertility rates plummet, and the idiots will inherit the Earth &agrave; la the movie \"Idiocracy\".\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>More to come in subsequent parts....</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6FzkEzpPQxrLyu8eH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 1, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "27119", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"margin-bottom: 0in;\">I know that many people on LessWrong want nothing to do with \"<a title=\"neoreaction\" href=\"http://iamlegionnaire.wordpress.com/core-works-of-neoreaction/#sthash.evlTGYwH.dpbs\" target=\"_blank\">neoreaction</a>.\"&nbsp; It does seem strange that a website commonly associated with techno-futurism, such as LessWrong, would end up with even the most tangential networked association with an intellectual current, such as neoreaction, that commonly includes nostalgia for <a title=\"absolute monarchies\" href=\"http://techcrunch.com/2013/11/22/geeks-for-monarchy/\" target=\"_blank\">absolute monarchies</a> and other avatistic obessions.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Perhaps blame it on Yvain, AKA Scott Alexander of slatestarcodex.com for attaching this strange intellectual node to LessWrong. ; ) That's at least how I found out about neoreaction, and I doubt that I am alone in this.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Certainly many on LessWrong would view any association with \"neoreaction\" as a Greek gift to be avoided. I understand the concept of keeping \"<a title=\"gardens\" href=\"/lw/c1/wellkept_gardens_die_by_pacifism/\" target=\"_blank\">well-kept gardens</a>\" and of politics being the \"<a title=\"politics mindkiller\" href=\"/lw/gw/politics_is_the_mindkiller/\" target=\"_blank\">mind-killer</a>,\" although some at LessWrong have argued that some of the most important questions humanity will face in the next decades will be questions that are unavoidably \"political\" in nature. Yes, \"<a title=\"politics hard mode\" href=\"/lw/kkp/politics_is_hard_mode/\" target=\"_blank\">politics is hard mode</a>,\" but so is life itself, and you don't get better at hard mode without practicing in hard mode.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">LessWrong proclaims itself as a community devoted to refining the art of rationality. One aspect of the art of rationality is locating the true sources of disagreement between two parties who want to communicate with each other, but who can't help but talk past each other in different languages due to having radically different pre-existing assumptions.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I believe that this is the problem that any discourse between neoreaction and progressivism currently faces.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Even if you have no interest at all in neoreaction or progressivism as ideologies, I invite you to read this analysis as a case study in locating sources of disagreement between ideologies that have different unspoken assumptions. I will try to steelman neoreaction as much as I can, despite the fact that I am more sympathetic to the progressivist point of view.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">In particular, I am interested in the following question:&nbsp; to what extent do neoreactionary and progressive disagreements stem from judgments that merely differ in degree?&nbsp; (For example, being slightly more or less pessimistic about X, Y, and Z propositions).&nbsp; Or to what extent do neoreactionary and progressive disagreements stem from assumptions that are <em>qualitatively </em>different?</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Normative_vs__descriptive_assumptions\">Normative vs. descriptive assumptions</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">\"Normative\" statements are \"ought\" statements, or judgments of value. \"Descriptive\" statements are \"is\" statements, or depictions of reality. While neoreaction and progressivism have a lot of differing descriptive assumptions, there is really only one fundamental normative disagreement, which I will address first.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Normative_disagreement__1__Progressivism_s_subjective_values_vs__Neoreaction_s_objective____values\">Normative disagreement #1: Progressivism's subjective values vs. Neoreaction's objective[?] values</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">As I see it, <strong>Progressivism says</strong>, \"Our subjective values are worth pursuing in and of themselves just because it makes us feel good. It does not particularly matter where our values come from. Perhaps we are Cartesian dualists\u2014unmoved movers with free will\u2014who invent our values in an act of existential creation. Or perhaps our values are biological programming\u2014spandrels manufactured by Nature, or as the neoreactionaries personify it, \"Gnon.\" It doesn't matter. In principle, if we could rewire our reward circuits to give us pleasure/fun/novelty/happiness/sadness/tragedy/suffering/whatever we desire* in response to whatever Nature had the automatic (or modified) disposition to offer us, then those good feelings would be just as worthwhile as anything else. (This is why neoreactionaries perceive progressive values as \"nihilistic.\")</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">According to this formulation, most LessWrongers, being averse to <a title=\"wireheading\" href=\"http://wiki.lesswrong.com/wiki/Wireheading\" target=\"_blank\">wireheading </a>in principle, are not full-fledged progressives at this most fundamental level.&nbsp; (Perhaps this explains some of the counter-intuitive overlap between the LessWrong and neoreactionary thoughtsphere....)&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">[Editorial:&nbsp; In my view, coming to terms with the obvious benefit of wireheading is the ultimate \"red pill\" to swallow. I am a progressive who would happily wirehead as long as I had concluded beforehand that I had adequately secured its completely automatic perpetuation even in the absence of any further input from me...although an optional override to shut it down and return me to the non-wireheaded state would not be unwelcome, just in case I had miscalculated and found that the system did not attend to my every wish as anticipated.]</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">*Note that I am aware that our subjective values are complex and that we are \"<a title=\"Godshatter\" href=\"/lw/l3/thou_art_godshatter/\" target=\"_blank\">Godshatter</a>.\" Nevertheless, this does not seem to me to be a fundamental impediment to wireheading. In principle, we should be able to dissect every last little bit of this \"Godshatter\" and figure out exactly what we want in all of its diversity...and then we can start designing a system of wireheading to give it to us. Is this not what Friendly AI is all about? Doesn't Friendly AI = Wireheading Done \"Right\"? Alternatively, we could re-wire ourselves to not be Godshatter, and to have a very simple list of things that would make us feel good. I am open to either one. LessWrongers, being neoreactionaries at heart (see below), would insist on maintaining our human complexity, our Godshatter values, and making our wireheading laboriously work around that. Okay, fine. I'll compromise...as long as I get my wireheading in some form. ; )</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Neoreaction says</strong>, \"There is objective value in the principle of \"perpetuating biological and/or civilizational complexity\" itself*; the best way to perpetuate biological and/or civilizational complexity is to \"serve Gnon\" (i.e. devote our efforts to fulfilling nature's pre-requisites for perpetuating our biologial and/or civilizational complexity); our subjective values are spandrels manufactured by natural selection/Gnon; insofar as our subjective values motivate us to serve Gnon and thereby ensure the perpetuation of biological and/or civilizational complexity, our subjective values are useful. (For example, natural selection makes sex a subjective value by making it pleasurable, which then motivates us to perpetuate our biological complexity). But, insofar as our subjective values mislead us from serving Gnon (such as by making non-procreative sex still feel good) and jeopardize our biological/civilizational perpetuation, we must sacrifice our subjective values for the objective good of perpetuating our biological/civilizational complexity\" (such as by buckling down and having procreative sex even if one would personally rather not enjoy raising kids).</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">*Note that different NRx thinkers might have different definitions about what counts as biological or civilizational \"complexity\" worthy of perpetuating...it could be \"Western Civilization,\" \"the White Race,\" \"Homo sapiens,\" \"one's own genetic material,\" \"intelligence, whether encoded in human brains or silicon AI,\" \"human complexity/Godshatter,\" etc. This has led to the so-called \"neoreactionary trichotomy\"\u20143 wings of the neoreactionary movement: Christian traditionalists, ethno-nationalists, and techno-commercialists.&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Most LessWrongers probably agree with neoreactionaries on this fundamental normative assumption, with the typical objective good of LessWrongers being \"human complexity/Godshatter,\" and thus the \"techno-commercialist\" wing of neoreaction being the one that typically finds the most interest among LessWrongers.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">[Editorial:&nbsp; pesumably, each neoreactionary is choosing his/her objective target of allegiance (such as \"Western Civilization\") because of the warm fuzzies that the idea elicits in him/herself. Has it ever occurred to neoreactionaries that humans' occasional predilection for being awed by a system bigger than themselves (such as \"Western Civilization\") and sacrificing for that system is itself a \"mere\" evolutionary spandrel?]</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Now, in an attempt to <a title=\"Steelmanning\" href=\"http://wiki.lesswrong.com/wiki/Steel_man\" target=\"_blank\">steelman</a> neoreaction's normative assumption, I would characterize it thus: \"In the most ultimate sense, neoreactionaries find the pursuit of subjective values just as worthwhile as progressives do. However, neoreactionaries are aware that human beings are short-sighted creatures with finite discount windows. If we tell ourselves that we should pursue our subjective values, we won't end up pursuing those subjective values in a farsighted way that involves, for example, maintaining a functioning civilization so that people continue to follow laws and don't rob or stab each other. Instead, we will invariably party it up and pursue short-term subjective values to the detriment of our long-term subjective values. So instead of admitting to ourselves that we are really interested in subjective value in the long run, we have to tell ourselves a noble lie that we are actually serving some higher objective purpose in order to motivate our primate brains to stick to what will happen to be good for subjective values in the long run.\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Indeed, I have found some neoreactionary writers muse on the problem of wanting to believe in God because it would serve as a unifying and motivating objective good, and lamenting the fact that they cannot bring themselves to do so.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Now, onto the descriptive disagreements....</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Descriptive_assumption__1__Humanity_can_master_nature__progressivism__vs__Nature_will_always_end_up_mastering_humanity__neoreaction__\">Descriptive assumption #1: Humanity can master nature (progressivism) vs. Nature will always end up mastering humanity (neoreaction).</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">Whereas progressives tend to have optimism that humankind can incrementally master the laws of nature (not change them, but master them, as in intelligently work around them, much like how we have worked around but not changed gravitation by inventing airplanes), neoreactionaries have a dour pessimism that humankind under-estimates the extent to which the laws of nature constantly pull our puppet strings.&nbsp; Far from being able to ever master nature, <a title=\"Gnon\" href=\"http://www.moreright.net/capturing-gnon/\" target=\"_blank\">humankind will always be mastered by nature</a>, by nature's command to \"race to the bottom\" in order to out-reproduce, out-compete one's rivals, even if that means having to sacrifice the nice things in life.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">For specific ways in which nature threatens to master humanity unless humanity somehow finds a way to exert tremendous efforts at collective coordination against nature, see Scott Alexander's \"<a title=\"Meditations on Moloch\" href=\"http://slatestarcodex.com/2014/07/30/meditations-on-moloch/\" target=\"_blank\">Meditations on Moloch</a>.\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Most progressives presumably hold out hope that we can collectively coordinate to overcome Moloch.&nbsp; If nature and its incentives threaten humanity with the strongest and most ruthless conquering the weak and charitable, perhaps we create a world government to prevent that.&nbsp; If nature and its incentives drive down wages to subsistence level, perhaps we create a global minimum wage.&nbsp; If humanity is threatened with dysgenic decline, perhaps a democratic world government organizes a eugenics program.&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Descriptive_assumption__2___On_average__people_have__or_can_be_trained_to_have__far_sighted_discount_functions__progressivism___vs__people_typically_have_short_sighted_discount_functions__neoreaction____\">Descriptive assumption #2:&nbsp; On average, people have, or can be trained to have, far-sighted <a title=\"Discount functions\" href=\"http://en.wikipedia.org/wiki/Discount_function\" target=\"_blank\">discount functions</a> (progressivism), vs. people typically have short-sighted discount functions (neoreaction).&nbsp; </strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">Part of the progressive assumption about humanity being able to master nature is that ordinary people are rational enough to see the big picture and submit to such controls if they are needed to avoid the disasters of Moloch.&nbsp; Part of the neoreactionary assumption about nature always mastering humanity is that, except for some bright outliers, most people are short-sighted primates who will insist on trading long-term well-being for short-term frills.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Descriptive_assumption__3__Culture_is_a_variable_mostly_dependent_on_material_conditions__progressivism__vs__Culture_is_an_independent_variable_with_respect_to_material_conditions__neoreaction__\">Descriptive assumption #3: Culture is a variable mostly dependent on material conditions (progressivism) vs. Culture is an independent variable with respect to material conditions (neoreaction).</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">Neoreactionaries often claim that life seems so much better in modern times in comparison to, say, 400 years ago, only because of our technological advancement since then has compensated for, and hidden, how our culture has rotted in the meantime. Neoreactionaries argue that, if one could combine our modern technology with, let's say, an absolute monarchy, then life would be so much better. This assumption of being able to mix &amp; match material conditions and political systems, or material conditions and culture, depends on an assumption that culture and social institutions are essentially independent variables. Perhaps with enough will, we can try to make any set of technologies work well with any set of cultural and social institutions.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Progressives, whether they realize it or not, are probably subtly influenced, instead, by the \"historical materialist\" (AKA Marxist) view of society which argues that certain material conditions and material incentives tend to automatically generate certain cultural and social responses.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">For example, to Marx, increased agricultural productivity in the late middle ages and Renaissance due to better agricultural technologies was a pre-requisite for the \"Acts of Enclosure\" in England, which booted the \"surplus\" farmers off of the farms and into the cities as propertyless proletarians who would be willing to work for a wage. Likewise, technologies like steam power were pre-requisites for providing an unprecedentedly profitable way of employing these proletarians to make a profit. (Otherwise, the proletarians might have just been left to rot on the street unemployed, with their numbers dwindling in Malthusian fashion). And because there were new avenues for making a profit, the people who stood to gain from chasing these new profit incentives produced new cultural habits and laws that would enable them to pursue these incentives more effectively. One of these new sets of laws was \"laissez-faire\" economics. Another was liberal democracy.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">To a progressive, the proposition that we could, even theoretically, run our modern technological society through an absolute monarchy would probably seem preposterous. It is not even an option. Our modern society is too complex, with too many conflicting interests to reconcile through any system that prohibits the peaceful discovery and negotiation of these varied interests through a democratic process involving \"voice.\" In reality, people are not content with being able only to exercise the \"right of exit\" from institutions or governments that they don't like. Perhaps the powerless have no choice but to immigrate. But elites have, historically, more often chosen to stand and fight rather than gracefully exit. Hence, feudalism, civil wars brought on by crises of royal succession, Masonic orders, factions, political parties, \"special interest groups,\" and so on.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Progressives would say, \"Do you honestly think that you can tame these beasts, when even a dictator like Hitler was just as much beholden to juggling interest groups and power blocs around him as he was the real dictator of events?\" Ah, but the neoreactionaries will say, \"Hitler's Nazism was still \"<a title=\"Neoreactionary glossary\" href=\"http://www.moreright.net/neoreactionary-glossary/\" target=\"_blank\">demotist</a>.\" It made the mistake of trying to justify itself to the public, if not through elections, then at least implicitly. We won't do that.\" To which progressives might say, \"You might not want to justify yourself to the rabble and to elite power blocs, but they will demand it\u2014and not because they are all infected by some mysterious mental virus called the \"<a title=\"Neoreactionary glossary\" href=\"http://www.moreright.net/neoreactionary-glossary/\" target=\"_blank\">Cathedral</a>,\" but because they see a way to gain an advantage through politics, and in the modern era they have the means and coordination to effectively fight for it.\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">These are just examples. The take-away point is that, for progressives, culture appears to be more of a dependent variable, not a variable that is independent of material conditions. So, according to progressives, you can't say, \"Let's just combine today's technology with absolute monarchy, and voil\u00e0!\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Descriptive_assumption__4__Western_society_is_currently_anabolic_ascendant__progressivism__vs__catabolic_decadent__neoreaction__\">Descriptive assumption #4: Western society is currently anabolic/ascendant (progressivism) vs. catabolic/decadent (neoreaction).</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">Neoreaction often gets <strong>caricatured </strong>as claiming that \"things are getting worse\" or \"have been getting worse for the past x number of years.\" This paints a weak straw-man of neoreaction because, on the surface, things seem so much \"obviously\" better now than ever. However, this isn't quite what neoreactionaries claim.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Neoreactionaries actually claim that Western society is decaying (note the subtle difference). Western society is gradually weakening its ability to reproduce itself. It is, to use a farming metaphor, eating up its seed-corn on present consumption, on insant gratification, which causes things to seem really swell on the surface...for now. However, according to neoreactionaries, conditions might not yet be getting worse on average (although they will point to inner city violence and other signs that conditions already have started to get worse in some places), but Western society's \"capital stock\" is getting worse, is already dwindling.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Envisioned more broadly, a society's \"capital\" is not just its money. It is its entire basket of tangible and intangible assets that help it reproduce and expand itself. So a society's \"capital\" would also include things like its citizens, its birth rates, its habits of harmonious gender relations, its education, its habits of civil propriety, its sustaining myths (such as patriotism or religion), its infrastructure, its environmental health [although NRxers tend to not focus on this], etc.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Another term for \"decadence\" might be \"catabolic collapse.\" A catabolic collapse is when an organism starts consuming its own muscles, its own seed-corn, if you will, in a last-ditch effort to stay alive. By contrast, an \"anabolic\" process is one that builds muscle\u2014one that saves up capital, if you will. (Hence, \"anabolic\" steroids).</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Neoreactionaries believe that Western society is currently headed for a \"catabolic collapse.\"&nbsp; (See John Michael Greer, author of \"<a title=\"Catabolic Collapse\" href=\"http://www.ecoshock.org/transcripts/greer_on_collapse.pdf\" target=\"_blank\">How Civilizations Fall: A Theory of Catabolic Collapse</a>.\"&nbsp; Oddly enough, John Michael Greer started out 10 years ago as a trendy name in anarcho-primitivist intellectual circles.&nbsp; Now his ideas have been embraced by some neoreactionaries such as Nick Land, which makes me ponder whether anarcho-primitivism is really of the \"left\" or \"right\" to begin with...)</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">When it comes to progressives, most, I think, would argue that Western society is not currently catabolic/decadent. Granted, they would point to some problems with \"unsustainability,\" especially with regards to environmental pollution, resource depletion, and maybe public debt levels (especially worrisome to the libertarian-minded). But on the whole, progressives are still optimistic that these problems can be overcome without rolling back liberal democracy.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Now, let's look at some specific worries that neoreaction has about Western decadence....</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Descriptive_Assumption__5__Our_biggest_population_threat_is_overshoot_and_the_attendant_resource_depletion__environmental_pollution__and_immiseration_of_living_standards__progressivism__vs__Our_biggest_population_threat_is_a_demographic_death_spiral__neoreaction__\">Descriptive Assumption #5: Our biggest population threat is overshoot and the attendant resource depletion, environmental pollution, and immiseration of living standards (progressivism) vs. Our biggest population threat is a demographic death spiral (neoreaction).</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">One thing I have noticed when looking at neoreactionary websites is that they are really obsessed with birth rates! They argue that countries with fertility below replacement level are on the road to annihilation. I found this interesting because my first impulse is to feel like this globe is getting too damn crowded.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Perhaps neoreactionaries envision the birth rates to stay below replacement level from here on out\u2014that this is a permanent change. Perhaps they foresee world population following a sort of bell-shaped curve. My naive progressive assumption is that our population is already in a slight overshoot beyond what can be sustained at our current level of technology, and that any present declines in birth rates are probably just enough to bring us into the oscillating plateau of a typical <a title=\"S-shaped curve\" href=\"http://images.tutorvista.com/content/biotic-community/population-growth-forms.jpeg\" target=\"_blank\">S-shaped popoulation curve</a>, and that better economic prospects could easily reverse the trend. My naive progressive assumption is that raising kids will remain sufficiently fun and interesting to a large enough pool of adults that, given enough of a feeling of economic security, people will happily continue having kids in sufficient numbers to prevent a die-off of Homo sapiens. In other words, most progressives like myself would not see the need to roll back gender norms in Western society at the present time for the sake of popping out more babies.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Perhaps what worries neoreactionaries, though, is not so much the fear of a global planetary baby shortage, but rather a localized baby shortage among Westerners or Whites. Maybe they fear that all babies are not created equal....</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Descriptive_assumption__6___Immigrants_are_OK___progressivism__vs___Immigrants_will_jeopardize_Western_Civilization_the_White_Race_intelligent_human_complexity_etc____neoreaction_\">Descriptive assumption #6: \"Immigrants are OK\" (progressivism) vs. \"Immigrants will jeopardize Western Civilization/the White Race/intelligent human complexity/etc.\" (neoreaction)</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">Progressives say, \"It is not a big deal if Western society has to import some immigrants to keep its population topped off. Immigrant cultures will eventually blend with the \"nativist\" culture. Historically, this has turned out OK, despite xenophobic fears every time that it will end in disaster. The immigrants will mostly assimilate into the nativist culture. The nativist culture will pick up a few new habits from the immigrants (some of them helpful, some of them harmful, but on the balance nothing disastrous). Nor will the immigrants dirty the nativist gene pool with bad genes. As far as we can tell so far, no significant genetic differences in intelligence and/or physical vigor exist between immigrants and non-immigrants.\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Neoreactionaries say, \"It is a very big deal if Western society has to import some immigrants to keep its population topped off. Immigrant cultures will not assimilate with the nativist culture. Immigrant cultures will end up imparting a net influene of bad habits on the native culture. Civil decency will be eroded. Crime and societal dysfunction will increase. The native gene pool will also be dirtied with lower-intelligence immigrant genes. (And the only reason we can't see this is because the progressive Establishment AKA the \"Cathedral\" has systematically distorted <a title=\"IQ\" href=\"http://www.lagriffedulion.f2s.com/sft.htm\" target=\"_blank\">the research and discourse around IQ</a>). At worst, Western cities will act as \"<a title=\"IQ Shredders\" href=\"http://www.xenosystems.net/iq-shredders/\" target=\"_blank\">IQ Shredders</a>.\" Any intelligent immigrants who seize economic opportunities in wealthy Western cities will see their fertility rates plummet, and the idiots will inherit the Earth \u00e0 la the movie \"Idiocracy\".\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"More_to_come_in_subsequent_parts____\">More to come in subsequent parts....</strong></p>", "sections": [{"title": "Normative vs. descriptive assumptions", "anchor": "Normative_vs__descriptive_assumptions", "level": 1}, {"title": "Normative disagreement #1: Progressivism's subjective values vs. Neoreaction's objective[?] values", "anchor": "Normative_disagreement__1__Progressivism_s_subjective_values_vs__Neoreaction_s_objective____values", "level": 1}, {"title": "Descriptive assumption #1: Humanity can master nature (progressivism) vs. Nature will always end up mastering humanity (neoreaction).", "anchor": "Descriptive_assumption__1__Humanity_can_master_nature__progressivism__vs__Nature_will_always_end_up_mastering_humanity__neoreaction__", "level": 1}, {"title": "Descriptive assumption #2:\u00a0 On average, people have, or can be trained to have, far-sighted discount functions (progressivism), vs. people typically have short-sighted discount functions (neoreaction).\u00a0 ", "anchor": "Descriptive_assumption__2___On_average__people_have__or_can_be_trained_to_have__far_sighted_discount_functions__progressivism___vs__people_typically_have_short_sighted_discount_functions__neoreaction____", "level": 1}, {"title": "Descriptive assumption #3: Culture is a variable mostly dependent on material conditions (progressivism) vs. Culture is an independent variable with respect to material conditions (neoreaction).", "anchor": "Descriptive_assumption__3__Culture_is_a_variable_mostly_dependent_on_material_conditions__progressivism__vs__Culture_is_an_independent_variable_with_respect_to_material_conditions__neoreaction__", "level": 1}, {"title": "Descriptive assumption #4: Western society is currently anabolic/ascendant (progressivism) vs. catabolic/decadent (neoreaction).", "anchor": "Descriptive_assumption__4__Western_society_is_currently_anabolic_ascendant__progressivism__vs__catabolic_decadent__neoreaction__", "level": 1}, {"title": "Descriptive Assumption #5: Our biggest population threat is overshoot and the attendant resource depletion, environmental pollution, and immiseration of living standards (progressivism) vs. Our biggest population threat is a demographic death spiral (neoreaction).", "anchor": "Descriptive_Assumption__5__Our_biggest_population_threat_is_overshoot_and_the_attendant_resource_depletion__environmental_pollution__and_immiseration_of_living_standards__progressivism__vs__Our_biggest_population_threat_is_a_demographic_death_spiral__neoreaction__", "level": 1}, {"title": "Descriptive assumption #6: \"Immigrants are OK\" (progressivism) vs. \"Immigrants will jeopardize Western Civilization/the White Race/intelligent human complexity/etc.\" (neoreaction)", "anchor": "Descriptive_assumption__6___Immigrants_are_OK___progressivism__vs___Immigrants_will_jeopardize_Western_Civilization_the_White_Race_intelligent_human_complexity_etc____neoreaction_", "level": 1}, {"title": "More to come in subsequent parts....", "anchor": "More_to_come_in_subsequent_parts____", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "340 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 340, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tscc3e5eujrsEeFN4", "9weLK2AJ9JEt2Tt8f", "jxfu7CTc3NidinuXD", "cSXZpvqpa9vbGGLtG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-04T22:23:22.292Z", "modifiedAt": "2020-07-26T07:01:48.870Z", "url": null, "title": "Goal retention discussion with Eliezer", "slug": "goal-retention-discussion-with-eliezer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:07.963Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "MaxTegmark", "user": {"username": "MaxTegmark", "createdAt": "2014-02-16T13:56:23.295Z", "isAdmin": false, "displayName": "MaxTegmark"}, "userId": "tSyRZ2dnWXgi5M964", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FtNFhuXXtmSjnNvE7/goal-retention-discussion-with-eliezer", "pageUrlRelative": "/posts/FtNFhuXXtmSjnNvE7/goal-retention-discussion-with-eliezer", "linkUrl": "https://www.lesswrong.com/posts/FtNFhuXXtmSjnNvE7/goal-retention-discussion-with-eliezer", "postedAtFormatted": "Thursday, September 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Goal%20retention%20discussion%20with%20Eliezer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGoal%20retention%20discussion%20with%20Eliezer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFtNFhuXXtmSjnNvE7%2Fgoal-retention-discussion-with-eliezer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Goal%20retention%20discussion%20with%20Eliezer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFtNFhuXXtmSjnNvE7%2Fgoal-retention-discussion-with-eliezer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFtNFhuXXtmSjnNvE7%2Fgoal-retention-discussion-with-eliezer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1672, "htmlBody": "<p>Although I feel that Nick Bostrom\u2019s new book \u201cSuperintelligence\u201d is&nbsp;generally awesome and a well-needed&nbsp;milestone for the field, I&nbsp;do have one quibble: both he and Steve Omohundro appear to be more&nbsp;convinced than I am by the assumption that an AI will naturally tend to&nbsp;retain its goals as it reaches a deeper understanding of the world and of&nbsp;itself. I\u2019ve written a short essay on this issue from my physics perspective, available at <a href=\"http://arxiv.org/pdf/1409.0813.pdf\">http://arxiv.org/pdf/1409.0813.pdf</a>.</p><p>Eliezer&nbsp;Yudkowsky just sent the following extremely interesting comments, and told me he was OK with me sharing them here to spur a broader discussion of these issues, so here goes.<br>&nbsp;</p><p><i>On Sep 3, 2014, at 17:21, Eliezer Yudkowsky &lt;yudkowsky@gmail.com&gt; wrote:</i></p><p><i>Hi Max! &nbsp;You're asking the right questions. &nbsp;Some of the answers we can</i><br><i>give you, some we can't, few have been written up and even fewer in any</i><br><i>well-organized way. &nbsp;Benja or Nate might be able to expound in more detail</i><br><i>while I'm in my seclusion.</i><br><br><i>Very briefly, though:</i><br><i>The problem of utility functions turning out to be ill-defined in light of</i><br><i>new discoveries of the universe is what Peter de Blanc named an</i><br><i>\"ontological crisis\" (not necessarily a particularly good name, but it's</i><br><i>what we've been using locally).</i><br><br><i>http://intelligence.org/files/OntologicalCrises.pdf</i><br><br><i>The way I would phrase this problem now is that an expected utility</i><br><i>maximizer makes comparisons between quantities that have the type</i><br><i>\"expected utility conditional on an action\", which means that the AI's</i><br><i>utility function must be something that can assign utility-numbers to the</i><br><i>AI's model of reality, and these numbers must have the further property</i><br><i>that there is some computationally feasible approximation for calculating</i><br><i>expected utilities relative to the AI's probabilistic beliefs. &nbsp;This is a</i><br><i>constraint that rules out the vast majority of all completely chaotic and</i><br><i>uninteresting utility functions, but does not rule out, say, \"make lots of</i><br><i>paperclips\".</i><br><br><i>Models also have the property of being Bayes-updated using sensory</i><br><i>information; for the sake of discussion let's also say that models are</i><br><i>about universes that can generate sensory information, so that these</i><br><i>models can be probabilistically falsified or confirmed. &nbsp;Then an</i><br><i>\"ontological crisis\" occurs when the hypothesis that best fits sensory</i><br><i>information corresponds to a model that the utility function doesn't run</i><br><i>on, or doesn't detect any utility-having objects in. &nbsp;The example of</i><br><i>\"immortal souls\" is a reasonable one. &nbsp;Suppose we had an AI that had a</i><br><i>naturalistic version of a Solomonoff prior, a language for specifying</i><br><i>universes that could have produced its sensory data. &nbsp;Suppose we tried to</i><br><i>give it a utility function that would look through any given model, detect</i><br><i>things corresponding to immortal souls, and value those things. &nbsp;Even if</i><br><i>the immortal-soul-detecting utility function works perfectly (it would in</i><br><i>fact detect all immortal souls) this utility function will not detect</i><br><i>anything in many (representations of) universes, and in particular it will</i><br><i>not detect anything in the (representations of) universes we think have</i><br><i>most of the probability mass for explaining our own world. &nbsp;In this case</i><br><i>the AI's behavior is undefined until you tell me more things about the AI;</i><br><i>an obvious possibility is that the AI would choose most of its actions</i><br><i>based on low-probability scenarios in which hidden immortal souls existed</i><br><i>that its actions could affect. &nbsp;(Note that even in this case the utility</i><br><i>function is stable!)</i><br><br><i>Since we don't know the final laws of physics and could easily be</i><br><i>surprised by further discoveries in the laws of physics, it seems pretty</i><br><i>clear that we shouldn't be specifying a utility function over exact</i><br><i>physical states relative to the Standard Model, because if the Standard</i><br><i>Model is even slightly wrong we get an ontological crisis. &nbsp;Of course</i><br><i>there are all sorts of extremely good reasons we should not try to do this</i><br><i>anyway, some of which are touched on in your draft; there just is no</i><br><i>simple function of physics that gives us something good to maximize. &nbsp;See</i><br><i>also Complexity of Value, Fragility of Value, indirect normativity, the</i><br><i>whole reason for a drive behind CEV, and so on. &nbsp;We're almost certainly</i><br><i>going to be using some sort of utility-learning algorithm, the learned</i><br><i>utilities are going to bind to modeled final physics by way of modeled</i><br><i>higher levels of representation which are known to be imperfect, and we're</i><br><i>going to have to figure out how to preserve the model and learned</i><br><i>utilities through shifts of representation. &nbsp;E.g., the AI discovers that</i><br><i>humans are made of atoms rather than being ontologically fundamental</i><br><i>humans, and furthermore the AI's multi-level representations of reality</i><br><i>evolve to use a different sort of approximation for \"humans\", but that's</i><br><i>okay because our utility-learning mechanism also says how to re-bind the</i><br><i>learned information through an ontological shift.</i><br><br><i>This sorta thing ain't going to be easy which is the other big reason to</i><br><i>start working on it well in advance. &nbsp;I point out however that this</i><br><i>doesn't seem unthinkable in human terms. &nbsp;We discovered that brains are</i><br><i>made of neurons but were nonetheless able to maintain an intuitive grasp</i><br><i>on what it means for them to be happy, and we don't throw away all that</i><br><i>info each time a new physical discovery is made. &nbsp;The kind of cognition we</i><br><i>want does not seem inherently self-contradictory.</i><br><br><i>Three other quick remarks:</i><br><br><i>*) &nbsp;Natural selection is not a consequentialist, nor is it the sort of</i><br><i>consequentialist that can sufficiently precisely predict the results of</i><br><i>modifications that the basic argument should go through for its stability.</i><br><i>The Omohundrian/Yudkowskian argument is not that we can take an arbitrary</i><br><i>stupid young AI and it will be smart enough to self-modify in a way that</i><br><i>preserves its values, but rather that most AIs that don't self-destruct</i><br><i>will eventually end up at a stable fixed-point of coherent</i><br><i>consequentialist values. &nbsp;This could easily involve a step where, e.g., an</i><br><i>AI that started out with a neural-style delta-rule policy-reinforcement</i><br><i>learning algorithm, or an AI that started out as a big soup of</i><br><i>self-modifying heuristics, is \"taken over\" by whatever part of the AI</i><br><i>first learns to do consequentialist reasoning about code. &nbsp;But this</i><br><i>process doesn't repeat indefinitely; it stabilizes when there's a</i><br><i>consequentialist self-modifier with a coherent utility function that can</i><br><i>precisely predict the results of self-modifications. &nbsp;The part where this</i><br><i>does happen to an initial AI that is under this threshold of stability is</i><br><i>a big part of the problem of Friendly AI and it's why MIRI works on tiling</i><br><i>agents and so on!</i><br><br><i>*) &nbsp;Natural selection is not a consequentialist, nor is it the sort of</i><br><i>consequentialist that can sufficiently precisely predict the results of</i><br><i>modifications that the basic argument should go through for its stability.</i><br><i>It built humans to be consequentialists that would value sex, not value</i><br><i>inclusive genetic fitness, and not value being faithful to natural</i><br><i>selection's optimization criterion. &nbsp;Well, that's dumb, and of course the</i><br><i>result is that humans don't optimize for inclusive genetic fitness.</i><br><i>Natural selection was just stupid like that. &nbsp;But that doesn't mean</i><br><i>there's a generic process whereby an agent rejects its \"purpose\" in the</i><br><i>light of exogenously appearing preference criteria. &nbsp;Natural selection's</i><br><i>anthropomorphized \"purpose\" in making human brains is just not the same as</i><br><i>the cognitive purposes represented in those brains. &nbsp;We're not talking</i><br><i>about spontaneous rejection of internal cognitive purposes based on their</i><br><i>causal origins failing to meet some exogenously-materializing criterion of</i><br><i>validity. &nbsp;Our rejection of \"maximize inclusive genetic fitness\" is not an</i><br><i>exogenous rejection of something that was explicitly represented in us,</i><br><i>that we were explicitly being consequentialists for. &nbsp;It's a rejection of</i><br><i>something that was never an explicitly represented terminal value in the</i><br><i>first place. &nbsp;Similarly the stability argument for sufficiently advanced</i><br><i>self-modifiers doesn't go through a step where the successor form of the</i><br><i>AI reasons about the intentions of the previous step and respects them</i><br><i>apart from its constructed utility function. &nbsp;So the lack of any universal</i><br><i>preference of this sort is not a general obstacle to stable</i><br><i>self-improvement.</i><br><br><i>*) &nbsp;&nbsp;The case of natural selection does not illustrate a universal</i><br><i>computational constraint, it illustrates something that we could</i><br><i>anthropomorphize as a foolish design error. &nbsp;Consider humans building Deep</i><br><i>Blue. &nbsp;We built Deep Blue to attach a sort of default value to queens and</i><br><i>central control in its position evaluation function, but Deep Blue is</i><br><i>still perfectly able to sacrifice queens and central control alike if the</i><br><i>position reaches a checkmate thereby. &nbsp;In other words, although an agent</i><br><i>needs crystallized instrumental goals, it is also perfectly reasonable to</i><br><i>have an agent which never knowingly sacrifices the terminally defined</i><br><i>utilities for the crystallized instrumental goals if the two conflict;</i><br><i>indeed \"instrumental value of X\" is simply \"probabilistic belief that X</i><br><i>leads to terminal utility achievement\", which is sensibly revised in the</i><br><i>presence of any overriding information about the terminal utility. &nbsp;To put</i><br><i>it another way, in a rational agent, the only way a loose generalization</i><br><i>about instrumental expected-value can conflict with and trump terminal</i><br><i>actual-value is if the agent doesn't know it, i.e., it does something that</i><br><i>it reasonably expected to lead to terminal value, but it was wrong.</i><br><br><i>This has been very off-the-cuff and I think I should hand this over to</i><br><i>Nate or Benja if further replies are needed, if that's all right.</i><br>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 2, "xHTXnyp65X8YX6ahT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FtNFhuXXtmSjnNvE7", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 60, "baseScore": 92, "extendedScore": null, "score": 0.00027, "legacy": true, "legacyId": "27120", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 61, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-05T03:52:58.778Z", "modifiedAt": null, "url": null, "title": "Meetup :  Sydney Meetup - September", "slug": "meetup-sydney-meetup-september-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taryneast", "createdAt": "2010-11-29T20:51:06.328Z", "isAdmin": false, "displayName": "taryneast"}, "userId": "xD8wjhiTvwbXdKirW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kFShCyqBDpS3bfCbS/meetup-sydney-meetup-september-0", "pageUrlRelative": "/posts/kFShCyqBDpS3bfCbS/meetup-sydney-meetup-september-0", "linkUrl": "https://www.lesswrong.com/posts/kFShCyqBDpS3bfCbS/meetup-sydney-meetup-september-0", "postedAtFormatted": "Friday, September 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%20Sydney%20Meetup%20-%20September&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%20Sydney%20Meetup%20-%20September%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFShCyqBDpS3bfCbS%2Fmeetup-sydney-meetup-september-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%20Sydney%20Meetup%20-%20September%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFShCyqBDpS3bfCbS%2Fmeetup-sydney-meetup-september-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFShCyqBDpS3bfCbS%2Fmeetup-sydney-meetup-september-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/142'> Sydney Meetup - September</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 September 2014 06:30:06PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Sydney City RSL, 565 George St, Sydney, Australia 2000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>6:30 PM for early discussion 7PM general dinner-discussion after dinner we'll have our rationality exercise and a more specific discussion-topic.</p>\n\n<p>We normally meet in the restaurant on level 2. So far they've always put it about 5m to the left of the lift, up against the kitchen area.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/142'> Sydney Meetup - September</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kFShCyqBDpS3bfCbS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.980444551925574e-06, "legacy": true, "legacyId": "27123", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____Sydney_Meetup___September\">Discussion article for the meetup : <a href=\"/meetups/142\"> Sydney Meetup - September</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 September 2014 06:30:06PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Sydney City RSL, 565 George St, Sydney, Australia 2000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>6:30 PM for early discussion 7PM general dinner-discussion after dinner we'll have our rationality exercise and a more specific discussion-topic.</p>\n\n<p>We normally meet in the restaurant on level 2. So far they've always put it about 5m to the left of the lift, up against the kitchen area.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup____Sydney_Meetup___September1\">Discussion article for the meetup : <a href=\"/meetups/142\"> Sydney Meetup - September</a></h2>", "sections": [{"title": "Discussion article for the meetup :  Sydney Meetup - September", "anchor": "Discussion_article_for_the_meetup____Sydney_Meetup___September", "level": 1}, {"title": "Discussion article for the meetup :  Sydney Meetup - September", "anchor": "Discussion_article_for_the_meetup____Sydney_Meetup___September1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-05T05:41:01.629Z", "modifiedAt": null, "url": null, "title": "What motivates politicians?", "slug": "what-motivates-politicians", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:39.480Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamzerner", "createdAt": "2013-08-12T18:18:47.957Z", "isAdmin": false, "displayName": "adamzerner"}, "userId": "6jLdWqegNefgaabhr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uby3PRdQw6jPuhory/what-motivates-politicians", "pageUrlRelative": "/posts/uby3PRdQw6jPuhory/what-motivates-politicians", "linkUrl": "https://www.lesswrong.com/posts/uby3PRdQw6jPuhory/what-motivates-politicians", "postedAtFormatted": "Friday, September 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20motivates%20politicians%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20motivates%20politicians%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuby3PRdQw6jPuhory%2Fwhat-motivates-politicians%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20motivates%20politicians%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuby3PRdQw6jPuhory%2Fwhat-motivates-politicians", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuby3PRdQw6jPuhory%2Fwhat-motivates-politicians", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 246, "htmlBody": "<p>It seems that politicians make a lot of decisions that aren't socially optimal because they want money from lobbyists and other campaign contributors. Presumably, the purpose this money serves is to keep them in office by allowing them to advertise a lot the next time they're up for reelection.</p>\n<p>So the question then becomes, \"why do they want to remain in office?\". I could think of two reasons: money and power. From what I know, politicians have a pretty high salary (<a href=\"http://www.ehow.com/info_8259430_much-congress-make-per-year.html\">congressmen make ~$175k</a>), so that's an understandable motivator. But power is the one I don't understand.</p>\n<p>Supposedly they want to remain in office so they could use their power to have an influence. I don't know too much about politics, but it seems that politicians spend most of their time catering to lobbyists and voters rather than pushing the things they actually believe in. So much so that they aren't actually exerting that much power. And it seems that most of this catering is to special interests and is socially suboptimal. (I may very well be wrong on these points. I really don't know but it's the impression I get.)</p>\n<p>Why are congressmen so motivated to stay in office, make $175k a year, exert a minimal amount of real power, and spend their time catering to lobbyists and making socially suboptimal decisions? I'm sure they could make twice as much in the private sector. I feel like there's something obvious that I'm missing here, but I'm genuinely confused.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uby3PRdQw6jPuhory", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 6, "extendedScore": null, "score": 1.9806347038709514e-06, "legacy": true, "legacyId": "27124", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-05T12:38:21.030Z", "modifiedAt": null, "url": null, "title": "I'm holding a birthday fundraiser", "slug": "i-m-holding-a-birthday-fundraiser", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:10.029Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BvsFg7xCsFBjos56J/i-m-holding-a-birthday-fundraiser", "pageUrlRelative": "/posts/BvsFg7xCsFBjos56J/i-m-holding-a-birthday-fundraiser", "linkUrl": "https://www.lesswrong.com/posts/BvsFg7xCsFBjos56J/i-m-holding-a-birthday-fundraiser", "postedAtFormatted": "Friday, September 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I'm%20holding%20a%20birthday%20fundraiser&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI'm%20holding%20a%20birthday%20fundraiser%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBvsFg7xCsFBjos56J%2Fi-m-holding-a-birthday-fundraiser%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I'm%20holding%20a%20birthday%20fundraiser%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBvsFg7xCsFBjos56J%2Fi-m-holding-a-birthday-fundraiser", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBvsFg7xCsFBjos56J%2Fi-m-holding-a-birthday-fundraiser", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<p><strong>EDIT: The fundraiser was successfully completed, raising the full $500 for worthwhile charities. Yay!</strong></p>\n<p>Today's my birthday! And per <a href=\"/user/peter_hurford/overview/\">Peter Hurford's</a> suggestion, I'm holding a <a href=\"http://www.youcaring.com/nonprofits/kaj-s-donating-his-birthday/227034\">birthday fundraiser</a>&nbsp;to help raise money for MIRI, GiveDirectly, and Mercy for Animals.&nbsp;If you like my activity on LW or elsewhere, please consider giving a few dollars to one of these organizations via the <a href=\"http://www.youcaring.com/nonprofits/kaj-s-donating-his-birthday/227034\">fundraiser page</a>. You can specify which organization you wish to donate in the comment of the donation, or just leave it unspecified, in which case I'll give your donation to MIRI.</p>\n<p>If you don't happen to be particularly altruistically motivated, just consider it a birthday gift to me - it will give me warm fuzzies to know that I helped move money for worthy organizations. And if you are altruistically motivated but don't care about me in particular, maybe you still can get yourself to donate more than usual by hacky stuff like someone you know on the Internet having a birthday. :)</p>\n<p>If someone else wants to hold their own birthday fundraiser, here are some tips: <a href=\"http://www.charityscience.com/birthday-fundraisers.html\">birthday fundraisers</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Z6DgiCrMtpSNxwuYW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BvsFg7xCsFBjos56J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 39, "extendedScore": null, "score": 0.000123, "legacy": true, "legacyId": "27125", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-05T15:53:39.325Z", "modifiedAt": null, "url": null, "title": "Meetup : Rationality Meetup Vienna - Superintelligence", "slug": "meetup-rationality-meetup-vienna-superintelligence", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaLeptikon", "createdAt": "2014-03-29T17:32:19.520Z", "isAdmin": false, "displayName": "AnnaLeptikon"}, "userId": "FyZibA2dPTe9zcmND", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4cjfiokKSPEwcEk7a/meetup-rationality-meetup-vienna-superintelligence", "pageUrlRelative": "/posts/4cjfiokKSPEwcEk7a/meetup-rationality-meetup-vienna-superintelligence", "linkUrl": "https://www.lesswrong.com/posts/4cjfiokKSPEwcEk7a/meetup-rationality-meetup-vienna-superintelligence", "postedAtFormatted": "Friday, September 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Rationality%20Meetup%20Vienna%20-%20Superintelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Rationality%20Meetup%20Vienna%20-%20Superintelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4cjfiokKSPEwcEk7a%2Fmeetup-rationality-meetup-vienna-superintelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Rationality%20Meetup%20Vienna%20-%20Superintelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4cjfiokKSPEwcEk7a%2Fmeetup-rationality-meetup-vienna-superintelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4cjfiokKSPEwcEk7a%2Fmeetup-rationality-meetup-vienna-superintelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 116, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/143'>Rationality Meetup Vienna - Superintelligence</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 September 2014 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Kaiserm\u00fchlenstra\u00dfe 24/2, 1220 Wien, meeting room behind the building</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>location:</strong> When arriving by U2 or Schnellbahn train: take the exit towards Kaiserm\u00fchlenstra\u00dfe, cross the street, step through the (very modern looking) building (on Erich Fried Weg) and go right, along the backside of the building until you get to the meeting room (it has a glass front so it should be hard to miss).\n<strong>Important</strong>: Google maps doesn't recognise the address, so what is being displayed here is nonsense.</p>\n\n<p><strong>topic</strong>: Marko will summarize \"Superintelligence: Paths, Dangers, Strategies\" by Nick Bostrom</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/143'>Rationality Meetup Vienna - Superintelligence</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4cjfiokKSPEwcEk7a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.981713474500778e-06, "legacy": true, "legacyId": "27126", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Rationality_Meetup_Vienna___Superintelligence\">Discussion article for the meetup : <a href=\"/meetups/143\">Rationality Meetup Vienna - Superintelligence</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 September 2014 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Kaiserm\u00fchlenstra\u00dfe 24/2, 1220 Wien, meeting room behind the building</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>location:</strong> When arriving by U2 or Schnellbahn train: take the exit towards Kaiserm\u00fchlenstra\u00dfe, cross the street, step through the (very modern looking) building (on Erich Fried Weg) and go right, along the backside of the building until you get to the meeting room (it has a glass front so it should be hard to miss).\n<strong>Important</strong>: Google maps doesn't recognise the address, so what is being displayed here is nonsense.</p>\n\n<p><strong>topic</strong>: Marko will summarize \"Superintelligence: Paths, Dangers, Strategies\" by Nick Bostrom</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Rationality_Meetup_Vienna___Superintelligence1\">Discussion article for the meetup : <a href=\"/meetups/143\">Rationality Meetup Vienna - Superintelligence</a></h2>", "sections": [{"title": "Discussion article for the meetup : Rationality Meetup Vienna - Superintelligence", "anchor": "Discussion_article_for_the_meetup___Rationality_Meetup_Vienna___Superintelligence", "level": 1}, {"title": "Discussion article for the meetup : Rationality Meetup Vienna - Superintelligence", "anchor": "Discussion_article_for_the_meetup___Rationality_Meetup_Vienna___Superintelligence1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-05T16:18:34.387Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-24", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:08.302Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ToSmEvJDYJRZ8hiB2/weekly-lw-meetups-24", "pageUrlRelative": "/posts/ToSmEvJDYJRZ8hiB2/weekly-lw-meetups-24", "linkUrl": "https://www.lesswrong.com/posts/ToSmEvJDYJRZ8hiB2/weekly-lw-meetups-24", "postedAtFormatted": "Friday, September 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FToSmEvJDYJRZ8hiB2%2Fweekly-lw-meetups-24%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FToSmEvJDYJRZ8hiB2%2Fweekly-lw-meetups-24", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FToSmEvJDYJRZ8hiB2%2Fweekly-lw-meetups-24", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 533, "htmlBody": "<p><strong>This summary was posted to LW main on August 29th. The following week's summary is <a href=\"/lw/kxj/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/13l\">Atlanta August meetup - Media representations:&nbsp;<span class=\"date\">30 August 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/13r\">Bratislava:&nbsp;<span class=\"date\">08 September 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/zr\">Houston, TX:&nbsp;<span class=\"date\">13 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13j\">Urbana-Champaign: Reconstituting:&nbsp;<span class=\"date\">31 August 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13a\">[Utrecht] Topic to be determinined:&nbsp;<span class=\"date\">06 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13b\">[Utrecht] Debiasing techniques:&nbsp;<span class=\"date\">20 September 2014 02:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/13m\">Canberra: Akrasia-busters!:&nbsp;<span class=\"date\">13 September 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/13o\">London Meetup - Effective Altruism:&nbsp;<span class=\"date\">31 August 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13h\">[Melbourne] September Rationality Dojo - Fixed and Growth Mindset:&nbsp;<span class=\"date\">07 September 2014 03:30PM</span></a></li>\n<li><a href=\"/meetups/13n\">Moscow Meetup:&nbsp;<span class=\"date\">31 August 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13q\">Washington, D.C.: Parkour:&nbsp;<span class=\"date\">31 August 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/13p\">West LA Meetup: Lightning Talks:&nbsp;<span class=\"date\">03 September 2014 07:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Buffalo.2C_NY\">Buffalo</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Moscow.2C_Russia\">Moscow</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong>&nbsp;</strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Sydney\">Sydney</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> </strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tel_Aviv.2C_Israel\">Tel Aviv</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Warsaw.2C_Poland\">Warsaw</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ToSmEvJDYJRZ8hiB2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.9817573737904068e-06, "legacy": true, "legacyId": "27057", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tvDAC8YjsJYeCEPk3", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-05T19:33:42.298Z", "modifiedAt": null, "url": null, "title": "A reason to see the future", "slug": "a-reason-to-see-the-future", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:20.913Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wFagkzCboLLhaicEK/a-reason-to-see-the-future", "pageUrlRelative": "/posts/wFagkzCboLLhaicEK/a-reason-to-see-the-future", "linkUrl": "https://www.lesswrong.com/posts/wFagkzCboLLhaicEK/a-reason-to-see-the-future", "postedAtFormatted": "Friday, September 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20reason%20to%20see%20the%20future&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20reason%20to%20see%20the%20future%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwFagkzCboLLhaicEK%2Fa-reason-to-see-the-future%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20reason%20to%20see%20the%20future%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwFagkzCboLLhaicEK%2Fa-reason-to-see-the-future", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwFagkzCboLLhaicEK%2Fa-reason-to-see-the-future", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 258, "htmlBody": "<p>I just learned of <a href=\"http://www.theguardian.com/books/2014/sep/05/margaret-atwood-new-work-unseen-century-future-library\">The Future Library project</a>. In short, famous authors will be asked to write new, original fiction that will not be released until 2114. First one announced was Margaret Atwood, of The Handmaiden's Tale fame.</p>\n<p>I learned of this when a friend posted on Facebook that \"I'm officially looking into being cryogenically frozen due to The Future Library project. See you all in 2114.\" She meant it as a joke, but after a couple comments she now knows about CI, and she didn't yesterday.</p>\n<p>What's one of the most common complaints we hear from Deathists? The future is unknown and scary and there won't be anything there they'd be interested in anyway. Now there will be, if they're Atwood fans.</p>\n<p>What's one of the ways artists who give away most of their work (almost all of them nowadays) try to entice people to pay for their albums/books/games/whatever? Including special content that is only available for people who pay (or who pay more). Now there is special content only available for people who are around post-2113.</p>\n<p>Which got me to thinking... could we incentivize seeing the future? I know it sounds kinda silly (\"What, escaping utter annihilation isn't incentive enough??\"), but it seems possible that we could save lives by compiling original work from popular artists (writers, musicians, etc), sealing it tight somewhere, and promising to release it in 100, 200, maybe 250 years. And of course, providing links to cryo resources with all publicity materials.</p>\n<p>Would this be worth pursuing? Are there any obvious downsides, aside from cost &amp; difficulty?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wFagkzCboLLhaicEK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 29, "extendedScore": null, "score": 0.000122, "legacy": true, "legacyId": "27128", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-05T20:15:59.281Z", "modifiedAt": "2019-12-24T20:22:05.601Z", "url": null, "title": "Deception detection machines", "slug": "deception-detection-machines", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:35.302Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "vpeitoi89GPGat77P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JApWmekjxuTBkoenz/deception-detection-machines", "pageUrlRelative": "/posts/JApWmekjxuTBkoenz/deception-detection-machines", "linkUrl": "https://www.lesswrong.com/posts/JApWmekjxuTBkoenz/deception-detection-machines", "postedAtFormatted": "Friday, September 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Deception%20detection%20machines&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADeception%20detection%20machines%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJApWmekjxuTBkoenz%2Fdeception-detection-machines%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Deception%20detection%20machines%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJApWmekjxuTBkoenz%2Fdeception-detection-machines", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJApWmekjxuTBkoenz%2Fdeception-detection-machines", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 352, "htmlBody": "<p>Assume you had access to:</p>\n<p>1. a self-improving artificial general intelligence (AGI) of at least human-capability or better; and</p>\n<p>2. a lie detector that the AGI is known to be susceptible to. A lie for this purpose is any intentional deception.</p>\n<p>The AI is not omnipotent. It is running on a physical machine in the real world with limited inputs. One of these inputs is mechanism by which you can ask it questions, and receive answers filtered through the lie detector. A little light on the box turns red if the machine was intentionally deceptive in any way, or green otherwise. As it happens the lie detector is probabilistic, but the chance of a false negative (lie escaping detection) is infinitesimal\u20141 in 10^40, let's say, but assume you could make it even smaller if you needed to. The chance of a false positive (non-deception being labeled a lie) is effectively zero.</p>\n<p>What could you do with this setup?</p>\n<p>\u00a0</p>\n<p>\n<hr />\n</p>\n<p>\u00a0</p>\n<p>EDIT: Some of the details above are provided simply to make the problem tractable. E.g. having the deception detector be probabilistic allows its operating requirements to be less than the AI itself. If the AI is running on a supercomputer, it seems intractable to require a super-supercomputer for the deception detector.</p>\n<p>EDIT2: Deception in this case simply means falsifying or omitting information. Loosely speaking, if the AI falsifies the computational traces fed into the deception detector, or leaves out any information in its response, this is detectable. Presumably the UFAI could output a very nice, very convincing plan of action with very tiny fine print hidden somewhere along the lines of \"PS: This is all a lie! You implement this and I'm going to turn you all into meaty paperclips. Haha!\" and it would get past the deception detector. But I would rather discussion not get sidetracked by such legalistic, nitpicky scenarios. Assume the humans involved are competent, conservative, responsible people who have setup institutional safeguards to prevent hasty action and make sure that output is sufficiently analyzed down to the very last digital bit by a competent, international team of highly rational people before being acted upon.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JApWmekjxuTBkoenz", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 1.9821757309968118e-06, "legacy": true, "legacyId": "27129", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-05T22:40:23.012Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver, Canada: Backyard BBQ", "slug": "meetup-vancouver-canada-backyard-bbq", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "eggman", "createdAt": "2011-10-09T00:24:15.183Z", "isAdmin": false, "displayName": "eggman"}, "userId": "irkySx7hExrK2XG53", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9TwFFJypbXbxYo2q8/meetup-vancouver-canada-backyard-bbq", "pageUrlRelative": "/posts/9TwFFJypbXbxYo2q8/meetup-vancouver-canada-backyard-bbq", "linkUrl": "https://www.lesswrong.com/posts/9TwFFJypbXbxYo2q8/meetup-vancouver-canada-backyard-bbq", "postedAtFormatted": "Friday, September 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%2C%20Canada%3A%20Backyard%20BBQ&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%2C%20Canada%3A%20Backyard%20BBQ%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9TwFFJypbXbxYo2q8%2Fmeetup-vancouver-canada-backyard-bbq%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%2C%20Canada%3A%20Backyard%20BBQ%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9TwFFJypbXbxYo2q8%2Fmeetup-vancouver-canada-backyard-bbq", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9TwFFJypbXbxYo2q8%2Fmeetup-vancouver-canada-backyard-bbq", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 195, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/144'>Vancouver, Canada: Backyard BBQ</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 September 2014 03:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">3461 12th Avenue West, Vancouver, Canada</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I've got access to a house with a big backyard, and I'm hosting a big meetup at my house on Sunday.</p>\n\n<p>My address is 3461 12th Ave W., in Vancouver.</p>\n\n<p>The meetup starts at 1500 hours, 3:00 p.m., The BBQ will go all evening, so feel free to show up anytime up until 10 p.m. If we're not outside in the backyard when you get there, knock on the front door, and we'll be outside.</p>\n\n<p>Bring your own food to grill on the BBQ (meat, fish, veggie burgers, whatever). B.Y.O.B. Feel free to bring snacks to share as well. I'll have a few general beverages sitting around.</p>\n\n<p>If you live nearby, and could spare an outside chair for seating, that would be appreciated, as I don't believe I own enough to accommodate everyone myself.</p>\n\n<p>Feel free to invite your friends. Feel free to bring board games, any topic to the table to discuss, opportunities to join your start-up, whatever...</p>\n\n<p>I'll see you there. Let me know if you have questions.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/144'>Vancouver, Canada: Backyard BBQ</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9TwFFJypbXbxYo2q8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.9824302519353463e-06, "legacy": true, "legacyId": "27130", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver__Canada__Backyard_BBQ\">Discussion article for the meetup : <a href=\"/meetups/144\">Vancouver, Canada: Backyard BBQ</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 September 2014 03:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">3461 12th Avenue West, Vancouver, Canada</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I've got access to a house with a big backyard, and I'm hosting a big meetup at my house on Sunday.</p>\n\n<p>My address is 3461 12th Ave W., in Vancouver.</p>\n\n<p>The meetup starts at 1500 hours, 3:00 p.m., The BBQ will go all evening, so feel free to show up anytime up until 10 p.m. If we're not outside in the backyard when you get there, knock on the front door, and we'll be outside.</p>\n\n<p>Bring your own food to grill on the BBQ (meat, fish, veggie burgers, whatever). B.Y.O.B. Feel free to bring snacks to share as well. I'll have a few general beverages sitting around.</p>\n\n<p>If you live nearby, and could spare an outside chair for seating, that would be appreciated, as I don't believe I own enough to accommodate everyone myself.</p>\n\n<p>Feel free to invite your friends. Feel free to bring board games, any topic to the table to discuss, opportunities to join your start-up, whatever...</p>\n\n<p>I'll see you there. Let me know if you have questions.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver__Canada__Backyard_BBQ1\">Discussion article for the meetup : <a href=\"/meetups/144\">Vancouver, Canada: Backyard BBQ</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver, Canada: Backyard BBQ", "anchor": "Discussion_article_for_the_meetup___Vancouver__Canada__Backyard_BBQ", "level": 1}, {"title": "Discussion article for the meetup : Vancouver, Canada: Backyard BBQ", "anchor": "Discussion_article_for_the_meetup___Vancouver__Canada__Backyard_BBQ1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-06T04:37:20.839Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington, D.C.: Fun & Games Meetup", "slug": "meetup-washington-d-c-fun-and-games-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinZ", "createdAt": "2009-07-08T20:34:05.168Z", "isAdmin": false, "displayName": "RobinZ"}, "userId": "eTMojvi4f2z3pDfsc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QZqXsz6kRsJWcpSdn/meetup-washington-d-c-fun-and-games-meetup-0", "pageUrlRelative": "/posts/QZqXsz6kRsJWcpSdn/meetup-washington-d-c-fun-and-games-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/QZqXsz6kRsJWcpSdn/meetup-washington-d-c-fun-and-games-meetup-0", "postedAtFormatted": "Saturday, September 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%2C%20D.C.%3A%20Fun%20%26%20Games%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%2C%20D.C.%3A%20Fun%20%26%20Games%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQZqXsz6kRsJWcpSdn%2Fmeetup-washington-d-c-fun-and-games-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%2C%20D.C.%3A%20Fun%20%26%20Games%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQZqXsz6kRsJWcpSdn%2Fmeetup-washington-d-c-fun-and-games-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQZqXsz6kRsJWcpSdn%2Fmeetup-washington-d-c-fun-and-games-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/145'>Washington, D.C.: Fun &amp; Games Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 September 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting in the Kogod Courtyard of the National Portrait Gallery (8th and F Sts or 8th and G Sts NW, go straight past the information desk from either entrance) to hang out, play games, and engage in fun conversation.\nA note to those who plan ahead: we are tentatively switching to a monthly schedule for Fun &amp; Games meetups, rather than every-three-meetings. Among the reasons for this is that the Mini Talks meetups seem to be fun enough and popular enough to deserve a regular slot as well. After this meetup, our tentative plan for the rest of the month is:</p>\n\n<ul>\n<li>Sept. 14 - Parkour (Backup: CFAR's Getting Things Done systems)</li>\n<li>Sept. 21 - Mini Talks</li>\n<li>Sept. 28 - Book Swap</li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/145'>Washington, D.C.: Fun &amp; Games Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QZqXsz6kRsJWcpSdn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.983059707803671e-06, "legacy": true, "legacyId": "27131", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington__D_C___Fun___Games_Meetup\">Discussion article for the meetup : <a href=\"/meetups/145\">Washington, D.C.: Fun &amp; Games Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 September 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting in the Kogod Courtyard of the National Portrait Gallery (8th and F Sts or 8th and G Sts NW, go straight past the information desk from either entrance) to hang out, play games, and engage in fun conversation.\nA note to those who plan ahead: we are tentatively switching to a monthly schedule for Fun &amp; Games meetups, rather than every-three-meetings. Among the reasons for this is that the Mini Talks meetups seem to be fun enough and popular enough to deserve a regular slot as well. After this meetup, our tentative plan for the rest of the month is:</p>\n\n<ul>\n<li>Sept. 14 - Parkour (Backup: CFAR's Getting Things Done systems)</li>\n<li>Sept. 21 - Mini Talks</li>\n<li>Sept. 28 - Book Swap</li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington__D_C___Fun___Games_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/145\">Washington, D.C.: Fun &amp; Games Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington, D.C.: Fun & Games Meetup", "anchor": "Discussion_article_for_the_meetup___Washington__D_C___Fun___Games_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington, D.C.: Fun & Games Meetup", "anchor": "Discussion_article_for_the_meetup___Washington__D_C___Fun___Games_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-06T10:46:59.780Z", "modifiedAt": null, "url": null, "title": "Meetup : Perth, Australia: Sunday lunch", "slug": "meetup-perth-australia-sunday-lunch", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "4r4aKXsWgDHchNwHh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6Ge2AqBDRbvG2Zw2P/meetup-perth-australia-sunday-lunch", "pageUrlRelative": "/posts/6Ge2AqBDRbvG2Zw2P/meetup-perth-australia-sunday-lunch", "linkUrl": "https://www.lesswrong.com/posts/6Ge2AqBDRbvG2Zw2P/meetup-perth-australia-sunday-lunch", "postedAtFormatted": "Saturday, September 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Perth%2C%20Australia%3A%20Sunday%20lunch&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Perth%2C%20Australia%3A%20Sunday%20lunch%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Ge2AqBDRbvG2Zw2P%2Fmeetup-perth-australia-sunday-lunch%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Perth%2C%20Australia%3A%20Sunday%20lunch%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Ge2AqBDRbvG2Zw2P%2Fmeetup-perth-australia-sunday-lunch", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Ge2AqBDRbvG2Zw2P%2Fmeetup-perth-australia-sunday-lunch", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/146'>Perth, Australia: Sunday lunch</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 September 2014 12:00:00PM (+0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Annalakshmi, Barrack Square, Perth, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come have lunch with other Less Wrongians! I'll describe a bunch of the more noteworthy cognitive biases, and ask everyone else to think of (1) examples \u2013 real or fictional \u2013 and (2) ideas for how to notice ourselves making these errors.</p>\n\n<p>We'll be at Annalakshmi (<a href=\"http://www.annalakshmi.com.au/\" rel=\"nofollow\">http://www.annalakshmi.com.au/</a>), a vegetarian pay-what-you-want restaurant.</p>\n\n<p>How to find me: I'll be by the entrance, carrying a silver water bottle labeled \"CFAR\" in orange.</p>\n\n<p>You can RSVP here: <a href=\"http://www.meetup.com/Perth-Less-Wrong/events/201429192/\" rel=\"nofollow\">http://www.meetup.com/Perth-Less-Wrong/events/201429192/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/146'>Perth, Australia: Sunday lunch</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6Ge2AqBDRbvG2Zw2P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "27132", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Perth__Australia__Sunday_lunch\">Discussion article for the meetup : <a href=\"/meetups/146\">Perth, Australia: Sunday lunch</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 September 2014 12:00:00PM (+0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Annalakshmi, Barrack Square, Perth, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come have lunch with other Less Wrongians! I'll describe a bunch of the more noteworthy cognitive biases, and ask everyone else to think of (1) examples \u2013 real or fictional \u2013 and (2) ideas for how to notice ourselves making these errors.</p>\n\n<p>We'll be at Annalakshmi (<a href=\"http://www.annalakshmi.com.au/\" rel=\"nofollow\">http://www.annalakshmi.com.au/</a>), a vegetarian pay-what-you-want restaurant.</p>\n\n<p>How to find me: I'll be by the entrance, carrying a silver water bottle labeled \"CFAR\" in orange.</p>\n\n<p>You can RSVP here: <a href=\"http://www.meetup.com/Perth-Less-Wrong/events/201429192/\" rel=\"nofollow\">http://www.meetup.com/Perth-Less-Wrong/events/201429192/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Perth__Australia__Sunday_lunch1\">Discussion article for the meetup : <a href=\"/meetups/146\">Perth, Australia: Sunday lunch</a></h2>", "sections": [{"title": "Discussion article for the meetup : Perth, Australia: Sunday lunch", "anchor": "Discussion_article_for_the_meetup___Perth__Australia__Sunday_lunch", "level": 1}, {"title": "Discussion article for the meetup : Perth, Australia: Sunday lunch", "anchor": "Discussion_article_for_the_meetup___Perth__Australia__Sunday_lunch1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-06T20:39:06.366Z", "modifiedAt": null, "url": null, "title": "[link] Reality Show 'Utopia'", "slug": "link-reality-show-utopia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:08.752Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MathieuRoy", "createdAt": "2013-06-16T02:41:27.071Z", "isAdmin": false, "displayName": "Mati_Roy"}, "userId": "Tw9etd8rMnHLeSQ9q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZzPyNFkHC3NTAv9JA/link-reality-show-utopia", "pageUrlRelative": "/posts/ZzPyNFkHC3NTAv9JA/link-reality-show-utopia", "linkUrl": "https://www.lesswrong.com/posts/ZzPyNFkHC3NTAv9JA/link-reality-show-utopia", "postedAtFormatted": "Saturday, September 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Reality%20Show%20'Utopia'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Reality%20Show%20'Utopia'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZzPyNFkHC3NTAv9JA%2Flink-reality-show-utopia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Reality%20Show%20'Utopia'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZzPyNFkHC3NTAv9JA%2Flink-reality-show-utopia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZzPyNFkHC3NTAv9JA%2Flink-reality-show-utopia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 306, "htmlBody": "<p>The TV series 'Utopia' just started.</p>\n<p>\"T<span style=\"color: #252525; font-family: sans-serif; font-size: 14px; line-height: 22.3999996185303px;\">he series follows a cast of 15 men and women who are placed in isolation and filmed twenty-four hours a day for one year. The cast must create their own society and figure out how to survive. The series will be shown twice a week, but there will be online streaming 24/7 with 129 hidden and unhidden cameras all over the Utopia compound. The live streams will begin on August 29, the day when the 15 pioneers will enter Utopia.</span><span style=\"color: #252525; font-family: sans-serif; font-size: 11px; line-height: 11px;\">&nbsp;</span><span style=\"color: #252525; font-family: sans-serif; font-size: 14px; line-height: 22.3999996185303px;\">Over 5,000 people auditioned for the series.</span><span style=\"color: #252525; font-family: sans-serif; font-size: 11px; line-height: 11px;\">&nbsp;</span><span style=\"color: #252525; font-family: sans-serif; font-size: 14px; line-height: 22.3999996185303px;\">Every month three pioneers will be nominated and could be sent back to their everyday lives. The live streamers will decide which new pioneers get their chance to become Utopian.\" (source:&nbsp;</span><a href=\"/he series follows a cast of 15 men and women who are placed in isolation and filmed twenty-four hours a day for one year. The cast must create their own society and figure out how to survive. The series will be shown twice a week, but there will be online streaming 24/7 with 129 hidden and unhidden cameras all over the Utopia compound. The live streams will begin on August 29, the day when the 15 pioneers will enter Utopia.[3] Over 5,000 people auditioned for the series.[4] Every month three pioneers will be nominated and could be sent back to their everyday lives. The live streamers will decide which new pioneers get their chance to become Utopian.\">http://en.wikipedia.org/wiki/Utopia_(U.S._reality_TV_series)</a>)</p>\n<p>Since every month new 'pioneers' will be introduced, you can still audition for the series; here's how: <a href=\"http://www.utopiatvcasting.com/how-to-audition\">http://www.utopiatvcasting.com/how-to-audition</a>. I would love to see a well-trained rationalist teaching \"the world\" some applied rationality principles, and I think this TV show would be an excellent medium to reach the \"average person\". It would also be nice to see someone explaining what Utopia means to a transhumanist. Let us know if you apply.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZzPyNFkHC3NTAv9JA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -12, "extendedScore": null, "score": 1.984757396344197e-06, "legacy": true, "legacyId": "27133", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-07T06:04:04.826Z", "modifiedAt": null, "url": null, "title": "[Link] How to see into the future (Financial Times)", "slug": "link-how-to-see-into-the-future-financial-times", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:22.870Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fortyeridania", "createdAt": "2010-07-21T15:35:12.558Z", "isAdmin": false, "displayName": "fortyeridania"}, "userId": "roBPqtzsvG6dC3YFT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FNXaLvgtguZiCrqBZ/link-how-to-see-into-the-future-financial-times", "pageUrlRelative": "/posts/FNXaLvgtguZiCrqBZ/link-how-to-see-into-the-future-financial-times", "linkUrl": "https://www.lesswrong.com/posts/FNXaLvgtguZiCrqBZ/link-how-to-see-into-the-future-financial-times", "postedAtFormatted": "Sunday, September 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20How%20to%20see%20into%20the%20future%20(Financial%20Times)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20How%20to%20see%20into%20the%20future%20(Financial%20Times)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFNXaLvgtguZiCrqBZ%2Flink-how-to-see-into-the-future-financial-times%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20How%20to%20see%20into%20the%20future%20(Financial%20Times)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFNXaLvgtguZiCrqBZ%2Flink-how-to-see-into-the-future-financial-times", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFNXaLvgtguZiCrqBZ%2Flink-how-to-see-into-the-future-financial-times", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 173, "htmlBody": "<p><a href=\"http://www.ft.com/intl/cms/s/2/3950604a-33bc-11e4-ba62-00144feabdc0.html#axzz3CbZ0Vd94\">How to see into the future</a>, by <a href=\"http://en.wikipedia.org/wiki/Tim_Harford\">Tim Harford</a></p>\n<p>The article may be gated. (I have a subscription through my school.)</p>\n<p>It is mainly about two things: the differing approaches to forecasting taken by Irving Fisher, John Maynard Keynes, and <a href=\"http://en.wikipedia.org/wiki/Roger_Babson\">Roger Babson</a>; and <a href=\"http://en.wikipedia.org/wiki/Philip_E._Tetlock\">Philip Tetlock</a>'s <a href=\"http://www.goodjudgmentproject.com/index.html\">Good Judgment Project</a>.</p>\n<p>Key paragraph:</p>\n<blockquote>\n<p class=\"assankaannotatable\" style=\"margin: 0px; padding: 0px; font-family: Arial, Helvetica, sans-serif; font-size: 16px; line-height: 18px;\">So what is the secret of looking into the future? Initial results from the Good Judgment Project suggest the following approaches. First, some basic training in probabilistic reasoning helps to produce better forecasts. Second, teams of good forecasters produce better results than good forecasters working alone. Third, actively open-minded people prosper as forecasters.</p>\n<p class=\"assankaannotatable\" style=\"margin: 0px; padding: 0px; font-family: Arial, Helvetica, sans-serif; font-size: 16px; line-height: 18px;\">&nbsp;</p>\n<p class=\"assankaannotatable\" style=\"margin: 0px; padding: 0px; font-family: Arial, Helvetica, sans-serif; font-size: 16px; line-height: 18px;\">But the Good Judgment Project also hints at why so many experts are such terrible forecasters. It&rsquo;s not so much that they lack training, teamwork and open-mindedness &ndash; although some of these qualities are in shorter supply than others. It&rsquo;s that most forecasters aren&rsquo;t actually seriously and single-mindedly trying to see into the future. If they were, they&rsquo;d keep score and try to improve their predictions based on past errors. They don&rsquo;t.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FNXaLvgtguZiCrqBZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 1.985755883685431e-06, "legacy": true, "legacyId": "27134", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-07T08:19:46.172Z", "modifiedAt": null, "url": null, "title": "Meetup : LW Copenhagen - September: This Wavefunction Has Uncollapsed", "slug": "meetup-lw-copenhagen-september-this-wavefunction-has", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ruby", "createdAt": "2014-04-03T03:38:23.914Z", "isAdmin": true, "displayName": "Ruby"}, "userId": "qgdGA4ZEyW7zNdK84", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/chP2zHZW2sRWTTk2a/meetup-lw-copenhagen-september-this-wavefunction-has", "pageUrlRelative": "/posts/chP2zHZW2sRWTTk2a/meetup-lw-copenhagen-september-this-wavefunction-has", "linkUrl": "https://www.lesswrong.com/posts/chP2zHZW2sRWTTk2a/meetup-lw-copenhagen-september-this-wavefunction-has", "postedAtFormatted": "Sunday, September 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LW%20Copenhagen%20-%20September%3A%20This%20Wavefunction%20Has%20Uncollapsed&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LW%20Copenhagen%20-%20September%3A%20This%20Wavefunction%20Has%20Uncollapsed%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FchP2zHZW2sRWTTk2a%2Fmeetup-lw-copenhagen-september-this-wavefunction-has%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LW%20Copenhagen%20-%20September%3A%20This%20Wavefunction%20Has%20Uncollapsed%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FchP2zHZW2sRWTTk2a%2Fmeetup-lw-copenhagen-september-this-wavefunction-has", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FchP2zHZW2sRWTTk2a%2Fmeetup-lw-copenhagen-september-this-wavefunction-has", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/147'>LW Copenhagen - September: This Wavefunction Has Uncollapsed</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 September 2014 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Studenterhuset, K\u00f8bmagergade 52, 1150 K\u00f8benhavn, Denmark</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Less Wrong Copenhagen is back!</p>\n\n<p>Join us for this coming Saturday at Studenterhuset. The meetup will have two components:</p>\n\n<ul>\n<li><p>Rationality Dojo: a section intended to be a serious self-improvement session for those committed to the Art of Rationality and personal growth. At this meetup I'll talk about \"Sphexishness, Agentiness, and Noticing\" and follow with related exercises.</p></li>\n<li><p>Socialising: second component is getting to know and enjoy the company of fellow humans who care about doing better. We'll drink, talk, maybe play a board game.</p></li>\n</ul>\n\n<p>I'll be there from slightly before 15:00, call me on 22 47 83 73 if you're having trouble finding the group.</p>\n\n<p>From the soon-to-published published CFAR Glossary:</p>\n\n<p>Agency:\nAgency is the property of agents. An agent has explicit goals which they strive to accomplish by planning and executing appropriate actions. Non-agents unreflectively act out default behaviours, without considering whether these actions achieve their goals. Agency is the opposite of sphexishness.</p>\n\n<p>Sphexishness:\nCoined by Douglas Hofstader in reference to the sphex wasp, sphexishness is the execution of seemingly intelligent behaviour by following a rigid algorithm. Sphexish behaviours, are repeated automatically, on habit, without checking for their effectiveness at achieving desired goals. Opposite of agency.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/147'>LW Copenhagen - September: This Wavefunction Has Uncollapsed</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "chP2zHZW2sRWTTk2a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.985995821339313e-06, "legacy": true, "legacyId": "27135", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LW_Copenhagen___September__This_Wavefunction_Has_Uncollapsed\">Discussion article for the meetup : <a href=\"/meetups/147\">LW Copenhagen - September: This Wavefunction Has Uncollapsed</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 September 2014 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Studenterhuset, K\u00f8bmagergade 52, 1150 K\u00f8benhavn, Denmark</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Less Wrong Copenhagen is back!</p>\n\n<p>Join us for this coming Saturday at Studenterhuset. The meetup will have two components:</p>\n\n<ul>\n<li><p>Rationality Dojo: a section intended to be a serious self-improvement session for those committed to the Art of Rationality and personal growth. At this meetup I'll talk about \"Sphexishness, Agentiness, and Noticing\" and follow with related exercises.</p></li>\n<li><p>Socialising: second component is getting to know and enjoy the company of fellow humans who care about doing better. We'll drink, talk, maybe play a board game.</p></li>\n</ul>\n\n<p>I'll be there from slightly before 15:00, call me on 22 47 83 73 if you're having trouble finding the group.</p>\n\n<p>From the soon-to-published published CFAR Glossary:</p>\n\n<p>Agency:\nAgency is the property of agents. An agent has explicit goals which they strive to accomplish by planning and executing appropriate actions. Non-agents unreflectively act out default behaviours, without considering whether these actions achieve their goals. Agency is the opposite of sphexishness.</p>\n\n<p>Sphexishness:\nCoined by Douglas Hofstader in reference to the sphex wasp, sphexishness is the execution of seemingly intelligent behaviour by following a rigid algorithm. Sphexish behaviours, are repeated automatically, on habit, without checking for their effectiveness at achieving desired goals. Opposite of agency.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LW_Copenhagen___September__This_Wavefunction_Has_Uncollapsed1\">Discussion article for the meetup : <a href=\"/meetups/147\">LW Copenhagen - September: This Wavefunction Has Uncollapsed</a></h2>", "sections": [{"title": "Discussion article for the meetup : LW Copenhagen - September: This Wavefunction Has Uncollapsed", "anchor": "Discussion_article_for_the_meetup___LW_Copenhagen___September__This_Wavefunction_Has_Uncollapsed", "level": 1}, {"title": "Discussion article for the meetup : LW Copenhagen - September: This Wavefunction Has Uncollapsed", "anchor": "Discussion_article_for_the_meetup___LW_Copenhagen___September__This_Wavefunction_Has_Uncollapsed1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-07T11:26:52.626Z", "modifiedAt": null, "url": null, "title": "Meetup : London social meetup", "slug": "meetup-london-social-meetup-3", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pvQYNPqvuwp7xcyJi/meetup-london-social-meetup-3", "pageUrlRelative": "/posts/pvQYNPqvuwp7xcyJi/meetup-london-social-meetup-3", "linkUrl": "https://www.lesswrong.com/posts/pvQYNPqvuwp7xcyJi/meetup-london-social-meetup-3", "postedAtFormatted": "Sunday, September 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpvQYNPqvuwp7xcyJi%2Fmeetup-london-social-meetup-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpvQYNPqvuwp7xcyJi%2Fmeetup-london-social-meetup-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpvQYNPqvuwp7xcyJi%2Fmeetup-london-social-meetup-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 165, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/148'>London social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 September 2014 02:00:00AM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next LW London meetup will be on September 14th. Join us from 2pm to talk about the sorts of things that your other friends will look funny at you for talking about.</p>\n\n<p>I'll try to remember to bring an identifying sign, but if you have difficulty finding us, my number is 07792009646.</p>\n\n<p><strong>About London LessWrong</strong>:</p>\n\n<p>We run this meetup approximately every other week; these days we tend to get in the region of 5-15 people in attendance. By default, meetups are just unstructured social discussion about whatever strikes our fancy: books we're reading, recent posts on LW/related blogs, logic puzzles, toilet usage statistics....</p>\n\n<p>Sometimes we play <a href=\"http://en.wikipedia.org/wiki/The_Resistance_%28game%29\" rel=\"nofollow\">The Resistance</a> or other games. We usually finish around 7pm, give or take an hour, but people arrive and leave whenever suits them.</p>\n\n<p>Related discussion happens on both our <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">google group</a> and our <a href=\"https://www.facebook.com/groups/380103898766356/\" rel=\"nofollow\">facebook group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/148'>London social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pvQYNPqvuwp7xcyJi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "27136", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/148\">London social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 September 2014 02:00:00AM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next LW London meetup will be on September 14th. Join us from 2pm to talk about the sorts of things that your other friends will look funny at you for talking about.</p>\n\n<p>I'll try to remember to bring an identifying sign, but if you have difficulty finding us, my number is 07792009646.</p>\n\n<p><strong>About London LessWrong</strong>:</p>\n\n<p>We run this meetup approximately every other week; these days we tend to get in the region of 5-15 people in attendance. By default, meetups are just unstructured social discussion about whatever strikes our fancy: books we're reading, recent posts on LW/related blogs, logic puzzles, toilet usage statistics....</p>\n\n<p>Sometimes we play <a href=\"http://en.wikipedia.org/wiki/The_Resistance_%28game%29\" rel=\"nofollow\">The Resistance</a> or other games. We usually finish around 7pm, give or take an hour, but people arrive and leave whenever suits them.</p>\n\n<p>Related discussion happens on both our <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">google group</a> and our <a href=\"https://www.facebook.com/groups/380103898766356/\" rel=\"nofollow\">facebook group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/148\">London social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : London social meetup", "anchor": "Discussion_article_for_the_meetup___London_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : London social meetup", "anchor": "Discussion_article_for_the_meetup___London_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-08T00:00:36.118Z", "modifiedAt": null, "url": null, "title": "Is it a good idea to use Soylent once/twice a day?", "slug": "is-it-a-good-idea-to-use-soylent-once-twice-a-day", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:37.713Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamzerner", "createdAt": "2013-08-12T18:18:47.957Z", "isAdmin": false, "displayName": "adamzerner"}, "userId": "6jLdWqegNefgaabhr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vMCDWc62b6TY7Chak/is-it-a-good-idea-to-use-soylent-once-twice-a-day", "pageUrlRelative": "/posts/vMCDWc62b6TY7Chak/is-it-a-good-idea-to-use-soylent-once-twice-a-day", "linkUrl": "https://www.lesswrong.com/posts/vMCDWc62b6TY7Chak/is-it-a-good-idea-to-use-soylent-once-twice-a-day", "postedAtFormatted": "Monday, September 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20it%20a%20good%20idea%20to%20use%20Soylent%20once%2Ftwice%20a%20day%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20it%20a%20good%20idea%20to%20use%20Soylent%20once%2Ftwice%20a%20day%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvMCDWc62b6TY7Chak%2Fis-it-a-good-idea-to-use-soylent-once-twice-a-day%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20it%20a%20good%20idea%20to%20use%20Soylent%20once%2Ftwice%20a%20day%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvMCDWc62b6TY7Chak%2Fis-it-a-good-idea-to-use-soylent-once-twice-a-day", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvMCDWc62b6TY7Chak%2Fis-it-a-good-idea-to-use-soylent-once-twice-a-day", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1116, "htmlBody": "<p>Edit: Sorry, I didn't realize there has been so much discussion on this already! I thought I had just stumbled across some obscure product haha. Anyway, I've been reading through discussions here, on Hacker News, Tim Ferris' blog etc. There's been a lot of talk about whether or not this is truly a \"replacement for eating\" (or whatever the term is). I think the more interesting question is whether it's a good idea to:</p>\n<ul>\n<li>Have Soylent once or twice a day.</li>\n<li>Have whole food snacks throughout the day like cheerios, trail mix, fruits etc.</li>\n<li>Have a nice big dinner each day.</li>\n<li>Maybe focus more on whole foods on weekends when you have more time.</li>\n</ul>\n<div>I don't think this particular question has been discussed enough. And it seems that it hasn't been discussed here for a while. So perhaps we should start a new conversation!</div>\n<div><br /></div>\n<p>My initial impression is that it is a good idea to use it as a once or twice a day thing.</p>\n<ol>\n<li>It saves time. To me, this is huge.</li>\n<li>It saves money.</li>\n<li>It makes it easier to eat fewer calories, fat, sugar and salt. I'm surprised this health benefit isn't talked about more. Most american diets have way too much of these four things. I think Soylent helps in this area for two main reasons: a) It makes you full faster. b) It doesn't have as much calories/fat/sugar/salt as you a typical diet probably does.</li>\n<li>It is probably way more nutritious than the meal it's replacing. Typical diets probably are lacking in certain nutrients, and Soylent will probably help to \"fill in these gaps\". Again, another huge benefit that I'm surprised doesn't get talked about as much (although this doesn't apply for people who use multivitamins).</li>\n<li>There really don't seem to be anything unhealthy about having it once or twice a day. I'm not very confident about this claim because it hasn't been studied enough, but so far I haven't heard of anyone experiencing health problems from Soylent* as a once or twice a day thing, and meal replacement stuff like Soylent seems to have been around for a while and hasn't caused anyone any problems.<br />*The two main problems (digestive issues and headaches) seem to be <a href=\"http://blog.soylent.me/post/92183684822/soylent-1-0-update\">sufficiently addressed</a> by 1. Adopting it slowly into your diet (over the course of 5 days or so) and 2. Making sure you get enough salt.</li>\n</ol>\n<div><strong>So what do you guys think about using Soylent once or twice a day as I describe?</strong></div>\n<div><br /></div>\n<div>Edit: I just found an informative article -&nbsp;http://www.meghantelpner.com/blog/the-soylent-killer.</div>\n<p>&nbsp;</p>\n<hr />\n<p><em>Original Post: you could ignore this if you're familiar with Soylent</em></p>\n<p><em>I've just came across a meal replacement drink called Soylent -&nbsp;http://www.soylent.me/.</em></p>\n<p><em>It is...</em></p>\n<ul>\n<li><em>Cheap (~$3/meal)</em></li>\n<li><em>Fast (just add water to the powder, no cooking or cleaning)</em></li>\n<li><em>I could work while I drink it (I'm a slow eater and don't like to work while I'm eating, so this would save me a lot of time)</em></li>\n<li><em>Nutritious</em></li>\n<li><em>Doesn't go bad for about 2 years</em></li>\n</ul>\n<div><em>And thus it sounds amazing! I try to be healthy and cook my own meals but I find that I spend way too much time cooking, eating and cleaning (at least 2-3 hours a day). And despite my efforts to be healthy, I still think my diet has too much fat, sugar, salt, processed ingredients etc. (like most Americans).</em></div>\n<div><em><br /></em></div>\n<div><em>The two downsides that come to mind are:</em></div>\n<div><ol>\n<li><em>It may be lacking certain essential nutrients.</em></li>\n<li><em>It may have detrimental effects on my health in the long-term.</em></li>\n</ol></div>\n<div><em><br /></em></div>\n<div><em>1) It seems that they tried to create this drink to have all the macro and micronutrients you'd need. I'm not a nutritionist so I can't say, but I can't be sure that it is indeed comprehensive. However, I was just talking to a nutritionist and she mentioned that people in comas and stuff do survive on a formula similar to this that's injected into their stomach (or another part of their digestive system). So that does make me relatively confident that it's at least possible for humans to survive off a formula like this.</em></div>\n<div><em><br /></em></div>\n<div><em>2) This is the biggest concern. This <a href=\"http://biology.stackexchange.com/questions/20995/why-do-we-absorb-vitamins-better-from-whole-foods-than-from-pills/21103#21103\">Stack Exchange answer</a>&nbsp;seems reliable and said the long-term effects haven't been studied. And I recall hearing this before. I don't really see any reason to think there'd be detrimental long-term effects.</em></div>\n<div>\n<ul>\n<li><em>Tube feeding has been around for a while and doesn't seem to have any long-term effects (from what I know).</em></li>\n<li><em>There doesn't seem to be anything odd about the ingredients that would be detrimental. When you eat food and digest it, it becomes something pretty similar to what's in the formula. In fact, it seems that the ingredients in the formula are simpler than the components of whole foods, and thus there should be less stress on your digestive system.</em></li>\n<li><em>Meal replacement drinks have been around for a while and don't seem to have any long-term effects (from what I know).</em></li>\n</ul>\n</div>\n<p><em>However I really don't have enough information to make any reasonably strong conclusions. Those bullet points above are more vague suspicions than evidence backed knowledge.</em></p>\n<hr />\n<p><em>So do any of you guys know anything about Soylent or meal replacement drinks/bars/etc.? Are they healthy? Are there things I haven't accounted for?</em></p>\n<p><em>Also, I'm sorta surprised this isn't more popular. Most people I know hate cooking and cleaning and shopping and spending so much time and money on food. I think most people would be more than happy to have Soylent (or something similar) for a meal or two each day, and then have a big dinner or something. It would save a ton of money and time, and would reduce the amount of fat and sugar in the persons diet. And because you're spending less money on food and consuming less fat and sugar, you could justify eating out or ordering in a splurge meal more often! What do you guys think? Why isn't this more popular? Are people really that afraid of the health effects?</em></p>\n<p><em>(I'm not being hypocritical. I know that *I've* been asking about the health effects and seem to be worried about them, but I wouldn't think most people would approach this the same way I am. If I lived on an island isolated from other people, was told about Soylent and asked what I think it's popularity is, I would guess it to be very high. I would think people would see that it's pretty nutritious, aren't really any known risks or reason to think there would be risks, and be eager to save time and money by using Soylent).</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vMCDWc62b6TY7Chak", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 8, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "27138", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-08T12:31:36.779Z", "modifiedAt": "2021-08-25T02:22:59.981Z", "url": null, "title": "Open thread, September 8-14, 2014", "slug": "open-thread-september-8-14-2014", "viewCount": null, "lastCommentedAt": "2014-09-21T22:27:30.424Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z4qvgB4ZEWzhCqYRx/open-thread-september-8-14-2014", "pageUrlRelative": "/posts/z4qvgB4ZEWzhCqYRx/open-thread-september-8-14-2014", "linkUrl": "https://www.lesswrong.com/posts/z4qvgB4ZEWzhCqYRx/open-thread-september-8-14-2014", "postedAtFormatted": "Monday, September 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20September%208-14%2C%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20September%208-14%2C%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz4qvgB4ZEWzhCqYRx%2Fopen-thread-september-8-14-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20September%208-14%2C%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz4qvgB4ZEWzhCqYRx%2Fopen-thread-september-8-14-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz4qvgB4ZEWzhCqYRx%2Fopen-thread-september-8-14-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<div id=\"entry_t3_kwc\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px; font-weight: bold;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px; font-weight: bold;\"><br /></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">3.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should be posted in Discussion, and not Main.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">4.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should start on Monday, and end on Sunday.</span></p>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z4qvgB4ZEWzhCqYRx", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 1.988991803423197e-06, "legacy": true, "legacyId": "27139", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 296, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2014-09-08T12:31:36.779Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-08T14:22:36.130Z", "modifiedAt": null, "url": null, "title": "Omission vs commission and conservation of expected moral evidence", "slug": "omission-vs-commission-and-conservation-of-expected-moral", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:33.767Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GJ482ZyyHWf2e5D5a/omission-vs-commission-and-conservation-of-expected-moral", "pageUrlRelative": "/posts/GJ482ZyyHWf2e5D5a/omission-vs-commission-and-conservation-of-expected-moral", "linkUrl": "https://www.lesswrong.com/posts/GJ482ZyyHWf2e5D5a/omission-vs-commission-and-conservation-of-expected-moral", "postedAtFormatted": "Monday, September 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Omission%20vs%20commission%20and%20conservation%20of%20expected%20moral%20evidence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOmission%20vs%20commission%20and%20conservation%20of%20expected%20moral%20evidence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGJ482ZyyHWf2e5D5a%2Fomission-vs-commission-and-conservation-of-expected-moral%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Omission%20vs%20commission%20and%20conservation%20of%20expected%20moral%20evidence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGJ482ZyyHWf2e5D5a%2Fomission-vs-commission-and-conservation-of-expected-moral", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGJ482ZyyHWf2e5D5a%2Fomission-vs-commission-and-conservation-of-expected-moral", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<p>Consequentialism traditionally doesn't distinguish between acts of commission or acts of omission. Not flipping the lever to the left is equivalent with flipping it to the right.</p>\n<p>But there seems one clear case where the distinction is important. Consider a <a href=\"/lw/f32/value_loading/\">moral learning</a> agent. It must act in accordance with human morality and desires, which it is currently unclear about.</p>\n<p>For example, it may consider whether to forcibly wirehead everyone. If it does so, they everyone will agree, for the rest of their existence, that the wireheading was the right thing to do. Therefore across the whole future span of human preferences, humans agree that wireheading was correct, apart from a very brief period of objection in the immediate future. Given that human preferences are known to be inconsistent, this seems to imply that forcible wireheading is the right thing to do (if you happen to personally approve of forcible wireheading, replace that example with some other forcible rewriting of human preferences).</p>\n<p>What went wrong there? Well, this doesn't respect \"<a href=\"/lw/jy2/value_learning_ultrasophisticated_cake_or_death/\">conversation of moral evidence</a>\": the AI got the moral values it wanted, but only though the actions it took. This is very close to the omission/commission distinction. We'd want the AI to not take actions (commission) that determines the (expectation of the) moral evidence it gets. Instead, we'd want the moral evidence to accrue \"naturally\", without interference and manipulation from the AI (omission).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GJ482ZyyHWf2e5D5a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 4, "extendedScore": null, "score": 1.9891886268134527e-06, "legacy": true, "legacyId": "27140", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Z8WRsxYjmrxNGyPPk", "f387EfBAbpSTDerz2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-09T04:44:32.417Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels - September meetup", "slug": "meetup-brussels-september-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roxolan", "createdAt": "2011-10-23T19:06:17.298Z", "isAdmin": false, "displayName": "Roxolan"}, "userId": "jXG7tMhkQMNpCCXPN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DHyYdutssjt986eMF/meetup-brussels-september-meetup", "pageUrlRelative": "/posts/DHyYdutssjt986eMF/meetup-brussels-september-meetup", "linkUrl": "https://www.lesswrong.com/posts/DHyYdutssjt986eMF/meetup-brussels-september-meetup", "postedAtFormatted": "Tuesday, September 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20-%20September%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20-%20September%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDHyYdutssjt986eMF%2Fmeetup-brussels-september-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20-%20September%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDHyYdutssjt986eMF%2Fmeetup-brussels-september-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDHyYdutssjt986eMF%2Fmeetup-brussels-september-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/149'>Brussels - September meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 September 2014 01:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at 1 pm at \"La Fleur en papier dor\u00e9, close to the Brussels Central station. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform\" rel=\"nofollow\">this one minute form</a> to share your contact information.</p>\n\n<p>The Brussels meetup group communicates through a <a href=\"https://groups.google.com/forum/#!forum/lesswrong-brussels\">Google Group</a>.</p>\n\n<p>Meetup announcements are also mirrored on <a href=\"http://www.meetup.com/LWBrussels/\" rel=\"nofollow\">meetup.com</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/149'>Brussels - September meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DHyYdutssjt986eMF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.9907183244589324e-06, "legacy": true, "legacyId": "27143", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels___September_meetup\">Discussion article for the meetup : <a href=\"/meetups/149\">Brussels - September meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 September 2014 01:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at 1 pm at \"La Fleur en papier dor\u00e9, close to the Brussels Central station. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform\" rel=\"nofollow\">this one minute form</a> to share your contact information.</p>\n\n<p>The Brussels meetup group communicates through a <a href=\"https://groups.google.com/forum/#!forum/lesswrong-brussels\">Google Group</a>.</p>\n\n<p>Meetup announcements are also mirrored on <a href=\"http://www.meetup.com/LWBrussels/\" rel=\"nofollow\">meetup.com</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels___September_meetup1\">Discussion article for the meetup : <a href=\"/meetups/149\">Brussels - September meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels - September meetup", "anchor": "Discussion_article_for_the_meetup___Brussels___September_meetup", "level": 1}, {"title": "Discussion article for the meetup : Brussels - September meetup", "anchor": "Discussion_article_for_the_meetup___Brussels___September_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-09T16:56:38.149Z", "modifiedAt": null, "url": null, "title": "Talking to yourself: A useful thinking tool that seems understudied and underdiscussed", "slug": "talking-to-yourself-a-useful-thinking-tool-that-seems", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:03.553Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DSfH27utWBPRCYjod/talking-to-yourself-a-useful-thinking-tool-that-seems", "pageUrlRelative": "/posts/DSfH27utWBPRCYjod/talking-to-yourself-a-useful-thinking-tool-that-seems", "linkUrl": "https://www.lesswrong.com/posts/DSfH27utWBPRCYjod/talking-to-yourself-a-useful-thinking-tool-that-seems", "postedAtFormatted": "Tuesday, September 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Talking%20to%20yourself%3A%20A%20useful%20thinking%20tool%20that%20seems%20understudied%20and%20underdiscussed&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATalking%20to%20yourself%3A%20A%20useful%20thinking%20tool%20that%20seems%20understudied%20and%20underdiscussed%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDSfH27utWBPRCYjod%2Ftalking-to-yourself-a-useful-thinking-tool-that-seems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Talking%20to%20yourself%3A%20A%20useful%20thinking%20tool%20that%20seems%20understudied%20and%20underdiscussed%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDSfH27utWBPRCYjod%2Ftalking-to-yourself-a-useful-thinking-tool-that-seems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDSfH27utWBPRCYjod%2Ftalking-to-yourself-a-useful-thinking-tool-that-seems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 772, "htmlBody": "<p>I have returned from a particularly fruitful Google search, with unexpected results.</p>\n<p>My question was simple. I was pretty sure that talking to myself aloud makes me temporarily better at solving problems that need a lot of working memory. It is a <a href=\"http://www.sac.sa.edu.au/Library/Library/Topics/thinking_skills/thinking.html\">thinking tool</a> that I find to be of great value, and that I imagine would be of interest to anyone who'd like to optimize their problem solving. I just wanted to collect some evidence on that, make sure I'm not deluding myself, and possibly learn how to enhance the effect.</p>\n<p>This might be just lousy Googling on my part, but the evidence is surprisingly unclear and disorganized. There are at least three seperate Wiki pages for it. They don't link to each other. Instead they present the distinct models of three seperate fields: <a href=\"http://en.wikipedia.org/wiki/Autocommunication\">autocommunication</a> in communication studies, semiotics and other cultural studies, <a href=\"http://en.wikipedia.org/wiki/Intrapersonal_communication\">intrapersonal communication</a> (\"self-talk\" redirects here) in anthropology and (older) psychology and <a href=\"http://en.wikipedia.org/wiki/Private_speech\">private speech</a> in developmental psychology. The first is useless for my purpose, the second mentions \"may increase concentration and retention\" with no source, the third confirms my suspicion that this behavior boosts memory, motivation and creativity, but it only talks about children.</p>\n<p>Google Scholar yields lots of sports-related results for \"self-talk\" because it can apparently improve the performance of athletes and if there's something that obviously needs the optimization power of psychology departments, it is competitive sports. For \"intrapersonal communication\" it has papers indicating it helps in language acquisition and in dealing with social anxiety. Both are dwarfed by the results for \"private speech\", which again focus on children. There's very little on \"autocommunication\" and what is there has nothing to do with the functioning of individual minds.</p>\n<p>So there's a bunch of converging pieces of evidence supporting the usefulness of this behavior, but they're from several seperate fields that don't seem to have noticed each other very much. How often do you find that?</p>\n<p>Let me quickly list a few ways that I find it plausible to imagine talking to yourself could enhance rational thought.</p>\n<ul>\n<li>It taps the <a href=\"http://en.wikipedia.org/wiki/Baddeley%27s_model_of_working_memory#Phonological_loop\">phonological loop</a>, a distinct part of working memory that might otherwise sit idle in non-auditory tasks. More memory is always better, right?</li>\n<li>Auditory information is retained more easily, so making thoughts auditory helps remember them later.</li>\n<li>It lets you commit to thoughts, and build upon them, in a way that is more powerful (and slower) than unspoken thought while less powerful (but quicker) than action. (I don't have a good online source for this one, but <em><a href=\"http://www.amazon.de/Inside-Jokes-Using-Humor-Reverse-Engineer/dp/0262518694\">Inside Jokes</a></em> should convince you, and has lots of new cognitive science to boot.)</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Dual_process_theory#Dual-process_accounts_of_reasoning\">System 1</a> does seem to understand language, especially if it does not use complex grammar - so this might be a useful way for results of <a href=\"http://en.wikipedia.org/wiki/Dual_process_theory#Dual-process_accounts_of_reasoning\">System 2</a> reasoning to be propagated. Compare <a href=\"http://en.wikipedia.org/wiki/Affirmations_%28New_Age%29\">affirmations</a>. Anecdotally, whenever I'm starting a complex task, I find stating my intent out loud makes a <em>huge</em> difference in how well the various submodules of my mind cooperate.</li>\n<li>It lets separate parts of your mind communicate in a fairly natural fashion, slows each of them down to the speed of your tongue and makes them not interrupt each other so much. (This is being used as a <a href=\"http://en.wikipedia.org/wiki/Hal_and_Sidra_Stone\">psychotherapy method</a>.) In effect, your mouth becomes a kind of <a href=\"http://en.wikipedia.org/wiki/Talking_stick\">talking stick</a> in their discussion.</li>\n</ul>\n<p>All told, if you're talking to yourself you should be more able to solve complex problems than somebody of your IQ who doesn't, although somebody of your IQ with a pen and a piece of paper should still outthink both of you.</p>\n<p>Given all that, I'm surprised this doesn't appear to have been discussed on LessWrong. <a href=\"/lw/101/honesty_beyond_internal_truth/\">Honesty: Beyond Internal Truth</a> comes close but goes past it. Again, this might be me failing to use a search engine, but I think this is worth more of our attention that it has gotten so far.</p>\n<p>I'm now almost certain talking to myself is useful, and I already find hindsight bias trying to convince me I've always been so sure. But I wasn't - I was suspicious because talking to yourself is an early warning sign of schizophrenia, and is frequent in dementia. But in those cases, it might simply be an autoregulatory response to failing working memory, not a pathogenetic element. After all, its memory enhancing effect is what the developmental psychologists say the kids use it for. I do expect social stigma, which is why I avoid talking to myself when around uninvolved or unsympathetic people, but my solving of complex problems tends to happen away from those anyway so that hasn't been an issue really.</p>\n<p>So, what do you think? Useful?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1, "BzghQYM9GnkMHxZKb": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DSfH27utWBPRCYjod", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 44, "extendedScore": null, "score": 1.99201922323859e-06, "legacy": true, "legacyId": "27148", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pZSpbxPrftSndTdSf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-10T00:53:05.455Z", "modifiedAt": null, "url": null, "title": "Meetup : Passive Investing and Financial Independence", "slug": "meetup-passive-investing-and-financial-independence", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anders_H", "createdAt": "2013-07-28T20:46:58.747Z", "isAdmin": false, "displayName": "Anders_H"}, "userId": "jfdosp4Hn7tFNbf9k", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q2TW6PDXLWw6ci9oH/meetup-passive-investing-and-financial-independence", "pageUrlRelative": "/posts/Q2TW6PDXLWw6ci9oH/meetup-passive-investing-and-financial-independence", "linkUrl": "https://www.lesswrong.com/posts/Q2TW6PDXLWw6ci9oH/meetup-passive-investing-and-financial-independence", "postedAtFormatted": "Wednesday, September 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Passive%20Investing%20and%20Financial%20Independence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Passive%20Investing%20and%20Financial%20Independence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ2TW6PDXLWw6ci9oH%2Fmeetup-passive-investing-and-financial-independence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Passive%20Investing%20and%20Financial%20Independence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ2TW6PDXLWw6ci9oH%2Fmeetup-passive-investing-and-financial-independence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ2TW6PDXLWw6ci9oH%2Fmeetup-passive-investing-and-financial-independence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14a'>Passive Investing and Financial Independence</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 September 2014 03:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames Street , Cambridge MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Chase will be dispensing his financial wisdom.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 3:30pm, and have an alternating location:</p>\n\n<ul>\n<li><p>1st Sunday meetups are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p></li>\n<li><p>3rd Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p></li>\n</ul>\n\n<p>(We also have last Wednesday meetups at Citadel at 7pm.)</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation. <br />\n\u2014Phase 2: The headline event. This starts promptly at 4pm, and lasts 30-60 minutes. <br />\n\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups. <br />\n\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14a'>Passive Investing and Financial Independence</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q2TW6PDXLWw6ci9oH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.9928666694764985e-06, "legacy": true, "legacyId": "27150", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Passive_Investing_and_Financial_Independence\">Discussion article for the meetup : <a href=\"/meetups/14a\">Passive Investing and Financial Independence</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 September 2014 03:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames Street , Cambridge MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Chase will be dispensing his financial wisdom.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 3:30pm, and have an alternating location:</p>\n\n<ul>\n<li><p>1st Sunday meetups are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p></li>\n<li><p>3rd Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p></li>\n</ul>\n\n<p>(We also have last Wednesday meetups at Citadel at 7pm.)</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation. <br>\n\u2014Phase 2: The headline event. This starts promptly at 4pm, and lasts 30-60 minutes. <br>\n\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups. <br>\n\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Passive_Investing_and_Financial_Independence1\">Discussion article for the meetup : <a href=\"/meetups/14a\">Passive Investing and Financial Independence</a></h2>", "sections": [{"title": "Discussion article for the meetup : Passive Investing and Financial Independence", "anchor": "Discussion_article_for_the_meetup___Passive_Investing_and_Financial_Independence", "level": 1}, {"title": "Discussion article for the meetup : Passive Investing and Financial Independence", "anchor": "Discussion_article_for_the_meetup___Passive_Investing_and_Financial_Independence1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-10T00:55:23.241Z", "modifiedAt": null, "url": null, "title": "Meetup : Social Skills", "slug": "meetup-social-skills", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:37.222Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anders_H", "createdAt": "2013-07-28T20:46:58.747Z", "isAdmin": false, "displayName": "Anders_H"}, "userId": "jfdosp4Hn7tFNbf9k", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p26zpACRzZPfdbKFF/meetup-social-skills", "pageUrlRelative": "/posts/p26zpACRzZPfdbKFF/meetup-social-skills", "linkUrl": "https://www.lesswrong.com/posts/p26zpACRzZPfdbKFF/meetup-social-skills", "postedAtFormatted": "Wednesday, September 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Social%20Skills&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Social%20Skills%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp26zpACRzZPfdbKFF%2Fmeetup-social-skills%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Social%20Skills%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp26zpACRzZPfdbKFF%2Fmeetup-social-skills", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp26zpACRzZPfdbKFF%2Fmeetup-social-skills", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 109, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14b'>Social Skills</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 September 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">98 Elm Street, Somerville MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come hear the amazing and charismatic Sam Rosen speak! Cambridge/Boston-area Less Wrong Wednesday meetups are once a month on the last Wednesday at 7pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square). All other meetups are on Sundays.\nOur default schedule is as follows:\n\u2014Phase 1: Arrival, greetings, unstructured conversation. \u2014Phase 2: The headline event. This starts promptly at 7:30pm, and lasts 30-60 minutes. \u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14b'>Social Skills</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p26zpACRzZPfdbKFF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.992870755539776e-06, "legacy": true, "legacyId": "27151", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Social_Skills\">Discussion article for the meetup : <a href=\"/meetups/14b\">Social Skills</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 September 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">98 Elm Street, Somerville MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come hear the amazing and charismatic Sam Rosen speak! Cambridge/Boston-area Less Wrong Wednesday meetups are once a month on the last Wednesday at 7pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square). All other meetups are on Sundays.\nOur default schedule is as follows:\n\u2014Phase 1: Arrival, greetings, unstructured conversation. \u2014Phase 2: The headline event. This starts promptly at 7:30pm, and lasts 30-60 minutes. \u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Social_Skills1\">Discussion article for the meetup : <a href=\"/meetups/14b\">Social Skills</a></h2>", "sections": [{"title": "Discussion article for the meetup : Social Skills", "anchor": "Discussion_article_for_the_meetup___Social_Skills", "level": 1}, {"title": "Discussion article for the meetup : Social Skills", "anchor": "Discussion_article_for_the_meetup___Social_Skills1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-10T02:26:47.861Z", "modifiedAt": null, "url": null, "title": "Meetup : Portland Teachable Skills Discussion", "slug": "meetup-portland-teachable-skills-discussion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VAuroch", "createdAt": "2013-11-07T11:01:09.015Z", "isAdmin": false, "displayName": "VAuroch"}, "userId": "idJgwEhhiRhTzHpst", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nSGWB7dGogfdWKmzg/meetup-portland-teachable-skills-discussion", "pageUrlRelative": "/posts/nSGWB7dGogfdWKmzg/meetup-portland-teachable-skills-discussion", "linkUrl": "https://www.lesswrong.com/posts/nSGWB7dGogfdWKmzg/meetup-portland-teachable-skills-discussion", "postedAtFormatted": "Wednesday, September 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Portland%20Teachable%20Skills%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Portland%20Teachable%20Skills%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnSGWB7dGogfdWKmzg%2Fmeetup-portland-teachable-skills-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Portland%20Teachable%20Skills%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnSGWB7dGogfdWKmzg%2Fmeetup-portland-teachable-skills-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnSGWB7dGogfdWKmzg%2Fmeetup-portland-teachable-skills-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14c'>Portland Teachable Skills Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 September 2014 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\"> 2945 NE 64th Ave, Portland, OR</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>First meetup in Portland after a long break.\nThis month's meetup will be a discussion on Teachable Skills; what are you learning, what can you teach, what do you need practice in, etc. It may break off into groups practicing specific skills, or we may arrange to do so at future meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14c'>Portland Teachable Skills Discussion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nSGWB7dGogfdWKmzg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.993033415879839e-06, "legacy": true, "legacyId": "27152", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Portland_Teachable_Skills_Discussion\">Discussion article for the meetup : <a href=\"/meetups/14c\">Portland Teachable Skills Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 September 2014 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\"> 2945 NE 64th Ave, Portland, OR</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>First meetup in Portland after a long break.\nThis month's meetup will be a discussion on Teachable Skills; what are you learning, what can you teach, what do you need practice in, etc. It may break off into groups practicing specific skills, or we may arrange to do so at future meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Portland_Teachable_Skills_Discussion1\">Discussion article for the meetup : <a href=\"/meetups/14c\">Portland Teachable Skills Discussion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Portland Teachable Skills Discussion", "anchor": "Discussion_article_for_the_meetup___Portland_Teachable_Skills_Discussion", "level": 1}, {"title": "Discussion article for the meetup : Portland Teachable Skills Discussion", "anchor": "Discussion_article_for_the_meetup___Portland_Teachable_Skills_Discussion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-10T10:26:49.779Z", "modifiedAt": null, "url": null, "title": "[Link] 3 Short Walking Breaks Can Reverse Harm From 3 Hours of Sitting", "slug": "link-3-short-walking-breaks-can-reverse-harm-from-3-hours-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:09.531Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8YBthcJ88aHSKoeAh/link-3-short-walking-breaks-can-reverse-harm-from-3-hours-of", "pageUrlRelative": "/posts/8YBthcJ88aHSKoeAh/link-3-short-walking-breaks-can-reverse-harm-from-3-hours-of", "linkUrl": "https://www.lesswrong.com/posts/8YBthcJ88aHSKoeAh/link-3-short-walking-breaks-can-reverse-harm-from-3-hours-of", "postedAtFormatted": "Wednesday, September 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%203%20Short%20Walking%20Breaks%20Can%20Reverse%20Harm%20From%203%20Hours%20of%20Sitting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%203%20Short%20Walking%20Breaks%20Can%20Reverse%20Harm%20From%203%20Hours%20of%20Sitting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8YBthcJ88aHSKoeAh%2Flink-3-short-walking-breaks-can-reverse-harm-from-3-hours-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%203%20Short%20Walking%20Breaks%20Can%20Reverse%20Harm%20From%203%20Hours%20of%20Sitting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8YBthcJ88aHSKoeAh%2Flink-3-short-walking-breaks-can-reverse-harm-from-3-hours-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8YBthcJ88aHSKoeAh%2Flink-3-short-walking-breaks-can-reverse-harm-from-3-hours-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 205, "htmlBody": "<p>I found the below link which is in the spirit of <a href=\"/lw/jrt/lifestyle_interventions_to_increase_longevity/\">Lifestyle interventions to increase longevity</a>:</p>\n<p><a href=\"http://journals.lww.com/acsm-msse/pages/articleviewer.aspx?year=9000&amp;issue=00000&amp;article=97944&amp;type=abstract\">3 Short Walking Breaks Can Reverse Harm From 3 Hours of Sitting</a>\"</p>\n<p>The /.-summary:</p>\n<p>&gt;&nbsp;<span style=\"font-family: inherit; font-style: inherit; color: #363636; font-size: 12.800000190734863px; line-height: 15.600000381469727px;\">Medical researchers have been steadily building evidence that prolonged sitting is awful for your health. One major problem is that blood can pool in the legs of a seated person, causing arteries to start losing their ability to control the rate of blood flow. A new experimental study (</span><a style=\"font-family: inherit; font-style: inherit; font-size: 12.800000190734863px; line-height: 15.600000381469727px; outline: none; vertical-align: baseline; padding: 0px; margin: 0px; color: #002f2f; cursor: pointer;\" href=\"http://journals.lww.com/acsm-msse/pages/articleviewer.aspx?year=9000&amp;issue=00000&amp;article=97944&amp;type=abstract\">abstract</a><span style=\"font-family: inherit; font-style: inherit; color: #363636; font-size: 12.800000190734863px; line-height: 15.600000381469727px;\">) has discovered it's quite easy to negate these detrimental health effects:</span><span style=\"font-family: inherit; font-style: inherit; color: #363636; font-size: 12.800000190734863px; line-height: 15.600000381469727px;\">&nbsp;</span><a style=\"font-family: inherit; font-style: inherit; font-size: 12.800000190734863px; line-height: 15.600000381469727px; outline: none; vertical-align: baseline; padding: 0px; margin: 0px; color: #002f2f; cursor: pointer;\" href=\"http://news.indiana.edu/releases/iu/2014/09/slow-walking-sitting-study.shtml\">all you need to do is take a leisurely, 5-minute walk for every hour you sit</a><span style=\"font-family: inherit; font-style: inherit; color: #363636; font-size: 12.800000190734863px; line-height: 15.600000381469727px;\">. \"The researchers were able to demonstrate that during a three-hour period, the flow-mediated dilation, or the expansion of the arteries as a result of increased blood flow, of the main artery in the legs was impaired by as much as 50 percent after just one hour. The study participants who walked for five minutes for each hour of sitting saw their arterial function stay the same &mdash; it did not drop throughout the three-hour period. Thosar says it is likely that the increase in muscle activity and blood flow accounts for this.\"</span></p>\n<div><span style=\"font-family: inherit; font-style: inherit; color: #363636; font-size: 12.800000190734863px; line-height: 15.600000381469727px;\">One way to incorporate this into ones habits is to use <a href=\"http://www.workrave.org/\">WorkRave</a>.</span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8YBthcJ88aHSKoeAh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 25, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "27159", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PhXENjdXiHhsWGfQo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-11T00:27:06.000Z", "modifiedAt": null, "url": null, "title": "Society Is Fixed, Biology Is Mutable", "slug": "society-is-fixed-biology-is-mutable", "viewCount": null, "lastCommentedAt": "2018-11-12T15:15:58.671Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2HafkDSNdtMzptzcN/society-is-fixed-biology-is-mutable", "pageUrlRelative": "/posts/2HafkDSNdtMzptzcN/society-is-fixed-biology-is-mutable", "linkUrl": "https://www.lesswrong.com/posts/2HafkDSNdtMzptzcN/society-is-fixed-biology-is-mutable", "postedAtFormatted": "Thursday, September 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Society%20Is%20Fixed%2C%20Biology%20Is%20Mutable&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASociety%20Is%20Fixed%2C%20Biology%20Is%20Mutable%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2HafkDSNdtMzptzcN%2Fsociety-is-fixed-biology-is-mutable%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Society%20Is%20Fixed%2C%20Biology%20Is%20Mutable%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2HafkDSNdtMzptzcN%2Fsociety-is-fixed-biology-is-mutable", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2HafkDSNdtMzptzcN%2Fsociety-is-fixed-biology-is-mutable", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 849, "htmlBody": "<p>Today during an otherwise terrible lecture on ADHD I realized something important we get sort of backwards.</p>\n<p>There&#8217;s this stereotype that the Left believes that human characteristics are socially determined, and therefore mutable. And social problems are easy to fix, through things like education and social services and <A HREF=\"http://slatestarcodex.com/2013/12/22/public-awareness-campaigns/\">public awareness campaigns</A> and &#8220;calling people out&#8221;, and so we have a responsiblity to fix them, thus radically improving society and making life better for everyone.</p>\n<p>But the Right (by now I guess the far right) believes human characteristics are <i>biologically</i> determined, and biology is fixed. Therefore we shouldn&#8217;t bother trying to improve things, and any attempt is just utopianism or &#8220;immanentizing the eschaton&#8221; or a shady justification for tyranny and busybodyness.</p>\n<p>And I think I reject this whole premise.</p>\n<p>See, my terrible lecture on ADHD suggested several reasons for the increasing prevalence of the disease. Of these I remember two: the spiritual desert of modern adolescence, and insufficient iron in the diet. And I remember thinking &#8220;Man, I hope it&#8217;s the iron one, because that seems a <i>lot</i> easier to fix.&#8221;</p>\n<p>Society is <i>really hard to change</i>. We figured drug use was &#8220;just&#8221; a social problem, and it&#8217;s <i>obvious</i> how to solve social problems, so we gave kids nice little lessons in school about how you should Just Say No. There were advertisements in sports and video games about how Winners Don&#8217;t Do Drugs. And just in case that didn&#8217;t work, the cherry on the social engineering sundae was putting all the drug users in jail, where they would have a lot of time to think about what they&#8217;d done and be so moved by the prospect of further punishment that they would come clean.</p>\n<p>And that is why, even to this day, nobody uses drugs.</p>\n<p>On the other hand, biology is gratifyingly easy to change. Sometimes it&#8217;s just giving people more iron supplements. But the best example is lead. Banning lead was probably kind of controversial at the time, but in the end some refineries probably had to change their refining process and some gas stations had to put up &#8220;UNLEADED&#8221; signs and then we were done. And crime <A HREF=\"http://www3.amherst.edu/~jwreyes/papers/LeadCrimeNBERWP13097.pdf\">dropped</A> like fifty percent in a couple of decades &#8211; including many forms of drug abuse.</p>\n<p>Saying &#8220;Tendency toward drug abuse is primarily determined by fixed brain structure&#8221; sounds callous, like you&#8217;re abandoning drug abusers to die. But maybe it means you can fight the problem head-on instead of forcing kids to attend more and more <A HREF=\"http://www.pbs.org/wgbh/pages/frontline/shows/dope/dare/effectiveness.html\">useless</A> classes where cartoon animals sing about how happy they are not using cocaine.</p>\n<p>What about obesity? We put a <i>lot</i> of social effort into fighting obesity: labeling foods, banning soda machines from school, banning large sodas from New York, programs in schools to promote healthy eating, doctors chewing people out when they gain weight, the profusion of gyms and Weight Watchers programs, and let&#8217;s not forget a level of stigma against obese people so strong that I am <i>constantly</i> having to deal with their weight-related suicide attempts. As a result, everyone&#8230;keeps gaining weight at exactly the same rate they have been for the past couple decades. Wouldn&#8217;t it be nice if increasing obesity was driven at least in part by <A HREF=\"http://www.sciencedirect.com/science/article/pii/S0092867414008216\">changes in the intestinal microbiota</A> that we could reverse through careful antibiotic use? Or by trans-fats?</p>\n<p>What about poor school performance? From the social angle, we try No Child Left Behind, Common Core Curriculum, stronger teachers&#8217; unions, weaker teachers&#8217; unions, more pay for teachers, less pay for teachers, more prayer in school, banning prayer in school, condemning racism, condemning racism even more, et cetera. But the poorest fifth or so of kids <A HREF=\"http://www.ncbi.nlm.nih.gov/pubmed/10706232\">show spectacular cognitive gains from multivitamin supplementation</A>, and doctors continue <A HREF=\"http://time.com/3162265/school-should-start-later-so-teens-can-sleep-urge-doctors/\">to tell everyone schools should start later so children can get enough sleep</A> and continue to be totally ignored despite <A HREF=\"http://trib.com/news/local/education/study-later-school-starts-improve-student-grades/article_b0dfe211-2a59-50ef-98dd-19948dd19a3d.html\">strong evidence in favor</A>.</p>\n<p>Even the most politically radioactive biological explanation &#8211; genetics &#8211; doesn&#8217;t seem that scary to me. The more things turn out to be genetic, the more I support universal funding for implantable contraception that allow people to choose when they do or don&#8217;t want children &#8211; thus breaking the cycle where people too impulsive or confused to use contraception have more children and increase frequency of those undesirable genes. I think I&#8217;d have a heck of a lot easier a time changing gene frequency in the population than you would changing people&#8217;s locus of control or self-efficacy or whatever, even if I wasn&#8217;t allowed to do anything immoral (except by very silly religious standards of &#8220;immoral&#8221;).</p>\n<p>I&#8217;m not saying that all problems are purely biological and none are social. But I do worry there&#8217;s a consensus that biological things are unfixable but social things are easy &#8211; or that social solutions are morally unambiguous but biological solutions necessarily monstrous &#8211; and so for any given biological/social breakdown of a problem, we figure we might as well put all our resources into attacking the more tractable social side and dismiss the biological side. I think there&#8217;s a sense in which that&#8217;s backwards, and in which it&#8217;s possible to marry scientific rigor with human compassion for the evils of the world.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jaf5zfcGgCB2REXGw": 6, "gHCNhqxuJq2bZ2akb": 2, "AHK82ypfxF45rqh9D": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2HafkDSNdtMzptzcN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 32, "extendedScore": null, "score": 9.1e-05, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "rNuPrZvabXe2MaZv8", "canonicalCollectionSlug": "codex", "canonicalBookId": "kcCvSNNZd8pfQvf9E", "canonicalNextPostSlug": "a-philosopher-walks-into-a-coffee-shop", "canonicalPrevPostSlug": "albion-s-seed-genotyped", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-11T04:22:47.774Z", "modifiedAt": null, "url": null, "title": "Overcoming Decision Anxiety", "slug": "overcoming-decision-anxiety", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:35.949Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TimMartin", "createdAt": "2014-02-19T03:44:52.589Z", "isAdmin": false, "displayName": "TimMartin"}, "userId": "aq43ZHqYMv7Qu7WBR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eWnepYhtQnr2ub7sg/overcoming-decision-anxiety", "pageUrlRelative": "/posts/eWnepYhtQnr2ub7sg/overcoming-decision-anxiety", "linkUrl": "https://www.lesswrong.com/posts/eWnepYhtQnr2ub7sg/overcoming-decision-anxiety", "postedAtFormatted": "Thursday, September 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Overcoming%20Decision%20Anxiety&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOvercoming%20Decision%20Anxiety%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeWnepYhtQnr2ub7sg%2Fovercoming-decision-anxiety%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Overcoming%20Decision%20Anxiety%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeWnepYhtQnr2ub7sg%2Fovercoming-decision-anxiety", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeWnepYhtQnr2ub7sg%2Fovercoming-decision-anxiety", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 871, "htmlBody": "<h2 style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\"><span style=\"color: #222222; font-family: arial; font-weight: normal; line-height: normal; font-size: small;\">I get pretty anxious about open-ended decisions. I often spend an unacceptable amount of time agonizing over things like what design options to get on a custom suit, or what kind of job I want to pursue, or what apartment I want to live in. Some of these decisions are obviously important ones, with implications for my future happiness. However, in general my sense of anxiety is poorly calibrated with the importance of the decision. This makes life harder than it has to be, and lowers my productivity.</span></h2>\n<p style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\" align=\"JUSTIFY\"><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">I moved apartments recently, and I decided that this would be a good time to address my anxiety about open-ended decisions. My hope is to present some ideas that will be helpful for others with similar anxieties, or to stimulate helpful discussion.</span></p>\n<p style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\" align=\"JUSTIFY\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><br /></span></p>\n<h3 style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\"><strong style=\"line-height: 0.15in;\">Solutions</strong></h3>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\" align=\"JUSTIFY\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong>Exposure therapy</strong></span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">One promising way of dealing with decision anxiety is to practice making decisions without worrying about them quite so much. Match your clothes together in a new way, even if you're not 100% sure that you like the resulting outfit. Buy a new set of headphones, even if it isn't the &ldquo;perfect choice.&rdquo; Aim for good enough. Remind yourself that life will be okay if your clothes are slightly mismatched for one day.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">This is basically <a href=\"http://en.wikipedia.org/wiki/Exposure_therapy\" target=\"_self\">exposure therapy</a> &ndash; exposing oneself to a slightly aversive stimulus while remaining calm about it. Doing something you're (mildly) afraid to do can have a tremendously positive impact when you try it and realize that it wasn't all that bad. Of course, you can always start small and build up to bolder activities as your anxieties diminish.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">For the past several months, I had been practicing this with small decisions. With the move approaching in July, I needed some more tricks for dealing with a bigger, more important decision.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong>Reasoning with yourself</strong></span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">It helps to think up reasons why your anxieties aren't justified. As in actual, honest-to-goodness reasons that you think are true. Check out this conversation between my <a href=\"http://en.wikipedia.org/wiki/Dual_process_theory#Dual-process_accounts_of_reasoning\" target=\"_self\">System 1 and System 2</a> that happened just after my roommates and I made a decision on an apartment:</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong>System 1:</strong></span><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"> Oh man, this neighborhood [the old neighborhood] is such a great place to go for walks. It's so scenic and calm. I'm going to miss that. The new neighborhood isn't as pretty.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong>System 2:</strong></span><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"> Well that's true, but how many walks did we actually take in five years living in the old neighborhood? If I recall correctly, we didn't even take two per year.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong>System 1:</strong></span><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"> Well, yeah... but...</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong>System 2:</strong></span><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"> So maybe &ldquo;how good the neighborhood is for taking walks&rdquo; isn't actually that important to us. At least not to the extent that you're feeling. There were things that we really liked about our old living situation, but taking walks really wasn't one of them.<br /><strong>System 1: </strong>Yeah, you may be right...</span></p>\n<p style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\" align=\"JUSTIFY\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">Of course, this &ldquo;conversation&rdquo; took place after the decision had already been made. But making a difficult decision often entails second-guessing oneself, and this too can be a source of great anxiety. As in the above, I find that poking holes in my own anxieties really makes me feel better. I do this by being a good skeptic and turning on my critical thinking skills &ndash; only instead of, say, debunking an article on pseudoscience, I'm debunking my own worries about how bad things are going to be. This helps me remain calm.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong>Re-calibration</strong></span></p>\n<p style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\" align=\"JUSTIFY\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong></strong></span><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">The last piece of this process is something that should help when making future decisions. I reasoned that if my System 1 feels anxiety about things that aren't very important &ndash; if it is, as I said, poorly calibrated &ndash; then I perhaps I can re-calibrate it.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">Before moving apartments, I decided to make predictions about what aspects of the new living situation would affect my happiness. &ldquo;How good the neighborhood is for walks&rdquo; may not be important to me, but surely there are some factors that are important. So I wrote down things that I thought would be good and bad about the new place. I also rated them on how good or bad I thought they would be.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">In several months, I plan to go back over that list and compare my predicted feelings to my actual feelings. What was I right about? This will hopefully give my System 1 a strong impetus to re-calibrate, and only feel anxious about aspects of a decision that are strongly correlated with my future happiness.<br /></span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\" /></p>\n<h3><strong>Future Benefits</strong></h3>\n<p style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\" align=\"JUSTIFY\"><strong></strong><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">I think we each carry in our heads a model of what is possible for us to achieve, and anxiety about the choices we make limits how bold we can be in trying new things. As a result, I think that my attempts to feel less anxiety about decisions will be very valuable to me, and allow me to do things that I couldn't do before. At the same time, I expect that making decisions of all kinds will be a quicker and more pleasant process, which is a great outcome in and of itself.</span></p>\n<ul style=\"margin: 15px 0px 0px; padding: 0px; border: 0px; outline: 0px; font-size: 13.333333969116211px; vertical-align: baseline; list-style-position: initial; color: #333333; font-family: 'Helvetica Neue', Arial, sans-serif; line-height: 17px; background: transparent;\">\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eWnepYhtQnr2ub7sg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 21, "extendedScore": null, "score": 1.9958058486966067e-06, "legacy": true, "legacyId": "27075", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\" id=\"I_get_pretty_anxious_about_open_ended_decisions__I_often_spend_an_unacceptable_amount_of_time_agonizing_over_things_like_what_design_options_to_get_on_a_custom_suit__or_what_kind_of_job_I_want_to_pursue__or_what_apartment_I_want_to_live_in__Some_of_these_decisions_are_obviously_important_ones__with_implications_for_my_future_happiness__However__in_general_my_sense_of_anxiety_is_poorly_calibrated_with_the_importance_of_the_decision__This_makes_life_harder_than_it_has_to_be__and_lowers_my_productivity_\"><span style=\"color: #222222; font-family: arial; font-weight: normal; line-height: normal; font-size: small;\">I get pretty anxious about open-ended decisions. I often spend an unacceptable amount of time agonizing over things like what design options to get on a custom suit, or what kind of job I want to pursue, or what apartment I want to live in. Some of these decisions are obviously important ones, with implications for my future happiness. However, in general my sense of anxiety is poorly calibrated with the importance of the decision. This makes life harder than it has to be, and lowers my productivity.</span></h2>\n<p style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\" align=\"JUSTIFY\"><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">I moved apartments recently, and I decided that this would be a good time to address my anxiety about open-ended decisions. My hope is to present some ideas that will be helpful for others with similar anxieties, or to stimulate helpful discussion.</span></p>\n<p style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\" align=\"JUSTIFY\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><br></span></p>\n<h3 style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\" id=\"Solutions\"><strong style=\"line-height: 0.15in;\">Solutions</strong></h3>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\" align=\"JUSTIFY\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong>Exposure therapy</strong></span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">One promising way of dealing with decision anxiety is to practice making decisions without worrying about them quite so much. Match your clothes together in a new way, even if you're not 100% sure that you like the resulting outfit. Buy a new set of headphones, even if it isn't the \u201cperfect choice.\u201d Aim for good enough. Remind yourself that life will be okay if your clothes are slightly mismatched for one day.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">This is basically <a href=\"http://en.wikipedia.org/wiki/Exposure_therapy\" target=\"_self\">exposure therapy</a> \u2013 exposing oneself to a slightly aversive stimulus while remaining calm about it. Doing something you're (mildly) afraid to do can have a tremendously positive impact when you try it and realize that it wasn't all that bad. Of course, you can always start small and build up to bolder activities as your anxieties diminish.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">For the past several months, I had been practicing this with small decisions. With the move approaching in July, I needed some more tricks for dealing with a bigger, more important decision.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong>Reasoning with yourself</strong></span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">It helps to think up reasons why your anxieties aren't justified. As in actual, honest-to-goodness reasons that you think are true. Check out this conversation between my <a href=\"http://en.wikipedia.org/wiki/Dual_process_theory#Dual-process_accounts_of_reasoning\" target=\"_self\">System 1 and System 2</a> that happened just after my roommates and I made a decision on an apartment:</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong>System 1:</strong></span><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"> Oh man, this neighborhood [the old neighborhood] is such a great place to go for walks. It's so scenic and calm. I'm going to miss that. The new neighborhood isn't as pretty.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong>System 2:</strong></span><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"> Well that's true, but how many walks did we actually take in five years living in the old neighborhood? If I recall correctly, we didn't even take two per year.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong>System 1:</strong></span><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"> Well, yeah... but...</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong>System 2:</strong></span><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"> So maybe \u201chow good the neighborhood is for taking walks\u201d isn't actually that important to us. At least not to the extent that you're feeling. There were things that we really liked about our old living situation, but taking walks really wasn't one of them.<br><strong>System 1: </strong>Yeah, you may be right...</span></p>\n<p style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\" align=\"JUSTIFY\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">Of course, this \u201cconversation\u201d took place after the decision had already been made. But making a difficult decision often entails second-guessing oneself, and this too can be a source of great anxiety. As in the above, I find that poking holes in my own anxieties really makes me feel better. I do this by being a good skeptic and turning on my critical thinking skills \u2013 only instead of, say, debunking an article on pseudoscience, I'm debunking my own worries about how bad things are going to be. This helps me remain calm.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong>Re-calibration</strong></span></p>\n<p style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\" align=\"JUSTIFY\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\"><strong></strong></span><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">The last piece of this process is something that should help when making future decisions. I reasoned that if my System 1 feels anxiety about things that aren't very important \u2013 if it is, as I said, poorly calibrated \u2013 then I perhaps I can re-calibrate it.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">Before moving apartments, I decided to make predictions about what aspects of the new living situation would affect my happiness. \u201cHow good the neighborhood is for walks\u201d may not be important to me, but surely there are some factors that are important. So I wrote down things that I thought would be good and bad about the new place. I also rated them on how good or bad I thought they would be.</span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">In several months, I plan to go back over that list and compare my predicted feelings to my actual feelings. What was I right about? This will hopefully give my System 1 a strong impetus to re-calibrate, and only feel anxious about aspects of a decision that are strongly correlated with my future happiness.<br></span><br style=\"color: #222222; font-family: arial; line-height: normal; text-align: start;\"></p>\n<h3 id=\"Future_Benefits\"><strong>Future Benefits</strong></h3>\n<p style=\"margin-bottom: 0in; font-style: normal; line-height: 0.15in;\" align=\"JUSTIFY\"><strong></strong><span style=\"font-weight: normal; color: #222222; font-family: arial; line-height: normal; text-align: start;\">I think we each carry in our heads a model of what is possible for us to achieve, and anxiety about the choices we make limits how bold we can be in trying new things. As a result, I think that my attempts to feel less anxiety about decisions will be very valuable to me, and allow me to do things that I couldn't do before. At the same time, I expect that making decisions of all kinds will be a quicker and more pleasant process, which is a great outcome in and of itself.</span></p>\n<ul style=\"margin: 15px 0px 0px; padding: 0px; border: 0px; outline: 0px; font-size: 13.333333969116211px; vertical-align: baseline; list-style-position: initial; color: #333333; font-family: 'Helvetica Neue', Arial, sans-serif; line-height: 17px; background: transparent;\">\n</ul>", "sections": [{"title": "I get pretty anxious about open-ended decisions. I often spend an unacceptable amount of time agonizing over things like what design options to get on a custom suit, or what kind of job I want to pursue, or what apartment I want to live in. Some of these decisions are obviously important ones, with implications for my future happiness. However, in general my sense of anxiety is poorly calibrated with the importance of the decision. This makes life harder than it has to be, and lowers my productivity.", "anchor": "I_get_pretty_anxious_about_open_ended_decisions__I_often_spend_an_unacceptable_amount_of_time_agonizing_over_things_like_what_design_options_to_get_on_a_custom_suit__or_what_kind_of_job_I_want_to_pursue__or_what_apartment_I_want_to_live_in__Some_of_these_decisions_are_obviously_important_ones__with_implications_for_my_future_happiness__However__in_general_my_sense_of_anxiety_is_poorly_calibrated_with_the_importance_of_the_decision__This_makes_life_harder_than_it_has_to_be__and_lowers_my_productivity_", "level": 1}, {"title": "Solutions", "anchor": "Solutions", "level": 2}, {"title": "Future Benefits", "anchor": "Future_Benefits", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-11T17:35:10.315Z", "modifiedAt": null, "url": null, "title": "Meetup : LessWrong Tel Aviv: Rational Career Development", "slug": "meetup-lesswrong-tel-aviv-rational-career-development", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Squark", "createdAt": "2013-02-04T19:29:04.489Z", "isAdmin": false, "displayName": "Squark"}, "userId": "k4QpNYXcigqfG85t6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pPcAFtFT63TvHhdhF/meetup-lesswrong-tel-aviv-rational-career-development", "pageUrlRelative": "/posts/pPcAFtFT63TvHhdhF/meetup-lesswrong-tel-aviv-rational-career-development", "linkUrl": "https://www.lesswrong.com/posts/pPcAFtFT63TvHhdhF/meetup-lesswrong-tel-aviv-rational-career-development", "postedAtFormatted": "Thursday, September 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LessWrong%20Tel%20Aviv%3A%20Rational%20Career%20Development&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LessWrong%20Tel%20Aviv%3A%20Rational%20Career%20Development%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpPcAFtFT63TvHhdhF%2Fmeetup-lesswrong-tel-aviv-rational-career-development%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LessWrong%20Tel%20Aviv%3A%20Rational%20Career%20Development%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpPcAFtFT63TvHhdhF%2Fmeetup-lesswrong-tel-aviv-rational-career-development", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpPcAFtFT63TvHhdhF%2Fmeetup-lesswrong-tel-aviv-rational-career-development", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 401, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14d'>LessWrong Tel Aviv: Rational Career Development</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 September 2014 08:33:43PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Yigal Alon Street 98, Tel Aviv-Yafo, Israel</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><em>The organization of this meetup was presided by Aur Saraf. The text below is by him.</em></p>\n\n<p>WHEN: 18 September 2014 07:00:00PM (+0300)\nWHERE: 98 Yigal Alon St., 29th floor, Tel Aviv</p>\n\n<p>We're going to have a meetup on Thursday, September 18th at Google Israel's offices, Electra Tower, 98 Yigal Alon st., Tel Aviv.</p>\n\n<p>This time will be a talk about Rational Career Development by Joshua Fox. It won't be a very long talk, so you're invited to prepare your own short perspective on the subject.</p>\n\n<p>We'll start the meetup at 19:00, and we'll go on as much as we like to or until Anatoly kicks us out.</p>\n\n<p>Please come on time, as we will begin the talk close to when we start. But, if you can only come later, thats totally ok!</p>\n\n<p>We'll meet at the 29th floor of the building (Note: Not where Google Campus is). If you arrive and cant find your way around, call Anatoly who is graciously hosting us at 054-245-1060.</p>\n\n<p>The Israeli Less Wrong Meetup happens once every two weeks. This is to allow people who cant make it to a meetup to not have to wait a whole month to meet again, and because we'd like to have both subject based and social meetups without having to wait for a month in between.</p>\n\n<p>If you have any question feel free to email me at sonoflilit@gmail.com or call me at 054-9400-840.</p>\n\n<p>Aur</p>\n\n<p>PS I misread Guy's proposed time for ending voting. He wrote tomorrow, I read today. Since it's not a vote where people are likely to wait for the last moment, since I already talked to both speakers and organized all the details, and since Joshua keeps Shabbas and would probably want some weekend time to prepare the talk, I'm closing the vote early and announcing the talk now. Sorry.</p>\n\n<p>PS2 I think everything went quite well this time. Please take a moment to go to Trello and post more talk proposals (or own a talk with many votes and no speaker) so that it would go as well next month!</p>\n\n<p>PS3 Thanks again to Anatoly for hosting us almost every two weeks us at such a great location, and thanks to Gal who almost never takes vacations from organizing meetings.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14d'>LessWrong Tel Aviv: Rational Career Development</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pPcAFtFT63TvHhdhF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.997220304133655e-06, "legacy": true, "legacyId": "27169", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Tel_Aviv__Rational_Career_Development\">Discussion article for the meetup : <a href=\"/meetups/14d\">LessWrong Tel Aviv: Rational Career Development</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 September 2014 08:33:43PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Yigal Alon Street 98, Tel Aviv-Yafo, Israel</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><em>The organization of this meetup was presided by Aur Saraf. The text below is by him.</em></p>\n\n<p>WHEN: 18 September 2014 07:00:00PM (+0300)\nWHERE: 98 Yigal Alon St., 29th floor, Tel Aviv</p>\n\n<p>We're going to have a meetup on Thursday, September 18th at Google Israel's offices, Electra Tower, 98 Yigal Alon st., Tel Aviv.</p>\n\n<p>This time will be a talk about Rational Career Development by Joshua Fox. It won't be a very long talk, so you're invited to prepare your own short perspective on the subject.</p>\n\n<p>We'll start the meetup at 19:00, and we'll go on as much as we like to or until Anatoly kicks us out.</p>\n\n<p>Please come on time, as we will begin the talk close to when we start. But, if you can only come later, thats totally ok!</p>\n\n<p>We'll meet at the 29th floor of the building (Note: Not where Google Campus is). If you arrive and cant find your way around, call Anatoly who is graciously hosting us at 054-245-1060.</p>\n\n<p>The Israeli Less Wrong Meetup happens once every two weeks. This is to allow people who cant make it to a meetup to not have to wait a whole month to meet again, and because we'd like to have both subject based and social meetups without having to wait for a month in between.</p>\n\n<p>If you have any question feel free to email me at sonoflilit@gmail.com or call me at 054-9400-840.</p>\n\n<p>Aur</p>\n\n<p>PS I misread Guy's proposed time for ending voting. He wrote tomorrow, I read today. Since it's not a vote where people are likely to wait for the last moment, since I already talked to both speakers and organized all the details, and since Joshua keeps Shabbas and would probably want some weekend time to prepare the talk, I'm closing the vote early and announcing the talk now. Sorry.</p>\n\n<p>PS2 I think everything went quite well this time. Please take a moment to go to Trello and post more talk proposals (or own a talk with many votes and no speaker) so that it would go as well next month!</p>\n\n<p>PS3 Thanks again to Anatoly for hosting us almost every two weeks us at such a great location, and thanks to Gal who almost never takes vacations from organizing meetings.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Tel_Aviv__Rational_Career_Development1\">Discussion article for the meetup : <a href=\"/meetups/14d\">LessWrong Tel Aviv: Rational Career Development</a></h2>", "sections": [{"title": "Discussion article for the meetup : LessWrong Tel Aviv: Rational Career Development", "anchor": "Discussion_article_for_the_meetup___LessWrong_Tel_Aviv__Rational_Career_Development", "level": 1}, {"title": "Discussion article for the meetup : LessWrong Tel Aviv: Rational Career Development", "anchor": "Discussion_article_for_the_meetup___LessWrong_Tel_Aviv__Rational_Career_Development1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-11T19:20:14.514Z", "modifiedAt": null, "url": null, "title": "Do Virtual Humans deserve human rights?", "slug": "do-virtual-humans-deserve-human-rights", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:17.352Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cameroncowan", "createdAt": "2014-07-18T02:54:28.718Z", "isAdmin": false, "displayName": "cameroncowan"}, "userId": "HNyXRDxMm3eGd2PjP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QZLnAAYr8ABeSyRo4/do-virtual-humans-deserve-human-rights", "pageUrlRelative": "/posts/QZLnAAYr8ABeSyRo4/do-virtual-humans-deserve-human-rights", "linkUrl": "https://www.lesswrong.com/posts/QZLnAAYr8ABeSyRo4/do-virtual-humans-deserve-human-rights", "postedAtFormatted": "Thursday, September 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Do%20Virtual%20Humans%20deserve%20human%20rights%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADo%20Virtual%20Humans%20deserve%20human%20rights%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQZLnAAYr8ABeSyRo4%2Fdo-virtual-humans-deserve-human-rights%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Do%20Virtual%20Humans%20deserve%20human%20rights%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQZLnAAYr8ABeSyRo4%2Fdo-virtual-humans-deserve-human-rights", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQZLnAAYr8ABeSyRo4%2Fdo-virtual-humans-deserve-human-rights", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<p style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19.5px; background-color: #f7f7f8;\">Do Virtual Humans deserve human rights?</span></span></p>\n<p style=\"text-align: justify;\"><a class=\"fullwidth\" href=\"http://www.slate.com/articles/technology/future_tense/2014/09/virtually_human_we_need_human_rights_for_cyberconscious_beings.2.html\" target=\"_blank\">Slate Article</a></p>\n<p style=\"text-align: justify;\">&nbsp;</p>\n<p style=\"text-align: justify;\">I think the idea of storing our minds in a machine so that we can keep on \"living\" (and I use that term loosely) is fascinating and certainly and oft discussed topic around here. However, in thinking about keeping our brains on a hard drive we have to think about rights and how that all works together. Indeed the technology may be here before we know it so I think its important to think about mindclones. If I create a little version of myself that can answer my emails for me, can I delete him when I'm done with him or just turn him in for a new model like I do iPhones?&nbsp;</p>\n<p style=\"text-align: justify;\">&nbsp;</p>\n<p style=\"text-align: justify;\">I look forward to the discussion.</p>\n<p style=\"text-align: justify;\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QZLnAAYr8ABeSyRo4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -4, "extendedScore": null, "score": 1.997407995829403e-06, "legacy": true, "legacyId": "27170", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-11T22:39:32.938Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington D.C.: Parkour (Backup: Task Management)", "slug": "meetup-washington-d-c-parkour-backup-task-management", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinZ", "createdAt": "2009-07-08T20:34:05.168Z", "isAdmin": false, "displayName": "RobinZ"}, "userId": "eTMojvi4f2z3pDfsc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6ydATFEfYtDYq48G2/meetup-washington-d-c-parkour-backup-task-management", "pageUrlRelative": "/posts/6ydATFEfYtDYq48G2/meetup-washington-d-c-parkour-backup-task-management", "linkUrl": "https://www.lesswrong.com/posts/6ydATFEfYtDYq48G2/meetup-washington-d-c-parkour-backup-task-management", "postedAtFormatted": "Thursday, September 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20D.C.%3A%20Parkour%20(Backup%3A%20Task%20Management)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20D.C.%3A%20Parkour%20(Backup%3A%20Task%20Management)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ydATFEfYtDYq48G2%2Fmeetup-washington-d-c-parkour-backup-task-management%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20D.C.%3A%20Parkour%20(Backup%3A%20Task%20Management)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ydATFEfYtDYq48G2%2Fmeetup-washington-d-c-parkour-backup-task-management", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ydATFEfYtDYq48G2%2Fmeetup-washington-d-c-parkour-backup-task-management", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14e'>Washington D.C.: Parkour (Backup: Task Management)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 September 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">U.S. Navy Memorial</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weather permitting, we will meet at the Navy Memorial (south on 8th St from the Portrait Gallery, between D St and Pennsylvania Ave) to practice basic parkour techniques.</p>\n\n<p>If the weather seems to be unsuited to an outdoors meetup, we will meet at the Kogod Courtyard of the National Portrait Gallery (8th and F Sts or 8th and G Sts NW, go straight past the information desk from either entrance) to discuss to-do lists, task management, prioritization, and related topics.</p>\n\n<p>In either case, we plan to congregate between 3:00 and 3:30 p.m. to begin the meeting proper at 3:30.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14e'>Washington D.C.: Parkour (Backup: Task Management)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6ydATFEfYtDYq48G2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.997764113628792e-06, "legacy": true, "legacyId": "27171", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_D_C___Parkour__Backup__Task_Management_\">Discussion article for the meetup : <a href=\"/meetups/14e\">Washington D.C.: Parkour (Backup: Task Management)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 September 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">U.S. Navy Memorial</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weather permitting, we will meet at the Navy Memorial (south on 8th St from the Portrait Gallery, between D St and Pennsylvania Ave) to practice basic parkour techniques.</p>\n\n<p>If the weather seems to be unsuited to an outdoors meetup, we will meet at the Kogod Courtyard of the National Portrait Gallery (8th and F Sts or 8th and G Sts NW, go straight past the information desk from either entrance) to discuss to-do lists, task management, prioritization, and related topics.</p>\n\n<p>In either case, we plan to congregate between 3:00 and 3:30 p.m. to begin the meeting proper at 3:30.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_D_C___Parkour__Backup__Task_Management_1\">Discussion article for the meetup : <a href=\"/meetups/14e\">Washington D.C.: Parkour (Backup: Task Management)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington D.C.: Parkour (Backup: Task Management)", "anchor": "Discussion_article_for_the_meetup___Washington_D_C___Parkour__Backup__Task_Management_", "level": 1}, {"title": "Discussion article for the meetup : Washington D.C.: Parkour (Backup: Task Management)", "anchor": "Discussion_article_for_the_meetup___Washington_D_C___Parkour__Backup__Task_Management_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-11T23:00:01.411Z", "modifiedAt": null, "url": null, "title": "How realistic would AI-engineered chatbots be?", "slug": "how-realistic-would-ai-engineered-chatbots-be", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:32.687Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kokotajlod", "createdAt": "2013-02-25T13:45:56.499Z", "isAdmin": false, "displayName": "kokotajlod"}, "userId": "Gf6b4eSgp4kCKj8WY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bfz6RcemQbcY8FbL4/how-realistic-would-ai-engineered-chatbots-be", "pageUrlRelative": "/posts/bfz6RcemQbcY8FbL4/how-realistic-would-ai-engineered-chatbots-be", "linkUrl": "https://www.lesswrong.com/posts/bfz6RcemQbcY8FbL4/how-realistic-would-ai-engineered-chatbots-be", "postedAtFormatted": "Thursday, September 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20realistic%20would%20AI-engineered%20chatbots%20be%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20realistic%20would%20AI-engineered%20chatbots%20be%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbfz6RcemQbcY8FbL4%2Fhow-realistic-would-ai-engineered-chatbots-be%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20realistic%20would%20AI-engineered%20chatbots%20be%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbfz6RcemQbcY8FbL4%2Fhow-realistic-would-ai-engineered-chatbots-be", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbfz6RcemQbcY8FbL4%2Fhow-realistic-would-ai-engineered-chatbots-be", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 506, "htmlBody": "<p>I'm interested in how easy it would be to simulate just one present-day person's life rather than an entire planet's worth of people. Currently our chatbots are bad enough that we could not populate the world with NPC's; the lone human would quickly figure out that everyone else was... different, duller, incomprehensibly stupid, etc.<br /><br />But what if the chatbots were designed by a superintelligent AI?<br /><br />If a superintelligent AI was simulating my entire life from birth, would it be able to do it (for reasonably low computational resources cost, i.e. less than the cost of simulating another person) without simulating any other people in sufficient detail that they would be people?<br /><br />I suspect that the answer is yes. If the answer is \"maybe\" or \"no,\" I would very much like to hear tips on how to tell whether someone is an ideal chatbot.<br /><br />Thoughts?<br /><br />EDIT: In the comments most people are asking me to clarify what I mean by various things. By popular demand:<br /><br />I interact with people in more ways than just textual communication. I also hear them, and see them move about. So when I speak of chatbots I don't mean bots that can do nothing but chat. I mean an algorithm governing the behavior of a simulated entire-human-body, that is nowhere near the complexity of a brain. (Modern chatbots are algorithms governing the behavior of a simulated human-hands-typing-on-keyboard, that are nowhere near the complexity of a brain.)<br /><br />When I spoke of \"simulating any other people in sufficient detail that they would be people\" I didn't mean to launch us into a philosophical discussion of consciousness or personhood. I take it to be common ground among all of us here that very simple algorithms, such as modern chatbots, are not people. By contrast, many of us think that a simulated human brain would be a person. Assuming a simulated human brain would be a person, but a simple chatbot-like algorithm would not, my question is: Would any algorithm complex enough to fool me into thinking it was a person over the course of repeated interactions actually be a person? Or could all the bodies around me be governed by algorithms which are too simple to be people?<br /><br />I realize that we have no consensus on how complex an algorithm needs to be to be a person. That's OK. I'm hoping that this conversation can answer my questions anyhow; I'm expecting answers along the lines of</p>\n<p>(A) \"For a program only a few orders of magnitude more complicated than current chatbots, you could be reliably fooled your whole life\" or</p>\n<p>(B) \"Any program capable of fooling you would either draw from massive databases of pre-planned responses, which would be impractical, or actually simulate human-like reasoning.\"<br /><br />These answers wouldn't settle the question for good without a theory of personhood, but that's OK with me, these answers would be plenty good enough.<br /><br /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bfz6RcemQbcY8FbL4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 1, "extendedScore": null, "score": 1.9978007034758575e-06, "legacy": true, "legacyId": "27172", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-12T08:12:15.201Z", "modifiedAt": null, "url": null, "title": "Meetup : Frankfurt: How to improve your life", "slug": "meetup-frankfurt-how-to-improve-your-life", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kendra", "createdAt": "2012-02-29T23:10:44.583Z", "isAdmin": false, "displayName": "Kendra"}, "userId": "BPB6kHkfZwFLrhcbG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zhY8nYTdwubitKv7o/meetup-frankfurt-how-to-improve-your-life", "pageUrlRelative": "/posts/zhY8nYTdwubitKv7o/meetup-frankfurt-how-to-improve-your-life", "linkUrl": "https://www.lesswrong.com/posts/zhY8nYTdwubitKv7o/meetup-frankfurt-how-to-improve-your-life", "postedAtFormatted": "Friday, September 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Frankfurt%3A%20How%20to%20improve%20your%20life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Frankfurt%3A%20How%20to%20improve%20your%20life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzhY8nYTdwubitKv7o%2Fmeetup-frankfurt-how-to-improve-your-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Frankfurt%3A%20How%20to%20improve%20your%20life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzhY8nYTdwubitKv7o%2Fmeetup-frankfurt-how-to-improve-your-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzhY8nYTdwubitKv7o%2Fmeetup-frankfurt-how-to-improve-your-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14f'>Frankfurt: How to improve your life</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 September 2014 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Frankfurt</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're having another meetup! Join our mailing list (<a href=\"https://groups.google.com/forum/#!forum/less-wrong-frankfurt\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/less-wrong-frankfurt</a>) for more information. This meetup will have the theme \"How to improve your life\". Details are going to be published in time.\nThe meetup will happen at Ginnheimer Landstra\u00dfe again.\nIf you have special needs re. the meetup (because of social anxiety, being a wheel chair user, whatever) please tell me in advance, I'll try to accommodate them as best as I can.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14f'>Frankfurt: How to improve your life</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zhY8nYTdwubitKv7o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.9987880353592154e-06, "legacy": true, "legacyId": "27181", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Frankfurt__How_to_improve_your_life\">Discussion article for the meetup : <a href=\"/meetups/14f\">Frankfurt: How to improve your life</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 September 2014 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Frankfurt</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're having another meetup! Join our mailing list (<a href=\"https://groups.google.com/forum/#!forum/less-wrong-frankfurt\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/less-wrong-frankfurt</a>) for more information. This meetup will have the theme \"How to improve your life\". Details are going to be published in time.\nThe meetup will happen at Ginnheimer Landstra\u00dfe again.\nIf you have special needs re. the meetup (because of social anxiety, being a wheel chair user, whatever) please tell me in advance, I'll try to accommodate them as best as I can.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Frankfurt__How_to_improve_your_life1\">Discussion article for the meetup : <a href=\"/meetups/14f\">Frankfurt: How to improve your life</a></h2>", "sections": [{"title": "Discussion article for the meetup : Frankfurt: How to improve your life", "anchor": "Discussion_article_for_the_meetup___Frankfurt__How_to_improve_your_life", "level": 1}, {"title": "Discussion article for the meetup : Frankfurt: How to improve your life", "anchor": "Discussion_article_for_the_meetup___Frankfurt__How_to_improve_your_life1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-12T15:38:25.361Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tvDAC8YjsJYeCEPk3/weekly-lw-meetups-1", "pageUrlRelative": "/posts/tvDAC8YjsJYeCEPk3/weekly-lw-meetups-1", "linkUrl": "https://www.lesswrong.com/posts/tvDAC8YjsJYeCEPk3/weekly-lw-meetups-1", "postedAtFormatted": "Friday, September 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtvDAC8YjsJYeCEPk3%2Fweekly-lw-meetups-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtvDAC8YjsJYeCEPk3%2Fweekly-lw-meetups-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtvDAC8YjsJYeCEPk3%2Fweekly-lw-meetups-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 586, "htmlBody": "<p><strong>This summary was posted to LW Main on September 5th. The following week's summary is <a href=\"/lw/kz4/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/13r\">Bratislava:&nbsp;<span class=\"date\">08 September 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/147\">Copenhagen - September: This Wavefunction Has Uncollapsed:&nbsp;<span class=\"date\">13 September 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/zr\">Houston, TX:&nbsp;<span class=\"date\">13 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13s\">Michigan Meetup:&nbsp;<span class=\"date\">07 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13x\">Urbana-Champaign: Practical Rationality:&nbsp;<span class=\"date\">07 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13a\">Utrecht: Improve your productivity:&nbsp;<span class=\"date\">06 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13b\">Utrecht: Debiasing techniques:&nbsp;<span class=\"date\">21 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13u\">Utrecht: Effective Altruism and Politics:&nbsp;<span class=\"date\">05 October 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13v\">Utrecht: Artificial Intelligence:&nbsp;<span class=\"date\">19 October 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13w\">Utrecht: Climate Change:&nbsp;<span class=\"date\">02 November 2014 03:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">06 September 2025 01:30PM</span></a></li>\n<li><a href=\"/meetups/13z\">[Cambridge MA] Prediction Markets and Futarchy:&nbsp;<span class=\"date\">07 September 2014 03:30PM</span></a></li>\n<li><a href=\"/meetups/13m\">Canberra: Akrasia-busters!:&nbsp;<span class=\"date\">13 September 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/13h\">[Melbourne] September Rationality Dojo - Fixed and Growth Mindset:&nbsp;<span class=\"date\">07 September 2014 03:30PM</span></a></li>\n<li><a href=\"/meetups/140\">Moscow Meetup: Codename Felix:&nbsp;<span class=\"date\">14 September 2014 10:11PM</span></a></li>\n<li><a href=\"/meetups/13t\">Sydney Rationality Dojo - Habits:&nbsp;<span class=\"date\">07 September 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/142\">Sydney Meetup - September:&nbsp;<span class=\"date\">24 September 2014 06:30PM</span></a></li>\n<li><a href=\"/meetups/143\">Vienna - Superintelligence:&nbsp;<span class=\"date\">27 September 2014 03:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Buffalo.2C_NY\">Buffalo</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Moscow.2C_Russia\">Moscow</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong>&nbsp;</strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Sydney\">Sydney</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> </strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tel_Aviv.2C_Israel\">Tel Aviv</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Warsaw.2C_Poland\">Warsaw</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tvDAC8YjsJYeCEPk3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.9995863725488973e-06, "legacy": true, "legacyId": "27127", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rWBZAB85996yAmveq", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-13T13:37:30.719Z", "modifiedAt": "2022-04-26T21:30:32.791Z", "url": null, "title": "[meta] New LW moderator: Viliam_Bur", "slug": "meta-new-lw-moderator-viliam_bur", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:38.815Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Kaj_Sotala", "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vrnB5nPrjTXuwv5nN/meta-new-lw-moderator-viliam_bur", "pageUrlRelative": "/posts/vrnB5nPrjTXuwv5nN/meta-new-lw-moderator-viliam_bur", "linkUrl": "https://www.lesswrong.com/posts/vrnB5nPrjTXuwv5nN/meta-new-lw-moderator-viliam_bur", "postedAtFormatted": "Saturday, September 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bmeta%5D%20New%20LW%20moderator%3A%20Viliam_Bur&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bmeta%5D%20New%20LW%20moderator%3A%20Viliam_Bur%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvrnB5nPrjTXuwv5nN%2Fmeta-new-lw-moderator-viliam_bur%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bmeta%5D%20New%20LW%20moderator%3A%20Viliam_Bur%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvrnB5nPrjTXuwv5nN%2Fmeta-new-lw-moderator-viliam_bur", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvrnB5nPrjTXuwv5nN%2Fmeta-new-lw-moderator-viliam_bur", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 128, "htmlBody": "<p>Some time back, <a href=\"/lw/krw/meta_future_moderation_and_investigation_of/\">I wrote that</a> I was unwilling to continue with investigations into mass downvoting, and asked people for suggestions on how to deal with them from now on. The <a href=\"/lw/krw/meta_future_moderation_and_investigation_of/b8ef\">top-voted proposal in that thread</a> suggested making Viliam_Bur into a moderator, and <a href=\"/lw/krw/meta_future_moderation_and_investigation_of/b8g0\">Viliam gracefully accepted the nomination</a>. So I have given him moderator privileges and also put him in contact with jackk, who provided me with the information necessary to deal with the previous cases. Future requests about mass downvote investigations should be directed to Viliam.</p>\n<p>Thanks a lot for agreeing to take up this responsibility, Viliam! It's not an easy one, but I'm very grateful that you're willing to do it. Please post a comment here so that we can reward you with some extra upvotes. :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vrnB5nPrjTXuwv5nN", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 62, "extendedScore": null, "score": 0.000598267051676661, "legacy": true, "legacyId": "27190", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": true, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["Hmc8MaThCG8xX8snT"]}, "moderationGuidelinesVersion": "1.1.0", "customHighlightVersion": "1.1.0", "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-13T17:05:52.884Z", "modifiedAt": null, "url": null, "title": "Causal decision theory is unsatisfactory", "slug": "causal-decision-theory-is-unsatisfactory", "viewCount": null, "lastCommentedAt": "2020-07-24T00:48:40.041Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5oBw9T5y5DLpJZL63/causal-decision-theory-is-unsatisfactory", "pageUrlRelative": "/posts/5oBw9T5y5DLpJZL63/causal-decision-theory-is-unsatisfactory", "linkUrl": "https://www.lesswrong.com/posts/5oBw9T5y5DLpJZL63/causal-decision-theory-is-unsatisfactory", "postedAtFormatted": "Saturday, September 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Causal%20decision%20theory%20is%20unsatisfactory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACausal%20decision%20theory%20is%20unsatisfactory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5oBw9T5y5DLpJZL63%2Fcausal-decision-theory-is-unsatisfactory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Causal%20decision%20theory%20is%20unsatisfactory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5oBw9T5y5DLpJZL63%2Fcausal-decision-theory-is-unsatisfactory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5oBw9T5y5DLpJZL63%2Fcausal-decision-theory-is-unsatisfactory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2804, "htmlBody": "<p><em>This is <a href=\"http://mindingourway.com/causal-reasoning-is-unsatisfactory/\">crossposted</a> from <a href=\"http://mindingourway.com/\">my new blog</a>. I was planning to write a short post explaining how Newcomblike problems are the norm and why any sufficiently powerful intelligence built to use causal decision theory would self-modify to stop using causal decision theory in short order. Turns out it's not such a short topic, and it's turning into a short intro to decision theory.</em></p>\n<p><em>I've been motivating MIRI's technical agenda (decision theory and otherwise) to outsiders quite frequently recently, and I received a few comments of the form \"Oh cool, I've seen lots of decision theory type stuff on LessWrong, but I hadn't understood the connection.\" While the intended audience of my blog is wider than the readerbase of LW (and thus, the tone might seem off and the content a bit basic), I've updated towards these posts being useful here. I also hope that some of you will correct my mistakes!</em></p>\n<p><em>This sequence will probably run for four or five posts, during which I'll motivate the use of decision theory, the problems with the modern standard of decision theory (CDT), and some of the reasons why these problems are an FAI concern.</em></p>\n<p><em>I'll be <a href=\"http://intelligence.org/2014/09/12/nate-soares-speaking-purdue-september-18th/\">giving a talk</a>&nbsp;on the material from this sequence at Purdue next week.</em></p>\n<h1>1</h1>\n<p>Choice is a crucial component of reasoning. Given a set of available actions, which action do you take? Do you go out to the movies or stay in with a book? Do you capture the bishop or fork the king? Somehow, we must reason about our options and choose the best one.</p>\n<p>Of course, we humans don't consciously weigh all of our actions. Many of our choices are made subconsciously. (Which letter will I type next? When will I get a drink of water?) Yet even if the choices are made by subconscious heuristics, they must be made somehow.</p>\n<p>In practice, decisions are often made on autopilot. We don't weigh every available alternative when it's time to prepare for work in the morning, we just pattern-match the situation and carry out some routine. This is a shortcut that saves time and cognitive energy. Yet, no matter how much we stick to routines, we still spend <em>some</em> of our time making hard choices, weighing alternatives, and predicting which available action will serve us best.</p>\n<p>The study of how to make these sorts of decisions is known as <em>Decision Theory</em>. This field of research is closely intertwined with Economics, Philosophy, Mathematics, and (of course) Game Theory. It will be the subject of today's post.</p>\n<p><a id=\"more\"></a></p>\n<h1>2</h1>\n<p>Decisions about what action to choose necessarily involve <em>counterfactual reasoning</em>, in the sense that we reason about what <em>would</em> happen if we took actions which we (in fact) will not take.</p>\n<p>We all have some way of performing this counterfactual reasoning. Most of us can visualize what would happen if we did something that we aren't going to do. For example, consider shouting \"PUPPIES!\" at the top of your lungs right now. I bet you won't do it, but I <em>also</em> bet that you can picture the results.</p>\n<p>One of the major goals of decision theory is to formalize this counterfactual reasoning: if we had unlimited resources then how would we compute alternatives so as to ensure that we always pick the best possible action? This question is harder than it looks, for reasons explored below: counterfactal reasoning can encounter many pitfalls.</p>\n<p>A second major goal of decision theory is this: human counterfactual reasoning sometimes runs afoul of those pitfalls, and a formal understanding of decision theory can help humans make better decisions. It's no coincidence that Game Theory was developed during the cold war!</p>\n<p>(<em>My</em> major goal in decision theory is to understand it as part of the process of learning how to construct a machine intelligence that reliably reasons well. This provides the motivation to nitpick existing decision theories. If they're broken then we had better learn that sooner rather than later.)</p>\n<h1>3</h1>\n<p>Sometimes, it's easy to choose the best available action. You consider each action in turn, and predict the outcome, and then pick the action that leads to the best outcome. This can be difficult when accurate predictions are unavailable, but that's not the problem that we address with decision theory. The problem we address is that sometimes <em>it is difficult to reason about what would happen if you took a given action</em>.</p>\n<p>For example, imagine that you know of a fortune teller who can reliably read palms to divine destinies. Most people who get a good fortune wind up happy, while most people who get a bad fortune wind up sad. It's been experimentally verified that she can use information on palms to reliably make inferences about the palm-owner's destiny.</p>\n<p>So... should you <a href=\"http://www.thedailybeast.com/articles/2013/07/12/your-future-is-in-the-palm-of-your-surgeon-s-hand.html\">get palm surgery to change your fate</a>?</p>\n<p>If you're bad at reasoning about counterfactuals, you might reason as follows:</p>\n<blockquote>\n<p>Nine out of ten people who get a good fortune do well in life. I had better use the palm surgery to ensure a good fortune!</p>\n</blockquote>\n<p>Now admittedly, if palm reading is shown to work, the first thing you should do is check whether you can alter destiny by altering your palms. However, <em>assuming</em> that changing your palm doesn't drastically affect your fate, this sort of reasoning is quite silly.</p>\n<p>The above reasoning process conflates <em>correlation</em> with <em>causal control</em>. The above reasoner gets palm surgery because they want a good destiny. But while your palm may give <em>information</em> about their destiny, your palm does not <em>control</em> your fate.</p>\n<p>If we find out that we've been using this sort of reasoning, we can usually do better by considering actions only on the basis of what they <em>cause</em>.</p>\n<h1>4</h1>\n<p>This idea leads us to <em>causal decision theory (CDT)</em>, which demands that we consider actions based only upon the causal effects of those actions.</p>\n<p>Actions are considered using <em>causal counterfactual reasoning</em>. Though causal counterfactual reasoning can be formalized in many ways, we will consider graphical models specifically. Roughly, a <em>causal graph</em> is a graph where the world model is divided into a series of nodes, with arrows signifying the causal connections between the nodes. For a more formal introduction, you'll need to consult a <a href=\"http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/0521773628\">textbook</a>. As an example, here's a causal graph for the palm-reading scenario above:</p>\n<p style=\"text-align: center;\"><img src=\"http://mindingourway.com/content/images/2014/Sep/palm-surgery.png\" alt=\"\" width=\"614\" height=\"555\" /></p>\n<p>The choice is denoted by the dotted <code>Surgery?</code> node. Your payoff is the <code>$</code> diamond. Each node is specified as a function of all nodes causing it.</p>\n<p>For example, in a very simple deterministic version of the palm-reading scenario, the nodes could be specified as follows:</p>\n<ol>\n<li><code>Surgery?</code> is a program implementing the agent, and must output either <em>yes</em> or <em>no</em>.</li>\n<li><code>Destiny</code> is either <em>good</em> or <em>bad</em>.</li>\n<li><code>Fortune</code> is always <em>good</em> if <code>Surgery?</code> is <em>yes</em>, and is the same as <code>Destiny</code> otherwise.</li>\n<li><code>$</code> is $100 if <code>Destiny</code> is <em>good</em> and $10 otherwise, minus $10 if <code>Surgery</code> is <em>yes</em>. Surgery is expensive!</li>\n</ol>\n<p>Now let's say that you expect even odds on whether or not your destiny is good or bad, e.g. the probability that <code>Destiny</code>=<em>good</em> is 50%.</p>\n<p>If the <code>Surgery?</code> node is a program that implements causal decision theory, then that program will choose between <em>yes</em> and <em>no</em> using the following reasoning:</p>\n<ul>\n<li>The action node is <code>Surgery?</code></li>\n<li>The available actions are <em>yes</em> and <em>no</em></li>\n<li>The payoff node is <code>$</code></li>\n<li>Consider the action <em>yes</em> \n<ul>\n<li>Replace the value of <code>Surgery?</code> with a function that always returns <em>yes</em></li>\n<li>Calculate the value of <code>$</code></li>\n<li>We would get $90 if <code>Destiny</code>=<em>good</em></li>\n<li>We would get $0 if <code>Destiny</code>=<em>bad</em></li>\n<li>This is $45 in expectation.</li>\n</ul>\n</li>\n<li>Consider the action <em>no</em> \n<ul>\n<li>Replace the value of <code>Surgery?</code> with a function that always returns <em>no</em></li>\n<li>Calculate the value of <code>$</code></li>\n<li>We would get $100 if <code>Destiny</code>=<em>good</em></li>\n<li>We would get $10 if <code>Destiny</code>=<em>bad</em></li>\n<li>This is $55 in expectation.</li>\n</ul>\n</li>\n<li>Return <em>no</em>, as that yields the higher value of <code>$</code>.</li>\n</ul>\n<p>More generally, the CDT reasoning procedure works as follows:</p>\n<ol>\n<li>Identify your action node <strong>A</strong></li>\n<li>Identify your available actions <em>Acts</em>.</li>\n<li>Identify your payoff node <strong>U</strong>.</li>\n<li>For each action <em>a</em> \n<ul>\n<li>Set&nbsp;<strong>A</strong> to <em>a</em> by replacing the value of <strong>A</strong> with a function that ignores its input and returns <em>a</em></li>\n<li>Evaluate the expectation of <strong>U</strong> given that <strong>A</strong>=a</li>\n</ul>\n</li>\n<li>Take the <em>a</em> with the highest associated expectation of <strong>U</strong>.</li>\n</ol>\n<p>Notice how CDT evaluates counterfactuals by setting the value of its action node in a causal graph, and then calculating its payoff accordingly. Done correctly, this allows a reasoner to figure out the causal implications of taking a specific action without getting confused by additional variables.</p>\n<p>CDT is the academic standard decision theory. Economics, statistics, and philosophy all assume (or, indeed, <em>define</em>) that rational reasoners use causal decision theory to choose between available actions.</p>\n<p>Furthermore, narrow AI systems which consider their options using this sort of causal counterfactual reasoning are implicitly acting like they use causal decision theory.</p>\n<p>Unfortunately, causal decision theory is broken.</p>\n<h1>5</h1>\n<p>Before we dive into the problems with CDT, let's flesh it out a bit more. Game Theorists often talk about scenarios in terms of tables that list the payoffs associated with each action. This might seem a little bit like cheating, because it often takes a lot of hard work to determine what the payoff of any given action is. However, these tables will allow us to explore some simple examples of how causal reasoning works.</p>\n<p>I will describe a variant of the classic <a href=\"http://en.wikipedia.org/wiki/Prisoner's_dilemma\">Prisoner's Dilemma</a> which I refer to as the <em>token trade</em>. There are two players in two separate rooms, one red and one green. The red player starts with the green token, and vice versa. Each must decide (in isolation, without communication) whether or not to give their token to me, in which case I will give it to the other player.</p>\n<p>Afterwards, they may cash their token out. The red player gets $200 for cashing out the red token and $100 for the green token, and vice versa. The payoff table looks like this:</p>\n<table style=\"border-collapse: collapse; margin-top: 2em; margin-bottom: 2em;\" border=\"0\" cellpadding=\"10\" align=\"center\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td style=\"color:red; text-align: center; border-right: 1px solid black; border-left: 1px solid black;\">Give</td>\n<td style=\"color:red; text-align: center; border-right: 1px solid black;\">Keep</td>\n</tr>\n<tr>\n<td style=\"color:green; text-align: center; border-top: 1px solid black;\">Give</td>\n<td style=\"border: 1px solid black; text-align: center;\">( <span style=\"color:green\">$200</span>, <span style=\"color:red\">$200</span> )</td>\n<td style=\"border: 1px solid black; text-align: center;\">( <span style=\"color:green\">$0</span>, <span style=\"color:red\">$300</span> )</td>\n</tr>\n<tr>\n<td style=\"color:green; text-align: center; border-top: 1px solid black; border-bottom: 1px solid black;\">Keep</td>\n<td style=\"border: 1px solid black; text-align: center;\">( <span style=\"color:green\">$300</span>, <span style=\"color:red\">$0</span> )</td>\n<td style=\"border: 1px solid black; text-align: center;\">( <span style=\"color:green\">$100</span>, <span style=\"color:red\">$100</span> )</td>\n</tr>\n</tbody>\n</table>\n<p>For example, if the green player gives the red token away, and the red player keeps the green token, then the red player gets $300 while the green player gets nothing.</p>\n<p>Now imagine a causal decision theorist facing this scenario. Their causal graph might look something like this:</p>\n<p style=\"text-align: center;\"><img src=\"http://mindingourway.com/content/images/2014/Sep/token-trade.png\" alt=\"\" width=\"732\" height=\"525\" /></p>\n<p>Let's evaluate this using CDT. The action node is <code>Give?</code>, the payoff node is <code>$</code>. We must evaluate the expectation of <code>$</code> given <code>Give?</code>=<em>yes</em> and <code>Give?</code>=<em>no</em>. This, of course, depends upon the expected value of <code>TheirDecision</code>.</p>\n<p>In Game Theory, we usually assume that the opponent is reasoning using something like CDT. Then we can reason about <code>TheirDecision</code> given that they are doing similar reasoning about our decision and so on. This threatens to lead to infinite regress, but in fact there are some tricks you can use to guarantee at least one equilibrium. (These are the famous <a href=\"http://en.wikipedia.org/wiki/Nash_equilibrium\">Nash equilibria</a>.) This sort of reasoning requires some modifications to the simple CDT procedure given above which we're going to ignore today. Because while <em>most</em> scenarios with multiple agents require more complicated reasoning, the token trade is an especially simple scenario where we can brush all that under the rug.</p>\n<p>In the token trade, the expected value of <code>TheirDecision</code> doesn't matter to a CDT agent. No matter what the probability p of <code>TheirDecision</code>=<em>give</em> happens to be, the CDT agent will do the following reasoning:</p>\n<ul>\n<li>Change <code>Give?</code> to be a constant function returning <em>yes</em> \n<ul>\n<li>If <code>TheirDecision</code>=<em>give</em> then we get $200</li>\n<li>If <code>TheirDecision</code>=<em>keep</em> then we get $0</li>\n<li>We get 200p dollars in expectation.</li>\n</ul>\n</li>\n<li>Change <code>Give?</code> to be a constant function returning <em>no</em> \n<ul>\n<li>If <code>TheirDecision</code>=<em>give</em> then we get $300</li>\n<li>If <code>TheirDecision</code>=<em>keep</em> then we get $100</li>\n<li>We get 300p + 100(1-p) dollars in expectation.</li>\n</ul>\n</li>\n</ul>\n<p>Obviously, 300p+100(1-p) will be larger than 200p, <em>no matter what probability p is</em>.</p>\n<p>A CDT agent in the token trade must have an expectation about <code>TheirDecision</code> captured by a probability p that they will give their token, and we have just shown that no matter what that p is, the CDT agent will keep their token.</p>\n<p>When something like this occurs (where <code>Give?</code>=<em>no</em> is better regardless of the value of <code>TheirDecision</code>) we say that <code>Give?</code>=<em>no</em> is a \"dominant strategy\". CDT executes this dominant strategy, and keeps its token.</p>\n<h1>6</h1>\n<p>Of course, this means that each player will get $100, when they could have both recieved $200. This may seem unsatisfactory. Both players would agree that they could do better by trading tokens. Why don't they coordinate?</p>\n<p>The classic response is that the token trade (better known as the Prisoner's Dilemma) is a game that explicitly disallows coordination. If players do have an opportunity to coordinate (or even if they expect to play the game mulitple times) then they can (and do!) do better than this.</p>\n<p>I won't object much here, except to note that this answer is still unsatisfactory. CDT agents fail to cooperate on a one-shot Prisoner's Dilemma. That's a bullet that causal decision theorists willingly bite, but don't forget that it's still a bullet.</p>\n<h1>7</h1>\n<p>Failure to cooperate on the one-shot Prisoner's Dilemma is not necessarily a problem. Indeed, if you ever find yourself playing a token trade against an opponent using CDT, then you had better hold on to your token, because they surely aren't going to give you theirs.</p>\n<p>However, CDT <em>does</em> fail on a very similar problem where it seems insane to fail. CDT fails at the token trade <em>even when it knows it is playing against a perfect copy of itself.</em></p>\n<p>I call this the \"mirror token trade\", and it works as follows: first, I clone you. Then, I make you play a token trade against yourself.</p>\n<p>In this case, your opponent is guaranteed to pick exactly the same action that you pick. (Well, mostly: the game isn't completely symmetric. If you want to nitpick, consider that instead of playing against a copy of yourself, you must write a red/green colorblind deterministic computer program which will play against a copy of itself.)</p>\n<p>The causal graph for this game looks like this:</p>\n<p style=\"text-align: center;\"><img src=\"http://mindingourway.com/content/images/2014/Sep/token-mirror.png\" alt=\"\" width=\"733\" height=\"554\" /></p>\n<p>Because I've swept questions of determinism and asymmetry under the rug, both decisions will be identical. The red copy should trade its token, because that's guaranteed to get it the green token (and it's the only way to do so).</p>\n<p>Yet CDT would have you evaluate an action by considering what happens if you replace the node <code>Give?</code> with a function that always returns that action. But this intervention does not affect the opponent, which reasons the same way! Just as before, a CDT agent treats <code>TheirDecision</code> as if it has some probability of being <em>give</em> that is independent from the agent's action, and reasons that \"I always keep my token while they act independently\" dominates \"I always give my token while they act independently\".</p>\n<p>Do you see the problem here? CDT is evaluating its action by changing the value of its action node <code>Give?</code>, assuming that this only affects things that are <em>caused</em> by <code>Give?</code>. The agent reasons counterfactually by considering \"what if <code>Give?</code> were a constant function that always returned <em>yes</em>?\" while failing to note that overwriting <code>Give?</code> in this way neglects the fact that <code>Give?</code> and <code>TheirDecision</code> are necessarily equal.</p>\n<p>Or, to put it another way, CDT evaluates counterfactuals <em>assuming</em> that all nodes uncaused by its action are independent of its action. It thinks it can change its action and only look at the downstream effects. This can break down when there are acausal connections between the nodes.</p>\n<p>After the red agent has been created from the template, its decision no longer <em>causally</em> affects the decision of the green agent. But both agents will do the same thing! There is a <em>logical</em> connection, even though there is no causal connection. It is these logical connections that are ignored by causal counterfactual reasoning.</p>\n<p>This is a subtle point, but an important one: the values of <code>Give?</code> and <code>TheirDecision</code> are logically connected, but CDT's method of reasoning about counterfactuals neglects this connection.</p>\n<h1>8</h1>\n<p>This is a know failure mode for causal decision theory. The mirror token trade is an example of what's known as a \"Newcomblike problem\".</p>\n<p>Decision theorists occasionally dismiss Newcomblike problems as edge cases, or as scenarios specifically designed to punish agents for being \"rational\". I disagree.</p>\n<p>And finally, eight sections in, I'm ready to articulate the original point: Newcomblike problems aren't a special case. <em>They're the norm.</em></p>\n<p>But this post has already run on for far too long, so that discussion will have to wait until next time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb28e": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5oBw9T5y5DLpJZL63", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 35, "extendedScore": null, "score": 0.00012, "legacy": true, "legacyId": "27189", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This is <a href=\"http://mindingourway.com/causal-reasoning-is-unsatisfactory/\">crossposted</a> from <a href=\"http://mindingourway.com/\">my new blog</a>. I was planning to write a short post explaining how Newcomblike problems are the norm and why any sufficiently powerful intelligence built to use causal decision theory would self-modify to stop using causal decision theory in short order. Turns out it's not such a short topic, and it's turning into a short intro to decision theory.</em></p>\n<p><em>I've been motivating MIRI's technical agenda (decision theory and otherwise) to outsiders quite frequently recently, and I received a few comments of the form \"Oh cool, I've seen lots of decision theory type stuff on LessWrong, but I hadn't understood the connection.\" While the intended audience of my blog is wider than the readerbase of LW (and thus, the tone might seem off and the content a bit basic), I've updated towards these posts being useful here. I also hope that some of you will correct my mistakes!</em></p>\n<p><em>This sequence will probably run for four or five posts, during which I'll motivate the use of decision theory, the problems with the modern standard of decision theory (CDT), and some of the reasons why these problems are an FAI concern.</em></p>\n<p><em>I'll be <a href=\"http://intelligence.org/2014/09/12/nate-soares-speaking-purdue-september-18th/\">giving a talk</a>&nbsp;on the material from this sequence at Purdue next week.</em></p>\n<h1 id=\"1\">1</h1>\n<p>Choice is a crucial component of reasoning. Given a set of available actions, which action do you take? Do you go out to the movies or stay in with a book? Do you capture the bishop or fork the king? Somehow, we must reason about our options and choose the best one.</p>\n<p>Of course, we humans don't consciously weigh all of our actions. Many of our choices are made subconsciously. (Which letter will I type next? When will I get a drink of water?) Yet even if the choices are made by subconscious heuristics, they must be made somehow.</p>\n<p>In practice, decisions are often made on autopilot. We don't weigh every available alternative when it's time to prepare for work in the morning, we just pattern-match the situation and carry out some routine. This is a shortcut that saves time and cognitive energy. Yet, no matter how much we stick to routines, we still spend <em>some</em> of our time making hard choices, weighing alternatives, and predicting which available action will serve us best.</p>\n<p>The study of how to make these sorts of decisions is known as <em>Decision Theory</em>. This field of research is closely intertwined with Economics, Philosophy, Mathematics, and (of course) Game Theory. It will be the subject of today's post.</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"2\">2</h1>\n<p>Decisions about what action to choose necessarily involve <em>counterfactual reasoning</em>, in the sense that we reason about what <em>would</em> happen if we took actions which we (in fact) will not take.</p>\n<p>We all have some way of performing this counterfactual reasoning. Most of us can visualize what would happen if we did something that we aren't going to do. For example, consider shouting \"PUPPIES!\" at the top of your lungs right now. I bet you won't do it, but I <em>also</em> bet that you can picture the results.</p>\n<p>One of the major goals of decision theory is to formalize this counterfactual reasoning: if we had unlimited resources then how would we compute alternatives so as to ensure that we always pick the best possible action? This question is harder than it looks, for reasons explored below: counterfactal reasoning can encounter many pitfalls.</p>\n<p>A second major goal of decision theory is this: human counterfactual reasoning sometimes runs afoul of those pitfalls, and a formal understanding of decision theory can help humans make better decisions. It's no coincidence that Game Theory was developed during the cold war!</p>\n<p>(<em>My</em> major goal in decision theory is to understand it as part of the process of learning how to construct a machine intelligence that reliably reasons well. This provides the motivation to nitpick existing decision theories. If they're broken then we had better learn that sooner rather than later.)</p>\n<h1 id=\"3\">3</h1>\n<p>Sometimes, it's easy to choose the best available action. You consider each action in turn, and predict the outcome, and then pick the action that leads to the best outcome. This can be difficult when accurate predictions are unavailable, but that's not the problem that we address with decision theory. The problem we address is that sometimes <em>it is difficult to reason about what would happen if you took a given action</em>.</p>\n<p>For example, imagine that you know of a fortune teller who can reliably read palms to divine destinies. Most people who get a good fortune wind up happy, while most people who get a bad fortune wind up sad. It's been experimentally verified that she can use information on palms to reliably make inferences about the palm-owner's destiny.</p>\n<p>So... should you <a href=\"http://www.thedailybeast.com/articles/2013/07/12/your-future-is-in-the-palm-of-your-surgeon-s-hand.html\">get palm surgery to change your fate</a>?</p>\n<p>If you're bad at reasoning about counterfactuals, you might reason as follows:</p>\n<blockquote>\n<p>Nine out of ten people who get a good fortune do well in life. I had better use the palm surgery to ensure a good fortune!</p>\n</blockquote>\n<p>Now admittedly, if palm reading is shown to work, the first thing you should do is check whether you can alter destiny by altering your palms. However, <em>assuming</em> that changing your palm doesn't drastically affect your fate, this sort of reasoning is quite silly.</p>\n<p>The above reasoning process conflates <em>correlation</em> with <em>causal control</em>. The above reasoner gets palm surgery because they want a good destiny. But while your palm may give <em>information</em> about their destiny, your palm does not <em>control</em> your fate.</p>\n<p>If we find out that we've been using this sort of reasoning, we can usually do better by considering actions only on the basis of what they <em>cause</em>.</p>\n<h1 id=\"4\">4</h1>\n<p>This idea leads us to <em>causal decision theory (CDT)</em>, which demands that we consider actions based only upon the causal effects of those actions.</p>\n<p>Actions are considered using <em>causal counterfactual reasoning</em>. Though causal counterfactual reasoning can be formalized in many ways, we will consider graphical models specifically. Roughly, a <em>causal graph</em> is a graph where the world model is divided into a series of nodes, with arrows signifying the causal connections between the nodes. For a more formal introduction, you'll need to consult a <a href=\"http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/0521773628\">textbook</a>. As an example, here's a causal graph for the palm-reading scenario above:</p>\n<p style=\"text-align: center;\"><img src=\"http://mindingourway.com/content/images/2014/Sep/palm-surgery.png\" alt=\"\" width=\"614\" height=\"555\"></p>\n<p>The choice is denoted by the dotted <code>Surgery?</code> node. Your payoff is the <code>$</code> diamond. Each node is specified as a function of all nodes causing it.</p>\n<p>For example, in a very simple deterministic version of the palm-reading scenario, the nodes could be specified as follows:</p>\n<ol>\n<li><code>Surgery?</code> is a program implementing the agent, and must output either <em>yes</em> or <em>no</em>.</li>\n<li><code>Destiny</code> is either <em>good</em> or <em>bad</em>.</li>\n<li><code>Fortune</code> is always <em>good</em> if <code>Surgery?</code> is <em>yes</em>, and is the same as <code>Destiny</code> otherwise.</li>\n<li><code>$</code> is $100 if <code>Destiny</code> is <em>good</em> and $10 otherwise, minus $10 if <code>Surgery</code> is <em>yes</em>. Surgery is expensive!</li>\n</ol>\n<p>Now let's say that you expect even odds on whether or not your destiny is good or bad, e.g. the probability that <code>Destiny</code>=<em>good</em> is 50%.</p>\n<p>If the <code>Surgery?</code> node is a program that implements causal decision theory, then that program will choose between <em>yes</em> and <em>no</em> using the following reasoning:</p>\n<ul>\n<li>The action node is <code>Surgery?</code></li>\n<li>The available actions are <em>yes</em> and <em>no</em></li>\n<li>The payoff node is <code>$</code></li>\n<li>Consider the action <em>yes</em> \n<ul>\n<li>Replace the value of <code>Surgery?</code> with a function that always returns <em>yes</em></li>\n<li>Calculate the value of <code>$</code></li>\n<li>We would get $90 if <code>Destiny</code>=<em>good</em></li>\n<li>We would get $0 if <code>Destiny</code>=<em>bad</em></li>\n<li>This is $45 in expectation.</li>\n</ul>\n</li>\n<li>Consider the action <em>no</em> \n<ul>\n<li>Replace the value of <code>Surgery?</code> with a function that always returns <em>no</em></li>\n<li>Calculate the value of <code>$</code></li>\n<li>We would get $100 if <code>Destiny</code>=<em>good</em></li>\n<li>We would get $10 if <code>Destiny</code>=<em>bad</em></li>\n<li>This is $55 in expectation.</li>\n</ul>\n</li>\n<li>Return <em>no</em>, as that yields the higher value of <code>$</code>.</li>\n</ul>\n<p>More generally, the CDT reasoning procedure works as follows:</p>\n<ol>\n<li>Identify your action node <strong>A</strong></li>\n<li>Identify your available actions <em>Acts</em>.</li>\n<li>Identify your payoff node <strong>U</strong>.</li>\n<li>For each action <em>a</em> \n<ul>\n<li>Set&nbsp;<strong>A</strong> to <em>a</em> by replacing the value of <strong>A</strong> with a function that ignores its input and returns <em>a</em></li>\n<li>Evaluate the expectation of <strong>U</strong> given that <strong>A</strong>=a</li>\n</ul>\n</li>\n<li>Take the <em>a</em> with the highest associated expectation of <strong>U</strong>.</li>\n</ol>\n<p>Notice how CDT evaluates counterfactuals by setting the value of its action node in a causal graph, and then calculating its payoff accordingly. Done correctly, this allows a reasoner to figure out the causal implications of taking a specific action without getting confused by additional variables.</p>\n<p>CDT is the academic standard decision theory. Economics, statistics, and philosophy all assume (or, indeed, <em>define</em>) that rational reasoners use causal decision theory to choose between available actions.</p>\n<p>Furthermore, narrow AI systems which consider their options using this sort of causal counterfactual reasoning are implicitly acting like they use causal decision theory.</p>\n<p>Unfortunately, causal decision theory is broken.</p>\n<h1 id=\"5\">5</h1>\n<p>Before we dive into the problems with CDT, let's flesh it out a bit more. Game Theorists often talk about scenarios in terms of tables that list the payoffs associated with each action. This might seem a little bit like cheating, because it often takes a lot of hard work to determine what the payoff of any given action is. However, these tables will allow us to explore some simple examples of how causal reasoning works.</p>\n<p>I will describe a variant of the classic <a href=\"http://en.wikipedia.org/wiki/Prisoner's_dilemma\">Prisoner's Dilemma</a> which I refer to as the <em>token trade</em>. There are two players in two separate rooms, one red and one green. The red player starts with the green token, and vice versa. Each must decide (in isolation, without communication) whether or not to give their token to me, in which case I will give it to the other player.</p>\n<p>Afterwards, they may cash their token out. The red player gets $200 for cashing out the red token and $100 for the green token, and vice versa. The payoff table looks like this:</p>\n<table style=\"border-collapse: collapse; margin-top: 2em; margin-bottom: 2em;\" border=\"0\" cellpadding=\"10\" align=\"center\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td style=\"color:red; text-align: center; border-right: 1px solid black; border-left: 1px solid black;\">Give</td>\n<td style=\"color:red; text-align: center; border-right: 1px solid black;\">Keep</td>\n</tr>\n<tr>\n<td style=\"color:green; text-align: center; border-top: 1px solid black;\">Give</td>\n<td style=\"border: 1px solid black; text-align: center;\">( <span style=\"color:green\">$200</span>, <span style=\"color:red\">$200</span> )</td>\n<td style=\"border: 1px solid black; text-align: center;\">( <span style=\"color:green\">$0</span>, <span style=\"color:red\">$300</span> )</td>\n</tr>\n<tr>\n<td style=\"color:green; text-align: center; border-top: 1px solid black; border-bottom: 1px solid black;\">Keep</td>\n<td style=\"border: 1px solid black; text-align: center;\">( <span style=\"color:green\">$300</span>, <span style=\"color:red\">$0</span> )</td>\n<td style=\"border: 1px solid black; text-align: center;\">( <span style=\"color:green\">$100</span>, <span style=\"color:red\">$100</span> )</td>\n</tr>\n</tbody>\n</table>\n<p>For example, if the green player gives the red token away, and the red player keeps the green token, then the red player gets $300 while the green player gets nothing.</p>\n<p>Now imagine a causal decision theorist facing this scenario. Their causal graph might look something like this:</p>\n<p style=\"text-align: center;\"><img src=\"http://mindingourway.com/content/images/2014/Sep/token-trade.png\" alt=\"\" width=\"732\" height=\"525\"></p>\n<p>Let's evaluate this using CDT. The action node is <code>Give?</code>, the payoff node is <code>$</code>. We must evaluate the expectation of <code>$</code> given <code>Give?</code>=<em>yes</em> and <code>Give?</code>=<em>no</em>. This, of course, depends upon the expected value of <code>TheirDecision</code>.</p>\n<p>In Game Theory, we usually assume that the opponent is reasoning using something like CDT. Then we can reason about <code>TheirDecision</code> given that they are doing similar reasoning about our decision and so on. This threatens to lead to infinite regress, but in fact there are some tricks you can use to guarantee at least one equilibrium. (These are the famous <a href=\"http://en.wikipedia.org/wiki/Nash_equilibrium\">Nash equilibria</a>.) This sort of reasoning requires some modifications to the simple CDT procedure given above which we're going to ignore today. Because while <em>most</em> scenarios with multiple agents require more complicated reasoning, the token trade is an especially simple scenario where we can brush all that under the rug.</p>\n<p>In the token trade, the expected value of <code>TheirDecision</code> doesn't matter to a CDT agent. No matter what the probability p of <code>TheirDecision</code>=<em>give</em> happens to be, the CDT agent will do the following reasoning:</p>\n<ul>\n<li>Change <code>Give?</code> to be a constant function returning <em>yes</em> \n<ul>\n<li>If <code>TheirDecision</code>=<em>give</em> then we get $200</li>\n<li>If <code>TheirDecision</code>=<em>keep</em> then we get $0</li>\n<li>We get 200p dollars in expectation.</li>\n</ul>\n</li>\n<li>Change <code>Give?</code> to be a constant function returning <em>no</em> \n<ul>\n<li>If <code>TheirDecision</code>=<em>give</em> then we get $300</li>\n<li>If <code>TheirDecision</code>=<em>keep</em> then we get $100</li>\n<li>We get 300p + 100(1-p) dollars in expectation.</li>\n</ul>\n</li>\n</ul>\n<p>Obviously, 300p+100(1-p) will be larger than 200p, <em>no matter what probability p is</em>.</p>\n<p>A CDT agent in the token trade must have an expectation about <code>TheirDecision</code> captured by a probability p that they will give their token, and we have just shown that no matter what that p is, the CDT agent will keep their token.</p>\n<p>When something like this occurs (where <code>Give?</code>=<em>no</em> is better regardless of the value of <code>TheirDecision</code>) we say that <code>Give?</code>=<em>no</em> is a \"dominant strategy\". CDT executes this dominant strategy, and keeps its token.</p>\n<h1 id=\"6\">6</h1>\n<p>Of course, this means that each player will get $100, when they could have both recieved $200. This may seem unsatisfactory. Both players would agree that they could do better by trading tokens. Why don't they coordinate?</p>\n<p>The classic response is that the token trade (better known as the Prisoner's Dilemma) is a game that explicitly disallows coordination. If players do have an opportunity to coordinate (or even if they expect to play the game mulitple times) then they can (and do!) do better than this.</p>\n<p>I won't object much here, except to note that this answer is still unsatisfactory. CDT agents fail to cooperate on a one-shot Prisoner's Dilemma. That's a bullet that causal decision theorists willingly bite, but don't forget that it's still a bullet.</p>\n<h1 id=\"7\">7</h1>\n<p>Failure to cooperate on the one-shot Prisoner's Dilemma is not necessarily a problem. Indeed, if you ever find yourself playing a token trade against an opponent using CDT, then you had better hold on to your token, because they surely aren't going to give you theirs.</p>\n<p>However, CDT <em>does</em> fail on a very similar problem where it seems insane to fail. CDT fails at the token trade <em>even when it knows it is playing against a perfect copy of itself.</em></p>\n<p>I call this the \"mirror token trade\", and it works as follows: first, I clone you. Then, I make you play a token trade against yourself.</p>\n<p>In this case, your opponent is guaranteed to pick exactly the same action that you pick. (Well, mostly: the game isn't completely symmetric. If you want to nitpick, consider that instead of playing against a copy of yourself, you must write a red/green colorblind deterministic computer program which will play against a copy of itself.)</p>\n<p>The causal graph for this game looks like this:</p>\n<p style=\"text-align: center;\"><img src=\"http://mindingourway.com/content/images/2014/Sep/token-mirror.png\" alt=\"\" width=\"733\" height=\"554\"></p>\n<p>Because I've swept questions of determinism and asymmetry under the rug, both decisions will be identical. The red copy should trade its token, because that's guaranteed to get it the green token (and it's the only way to do so).</p>\n<p>Yet CDT would have you evaluate an action by considering what happens if you replace the node <code>Give?</code> with a function that always returns that action. But this intervention does not affect the opponent, which reasons the same way! Just as before, a CDT agent treats <code>TheirDecision</code> as if it has some probability of being <em>give</em> that is independent from the agent's action, and reasons that \"I always keep my token while they act independently\" dominates \"I always give my token while they act independently\".</p>\n<p>Do you see the problem here? CDT is evaluating its action by changing the value of its action node <code>Give?</code>, assuming that this only affects things that are <em>caused</em> by <code>Give?</code>. The agent reasons counterfactually by considering \"what if <code>Give?</code> were a constant function that always returned <em>yes</em>?\" while failing to note that overwriting <code>Give?</code> in this way neglects the fact that <code>Give?</code> and <code>TheirDecision</code> are necessarily equal.</p>\n<p>Or, to put it another way, CDT evaluates counterfactuals <em>assuming</em> that all nodes uncaused by its action are independent of its action. It thinks it can change its action and only look at the downstream effects. This can break down when there are acausal connections between the nodes.</p>\n<p>After the red agent has been created from the template, its decision no longer <em>causally</em> affects the decision of the green agent. But both agents will do the same thing! There is a <em>logical</em> connection, even though there is no causal connection. It is these logical connections that are ignored by causal counterfactual reasoning.</p>\n<p>This is a subtle point, but an important one: the values of <code>Give?</code> and <code>TheirDecision</code> are logically connected, but CDT's method of reasoning about counterfactuals neglects this connection.</p>\n<h1 id=\"8\">8</h1>\n<p>This is a know failure mode for causal decision theory. The mirror token trade is an example of what's known as a \"Newcomblike problem\".</p>\n<p>Decision theorists occasionally dismiss Newcomblike problems as edge cases, or as scenarios specifically designed to punish agents for being \"rational\". I disagree.</p>\n<p>And finally, eight sections in, I'm ready to articulate the original point: Newcomblike problems aren't a special case. <em>They're the norm.</em></p>\n<p>But this post has already run on for far too long, so that discussion will have to wait until next time.</p>", "sections": [{"title": "1", "anchor": "1", "level": 1}, {"title": "2", "anchor": "2", "level": 1}, {"title": "3", "anchor": "3", "level": 1}, {"title": "4", "anchor": "4", "level": 1}, {"title": "5", "anchor": "5", "level": 1}, {"title": "6", "anchor": "6", "level": 1}, {"title": "7", "anchor": "7", "level": 1}, {"title": "8", "anchor": "8", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "162 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 162, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-14T02:25:26.228Z", "modifiedAt": null, "url": null, "title": "Ways to improve LessWrong", "slug": "ways-to-improve-lesswrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:00.810Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamzerner", "createdAt": "2013-08-12T18:18:47.957Z", "isAdmin": false, "displayName": "adamzerner"}, "userId": "6jLdWqegNefgaabhr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cX2PjJErBc7wsMPPr/ways-to-improve-lesswrong", "pageUrlRelative": "/posts/cX2PjJErBc7wsMPPr/ways-to-improve-lesswrong", "linkUrl": "https://www.lesswrong.com/posts/cX2PjJErBc7wsMPPr/ways-to-improve-lesswrong", "postedAtFormatted": "Sunday, September 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ways%20to%20improve%20LessWrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWays%20to%20improve%20LessWrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcX2PjJErBc7wsMPPr%2Fways-to-improve-lesswrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ways%20to%20improve%20LessWrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcX2PjJErBc7wsMPPr%2Fways-to-improve-lesswrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcX2PjJErBc7wsMPPr%2Fways-to-improve-lesswrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 210, "htmlBody": "<p>I think it'd be a good idea to keep a list of the ways we'd like to see LessWrong improve, sorted by popularity. Ie. email alerts for new responses.</p>\n<p>So if you have an idea for how LessWrong could be better, post it in the comments. As people up/downvote, we'll get a sense for what the consensus opinions are.</p>\n<p>I think there's a pretty good amount to be gained by improving LessWrong.</p>\n<ul>\n<li>I think there's a lot of low-hanging fruit (like email alerts for new responses).</li>\n<li>Conversations here are actually useful and productive. Facilitating conversation should thus lead to more of these useful and productive conversations (as opposed to leading to more of an unproductive type of conversation). (Sorry, I didn't word this well; hopefully you guys know what I mean.)</li>\n<li>Perhaps something big would come out of this list (like meet-ups). Perhaps rationality hack-a-thons (whatever that means)?&nbsp;</li>\n</ul>\n<div>I'm not a good enough coder now, but once I am, I think I'd like to do what I can to make LessWrong better. I'm starting a coding bootcamp (Fullstack Academy) on Monday. By the end of it, I should at least be competent.</div>\n<p>&nbsp;</p>\n<p>Note: I say \"ways to improve\" instead of \"features\" because \"ways to improve\" is more general.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cX2PjJErBc7wsMPPr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 2.003328200320007e-06, "legacy": true, "legacyId": "27192", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 104, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-14T04:42:28.200Z", "modifiedAt": null, "url": null, "title": "A kind or reverse \"tragedy of the commons\" - any solution ideas?", "slug": "a-kind-or-reverse-tragedy-of-the-commons-any-solution-ideas", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:35.041Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D_Alex", "createdAt": "2009-07-17T08:21:38.505Z", "isAdmin": false, "displayName": "D_Alex"}, "userId": "Sriopfkdwx2qJBx4G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ansAkTazXHPn7Y3ek/a-kind-or-reverse-tragedy-of-the-commons-any-solution-ideas", "pageUrlRelative": "/posts/ansAkTazXHPn7Y3ek/a-kind-or-reverse-tragedy-of-the-commons-any-solution-ideas", "linkUrl": "https://www.lesswrong.com/posts/ansAkTazXHPn7Y3ek/a-kind-or-reverse-tragedy-of-the-commons-any-solution-ideas", "postedAtFormatted": "Sunday, September 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20kind%20or%20reverse%20%22tragedy%20of%20the%20commons%22%20-%20any%20solution%20ideas%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20kind%20or%20reverse%20%22tragedy%20of%20the%20commons%22%20-%20any%20solution%20ideas%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FansAkTazXHPn7Y3ek%2Fa-kind-or-reverse-tragedy-of-the-commons-any-solution-ideas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20kind%20or%20reverse%20%22tragedy%20of%20the%20commons%22%20-%20any%20solution%20ideas%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FansAkTazXHPn7Y3ek%2Fa-kind-or-reverse-tragedy-of-the-commons-any-solution-ideas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FansAkTazXHPn7Y3ek%2Fa-kind-or-reverse-tragedy-of-the-commons-any-solution-ideas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 134, "htmlBody": "<p>I have recently come across a very practical example of a kind of \"tragedy of the commons\" - the unwillingness to invest in assets that benefit stakeholders indiscriminately. Specifically, on large strata-title apartment projects there is a reluctance to implement such measures as:</p>\n<p>- central hot water heating (~ 10% lower all-up costs, ~20% lower operating costs)</p>\n<p>- Solar hot water heating (&gt;20% ROI)</p>\n<p>- Solar electric power (~10% ROI)</p>\n<p>UNLESS some kind of user-pays system is implemented, which would use up pretty much all of the gains.</p>\n<p>&nbsp;</p>\n<p>The concern is of course that providing the above systems would create a \"commons\" that would tend to be exploited.</p>\n<p>&nbsp;</p>\n<p>I am curious if there are any ideas on a usable solutions, perhaps some kind of workable protocol that would enable the above, or existing success stories - what made them work?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ansAkTazXHPn7Y3ek", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 2.00357432115261e-06, "legacy": true, "legacyId": "27194", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-14T07:40:00.839Z", "modifiedAt": null, "url": null, "title": "Should people be writing more or fewer LW posts?", "slug": "should-people-be-writing-more-or-fewer-lw-posts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:35.205Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oabpHeFKMJB6dxoEH/should-people-be-writing-more-or-fewer-lw-posts", "pageUrlRelative": "/posts/oabpHeFKMJB6dxoEH/should-people-be-writing-more-or-fewer-lw-posts", "linkUrl": "https://www.lesswrong.com/posts/oabpHeFKMJB6dxoEH/should-people-be-writing-more-or-fewer-lw-posts", "postedAtFormatted": "Sunday, September 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20people%20be%20writing%20more%20or%20fewer%20LW%20posts%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20people%20be%20writing%20more%20or%20fewer%20LW%20posts%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoabpHeFKMJB6dxoEH%2Fshould-people-be-writing-more-or-fewer-lw-posts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20people%20be%20writing%20more%20or%20fewer%20LW%20posts%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoabpHeFKMJB6dxoEH%2Fshould-people-be-writing-more-or-fewer-lw-posts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoabpHeFKMJB6dxoEH%2Fshould-people-be-writing-more-or-fewer-lw-posts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 701, "htmlBody": "<p>It's unlikely that by pure chance we are currently writing the correct number of LW posts. &nbsp;So it might be useful to try to figure out if we're currently writing too few or too many LW posts. &nbsp;If commenters are evenly divided on this question then we're probably close to the optimal number; otherwise we have an opportunity to improve. &nbsp;Here's my case for why we should be writing more posts.</p>\n<p>Let's say you came up with a new and useful life hack, you have a novel line of argument on an important topic, or you stumbled across some academic research that seems valuable and isn't frequently discussed on Less Wrong. &nbsp;How valuable would it be for you to share your findings by writing up at post for Less Wrong?</p>\n<p>Recently I visited a friend of mine and commented on the extremely bright lights he had in his room. &nbsp;He referenced <a href=\"/lw/gdl/my_simple_hack_for_increased_alertness_and/\">this</a> LW post written over a year ago. &nbsp;That got me thinking. &nbsp;The bright lights in my friend's room make his life better every day, for a small upfront cost. &nbsp;And my friend is probably just one of tens or hundreds of people to use bright lights this way as a result of that post. &nbsp;Given that the technique seems to be effective, that number will probably continue going up, and will grow exponentially via word of mouth (useful memes tend to spread). &nbsp;So by my reckoning, chaosmage has created and will create a lot of utility. &nbsp;If they had kept that idea to themselves, I suspect they would have captured less than 1% of the total value to be had from the idea.</p>\n<p>You can reach orders of magnitude more people writing an obscure Less Wrong comment than you can talking to a few people at a party in person. &nbsp;For example, at least 100 logged in users read&nbsp;<a href=\"/lw/fao/voting_is_like_donating_thousands_of_dollars_to/7ri1\">this</a>&nbsp;fairly obscure comment of mine. &nbsp;So if you're going to discuss an important topic, it's often best to do it online. &nbsp;Given enough eyeballs, all bugs in human reasoning are shallow.</p>\n<p>Yes, peoples' time does have opportunity costs. &nbsp;But people are on Less Wrong because they need a break anyway. &nbsp;(If you're a LW addict, you might try the technique I describe in <a href=\"/lw/flr/thoughts_on_designing_policies_for_oneself/\">this</a> post for dealing with your addiction. &nbsp;If you're dealing with serious cravings, for LW or video games or drugs or anything else, perhaps look at <a href=\"http://examine.com/supplements/N-Acetylcysteine/\">N-acetylcysteine</a>... a <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3044191/#__sec8\">variety of studies</a>&nbsp;suggest it helps reduce cravings (behavioral addictions are pretty similar to drug addictions neurologically btw), it has a good safety profile, and you can buy it on Amazon. &nbsp;Not prescribed by doctors because it's <a href=\"http://slatestarcodex.com/2014/06/15/fish-now-by-prescription/\">not approved by the FDA</a>. &nbsp;Yes, you could use willpower (it's worked so well in the past...) or you could hit the \"stop craving things as much\" button, and <em>then</em> try using willpower. &nbsp;Amazing what you can learn on Less Wrong isn't it?)</p>\n<p>And LW does a good job of indexing content by how much utility people are going to get out of it. &nbsp;It's easy to look at a post's keywords and score and guess if it's worth reading. &nbsp;If your post is bad it will vanish in to obscurity and few will be significantly harmed. &nbsp;(Unless it's bad and inflammatory, or bad with a linkbait title... please don't write posts like that.) &nbsp;If your post is good, it will spread virally on its own and you'll generate untold utility.</p>\n<p>Given that above-average posts get read much more than below-average posts, if you're post's expected quality is average, sharing it on Less Wrong has a high positive expected utility. &nbsp;<a href=\"http://paulgraham.com/swan.html\">Like Paul Graham</a>, I think we should be spreading our net wide and trying to capture all of the winners we can.</p>\n<p>I'm going to call out a particular subset of LW commenters in particular. &nbsp;If you're a commenter and you (a) have at least 100 karma, (b) it's over 80% positive, and (c) you have a draft post with valuable new ideas you've been sitting on for a while, you should totally polish it off and share it with us! &nbsp;In general, the better your track record, the more you should be inclined to share ideas that seem valuable. &nbsp;Worst case you can delete your post and cut your losses.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oabpHeFKMJB6dxoEH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 20, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "27195", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ag7oQifJQM5AnMCrR", "aNRYQFnMQbA7uu99u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-14T20:00:09.849Z", "modifiedAt": null, "url": null, "title": "Proposal: Use logical depth relative to human history as objective function for superintelligence", "slug": "proposal-use-logical-depth-relative-to-human-history-as", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:38.280Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sbenthall", "createdAt": "2012-12-23T03:31:10.842Z", "isAdmin": false, "displayName": "sbenthall"}, "userId": "pbnv8yAoxSjxvEZr8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XeMB6F4wB2rxNXjXF/proposal-use-logical-depth-relative-to-human-history-as", "pageUrlRelative": "/posts/XeMB6F4wB2rxNXjXF/proposal-use-logical-depth-relative-to-human-history-as", "linkUrl": "https://www.lesswrong.com/posts/XeMB6F4wB2rxNXjXF/proposal-use-logical-depth-relative-to-human-history-as", "postedAtFormatted": "Sunday, September 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proposal%3A%20Use%20logical%20depth%20relative%20to%20human%20history%20as%20objective%20function%20for%20superintelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProposal%3A%20Use%20logical%20depth%20relative%20to%20human%20history%20as%20objective%20function%20for%20superintelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXeMB6F4wB2rxNXjXF%2Fproposal-use-logical-depth-relative-to-human-history-as%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proposal%3A%20Use%20logical%20depth%20relative%20to%20human%20history%20as%20objective%20function%20for%20superintelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXeMB6F4wB2rxNXjXF%2Fproposal-use-logical-depth-relative-to-human-history-as", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXeMB6F4wB2rxNXjXF%2Fproposal-use-logical-depth-relative-to-human-history-as", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 972, "htmlBody": "<p>I attended Nick Bostrom's talk at UC Berkeley last Friday and got intrigued by these problems again. I wanted to pitch an idea here, with the question: Have any of you seen work along these lines before? Can you recommend any papers or posts? Are you interested in collaborating on this angle in further depth?</p>\n<p>The problem I'm thinking about (surely naively, relative to y'all) is: What would you want to program an omnipotent machine to optimize?</p>\n<p>For the sake of avoiding some baggage, I'm not going to assume this machine is \"superintelligent\" or an AGI. Rather, I'm going to call it a <em>super<a href=\"http://en.wikipedia.org/wiki/Control_theory\">controller</a></em>, just something omnipotently effective at optimizing some function of what it perceives in its environment.</p>\n<p>As has been noted in other arguments, a supercontroller that optimizes the number of paperclips in the universe would be a disaster. Maybe any supercontroller that was insensitive to human values would be a disaster. What constitutes a disaster? An end of human history. If we're all killed and our memories wiped out to make more efficient paperclip-making machines, then it's as if we never existed. That is <em>existential</em> risk.</p>\n<p>The challenge is: how can one formulate an abstract objective function that would preserve human history and its evolving continuity?</p>\n<p>I'd like to propose an answer that depends on the notion of <a href=\"http://researcher.ibm.com/researcher/files/us-bennetc/UTMX.pdf\">logical depth</a> as proposed by C.H. Bennett and outlined in section 7.7 of Li and Vitanyi's <em>An Introduction to Kolmogorov Complexity and Its Applications</em> which I'm sure many of you have handy. Logical depth is a super fascinating complexity measure that Li and Vitanyi summarize thusly:</p>\n<blockquote>\n<p>Logical depth is the necessary number of steps in the deductive or causal path connecting an object with its plausible origin. Formally, it is the time required by a universal computer to compute the object from its compressed original description.</p>\n</blockquote>\n<p>The mathematics is fascinating and better read in the original Bennett paper than here. Suffice it presently to summarize some of its interesting properties, for the sake of intuition.</p>\n<ul>\n<li>\"Plausible origins\" here are incompressible, i.e. algorithmically random.</li>\n<li>As a first pass, the depth <em>D(x) </em>of a string x is the least amount of time it takes to output the string from an incompressible program.</li>\n<li>There's a free parameter that has to do with precision that I won't get into here.&nbsp;</li>\n<li>Both a string of length <em>n</em> that is comprised entirely of 1's, and a string of length <em>n</em> of independent random bits are both shallow. The first is shallow because it can be produced by a constant-sized program in time <em>n. </em>The second is shallow because there exists an incompressible program that is the output string plus a constant sized print function that produces the output in time <em>n</em>.</li>\n<li>An example of a deeper string is the string of length <em>n</em>&nbsp;that for each digit <em>i</em>&nbsp;encodes the answer to the <em>i</em>th enumerated <a href=\"http://en.wikipedia.org/wiki/Boolean_satisfiability_problem\">satisfiability</a> problem. Very deep strings can involve diagonalization.</li>\n<li>Like Kolmogorov complexity, there is an absolute and a relative version. Let&nbsp;<em>D(x/w) </em>be the least time it takes to output <em>x</em> from a program that is incompressible relative to<em> w</em>,</li>\n</ul>\n<div>That's logical depth. Here is the conceptual leap to history-preserving objective functions. Suppose you have a digital representation of all of human society at some time step t, calling this <em>h<sub>t</sub>. </em>And suppose you have some representation of the future state of the universe <em>u</em> that you want to build an objective function around. What's important, I posit, is the preservation of the logical depth of human history in its computational continuation in the future.</div>\n<div><br /></div>\n<div>We have a tension between two values. First, we want there to be an interesting, evolving future. We would perhaps like to optimize <em>D(u)</em>.</div>\n<div><br /></div>\n<div>However, we want that future to be <em>our</em> future. If the supercontroller maximizes logical depth by chopping all the humans up and turning them into better computers and erasing everything we've accomplished as a species, that would be sad. However, if the supercontroller takes human history as an input and then expands on it, that's much better. <em>D(u/h<sub>t</sub>) </em>is the logical depth of the universe as computed by a machine that takes human history at time slice <em>t</em> as input.</div>\n<div><br /></div>\n<div>Working on intuitions here--and your mileage may vary, so bear with me--I think we are interested in deep futures and especially those futures that are deep with respect to human progress so far. As a conjecture, I submit that those will be futures most shaped by human will.</div>\n<div><br /></div>\n<div>So, here's my proposed objective for the supercontroller, as a function of the state of the universe. The objective is to maximize:</div>\n<div><br /></div>\n<div style=\"padding-left: 30px;\"><em>f(u) = D(u/h<sub>t</sub>) / D(u)</em></div>\n<div style=\"padding-left: 30px;\"><br /></div>\n<div>I've been rather fast and loose here and expect there to be serious problems with this formulation. I invite your feedback! I'd like to conclude by noting some properties of this function:</div>\n<div>\n<ul>\n<li>It can be updated with observed progress in human history at time <em>t' </em>by replacing&nbsp;<em>h</em><sub style=\"font-style: italic;\">t </sub>with<sub style=\"font-style: italic;\">&nbsp;</sub><em>h<sub>t'</sub>. </em>You could imagine generalizing this to something that dynamically updated in real time.</li>\n<li>This is a quite conservative function, in that it severely punishes computation that does not depend on human history for its input. It is so conservative that it might result in, just to throw it out there, unnecessary militancy against extra-terrestrial life.</li>\n<li>There are lots of devils in the details. The precision parameter I glossed over. The problem of representing human history and the state of the universe. The incomputability of logical depth (of course it's incomputable!). My purpose here is to contribute to the formal framework for modeling these kinds of problems. The difficult work, like in most machine learning problems, becomes feature representation, sensing, and efficient convergence on the objective.</li>\n</ul>\n<div>Thank you for your interest.</div>\n</div>\n<div><br /></div>\n<div>Sebastian Benthall</div>\n<div>PhD Candidate</div>\n<div>UC Berkeley School of Information</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XeMB6F4wB2rxNXjXF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 10, "extendedScore": null, "score": 2.0052222639433417e-06, "legacy": true, "legacyId": "27198", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-14T23:07:34.033Z", "modifiedAt": null, "url": null, "title": "[Link] Learning how to exert self-control", "slug": "link-learning-how-to-exert-self-control", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:22.724Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pinyaka", "createdAt": "2012-09-21T12:11:45.980Z", "isAdmin": false, "displayName": "pinyaka"}, "userId": "FscpDmNcKZdbeDNZ2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xRqTbWxJu6F6hnyAh/link-learning-how-to-exert-self-control", "pageUrlRelative": "/posts/xRqTbWxJu6F6hnyAh/link-learning-how-to-exert-self-control", "linkUrl": "https://www.lesswrong.com/posts/xRqTbWxJu6F6hnyAh/link-learning-how-to-exert-self-control", "postedAtFormatted": "Sunday, September 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Learning%20how%20to%20exert%20self-control&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Learning%20how%20to%20exert%20self-control%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxRqTbWxJu6F6hnyAh%2Flink-learning-how-to-exert-self-control%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Learning%20how%20to%20exert%20self-control%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxRqTbWxJu6F6hnyAh%2Flink-learning-how-to-exert-self-control", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxRqTbWxJu6F6hnyAh%2Flink-learning-how-to-exert-self-control", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<p><a href=\"http://www.nytimes.com/2014/09/14/opinion/sunday/learning-self-control.html?_r=0\" target=\"_blank\">Here's a link to a short op-ed about some tips to develop self-control</a>. The author get them from talking with Walter Mischel, a researcher who correlated impulsiveness as a child (measured by ability to delay eating sweets) and various metrics as an adult (education attainment/cocaine use/weight). Mischel has a new book coming out, but this is not a review of the book. I thought this might be of interest because it talks a little about how self-control is a skill that can be developed and even gave some specific things to do.</p>\n<p>1. If possible remove unhelpful triggers from your environment. If not possible, try to reduce the emotional appeal of the trigger by mentally associating it with something unpleasant. One example he gives is imagining a cockroach crawling on the chocolate mousse that a server at a restaurant offers.</p>\n<p>2. Develop specific if-then plans such as \"if it is before noon, I won't check email\" or \"If I feel angry, I will count backward from ten.\" The goal of these kinds of checks is to introduce a delay between impulse and action during which you are reminded of your goal and have a chance to consider the impact of following the impulse on that goal.</p>\n<p>3. Link the behavior that you want to modify to a \"burning goal\" so that you have emotional impetus to actually make the desired change.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xRqTbWxJu6F6hnyAh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 2.005561112393175e-06, "legacy": true, "legacyId": "27199", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-14T23:33:57.171Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA\u2014The Hierarchy of Requests", "slug": "meetup-west-la-the-hierarchy-of-requests", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PQNNtjsN4SubPYQvX/meetup-west-la-the-hierarchy-of-requests", "pageUrlRelative": "/posts/PQNNtjsN4SubPYQvX/meetup-west-la-the-hierarchy-of-requests", "linkUrl": "https://www.lesswrong.com/posts/PQNNtjsN4SubPYQvX/meetup-west-la-the-hierarchy-of-requests", "postedAtFormatted": "Sunday, September 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%E2%80%94The%20Hierarchy%20of%20Requests&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%E2%80%94The%20Hierarchy%20of%20Requests%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPQNNtjsN4SubPYQvX%2Fmeetup-west-la-the-hierarchy-of-requests%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%E2%80%94The%20Hierarchy%20of%20Requests%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPQNNtjsN4SubPYQvX%2Fmeetup-west-la-the-hierarchy-of-requests", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPQNNtjsN4SubPYQvX%2Fmeetup-west-la-the-hierarchy-of-requests", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14g'>West LA\u2014The Hierarchy of Requests</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 September 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to Find Us</strong>: Go into <a href=\"https://www.google.com/maps/place/Del+Taco/@34.047464,-118.443286,974m/data=!3m2!1e3!4b1!4m2!3m1!1s0x80c2bb777277de93:0x99f58a53b04bb89c?hl=en\" rel=\"nofollow\">this</a> Del Taco. We will be in the back room if possible.</p>\n\n<p><strong>Parking</strong> is free in the lot out front or on the street nearby.</p>\n\n<p><strong>Discussion</strong>: The hierarchy of requests is a useful way of thinking about many situations. I will present it and emphasize certain things.</p>\n\n<p><strong>Recommended Reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://celandine13.livejournal.com/72711.html\" rel=\"nofollow\">The Hierarchy of Requests</a></li>\n</ul>\n\n<p><em>No prior exposure to Less Wrong is required</em>; this will be generally accessible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14g'>West LA\u2014The Hierarchy of Requests</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PQNNtjsN4SubPYQvX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.005608592193551e-06, "legacy": true, "legacyId": "27200", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_The_Hierarchy_of_Requests\">Discussion article for the meetup : <a href=\"/meetups/14g\">West LA\u2014The Hierarchy of Requests</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 September 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to Find Us</strong>: Go into <a href=\"https://www.google.com/maps/place/Del+Taco/@34.047464,-118.443286,974m/data=!3m2!1e3!4b1!4m2!3m1!1s0x80c2bb777277de93:0x99f58a53b04bb89c?hl=en\" rel=\"nofollow\">this</a> Del Taco. We will be in the back room if possible.</p>\n\n<p><strong>Parking</strong> is free in the lot out front or on the street nearby.</p>\n\n<p><strong>Discussion</strong>: The hierarchy of requests is a useful way of thinking about many situations. I will present it and emphasize certain things.</p>\n\n<p><strong>Recommended Reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://celandine13.livejournal.com/72711.html\" rel=\"nofollow\">The Hierarchy of Requests</a></li>\n</ul>\n\n<p><em>No prior exposure to Less Wrong is required</em>; this will be generally accessible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_The_Hierarchy_of_Requests1\">Discussion article for the meetup : <a href=\"/meetups/14g\">West LA\u2014The Hierarchy of Requests</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA\u2014The Hierarchy of Requests", "anchor": "Discussion_article_for_the_meetup___West_LA_The_Hierarchy_of_Requests", "level": 1}, {"title": "Discussion article for the meetup : West LA\u2014The Hierarchy of Requests", "anchor": "Discussion_article_for_the_meetup___West_LA_The_Hierarchy_of_Requests1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-15T01:47:58.425Z", "modifiedAt": null, "url": null, "title": "Meetup : Canberra: More rationalist fun and games!", "slug": "meetup-canberra-more-rationalist-fun-and-games", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielFilan", "createdAt": "2014-01-30T11:04:39.341Z", "isAdmin": false, "displayName": "DanielFilan"}, "userId": "DgsGzjyBXN8XSK22q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/smCXoGAPY6DZprNCr/meetup-canberra-more-rationalist-fun-and-games", "pageUrlRelative": "/posts/smCXoGAPY6DZprNCr/meetup-canberra-more-rationalist-fun-and-games", "linkUrl": "https://www.lesswrong.com/posts/smCXoGAPY6DZprNCr/meetup-canberra-more-rationalist-fun-and-games", "postedAtFormatted": "Monday, September 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Canberra%3A%20More%20rationalist%20fun%20and%20games!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Canberra%3A%20More%20rationalist%20fun%20and%20games!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsmCXoGAPY6DZprNCr%2Fmeetup-canberra-more-rationalist-fun-and-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Canberra%3A%20More%20rationalist%20fun%20and%20games!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsmCXoGAPY6DZprNCr%2Fmeetup-canberra-more-rationalist-fun-and-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsmCXoGAPY6DZprNCr%2Fmeetup-canberra-more-rationalist-fun-and-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 91, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14h'>Canberra: More rationalist fun and games!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 September 2014 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">108 North Road, Acton, ACT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be practising our practical rationality skills by playing (some variant of) <a href=\"http://en.wikipedia.org/wiki/Liar%27s_dice\" rel=\"nofollow\">Liar&#39;s Dice</a> and <a href=\"http://en.wikipedia.org/wiki/Mafia_%28party_game%29\" rel=\"nofollow\">Mafia</a>. As always, vegan snacks will be provided.</p>\n\n<p>General meetup info:</p>\n\n<p>If you use Facebook, please join our <a href=\"https://www.facebook.com/groups/lwcanberra/\" rel=\"nofollow\">group</a>.</p>\n\n<p>Structured meetups are held on the second Saturday and fourth Friday of each month from 6 pm until late in the CSIT building, room N101.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14h'>Canberra: More rationalist fun and games!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "smCXoGAPY6DZprNCr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.0058497878076093e-06, "legacy": true, "legacyId": "27201", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Canberra__More_rationalist_fun_and_games_\">Discussion article for the meetup : <a href=\"/meetups/14h\">Canberra: More rationalist fun and games!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 September 2014 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">108 North Road, Acton, ACT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be practising our practical rationality skills by playing (some variant of) <a href=\"http://en.wikipedia.org/wiki/Liar%27s_dice\" rel=\"nofollow\">Liar's Dice</a> and <a href=\"http://en.wikipedia.org/wiki/Mafia_%28party_game%29\" rel=\"nofollow\">Mafia</a>. As always, vegan snacks will be provided.</p>\n\n<p>General meetup info:</p>\n\n<p>If you use Facebook, please join our <a href=\"https://www.facebook.com/groups/lwcanberra/\" rel=\"nofollow\">group</a>.</p>\n\n<p>Structured meetups are held on the second Saturday and fourth Friday of each month from 6 pm until late in the CSIT building, room N101.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Canberra__More_rationalist_fun_and_games_1\">Discussion article for the meetup : <a href=\"/meetups/14h\">Canberra: More rationalist fun and games!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Canberra: More rationalist fun and games!", "anchor": "Discussion_article_for_the_meetup___Canberra__More_rationalist_fun_and_games_", "level": 1}, {"title": "Discussion article for the meetup : Canberra: More rationalist fun and games!", "anchor": "Discussion_article_for_the_meetup___Canberra__More_rationalist_fun_and_games_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-15T02:36:10.377Z", "modifiedAt": null, "url": null, "title": "A Guide to Rational Investing", "slug": "a-guide-to-rational-investing", "viewCount": null, "lastCommentedAt": "2018-05-13T14:08:52.523Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ColbyDavis", "createdAt": "2014-08-30T15:53:17.445Z", "isAdmin": false, "displayName": "ColbyDavis"}, "userId": "Zd6LS4pfPkuJeTFPa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4bBAqd4EEBKLBwNuF/a-guide-to-rational-investing", "pageUrlRelative": "/posts/4bBAqd4EEBKLBwNuF/a-guide-to-rational-investing", "linkUrl": "https://www.lesswrong.com/posts/4bBAqd4EEBKLBwNuF/a-guide-to-rational-investing", "postedAtFormatted": "Monday, September 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Guide%20to%20Rational%20Investing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Guide%20to%20Rational%20Investing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4bBAqd4EEBKLBwNuF%2Fa-guide-to-rational-investing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Guide%20to%20Rational%20Investing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4bBAqd4EEBKLBwNuF%2Fa-guide-to-rational-investing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4bBAqd4EEBKLBwNuF%2Fa-guide-to-rational-investing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 8720, "htmlBody": "<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\">Hello Less Wrong, I don't post here much but I've been involved in the Bay Area Less Wrong community for several years, where many of you know me from. The following is a white paper I wrote earlier this year for my firm, RHS Financial, a San Francisco based private wealth management practice. A few months ago I presented it at a South Bay Less Wrong meetup. Since then many of you have encouraged me to post it here for the rest of the community to see. The original can be found <a href=\"http://media.wix.com/ugd/16d508_46b54eb228db4b19b8eb9a98c2e7a0b3.pdf\" target=\"_blank\">here</a>, please refer to the disclosures, especially if you are the SEC. I have added an afterword here beneath the citations to address some criticisms I have encountered since writing it. As a company white paper intended for a general audience, please forgive me if the following is a little too self-promoting or spends too much time on grounds already well-tread here, but I think many of you will find it of value. Hope you enjoy!</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin: 0pt -9pt; text-align: justify; padding-left: 30px;\" dir=\"ltr\"><span id=\"docs-internal-guid-c6c0f3c7-7716-8e81-aaee-abae2f7e9676\"><span style=\"font-size: 15px; font-family: Arial; font-weight: bold; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">Executive Summary: </span><span style=\"font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">Capital markets have created enormous amounts of wealth for the world and reward disciplined, long-term investors for their contribution to the productive capacity of the economy. Most individuals would do well to invest most of their wealth in the capital market assets, particularly equities. Most investors, however, consistently make poor investment decisions as a result of a poor theoretical understanding of financial markets as well as cognitive and emotional biases, leading to inferior investment returns and inefficient allocation of capital. Using an empirically rigorous approach, a rational investor may reasonably expect to exploit inefficiencies in the market and earn excess returns in so doing.</span></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Most people understand that they need to save money for their future, and surveys consistently find a large majority of Americans expressing a desire to save and invest more than they currently are. Yet the savings rate and percentage of people who report owning stocks has trended down in recent years,</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>1</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> despite the increasing ease with which individuals can participate in financial markets, thanks to the spread of discount brokers and employer 401(k) plans. Part of the reason for this is likely the unrealistically pessimistic expectations of would-be investors. According to a recent poll barely one third of Americans consider equities to be a good way to build wealth over time.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>2</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> The verdict of history, however, is against the skeptics.</span></p>\n<p><strong id=\"docs-internal-guid-c6c0f3c7-7582-d0b4-fcd8-fcf2223c31ef\" style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The Greatest Deal of all Time</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Equity ownership is probably the easiest, most powerful means of accumulating wealth over time, and people regularly forego millions of dollars over the course of their lifetimes letting their wealth sit in cash. Since its inception in 1926, the annualized total return on the S&amp;P 500 has been 9.8% as of the end of 2012.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>3</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> $1 invested back then would be worth $3,533 by the end of the period. More saliently, a 25 year old investor investing $5,000 per year at that rate would have about $2.1 million upon retirement at 65.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The strong performance of stock markets is robust to different times and places. Though the most accurate data on the US stock market goes back to 1926, financial historians have gathered information going back to 1802 and find the average annualized real return in earlier periods is remarkably close to the more recent official records. Looking at rolling 30 year returns between 1802 and 2006, the lowest and highest annualized real returns have been 2.6% and 10.6%, respectively.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>4</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> The United States is not unique in its experience, either. In a massive study of the sixteen countries that had data on local stock, bond, and cash returns available for every year of the twentieth century, the stock market in every one had significant, positive real returns that exceeded those of cash and fixed income alternatives.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>5</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> The historical returns of US stocks only slightly exceed those of the global average.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The opportunity cost of not holding stocks is enormous. Historically the interest earned on cash equivalent investments like savings accounts has barely kept up with inflation - over the same since-1926 period inflation has averaged 3.0% while the return on 30-day treasury bills (a good proxy for bank savings rates) has been 3.5%.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>6</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> That 3.5% rate would only earn an investor $422k over the same $5k/year scenario above. The situation today is even worse. Most banks are currently paying about 0.05% on savings. </span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Similarly, investment grade bonds, such as those issued by the US Treasury and highly rated corporations, though often an important component of a diversified portfolio, have offered returns only modestly better than cash over the long run. The average return on 10-year treasury bonds has been 5.1%</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">,</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>7</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> earning an investor $619k over the same 40 year scenario. The yield on the 10-year treasury is currently about 3%.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Homeownership has long been a part of the American dream, and many have been taught that building equity in your home is the safest and most prudent way to save for the future. The fact of the matter, however, is that residential housing is more of a consumption good than an investment. Over the last century the value of houses have barely kept up with inflation,</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>8</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and as the recent mortgage crisis demonstrated, home prices can crash just as any other market.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">In virtually every time and place we look, equities are the best performing asset available, a fact which is consistent with the economic theory that risky assets must offer a premium to their investors to compensate them for the additional uncertainty they bear. What has puzzled economists for decades is why the so-called equity risk premium is so large and why so many individuals invest so little in stocks.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>9</sup></span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Your Own Worst Enemy</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Recent insights from multidisciplinary approaches in cognitive science have shed light on the issue, demonstrating that instead of rationally optimizing between various trade-offs, human beings regularly rely on heuristics - mental shortcuts that require little cognitive effort - when making decisions.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>10</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> These heuristics lead to taking biased approaches to problems that deviate from optimal decision making in systematic and predictable ways. Such biases affect financial decisions in a large number of ways, one of the most profound and pervasive being the tendency of myopic loss aversion.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Myopic loss aversion refers to the combined result of two observed regularities in the way people think: that losses feel bad to a greater extent than equivalent gains feel good, and that people rely too heavily (anchor) on recent and readily available information.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>11</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Taken together, it is easy to see how these mental errors could bias an individual against holding stocks. Though the historical and expected return on equities greatly exceeds those of bonds and cash, over short time horizons they can suffer significant losses. And while the loss of one&rsquo;s home equity is generally a nebulous abstraction that may not manifest itself consciously for years, stock market losses are highly visible, drawing attention to themselves in brokerage statements and newspaper headlines. Not surprisingly, then, an all too common pattern among investors is to start investing at a time when the headlines are replete with stories of the riches being made in markets, only to suffer a pullback and quickly sell out at ten, twenty, thirty plus percent losses and sit on cash for years until the next bull market is again near its peak in a vicious circle of capital destruction. Indeed, in the 20 year period ending 2012, the S&amp;P 500 returned 8.2% and investment grade bonds returned 6.3% annualized. The inflation rate was 2.5%, and the average retail investor earned an annualized rate of 2.3%.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>12</sup></span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Even when investors can overcome their myopic loss aversion and stay in the stock market for the long haul, investment success is far from assured. The methods by which investors choose which stocks or stock managers to buy, hold, and sell are also subject to a host of biases which consistently lead to suboptimal investing and performance. Chief among these is overconfidence, the belief that one&rsquo;s judgements and skills are reliably superior.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Overconfidence is endemic to the human experience. The vast majority of people think of themselves as more intelligent, attractive, and competent than most of their peers,</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>13</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> even in the face of proof to the contrary. 93% of people consider themselves to be above-average drivers,</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>14</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> for example, and that percentage decreases only slightly if you ask people to evaluate their driving skill after being admitted to a hospital following a traffic accident.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>15</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Similarly, most investors are confident they can consistently beat the market. One survey found 74% of mutual fund investors believed the funds they held would &ldquo;consistently beat the S&amp;P 500 every year&rdquo; in spite of the statistical reality that more than half of US stock funds underperform in a given year and virtually none will outperform it each and every year. Many investors will even report having beaten the index despite having verifiably underperformed it by several percentage points.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>16</sup></span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Overconfidence leads investors to take outsized bets on what they know and are familiar with. Investors around the world commonly hold 80% or more of their portfolios in investments from their own country,</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>17</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and one third of 401(k) assets are invested in participants&rsquo; own employer&rsquo;s stock.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>18</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Such concentrated portfolios are demonstrably riskier than a broadly diversified portfolio, yet investors regularly evaluate their investments as less risky than the general market, even if their securities had recently lost significantly more than the overall market.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">If an investor believes himself to possess superior talent in selecting investments, he is likely to trade more as a result in an attempt to capitalize on each new opportunity that presents itself. In this endeavor, the harder investors try, the worse they do. In one major study, the quintile of investors who traded the most over a five year period earned an average annualized 7.1 percentage points less than the quintile that traded the least.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>19</sup></span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The Folly of Wall Street</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Relying on experts does little to help. Wall Street employs an army of analysts to follow the every move of all the major companies traded on the market, predicting their earnings and their expected performance relative to peers, but on the whole they are about as effective as a strategy of throwing darts. Burton Malkiel explains in his book </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A Random Walk Down Wall Street </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">how he tracked the one and five year earnings forecasts on companies in the S&amp;P 500 from analysts at 19 Wall Street firms and found that in aggregate the estimates had no more predictive power than if you had just assumed a given company&rsquo;s earnings would grow at the same rate as the long-term average rate of growth in the economy. This is consistent with a much broader body of literature demonstrating that the predictions of </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">statistical prediction rules </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">- formulas that make predictions based on simple statistical rules - reliably outperform those of human experts. Statistical prediction rules have been used to predict the auction price of bordeaux better than expert wine tasters,</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">20</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> marital happiness better than marriage counselors,</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>21</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> academic performance better than admissions officers,</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>22</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> criminal recidivism better than criminologists,</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>23</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and bankruptcy better than loan officers,</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>24</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> to name just a few examples. This is an incredible finding that&rsquo;s difficult to overstate. When considering complex issues such as these our natural intuition is to trust experts who can carefully weigh all the relevant information in determining the best course of action. But in reality experts are simply humans who have had more time to reinforce their preconceived notions on a particular topic and are more likely to anchor their attention on items that only introduce statistical noise.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Back in the world of finance, It turns out that to a first approximation the best estimate on the return to expect from a given stock is the long-run historical average of the stock market, and the best estimate of the return to expect from a stock picking mutual fund is the long-run historical average of the stock market minus its fees. The active stock pickers who manage mutual funds have on the whole demonstrated little ability to outperform the market. To be sure, at any given time there are plenty of managers who have recently beaten the market smartly, and if you look around you will even find a few with records that have been terrific over ten years or more. But just as a coin-flipping contest between thousands of contestants would no doubt yield a few who had uncannily &ldquo;called it&rdquo; a dozen or more times in a row, the number of market beating mutual fund managers is no greater than what you should expect as a result of pure luck.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>25</sup></span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Expert and amatuer investors alike underestimate how competitive the capital markets are. News is readily available and quickly acted upon, and any fact you know about that you think gives you an edge is probably already a value in the cells of thousands of spreadsheets of analysts trading billions of dollars. Professor of Finance at Yale and Nobel Laureate Robert Shiller makes this point in a lecture using an example of a hypothetical drug company that announces it has received FDA approval to market a new drug:</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: 31.5pt; margin-right: 31.5pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Suppose you then, the next day, read in </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The Wall Street Journal</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> about this new announcement. Do you think you have any chance of beating the market by trading on it? I mean, you're like twenty-four hours late, but I hear people tell me &mdash; I hear, \"I read in </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Business Week</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> that there was a new announcement, so I'm thinking of buying.\" I say, \"Well, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Business Week</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &mdash; that information is probably a week old.\" Even other people will talk about trading on information that's years old, so you kind of think that maybe these people are na&iuml;ve. First of all, you're not a drug company expert or whatever it is that's needed. Secondly, you don't know the math &mdash; you don't know how to calculate present values, probably. Thirdly, you're a month late. You get the impression that a lot of people shouldn't be trying to beat the market. You might say, to a first approximation, the market has it all right so don't even try.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>26</sup></span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">In that last sentence Shiller hints at one of the most profound and powerful ideas in finance: the efficient market hypothesis. The core of the efficient market hypothesis is that when news that impacts the value of a company is released, stock prices will adjust instantly to account for the new information and bring it back to equilibrium where it&rsquo;s no longer a &ldquo;good&rdquo; or &ldquo;bad&rdquo; investment but simply a fair one for its risk level. Because news is unpredictable by definition, it is impossible then to reliably outperform the market as a whole, and the seemingly ingenious investors on the latest cover of Forbes or Fortune are simply lucky.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A Noble Lie</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">In the 50s, 60s, and 70s several economists who would go on to win Nobel prizes worked out the implications of the efficient market hypothesis and created a new intellectual framework known as modern portfolio theory.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>27</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> The upshot is that capital markets reward investors for taking risk, and the more risk you take, the higher your return should be (in </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">expectation</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, it might not turn out to be the case, which is why it&rsquo;s risky). But the market doesn&rsquo;t reward unnecessary risk, such as taking out a second mortgage to invest in your friend&rsquo;s hot dog stand. It only rewards </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">systematic</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> risk, the risks associated with being exposed to the vagaries of the entire economy, such as interest rates, inflation, and productivity growth.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>28</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Stock of small companies are riskier and have a higher expected return than stocks of large companies, which are riskier than corporate bonds, which are riskier than Treasury bonds. But owning one small cap stock doesn&rsquo;t offer a higher expected return than another small cap stock, or a portfolio of hundreds of small caps for that matter. Owning more of a particular stock merely exposes you to the idiosyncratic risks that particular company faces and for which you are not compensated. Diversifying assets across as many securities as possible, it is possible to reduce the volatility of your portfolio without lowering its expected return.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This approach to investing dictates that you should determine an acceptable level of risk for your portfolio, then buy the largest basket of securities possible that targets that risk, ideally while paying the least amount possible in fees. Academic activism in favor of this passive approach gained momentum through the 70s, culminating in the launch of the first commercially available index fund in 1976, offered by The Vanguard Group. The typical index fund seeks to replicate the overall market performance of a broad class of investments such as large US stocks by owning all the securities in that market in proportion to their market weights. Thus if XYZ stock makes up 2% of the value of the relevant asset class, the index fund will allocate 2% of its funds to that stock. Because index funds only seek to replicate the market instead of beating it, they save costs on research and management teams and pass the savings along to investors through lower fees.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Index funds were originally derided and attracted little investment, but years of passionate advocacy by popularizers such as Jack Bogle and Burton Malkiel as well as the consensus of the economics profession has helped to lift them into the mainstream. Index funds now command trillions of dollars of assets and cover every segment of the market in stocks, bonds, and alternative assets in the US and abroad. In 2003 Vanguard launched its target retirement funds, which took the logic of passive investing even further by providing a single fund that would automatically shift from more aggressive to more conservative index investments as its investors approached retirement. Target retirement funds have since become especially popular options in 401(k) plans.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The rise of index investing has been a boon to individual investors, who have clearly benefited from the lower fees and greater diversification they offer. To the extent that investors have bought into the idea of passive investing over market timing and active security selection they have collectively saved themselves a fortune by not giving in to their value-destroying biases. For all the good index funds have done though, since their birth in the 70s, the intellectual foundation upon which they stand, the efficient market hypothesis, has been all but disproved.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The EMH is now the noble lie of the economics profession; while economists usually teach their students and the public that the capital markets are efficient and unbeatable, their research over the last few decades has shown otherwise. In a telling example, Paul Samuelson, who helped originate the EMH and advocated it in his best selling textbook, was a large, early investor in Berkshire Hathaway, Warren Buffett&rsquo;s active investment holding company.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>29</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> But real people regularly ruin their lives through sloppy investing, and for them perhaps it is better just to say that beating the market can&rsquo;t be done, so just buy, hold, and forget about it. We, on the other hand, believe a more nuanced understanding of the facts can be helpful.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Premium Investing</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Shortly after the efficient market hypothesis was first put forth researchers realized the idea had serious theoretical shortcomings.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>30</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Beginning as early as 1977 they also found empirical &ldquo;anomalies,&rdquo; factors other than systematic risk that seemed to predict returns.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>31</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Most of the early findings focused on valuation ratios - measures of a firm&rsquo;s market price in relation to an accounting measure such as book value or earnings - and found that &ldquo;cheap&rdquo; stocks on average outperformed &ldquo;expensive&rdquo; stocks, confirming the value investment philosophy first promulgated by the legendary Depression-era investor Benjamin Graham and popularized by his most famous student, Warren Buffett. In 1992 Eugene Fama, one of the fathers of the efficient market hypothesis, published, along with Ken French, a groundbreaking paper demonstrating that the cheapest decile stocks in the US, as measured by the price to book ratio, outperformed the highest decile stocks by an astounding 11.9% per year, despite there being little difference in risk between them.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>32</sup></span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A year later, researchers found convincing evidence of a momentum anomaly in US stocks: stocks that had the highest performance over the last 3-12 months continued to outperform relative to those with the lowest performance. The effect size was comparable to that of the value anomaly and again the discrepancy could not be explained with any conventional measure of risk.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>33</sup></span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Since then, researchers have replicated the value and momentum effects across larger and deeper datasets, finding comparably large effect sizes in different times, regions, and asset classes. In a highly ambitious 2012 paper, Clifford Asness (a former student of Fama&rsquo;s) and Tobias Moskowitz documented the significance of value and momentum across 18 national equity markets, 10 currencies, 10 government bonds, and 27 commodity futures.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Though value and momentum are the most pervasive and best documented of the market anomalies, many others have been discovered across the capital markets. Others include the small-cap premium</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>34</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> (small company stocks tend to outperform large company stocks even </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">in excess </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">of what should be expected by their risk), the liquidity premium</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>35</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> (less frequently traded securities tend to outperform more frequently traded securities), short-term reversal</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>36</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> (equities with the lowest one-week to one-month performance tend to outperform over short time horizons), carry</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>37</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> (high-yielding currencies tend to appreciate against low-yielding currencies), roll yield</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>38,39</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> (bonds and futures at steeply negatively sloped points along the yield curve tend to outperform those at flatter or positively sloped points), profitability</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>40</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> (equities of firms with higher proportions of profits over assets or equity tend to outperform those with lower profitability), calendar effects</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>41</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> (stocks tend to have stronger returns in January and weaker returns on Mondays), and corporate action premia</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>42</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> (securities of corporations that will, currently are, or have recently engaged in mergers, acquisitions, spin-offs, and other events tend to consistently under or outperform relative to what would be expected by their risk).</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Most of these market anomalies appear remarkably robust compared to findings in other social sciences</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">,</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>43</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> especially considering that they seem to imply trillions of dollars of easy money is being overlooked in plain sight. Intelligent observers often question how such inefficiencies could possibly persist in the face of such strong incentives to exploit them until they disappear. Several explanations have been put forth, some of which are conflicting but which all probably have some explanatory power.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The first interpretation of the anomalies is to deny that they are actually anomalous, but rather are compensation for risk that isn&rsquo;t captured by the standard asset pricing models. This is the view of Eugene Fama, who first postulated that the value premium was compensation for assuming risk of financial distress and bankruptcy that was not fully captured by simply measuring the standard deviation of a value stock&rsquo;s returns.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>44</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Subsequent research, however, disproved that the value effect was explained by exposure to financial distress.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>45</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> More sophisticated arguments point to the fact that the excess returns of value, momentum, and many other premiums exhibit greater skewness, kurtosis, or other statistical moments than the broad market: subtle statistical indications of greater risk, but the differences hardly seem large enough to justify the large return premiums observed.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>46</sup></span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The only sense in which e.g. value and momentum stocks seem genuinely &ldquo;riskier&rdquo; is in career risk; though the factor premiums are significant and robust in the long term, they are not consistent or predictable along short time horizons. Reaping their rewards requires patience, and an analyst or portfolio manager who recommends an investment for his clients based on these factors may end up waiting years before it pays off, typically more than enough time to be fired.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>47</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Though any investment strategy is bound to underperform at times, strategies that seek to exploit the factors most predictive of excess returns are especially susceptible to reputational hazard. Value stocks tend to be from unpopular companies in boring, slow growth industries. Momentum stocks are often from unproven companies with uncertain prospects or are from fallen angels who have only recently experienced a turn of luck. Conversely, stocks that score low on value and momentum factors are typically reputable companies with popular products that are growing rapidly and forging new industry standards in their wake. </span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Consider then, two companies in the same industry: Ol&rsquo;Timer Industries, which has been around for decades and is consistently profitable but whose product lines are increasingly considered uncool and outdated. Recent attempts to revamp the company&rsquo;s image by the firm&rsquo;s new CEO have had modest success but consumers and industry experts expect this to be just delaying further inevitable loss of market share to NuTime.ly, founded eight years ago and posting exponential revenue growth and rapid adoption by the coveted 18-35 year old demographic, who typically describe its products using a wide selection of contemporary idioms and slang indicating superior social status and functionality. Ol&rsquo;Timer Industries&rsquo; stock will likely score highly on value on momentum factors relative to NuTime.ly and so have a higher expected return. But consider the incentives of the investment professional choosing between the two: if he chooses Ol&rsquo;Timer and it outperforms he may be congratulated and rewarded perhaps slightly more than if he had chosen NuTime.ly and it outperforms, but if he chooses Ol&rsquo;Timer and it underperforms he is a fool and a laughingstock who wasted clients&rsquo; money on his pet theory when &ldquo;everyone knew&rdquo; NuTime.ly was going to win. At least if he chooses NuTime.ly and it underperforms it was a fluke that none of his peers saw coming, save for a few wingnuts who keep yammering about the arcane theories of Gene Fama and Benjamin Graham.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">For most investors, &ldquo;it is better for reputation to fail conventionally than to succeed unconventionally&rdquo; as John Maynard Keynes observed in his </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">General Theory</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Not that this is at all restricted to investors, professional or amateur. In a similar vein, professional soccer goalkeepers continue to jump left or right on penalty kicks when statistics show they&rsquo;d block more shots standing still.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>48</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> But standing in place while the ball soars into the upper right corner makes the goalkeeper look incompetent. The proclivity of middle managers and bureaucrats to default to uncontroversial decisions formed by groupthink is familiar enough to be the stuff of popular culture; nobody ever got fired for buying IBM, as the saying goes. Psychological experiments have shown that people will often affirm an obviously false observation about simple facts such as the relative lengths of straight lines on a board if others have affirmed it before them.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>49</sup></span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We find ourselves back to the nature of human thinking and the biases and other cognitive errors that afflict it. This is what most interpretations of the market anomalies focuses on. Both amatuer and professional investors are human beings that are apt to make investment decisions not through a methodical application of modern portfolio theory but based rather on stories, anecdotes, hunches, and ideologies. Most of the anomalies make sense in light of an understanding of some of the most common biases such as anchoring and availability bias, status quo bias, and herd behavior.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>50</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Rational investors seeking to exploit these inefficiencies may be able to do so to a limited extent, but if they are using other peoples&rsquo; money then they are constrained by the biases of their clients. The more aggressively they attempt to exploit market inefficiencies, the more they risk badly underperforming the market long enough to suffer devastating withdrawals of capital.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>51</sup></span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">It is no surprise then, that the most successful investors have found ways to rely on &ldquo;sticky&rdquo; capital unlikely to slip out of their control at the worst time. Warren Buffett invests the float of his insurance company holdings, which behaves in actuarially predictable ways; David Swensen manages the Yale endowment fund, which has an explicitly indefinite time horizon and a rules based spending rate; Renaissance Technologies, arguably the most successful hedge fund ever, only invests its own money; Dimensional Fund Advisors, one of the only mutual fund companies that has consistently earned excess returns through factor premiums, only sells through independent financial advisors who undergo a due diligence process to ensure they share similar investment philosophies.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Building a Better Portfolio</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">So what is an investor to do? The prospect of delicately crafting a portfolio that&rsquo;s adequately diversified while taking advantage of return premiums may seem daunting, and one may be tempted to simply buy a Vanguard target retirement fund appropriate for their age and be done with it. Doing so is certainly a reasonable option. But we believe that with a disciplined investment strategy informed by the findings discussed above superior results are possible.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The first place to start is an assessment of your risk tolerance. How far can your portfolio fall before it adversely affects your quality of life? For investors saving for retirement with many more years of work ahead of them, the answer will likely be &ldquo;quite a lot.&rdquo; With ten years or more to work with, your portfolio will likely recover from even the most extreme bear markets. But people do not naturally think in ten-year increments, and many must live off their portfolio principal; accept that in the short term your portfolio will sometimes be in the red and consider what percentage decline over a period of a few months to a year you are comfortable enduring. Over a one year period the &ldquo;worst case scenario&rdquo; on diversified stock portfolios is historically about a 40% decline. For a traditional &ldquo;moderate&rdquo; portfolio of 60% stocks, 40% bonds it has been about a 25% decline.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>52</sup></span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">With a target on how much risk to accept in your portfolio, modern portfolio theory shows us a technique for achieving the most efficient tradeoff between risk and return possible called mean-variance optimization. An adequate treatment of MVO is beyond the scope of this paper,</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>53</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> but essentially the task is to forecast expected returns on the major asset classes (e.g. US Stocks, International Stocks, and Investment Grade Bonds) then compute the weights for each that will achieve the highest expected return for a given amount of risk. We use an approach to mean variance optimization known as the Black-Litterman model</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>54</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and estimate expected returns using a limited number of simple inputs; for example, the expected return on an index of stocks can be closely approximated using the current dividend yield plus the long run growth rate of the economy.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>55</sup></span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">With optimal portfolio weights determined, next the investor must select the investment vehicles to use to gain exposure to the various asset classes. Though traditional index funds are a reasonable option, in recent years several &ldquo;enhanced index&rdquo; mutual fund and ETFs have been released that provide inexpensive, broad exposure to the hundreds or thousands of securities in a given asset classes while enhancing exposure to one or more of the major factor premiums discussed above such as value, profitability, or momentum. Research Affiliates, for example, licences a &ldquo;fundamental index&rdquo; that has been shown to provide efficient exposure to value and small-cap stocks across many markets.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>56</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> These &ldquo;RAFI&rdquo; indexes have been licensed to the asset management firms Charles Schwab and PowerShares to be made available through mutual funds and ETFs to the general investing public, and have generally outperformed their traditional index fund counterparts since inception.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Over the course of time, portfolio allocations will drift from their optimized allocations as particular asset classes inevitably outperform relative to other ones. Leaving this unchecked can lead to a portfolio that is no longer risk-return efficient. The investor must periodically rebalance the portfolio by selling securities that have become overweight and buying others that are underweight. Research suggests that by setting &ldquo;tolerance bands&rdquo; around target asset allocations, monitoring the portfolio frequently and trading when weights drift outside tolerance, investors can take further advantage of inter-asset-class value and momentum effects and boost return while reducing risk.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>57</sup></span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Most investors, however, do not rebalance systematically, perhaps in part because it can be psychologically distressing. Rebalancing necessarily entails regularly selling assets that have been performing well in order to buy ones that have been laggards, exactly when your cognitive biases are most likely to tell you that it&rsquo;s a bad idea. Indeed, neuroscientists have observed in laboratory experiments that when individuals consider the prospect of buying more of a risky asset that has lost them money, it activates the modules in the brain associated with anticipation of physical pain and anxiety.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>58</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Dealing with investment losses is literally painful for investors.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Many investors may find it helpful to their peace of mind as well as their portfolio to outsource the entire process to a party with less emotional attachment in their portfolio. Realistically, most investors have neither the time nor the motivation necessary to attain a firm understanding of modern portfolio theory, research the capital market expectations on various asset classes and securities, and regularly monitor and rebalance their portfolio, all with enough rigor to make it worth the effort compared to a simple indexing strategy. By utilizing the skills of a good financial advisor, however, an investor can leverage the expertise of a professional with the bandwidth to execute these tactics in a cost-efficient manner.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A financial advisor should be able to engage you as an investor and acquire a firm understanding of your goals, needs, and attitudes towards risk, money, and markets. Because he or she will have an entire practice over which to efficiently dedicate time and resources on portfolio research, optimization, and trading, the financial advisor should be able to craft a portfolio that&rsquo;s optimized for your personal situation. Financial advisors, as institutional investors, generally have access to institutional class funds that retail investors do not, including many of those that have demonstrated the greatest dedication to exploiting the factor premiums. Notably, DFA and AQR, the two fund families with the greatest academic support, are generally only available to individual investors through a financial advisor. Should your professionally managed portfolio provide a better risk adjusted return than a comparable do-it-yourself index fund approach, the FA&rsquo;s fees have paid for themselves.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Furthermore, a good financial advisor will make sure your investments are tax efficient and that you are making the most of tax-preferred accounts. Researchers have shown that after asset allocation, asset </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">location</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, the strategic placement of investments in accounts with different tax treatment, is one of the most important factors in net portfolio returns,</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>59</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> yet most individual investors largely ignore these effects.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>60</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Advisor&rsquo;s fees can generally be paid with pre-tax funds as well, further enhancing tax efficiency.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Invest with Purpose</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">There is something of a paradox involved in investing. Finance is a highly specialized and technical field, but money is a very personal and emotional topic. Achieving the joy and fulfillment associated with financial success requires a large measure of emotional detachment and impersonal pragmatism. Far too often people suffer great loss by confusing loyalties and aspirations, fears and regrets with the efficient allocation of their portfolio assets. We as advisors hate to see this happen; there is nothing to celebrate about the needless destruction of capital, it is truly a loss for us all. One of the greatest misconceptions about finance is that investing is just a zero-sum game, that one trader&rsquo;s gain is another&rsquo;s loss. Nothing could be further from the truth. Economists have shown that one of the greatest predictors of a nation&rsquo;s well being is its financial development.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>61</sup></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> The more liquid and active our capital markets, the greater our society&rsquo;s capacity for innovation and progress. When you invest in the stock market, you are contributing your share to the productive capacity of our world, your return is your reward for helping make it better, outperformance is a sign that you have steered capital to those with the greatest use for it. </span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">With the right accounts and investments in place and a process for managing them effectively, you the investor are freed to focus on what you are working and investing </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">for</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, and an advisor can work with you to help get you there. Whether you want to travel the world, buy the house of your dreams, send your children to the best college, maximize your philanthropic giving, or simply retire early, an advisor can help you develop a financial plan to turn the dollars and cents of your portfolio into the life you want to live, building more health, wealth, and happiness for you, your loved ones, and the world.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\"><strong>Notes</strong></span></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"line-height: 1; font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">1. &ldquo;</span><span style=\"line-height: 1; font-size: 13px; font-family: Arial; color: #252626; vertical-align: baseline; white-space: pre-wrap;\">U.S. Stock Ownership Stays at Record Low,&rdquo; Gallup.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"line-height: 1; font-size: 13px; font-family: Arial; color: #252626; vertical-align: baseline; white-space: pre-wrap;\">2. </span><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">&ldquo;</span><span style=\"font-size: 13px; font-family: Arial; color: #252626; vertical-align: baseline; white-space: pre-wrap;\">U.S. Investors Not Sold on Stock Market as Wealth Creator,&rdquo; Gallup.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 13px; font-family: Arial; color: #252626; vertical-align: baseline; white-space: pre-wrap;\">3. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Data provided by Morningstar.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">4. </span><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">Siegel, </span><span style=\"font-size: 13px; font-family: Arial; font-style: italic; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">Stocks for the Long Run</span><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">, 5-25</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">5. </span><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">Dimson et al, </span><span style=\"font-size: 13px; font-family: Arial; font-style: italic; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">Triumph of the Optimists.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 13px; font-family: Arial; font-style: italic; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">6. Ibid. 3</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 13px; font-family: Arial; font-style: italic; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">7. Ibid</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 13px; font-family: Arial; font-style: italic; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">8. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Shiller, &ldquo;Understanding Recent Trends in House Prices and Home Ownership.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">9. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Mankiw and Zeldes, for example, find that to justify the historical equity risk premium observed, investors would in aggregate need to be indifferent between a certain payoff of $51,209 and a 50/50 bet paying either $50,000 or $100,000. Mankiw and Zeldes, &ldquo;The consumption of stockholders and nonstockholders,&rdquo; 8.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">10. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">For a highly readable introduction to the idea of cognitive biases, see Daniel Kahneman&rsquo;s book &ldquo;Thinking: Fast and Slow.&rdquo; Kahneman has been a pioneer in the field and for his work won the 2002 Nobel prize in economics.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">11. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Benartzi and Thaler, &ldquo;Myopic Loss Aversion and the Equity Premium Puzzle.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">12. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">&ldquo;Guide to the Markets,&rdquo; J.P. Morgan Asset Management</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">13. </span><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">See, for example, Kruger and Dunning,</span><span style=\"font-size: 13px; font-family: Arial; color: #252525; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\"> &nbsp;\"Unskilled and Unaware of It: How Difficulties in Recognizing One's Own Incompetence Lead to Inflated Self-Assessments\" and Zuckerman a</span><span style=\"font-size: 13px; font-family: Arial; color: #252626; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">nd Jost, &nbsp;</span><a style=\"text-decoration: none;\" href=\"http://www.psych.nyu.edu/jost/Zuckerman%20&amp;%20Jost%20(2001)%20What%20Makes%20You%20Think%20You're%20So%20Popular1.pdf\"><span style=\"font-size: 13px; font-family: Arial; color: #252626; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">\"What Makes You Think You're So Popular? Self Evaluation Maintenance and the Subjective Side of the &lsquo;Friendship Paradox&rsquo;\"</span></a></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 13px; font-family: Arial; color: #252626; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">14. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Svenson, &ldquo;Are We All Less Risky and More Skillful than Our Fellow Drivers?&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">15. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Preston and Harris, &ldquo;Psychology of Drivers in Traffic Accidents.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">16. </span><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">Zweig, </span><span style=\"font-size: 13px; font-family: Arial; font-style: italic; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">Your Money and Your Brain. </span><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">88-91.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">17. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">French and Poterba, &ldquo;Investor Diversification and International Equity Markets.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">18. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Ibid. 14. p. 98-99.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">19. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Barber and Odean, &ldquo;Trading is Hazardous to Your Wealth: The Common Stock Investment Performance of Individual Investors.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">20. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Ashenfelter et al, &ldquo;Predicting the Quality and Prices of Bordeaux Wine.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">21. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Thornton, \"Toward a Linear Prediction of Marital Happiness.\"</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">22. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Swets et al, \"Psychological Science Can Improve Diagnostic Decisions.\"</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">23. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Carroll et al, \"Evaluation, Diagnosis, and Prediction in Parole Decision-Making.\"</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">24. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Stillwell et al, \"Evaluating Credit Applications: A Validation of Multiattribute Utility Weight Elicitation Techniques\"</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">25. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">See Fama and French, &ldquo;Luck versus Skill in the Cross-Section of Mutual Fund Returns.&rdquo; They do find modest evidence of skill at the right tail end of the distribution under the capital asset pricing model. After controlling for the value, size, and momentum factor premiums (discussed below), however, evidence of net-of-fee skill is not significantly different than zero.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">26. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Shiller, &ldquo;Efficient Markets vs. Excess Volatility.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">27. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Professor Goetzmann of the Yale School of Management has a introductory hyper-text textbook on modern portfolio theory available on his website, &ldquo;An Introduction to Investment Theory.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">28. </span><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">In the language of modern portfolio theory this risk is known at a security&rsquo;s </span><span style=\"font-size: 13px; font-family: Arial; font-style: italic; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">beta</span><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">. Mathematically it is the covariance of the security&rsquo;s returns with the market&rsquo;s returns, divided by the variance of the market&rsquo;s returns.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">29. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Setton, &ldquo;The Berkshire Bunch.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">30. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">For example, Grossman and Stiglitz prove in &ldquo;On the Impossibility of Informationally Efficient Markets&rdquo; that market efficiency cannot be an equilibrium because without excess returns there is no incentive for arbitrageurs to correct mispricings. More recently, Markowitz, one of fathers of modern portfolio theory, showed in &ldquo;Market Efficiency: A Theoretical Distinction and So What&rdquo; that if a couple key assumptions of MPT are relaxed, the market portfolio is no longer optimal for most investors.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">31. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Basu, &ldquo;Investment Performance of Common Stocks in Relation to their Price-Earnings Ratios: A Test of the Efficient Market Hypothesis.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">32. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Fama and French, &ldquo;The Cross-Section of Expected Stock Returns.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">33. </span><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">Jegadeesh and Titman, &ldquo;</span><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">Returns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">34. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Ibid. 31.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">35. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Pastor and Stambaugh, &ldquo;Liquidity Risk and Expected Stock Returns.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">36. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Jegadeesh, &ldquo;Evidence of Predictable Behavior or Security Returns.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">37. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Froot and Thaler, &ldquo;Anomalies: Foreign Exchange.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">38. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Campbell and Shiller, &ldquo;Yield Spreads and Interest Rate Movements: A Bird&rsquo;s Eye View.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">39. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Erb and Harvey, &ldquo;The Tactical and Strategic Value of Commodity Futures.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">40. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Novy-Marx, &ldquo;The Other Side of Value: The Gross Profitability Premium.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">41. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Thaler, &ldquo;Seasonal Movements in Security Prices.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">42. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Mitchell and Pulvino, &ldquo;Characteristics of Risk and Return in Risk Arbitrage.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">43. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">See McLean and Pontiff, &ldquo;Does Academic Research Destroy Stock Return Predictability?&rdquo; A meta analysis of 82 equity return factors was able to replicate 72 using out of sample data.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">44. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Fama and French, &ldquo;Size and Book-to-Market Factors in Earnings and Returns.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">45. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Daniel and Titman, &ldquo;Evidence on the Characteristics of Cross Sectional Variation in Stock Returns.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">46. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Hwang and Rubesam, &ldquo;Is Value Really Riskier than Growth?&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">47. </span><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">Numerous investor profiles have expounded on the difficulty of being a rational investor in an irrational market. In a recent article in </span><span style=\"font-size: 13px; font-family: Arial; font-style: italic; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">Institutional Investor</span><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">, Asness and Liew give a highly readable overview of the risk vs. mispricing debate and discuss the problems they encountered launching a value-oriented hedge fund in the middle of the dot-com bubble.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 13px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\">48. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Bar-Eli, &ldquo;Action Bias Among Elite Soccer Goalkeepers: The Case of Penalty Kicks. Journal of Economic Psychology.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">49. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Asch, &ldquo;Opinions and Social Pressure.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">50. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Daniel et al provides one of the most thorough theoretical discussions on how certain common cognitive biases can result in systematically biased security prices in &ldquo;Investor Psychology and Security Market Under- and Overreaction.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">51. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Schleifer and Vishny, &ldquo;The Limits of Arbitrage.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">52. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Data provided by Vanguard.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">53. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Chapter 2 of Goetzmann&rsquo;s &ldquo;An Introduction to Investment Theory&rdquo; provides an introductory discussion.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">54. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">The Black-Litterman model allows investors to combine their estimates of expected returns with equilibrium implied returns in a Bayesian framework that largely overcomes the input-sensitivity problems </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">associated with traditional mean-variance optimization. Idzorek offers a thorough introduction in &ldquo;A Step-By-Step Guide to the Black-Litterman Model.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">55. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Ilmanen&rsquo;s &ldquo;Expected Returns on Major Asset Classes&rdquo; provides a detailed explanation of the theory and evidence of forecasting expected returns.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">56. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Walkshausl and Lobe, &ldquo;Fundamental Indexing Around the World.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">57. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Buetow et al, &ldquo;The Benefits of Rebalancing.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">58. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Kuhnen and Knutson, &ldquo;The Neural Basis of Financial Risk Taking.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">59. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Dammon et al, &ldquo;Optimal Asset Location and Allocation with Taxable and Tax-Deferred Investing.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">60. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Bodie and Crane, &ldquo;Personal Investing: Advice, Theory, and Evidence from a Survey of TIAA-CREF Participants.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">61. </span><span style=\"font-family: Arial; font-size: 13px; white-space: pre-wrap; background-color: transparent;\">Yongseok Shin of the Federal Reserve provides a brief review of the literature on this research in &ldquo;Financial Markets: An Engine for Economic Growth.&rdquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><strong style=\"font-family: Arial; font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\">Works Cited</strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 13px; line-height: 1; white-space: pre-wrap; background-color: transparent;\">Asch, Solomon E. \"Opinions and Social Pressure.\" Scientific American 193, no. 5 (12 1955).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Ashenfelter, Orley. \"Predicting the Quality and Prices of Bordeaux Wine*.\" The Economic Journal 118, no. 529 (12 2008).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Asness, Clifford and Liew, John. &ldquo;The Great Divide over Market Efficiency.&rdquo; Institutional Investor, March 3, 2014.</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Asness, Clifford, Moskowitz, Tobias, and Pedersen, Lasse. &ldquo;Value and Momentum Everywhere.&rdquo; The Journal of Finance 68, no. 3 (6, 2013).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Bar-Eli, Michael, Ofer H. Azar, Ilana Ritov, Yael Keidar-Levin, and Galit Schein. \"Action Bias among Elite Soccer Goalkeepers: The Case of Penalty Kicks.\" Journal of Economic Psychology 28, no. 5 (12 2007).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Barber, Brad M., and Terrance Odean. \"Trading Is Hazardous to Your Wealth: The Common Stock Investment Performance of Individual Investors.\" The Journal of Finance 55, no. 2 (12 2000).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Basu, S. \"Investment Performance of Common Stocks in Relation to Their Price-Earnings Ratios: A Test of the Efficient Market Hypothesis.\"The Journal of Finance 32, no. 3 (12 1977).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Benartzi, S., and R. H. Thaler. \"Myopic Loss Aversion and the Equity Premium Puzzle.\" The Quarterly Journal of Economics110, no. 1 (12, 1995).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Bodie, Zvi, and Dwight B. Crane. \"Personal Investing: Advice, Theory, and Evidence.\" Financial Analysts Journal 53, no. 6 (12 1997).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Buetow, Gerald W., Ronald Sellers, Donald Trotter, Elaine Hunt, and Willie A. Whipple. \"The Benefits of Rebalancing.\" The Journal of Portfolio Management 28, no. 2 (12 2002).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Campbell, John and Shiller, Robert. &ldquo;Yield Spreads and Interest Rate Movements: A Bird&rsquo;s Eye View.&rdquo; The Econometrics of Financial Markets, 58 no. 3 (1991).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Carroll, John S., Richard L. Wiener, Dan Coates, Jolene Galegher, and James J. Alibrio. \"Evaluation, Diagnosis, and Prediction in Parole Decision Making.\" Law &amp; Society Review 17, no. 1 (12 1982).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Dammon, Robert M., Chester S. Spatt, and Harold H. Zhang. \"Optimal Asset Location and Allocation with Taxable and Tax-Deferred Investing.\" The Journal of Finance 59, no. 3 (12 2004).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Daniel, Kent, and Sheridan Titman. \"Evidence on the Characteristics of Cross Sectional Variation in Stock Returns.\" The Journal of Finance52, no. 1 (12 1997).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Daniel, Kent, Hirshleifer, David, and Subrahmanyam, Avanidhar. &ldquo;Investor Psychology and Security Market Under- and Overreactions.&rdquo; The Journal of Finance, 53 no. 6 (1998).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Dimson, Elroy, Marsh, Paul, and Staunton, Mike. </span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Triumph of the Optimists. </span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Princeton: Princeton University Press, 2002.</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Erb, Cfa Claude B., and Campbell R. Harvey. \"The Strategic and Tactical Value of Commodity Futures.\" CFA Digest 36, no. 3 (12 2006).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Fama, Eugene F., and Kenneth R. French. \"The Cross-Section of Expected Stock Returns.\" The Journal of Finance 47, no. 2 (12 1992).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Fama, Eugene F., and Kenneth R. French. \"Luck versus Skill in the Cross-Section of Mutual Fund Returns.\" The Journal of Finance65, no. 5 (12 2010).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Fama, Eugene F., and Kenneth R. French. \"Size and Book-to-Market Factors in Earnings and Returns.\"The Journal of Finance 50, no. 1 (12 1995).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">French, Kenneth and Poterba, James. &ldquo;Investor Diversification and International Equity Markets.&rdquo; American Economic Review (1991).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Froot, Kenneth A., and Richard H. Thaler. \"Anomalies: Foreign Exchange.\" Journal of Economic Perspectives 4, no. 3 (12 1990).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">&ldquo;Guide to the Markets.&rdquo; J.P. Morgan Asset Management. 2014</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Goetzmann, William. </span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">An Introduction to Investment Theory</span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">. Yale School of Management. Accessed April 09, 2014.</span><a style=\"text-decoration:none;\" href=\"http://viking.som.yale.edu/will/finman540/classnotes/notes.html\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\"> </span><span style=\"font-size: 13px; font-family: Arial; color: #1155cc; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline; background-color: transparent;\">http://viking.som.yale.edu/will/finman540/classnotes/notes.html</span></a></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Grossman, Sanford and Stiglitz, Joseph. &ldquo;On the Impossibility of Informationally Efficent Markets.&rdquo; The American Economic Review 70, no. 3 (6, 1980).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Hwang, Soosung and Rubesam, Alexandre. &ldquo;Is Value Really Riskier Than Growth? An Answer with Time-Varying Return Reversal.&rdquo; Journal of Banking and Finance, 37 no. 7 (2013).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Idzorek, Thomas. &ldquo;A Step-by-Step Guide to the Black-Litterman Model.&rdquo; Ibbotson Associates (2005).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Ilmanen, Antti. &ldquo;Expected Returns on Major Asset Classes.&rdquo; Research Foundation of CFA Institute (2012).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Jegadeesh, Narasimhan, and Sheridan Titman. \"Returns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency.\" The Journal of Finance48, no. 1 (12 1993).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Kahneman, Daniel. </span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Thinking, Fast and Slow</span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">. New York: Farrar, Straus and Giroux, 2011.</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Kruger, Justin, and David Dunning. \"Unskilled and Unaware of It: How Difficulties in Recognizing One's Own Incompetence Lead to Inflated Self-assessments.\" Journal of Personality and Social Psychology77, no. 6 (12 1999).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Kuhnen, Camelia M., and Brian Knutson. \"The Neural Basis of Financial Risk Taking.\" Neuron 47, no. 5 (12 2005).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Malkiel, Burton. </span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">A Random Walk Down Wall Street: Time-Tested Strategies for Successful Investing (Tenth Edition)</span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">. New York: W.W. Norton &amp; Company, 2012.</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Mankiw, N.gregory, and Stephen P. Zeldes. \"The Consumption of Stockholders and Nonstockholders.\" Journal of Financial Economics 29, no. 1 (12 1991).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Markowitz, Harry M. \"Market Efficiency: A Theoretical Distinction and So What?\" Financial Analysts Journal 61, no. 5 (12 2005).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">McLean, David and Pontiff, Jeffrey. &ldquo;Does Academic Research Destroy Stock Return Predictability?&rdquo; Working Paper, (2013).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Mitchell, Mark, and Todd Pulvino. \"Characteristics of Risk and Return in Risk Arbitrage.\" The Journal of Finance 56, no. 6 (12 2001).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Novy-Marx, Robert. \"The Other Side of Value: The Gross Profitability Premium.\" Journal of Financial Economics 108, no. 1 (12 2013).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Pastor, Lubos and Stambaugh, Robert. &ldquo;Liquidity Risk and Expected Stock Returns.&rdquo; The Journal of Political Economy, 111 no. 3 (6, 2003).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Preston, Caroline E., and Stanley Harris. \"Psychology of Drivers in Traffic Accidents.\" Journal of Applied Psychology 49, no. 4 (12 1965).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Setton, Dolly. </span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">&ldquo;</span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">The Berkshire Bunch.&rdquo; Forbes, October 12, 1998.</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Shleifer, Andrei, and Robert W. Vishny. \"The Limits of Arbitrage.\"The Journal of Finance 52, no. 1 (12 1997).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Siegel, Jeremy J. </span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Stocks for the Long Run: The Definitive Guide to Financial Market Returns and Long-term Investment Strategies (Forth Edition)</span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">. New York: McGraw-Hill, 2008.</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Shiller, Robert. &ldquo;Understanding Recent Trends in House Prices and Homeownership.&rdquo; </span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Housing, Housing Finance and Monetary Policy</span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">, Jackson Hole Conference Series, Federal Reserve Bank of Kansas City, 2008, pp. 85-123 </span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Shiller, Robert. &ldquo;Efficient Markets vs. Excess Volatility.&rdquo; Yale. Accessed April 09, 2014.</span><a style=\"text-decoration:none;\" href=\"http://oyc.yale.edu/economics/econ-252-08/lecture-6\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\"> </span><span style=\"font-size: 13px; font-family: Arial; color: #1155cc; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline; background-color: transparent;\">http://oyc.yale.edu/economics/econ-252-08/lecture-6</span></a></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Shin, Yongseok. &ldquo;Financial Markets: An Engine for Economic Growth.&rdquo; The Regional Economist (July 2013).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Stillwell, William G., F.hutton Barron, and Ward Edwards. \"Evaluating Credit Applications: A Validation of Multiattribute Utility Weight Elicitation Techniques.\"Organizational Behavior and Human Performance 32, no. 1 (12 1983).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Svenson, Ola. \"Are We All Less Risky and More Skillful than Our Fellow Drivers?\" Acta Psychologica47, no. 2 (12 1981).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Swets, J. A., R. M. Dawes, and J. Monahan. \"Psychological Science Can Improve Diagnostic Decisions.\"Psychological Science in the Public Interest 1, no. 1 (12, 2000).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Thaler, Richard. \"Anomalies: Seasonal Movements in Security Prices II: Weekend, Holiday, Turn of the Month, and Intraday Effects.\"Journal of Economic Perspectives1, no. 2 (12 1987).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Thornton, B. \"Toward a Linear Prediction Model of Marital Happiness.\" Personality and Social Psychology Bulletin 3, no. 4 (12, 1977).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">\"U.S. Stock Ownership Stays at Record Low.\" Gallup. Accessed April 09, 2014. http://www.gallup.com/poll/162353/stock-ownership-stays-record-low.aspx.</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Walksh&auml;usl, Christian, and Sebastian Lobe. \"Fundamental Indexing around the World.\" Review of Financial Economics 19, no. 3 (12 2010).</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Zuckerman, Ezra W., and John T. Jost. \"What Makes You Think You're so Popular? Self-Evaluation Maintenance and the Subjective Side of the \"Friendship Paradox\"\"Social Psychology Quarterly 64, no. 3 (12 2001).</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Zweig, Jason. </span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">Your Money and Your Brain: How the New Science of Neuroeconomics Can Help Make You Rich</span><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">. New York: Simon &amp; Schuster, 2007.</span></p>\n<p style=\"line-height:1;margin-top:0pt;margin-bottom:10pt;margin-left: -9pt;margin-right: -9pt;text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Afterword/Acknowledgements</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">I wish to thank Romeo Stevens for the feedback and proofreading he provided for early drafts of this paper. You should go buy his <a href=\"http://www.mealsquares.com/sign-up.html\" target=\"_blank\">Mealsquares</a> (just look how happy I look eating them there!)</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">If the section on statistical prediction rules sounded familiar it's probably because I stole all the examples from <a href=\"/lw/3gv/statistical_prediction_rules_outperform_expert/\" target=\"_blank\">this Less Wrong article</a> by lukeprog about them. After you're done giving this article karma you should go give that one some more.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">After I made my South Bay meetup presentation Peter McCluskey wrote on the Bay Area LW mailing list that \"Your paper's report of 'a massive study of the sixteen countries that had data on local stock, bond, and cash returns available for every year of the twentieth century' could be considered a study of survivorship bias, in that it uses criteria which exclude countries where stocks lost 100% at some point (Russia, Poland, China, Hungary).\" This is a good point and is worth addressing, which some researchers have done in recent years. <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=891620\" target=\"_blank\">Dimson, Marsh, and Staunton (2006)</a> find that the surviving markets of the 20th century I cite in my paper dominated the global market capitalization in 1900 and the effect of national stock-market implosions was mostly negligible on worldwide averages. Peter did go on to say that \"I don't know of better advice for the average person than to invest in equities, and I have most of my wealth in equities...\" so I think we're mostly on the same page at least in terms of practical advice.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\">In a conversation with Alyssa Vance she similarly expressed skepticism that the equity risk premium has been significantly greater than zero due to the fact that at some point in the 20th century most major economies experienced double-digit inflation and very high marginal rates of taxation on capital income. It is true that taxes and inflation significantly dilute an investor's return, and one would be foolish to ignore their effects. But while they may reduce the absolute attractiveness of equities, the effects of taxes and inflation actually make stocks look <em>more </em>attractive relative to the alternatives of bonds and cash investments. In the US and most jurisdictions, the dividends and capital gains earned on stocks are taxed at preferential rates relative to the interest earned on fixed income investments, which is typically taxed as ordinary income. Furthermore, the majority of individual investors hold a large fraction of their investments in tax-sheltered accounts (such as 401(k)s and IRAs in the US).</span></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\">At my South Bay meetup presentation, Patrick LaVictoire (among others) expressed incredulity at my claim that retail investors have on average badly underperformed relevant benchmarks and that by implication institutional investors have outperformed. The source I cite in my paper is gated but there is plenty of research on actual investor performance. <a href=\"http://news.morningstar.com/articlenet/article.aspx?id=637022\" target=\"_blank\">Morningstar regularly publishes info</a> on how investors routinely underperform the mutual funds they invest in by buying into and selling out of them at the wrong times. Finding data on institutional investors is a little trickier but <a href=\"http://www.hec.unil.ch/agoyal/docs/Persistence_JoF.pdf\" target=\"_blank\">Busse, Goyal, and Wahal (2010)</a> find that institutional investors managing e.g. pensions, foundations, and endowments on average outperform the broad US equity market in the US equity sleeve of their portfolios. (the language of that paper sounds much more pessimistic, with \"alphas are statistically indistinguishable from zero\" in the abstract. The key is that they are controlling for the size, value, and momentum effects discussed in my paper. In other words, once we account for the fact that institutional investors are taking advantage of the factor premiums that have been shown to most consistently outperform a simple index strategy, they aren't providing any <em>extra</em> value. This ties in with the idea of <a href=\"http://www.top1000funds.com/attachments/081_Is%20Alpha%20Just%20Beta.pdf\" target=\"_blank\">\"shrinking alpha\" or \"smart beta\"</a> that is currently <em>en vogue</em> in my industry.)</span></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; margin-left: -9pt; margin-right: -9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\">I'm happy to address further questions and criticisms in the comments.</span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jgcAJnksReZRuvgzp": 8, "fkABsGCJZ6y9qConW": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4bBAqd4EEBKLBwNuF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 52, "extendedScore": null, "score": 0.000158, "legacy": true, "legacyId": "27197", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CKW8c2Bngz9yXibSk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-15T06:41:32.577Z", "modifiedAt": null, "url": null, "title": "Meetup : Bratislava", "slug": "meetup-bratislava-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JaX2ytEcBHvz4Bp44/meetup-bratislava-1", "pageUrlRelative": "/posts/JaX2ytEcBHvz4Bp44/meetup-bratislava-1", "linkUrl": "https://www.lesswrong.com/posts/JaX2ytEcBHvz4Bp44/meetup-bratislava-1", "postedAtFormatted": "Monday, September 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bratislava&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bratislava%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJaX2ytEcBHvz4Bp44%2Fmeetup-bratislava-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bratislava%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJaX2ytEcBHvz4Bp44%2Fmeetup-bratislava-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJaX2ytEcBHvz4Bp44%2Fmeetup-bratislava-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 112, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14i'>Bratislava</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 September 2014 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Bistro The Peach, Mari\u00e1nska 3, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Pre t\u00fdch, \u010do chv\u00ed\u013eu nechodili: Bistro the Peach sa asi pred dvoma mesiacmi presunulo o ulicu \u010falej, a mo\u017eno st\u00e1le nie je poriadne ozna\u010den\u00e9, ale je to oproti predajni richmanov na Mari\u00e1nskej. Alebo sa ria\u010fte pod\u013ea google mapy a \u010d\u00edsla ulice. Vchod vyzer\u00e1 tak, \u017ee na za\u010diatku pas\u00e1\u017ee s\u00fa kr\u00e1tke schody do prvej \u010dasti bistra, ved\u013ea je druh\u00e1 \u010das\u0165, kde sed\u00edme, a potom je e\u0161te terasa.</p>\n\n<p>M\u00e1me aj <a href=\"https://groups.google.com/forum/?fromgroups#!forum/lesswrong-bratislava\">mailov\u00fa skupinu</a>, kde zvy\u010dajne posielam inform\u00e1cie o \u010fal\u0161\u00edch stret\u00e1vkach. Ale do konca roka 2014 to bude pravidelne, ka\u017ed\u00e9 3 t\u00fd\u017edne.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14i'>Bratislava</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JaX2ytEcBHvz4Bp44", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.006378299633449e-06, "legacy": true, "legacyId": "27202", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bratislava\">Discussion article for the meetup : <a href=\"/meetups/14i\">Bratislava</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 September 2014 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Bistro The Peach, Mari\u00e1nska 3, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Pre t\u00fdch, \u010do chv\u00ed\u013eu nechodili: Bistro the Peach sa asi pred dvoma mesiacmi presunulo o ulicu \u010falej, a mo\u017eno st\u00e1le nie je poriadne ozna\u010den\u00e9, ale je to oproti predajni richmanov na Mari\u00e1nskej. Alebo sa ria\u010fte pod\u013ea google mapy a \u010d\u00edsla ulice. Vchod vyzer\u00e1 tak, \u017ee na za\u010diatku pas\u00e1\u017ee s\u00fa kr\u00e1tke schody do prvej \u010dasti bistra, ved\u013ea je druh\u00e1 \u010das\u0165, kde sed\u00edme, a potom je e\u0161te terasa.</p>\n\n<p>M\u00e1me aj <a href=\"https://groups.google.com/forum/?fromgroups#!forum/lesswrong-bratislava\">mailov\u00fa skupinu</a>, kde zvy\u010dajne posielam inform\u00e1cie o \u010fal\u0161\u00edch stret\u00e1vkach. Ale do konca roka 2014 to bude pravidelne, ka\u017ed\u00e9 3 t\u00fd\u017edne.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bratislava1\">Discussion article for the meetup : <a href=\"/meetups/14i\">Bratislava</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bratislava", "anchor": "Discussion_article_for_the_meetup___Bratislava", "level": 1}, {"title": "Discussion article for the meetup : Bratislava", "anchor": "Discussion_article_for_the_meetup___Bratislava1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-15T09:17:20.308Z", "modifiedAt": null, "url": null, "title": "What are your contrarian views?", "slug": "what-are-your-contrarian-views", "viewCount": null, "lastCommentedAt": "2020-06-02T22:22:49.451Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Metus", "createdAt": "2011-01-23T21:54:34.357Z", "isAdmin": false, "displayName": "Metus"}, "userId": "mNQ4fSvro7LYgrii4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vsGJTr4bNhA4MutMs/what-are-your-contrarian-views", "pageUrlRelative": "/posts/vsGJTr4bNhA4MutMs/what-are-your-contrarian-views", "linkUrl": "https://www.lesswrong.com/posts/vsGJTr4bNhA4MutMs/what-are-your-contrarian-views", "postedAtFormatted": "Monday, September 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20your%20contrarian%20views%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20your%20contrarian%20views%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsGJTr4bNhA4MutMs%2Fwhat-are-your-contrarian-views%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20your%20contrarian%20views%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsGJTr4bNhA4MutMs%2Fwhat-are-your-contrarian-views", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsGJTr4bNhA4MutMs%2Fwhat-are-your-contrarian-views", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<p>As per <a href=\"/r/discussion/lw/kzf/should_people_be_writing_more_or_fewer_lw_posts/bbhg\">a recent comment</a> this thread is meant to voice contrarian opinions, that is anything this community tends not to agree with. Thus I ask you to post your contrarian views and upvote anything you do <strong>not</strong> agree with based on personal beliefs. Spam and trolling still needs to be downvoted.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6Qic6PwwBycopJFNN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vsGJTr4bNhA4MutMs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 15, "extendedScore": null, "score": 2.0066588780031305e-06, "legacy": true, "legacyId": "27203", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 812, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-15T10:50:28.009Z", "modifiedAt": null, "url": null, "title": "What are you learning?", "slug": "what-are-you-learning", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:49.748Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AD53LCM2tHEBDxf7w/what-are-you-learning", "pageUrlRelative": "/posts/AD53LCM2tHEBDxf7w/what-are-you-learning", "linkUrl": "https://www.lesswrong.com/posts/AD53LCM2tHEBDxf7w/what-are-you-learning", "postedAtFormatted": "Monday, September 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20you%20learning%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20you%20learning%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAD53LCM2tHEBDxf7w%2Fwhat-are-you-learning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20you%20learning%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAD53LCM2tHEBDxf7w%2Fwhat-are-you-learning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAD53LCM2tHEBDxf7w%2Fwhat-are-you-learning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>This is a thread to connect rationalists who are learning the same thing, so they can cooperate.</p>\n<p>The \"learning\" doesn't necessarily mean \"I am reading a textbook / learning an online course right now\". It can be something you are interested in long-term, and still want to learn more.</p>\n<p>&nbsp;</p>\n<p>Rules:</p>\n<p>Top-level comments contain only the topic to learn. (Plus one comment for \"meta\" debate.) Only one topic per comment, for easier search. Try to find a reasonable level of specificity: too narrow topic means less people; too wide topic means more people who actually are interested in something different than you are.</p>\n<p>Use the second-level comments if you are learning that topic. (Or if you are going to learn it now, not merely in the far future.) Technically, \"me too\" is okay in this thread, but providing more info is probably more useful. For example: What are you focusing on? What learning materials you use? What is your goal?</p>\n<p>Third- and deeper-level comments, that's debate as usual.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AD53LCM2tHEBDxf7w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 19, "extendedScore": null, "score": 2.006826629709231e-06, "legacy": true, "legacyId": "27204", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 128, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-15T12:18:41.594Z", "modifiedAt": null, "url": null, "title": "Open thread, September 15-21, 2014", "slug": "open-thread-september-15-21-2014-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:24.494Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DRk8TB3DMqQM3DZwp/open-thread-september-15-21-2014-0", "pageUrlRelative": "/posts/DRk8TB3DMqQM3DZwp/open-thread-september-15-21-2014-0", "linkUrl": "https://www.lesswrong.com/posts/DRk8TB3DMqQM3DZwp/open-thread-september-15-21-2014-0", "postedAtFormatted": "Monday, September 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20September%2015-21%2C%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20September%2015-21%2C%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDRk8TB3DMqQM3DZwp%2Fopen-thread-september-15-21-2014-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20September%2015-21%2C%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDRk8TB3DMqQM3DZwp%2Fopen-thread-september-15-21-2014-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDRk8TB3DMqQM3DZwp%2Fopen-thread-september-15-21-2014-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<div id=\"entry_t3_kxv\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_kwc\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px; font-weight: bold;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px; font-weight: bold;\"><br /></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">3.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should be posted in Discussion, and not Main.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">4.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should start on Monday, and end on Sunday.</span></p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DRk8TB3DMqQM3DZwp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "27205", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-15T12:24:53.165Z", "modifiedAt": null, "url": null, "title": "Open thread, September 15-21, 2014", "slug": "open-thread-september-15-21-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:39.274Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gjm", "createdAt": "2009-03-09T01:11:32.668Z", "isAdmin": false, "displayName": "gjm"}, "userId": "977L8MR7JmNrQx6df", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rXzSQEnnTW8R93ikX/open-thread-september-15-21-2014", "pageUrlRelative": "/posts/rXzSQEnnTW8R93ikX/open-thread-september-15-21-2014", "linkUrl": "https://www.lesswrong.com/posts/rXzSQEnnTW8R93ikX/open-thread-september-15-21-2014", "postedAtFormatted": "Monday, September 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20September%2015-21%2C%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20September%2015-21%2C%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrXzSQEnnTW8R93ikX%2Fopen-thread-september-15-21-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20September%2015-21%2C%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrXzSQEnnTW8R93ikX%2Fopen-thread-september-15-21-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrXzSQEnnTW8R93ikX%2Fopen-thread-september-15-21-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<div id=\"entry_t3_kxv\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_kwc\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px; font-weight: bold;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px; font-weight: bold;\"><br /></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one. (<em>Immediately</em> before; refresh the list-of-threads page before posting.)<br /></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">3.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should be posted in Discussion, and not Main.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">4.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should start on Monday, and end on Sunday.</span></p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rXzSQEnnTW8R93ikX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 2.006996732069477e-06, "legacy": true, "legacyId": "27206", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 342, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-15T19:30:54.251Z", "modifiedAt": null, "url": null, "title": "Unpopular ideas attract poor advocates: Be charitable", "slug": "unpopular-ideas-attract-poor-advocates-be-charitable", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:04.475Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "LADT3SBgX6o98YmYH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/APRxNpqrEZrpbSxWE/unpopular-ideas-attract-poor-advocates-be-charitable", "pageUrlRelative": "/posts/APRxNpqrEZrpbSxWE/unpopular-ideas-attract-poor-advocates-be-charitable", "linkUrl": "https://www.lesswrong.com/posts/APRxNpqrEZrpbSxWE/unpopular-ideas-attract-poor-advocates-be-charitable", "postedAtFormatted": "Monday, September 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Unpopular%20ideas%20attract%20poor%20advocates%3A%20Be%20charitable&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUnpopular%20ideas%20attract%20poor%20advocates%3A%20Be%20charitable%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAPRxNpqrEZrpbSxWE%2Funpopular-ideas-attract-poor-advocates-be-charitable%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Unpopular%20ideas%20attract%20poor%20advocates%3A%20Be%20charitable%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAPRxNpqrEZrpbSxWE%2Funpopular-ideas-attract-poor-advocates-be-charitable", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAPRxNpqrEZrpbSxWE%2Funpopular-ideas-attract-poor-advocates-be-charitable", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 561, "htmlBody": "<p>Unfamiliar or unpopular ideas will tend to reach you via proponents who:</p>\n<ul>\n<li>&nbsp;...hold extreme interpretations of these ideas.</li>\n<li>...have unpleasant social characteristics.</li>\n<li>...generally come across as cranks.</li>\n</ul>\n<p>The basic idea: It's unpleasant to promote ideas that result in social sanction, and frustrating when your ideas are met with indifference. Both situations are more likely when talking to an ideological out-group. Given a range of positions on an in-group belief, who will decide to promote the belief to outsiders? On average, it will be those who believe the benefits of the idea are large relative to in-group opinion (extremists), those who view the social costs as small (disagreeable people), and those who are dispositionally drawn to promoting weird ideas (cranks).<br /><br />I don't want to push this pattern too far. This isn't a refutation of any particular idea. There are reasonable people in the world, and some of them even express their opinions in public, (in spite of being reasonable). And sometimes the truth will be unavoidably unfamiliar and unpopular, etc. But there are also...<br /><br />Some benefits that stem from recognizing these selection effects:</p>\n<ul>\n<li>It's easier to be charitable to controversial ideas, when you recognize that you're interacting with people who are terribly suited to persuade you. I'm not sure \"steelmanning\" is the best idea (trying to present the best argument for an opponent's position). Based on the extremity effect, another technique is to construct a much diluted version of the belief, and then try to steelman the diluted belief.</li>\n<li>If your group holds fringe or unpopular ideas, you can avoid these patterns when you want to influence outsiders.</li>\n<li>If you want to learn about an afflicted issue, you might ignore the public representatives and speak to the non-evangelical instead (you'll probably have to start the conversation).</li>\n<li>You can resist certain polarizing situations, in which the most visible camps hold extreme and opposing views. This situation worsens when those with non-extreme views judge the risk of participation as excessive, and leave the debate to the extremists (who are willing to take substantial risks for their beliefs). This leads to the perception that the current camps represent the only valid positions, which creates a polarizing loop. Because this is a sort of coordination failure among non-extremists, knowing to covertly look for other non-vocal moderates is a first step toward a solution. (Note: Sometimes there really aren't any moderates.)</li>\n<li>Related to the previous point: You can avoid exaggerating the ideological unity of a group based on the group's leadership, or believing that the entire group has some obnoxious trait present in the leadership. (Note: In things like elections and war, the views of the leadership are what you care about. But you still don't want to be confused about other group members.)</li>\n</ul>\n<p>&nbsp;</p>\n<p>I think the first benefit listed is the most useful.<br /><br />To sum up: An unpopular idea will tend to get poor representation for social reasons, which will makes it seem like a worse idea than it really is, even granting that many unpopular ideas are unpopular for good reason. So when you encounter a idea that seem unpopular, you're probably hearing about it from a sub-optimal source, and you should try to be charitable towards the idea before dismissing it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MXcpQvaPGtXpB6vkM": 1, "RE6h98Ziwcfh4EP9T": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "APRxNpqrEZrpbSxWE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 43, "extendedScore": null, "score": 2.0077645483316526e-06, "legacy": true, "legacyId": "27208", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-16T01:00:40.991Z", "modifiedAt": null, "url": null, "title": "Superintelligence Reading Group - Section 1: Past Developments and Present Capabilities", "slug": "superintelligence-reading-group-section-1-past-developments", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:37.472Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mmZ2PaRo86pDXu8ii/superintelligence-reading-group-section-1-past-developments", "pageUrlRelative": "/posts/mmZ2PaRo86pDXu8ii/superintelligence-reading-group-section-1-past-developments", "linkUrl": "https://www.lesswrong.com/posts/mmZ2PaRo86pDXu8ii/superintelligence-reading-group-section-1-past-developments", "postedAtFormatted": "Tuesday, September 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Superintelligence%20Reading%20Group%20-%20Section%201%3A%20Past%20Developments%20and%20Present%20Capabilities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuperintelligence%20Reading%20Group%20-%20Section%201%3A%20Past%20Developments%20and%20Present%20Capabilities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmmZ2PaRo86pDXu8ii%2Fsuperintelligence-reading-group-section-1-past-developments%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Superintelligence%20Reading%20Group%20-%20Section%201%3A%20Past%20Developments%20and%20Present%20Capabilities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmmZ2PaRo86pDXu8ii%2Fsuperintelligence-reading-group-section-1-past-developments", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmmZ2PaRo86pDXu8ii%2Fsuperintelligence-reading-group-section-1-past-developments", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2164, "htmlBody": "<p><em>This is part of a weekly reading group on Nick Bostrom's book, Superintelligence. For more information about the group, see the&nbsp;<a href=\"/lw/kw4/superintelligence_reading_group/\">announcement post</a>. For the schedule of future topics, see <a href=\"https://intelligence.org/wp-content/uploads/2014/08/Superintelligence-Readers-Guide-early-version.pdf\">MIRI's reading guide</a>.</em></p>\n<hr />\n<p>Welcome to the <a href=\"http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111\"><em>Superintelligence</em></a> reading group. This week we discuss the first section in the reading guide, <em><strong>Past developments and present capabilities</strong></em>. This section considers the behavior of the economy over very long time scales, and the recent history of artificial intelligence (henceforth, 'AI'). These two areas are excellent background if you want to think about large economic transitions caused by AI.</p>\n<p>This post summarizes the section, and offers a few relevant notes, thoughts, and ideas for further investigation. My own thoughts and questions for discussion are in the comments.</p>\n<p>There is no need to proceed in order through this post. Feel free to jump straight to the discussion. Where applicable, page numbers indicate the rough part of the chapter that is most related (not necessarily that the chapter is being cited for the specific claim).</p>\n<p><strong>Reading</strong>: <em>Foreword,</em> and <em>Growth modes</em> through <em>State of the art</em> from Chapter 1 (p1-18)</p>\n<hr />\n<h1>Summary</h1>\n<p>Economic growth:</p>\n<ol>\n<li>Economic growth has become radically faster over the course of human history. (p1-2)</li>\n<li>This growth has been uneven rather than continuous, perhaps corresponding to the farming and industrial revolutions. (p1-2)</li>\n<li>Thus history suggests large changes in the growth rate of the economy are plausible. (p2)</li>\n<li>This makes it more plausible that human-level AI will arrive and produce unprecedented levels of economic productivity. </li>\n<li>Predictions of much faster growth rates might also suggest the arrival of machine intelligence, because it is hard to imagine humans - slow as they are - sustaining such a rapidly growing economy. (p2-3)</li>\n<li>Thus economic history suggests that rapid growth caused by AI is more plausible than you might otherwise think.</li>\n</ol>\n<p>The history of AI:</p>\n<ol>\n<li>Human-level AI has been predicted since the 1940s.&nbsp;(p3-4)</li>\n<li>Early predictions were often optimistic about when human-level AI would come, but rarely considered whether it would pose a risk. (p4-5)</li>\n<li>AI research has been through several cycles of relative popularity and unpopularity. (p5-11)</li>\n<li>By around the 1990s, 'Good Old-Fashioned Artificial Intelligence' (GOFAI) techniques based on symbol manipulation gave way to new methods such as artificial neural networks and genetic algorithms. These are widely considered more promising, in part because they are less brittle and can learn from experience more usefully. Researchers have also lately developed a better understanding of the underlying mathematical relationships between various modern approaches. (p5-11)</li>\n<li>AI is very good at playing board games. (12-13)</li>\n<li>AI is used in many applications today (e.g. hearing aids, route-finders, recommender systems, medical decision support systems, machine translation, face recognition, scheduling, the financial market). (p14-16)</li>\n<li>In general, tasks we thought were intellectually demanding (e.g. board games) have turned out to be easy to do with AI, while tasks which seem easy to us (e.g. identifying objects) have turned out to be hard. (p14)</li>\n<li>An 'optimality notion' is the combination of a rule for learning, and a rule for making decisions. Bostrom describes one of these: a kind of ideal Bayesian agent. This is impossible to actually make, but provides a useful measure for judging imperfect agents against. (p10-11)</li>\n</ol>\n<h1>Notes on a few things</h1>\n<ol>\n<li><strong>What is 'superintelligence'?</strong> (p22 spoiler)<br />In case you are too curious about what the topic of this book is to wait until week 3, a 'superintelligence' will soon be described as <em>'any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest'</em>. Vagueness in this definition will be cleared up later.&nbsp;</li>\n<li><strong>What is 'AI'? <br /></strong>In particular, how does 'AI' differ from other computer software? The line is blurry, but basically AI research seeks to replicate the useful 'cognitive' functions of human brains ('cognitive' is perhaps unclear, but for instance it doesn't have to be squishy or prevent your head from imploding). Sometimes AI research tries to copy the methods used by human brains. Other times it tries to carry out the same broad functions as a human brain, perhaps better than a human brain. <a href=\"http://www.amazon.com/Artificial-Intelligence-Modern-Approach-Edition/dp/0136042597\">Russell and Norvig</a> (p2) divide prevailing definitions of AI into four categories: 'thinking humanly', 'thinking rationally', 'acting humanly' and 'acting rationally'. For our purposes however, the distinction is probably not too important.</li>\n<li><strong>What is 'human-level' AI?&nbsp;<br /></strong>We are going to talk about 'human-level' AI a lot, so it would be good to be clear on what that is. Unfortunately the term is used in various ways, and often ambiguously. So we probably can't be that clear on it, but let us at least be clear on how the term is unclear.&nbsp;<br /><br />One big ambiguity is whether you are talking about a machine that can carry out tasks as well as a human&nbsp;<em>at any price</em>, or a machine that can carry out tasks as well as a human&nbsp;<em>at the price of a human</em>. These are quite different, especially in their immediate social implications.<br /><br />Other ambiguities arise in how 'levels' are measured. If AI systems were to replace almost all humans in the economy, but only because they are so much cheaper - though they often do a lower quality job - are they human level?&nbsp;What exactly does the AI need to be human-level&nbsp;at? Anything you can be paid for? Anything a human is good for? Just mental tasks? Even mental tasks like daydreaming?&nbsp;Which or how many humans does the AI need to be the same level as? Note that in a sense most humans have been replaced in their jobs before (almost everyone used to work in farming), so if you use that metric for human-level AI, it was reached long ago, and perhaps farm machinery is human-level AI. This is probably not what we want to point at.<br /><br />Another thing to be aware of is the diversity of mental skills. If by 'human-level' we mean a machine that is&nbsp;at least&nbsp;as good as a human at each of these skills, then in practice the first 'human-level' machine will be much better than a human on many of those skills. It may not seem 'human-level' so much as&nbsp;'very super-human'.<br /><br />We could instead think of human-level as closer to 'competitive with a human' - where the machine has some super-human talents and lacks some skills humans have. This is not usually used, I think because it is hard to define in a meaningful way. There are already machines for which a company is willing to pay more than a human: in this sense a microscope might be 'super-human'. There is no reason for a machine which is equal in value to a human to have the traits we are interested in talking about here, such as agency, superior cognitive abilities or the tendency to drive humans out of work and shape the future. Thus we talk about AI which is at least as good as a human, but you should beware that the predictions made about such an entity may apply before the entity is technically 'human-level'.<br /><br /><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_ku6_1.png\" alt=\"\" width=\"334\" height=\"256\" /><br />Example of how the first 'human-level' AI may surpass humans in many ways.<br /><br />Because of these ambiguities, AI researchers are sometimes hesitant to&nbsp;use the term. e.g. in&nbsp;<a href=\"/r/discussion/lw/999/qa_with_experts_on_risks_from_ai_1/\">these</a>&nbsp;<a href=\"/r/discussion/lw/9a1/qa_with_experts_on_risks_from_ai_2/\">interviews</a>.</li>\n<li><strong>Growth modes (p1)</strong>&nbsp;<br />Robin Hanson wrote the <a href=\"http://hanson.gmu.edu/longgrow.pdf\">seminal paper</a> on this issue. Here's a figure from it, showing the step changes in growth rates. Note that both axes are logarithmic. Note also that the changes between modes don't happen overnight. According to Robin's model, we are still transitioning into the industrial era (p10 in his paper). <img src=\"http://images.lesswrong.com/t3_ku6_0.png?v=b459222bb385e9cd35f91af9b62c7d92\" alt=\"\" width=\"607\" height=\"378\" /> </li>\n<li><strong>What causes these transitions between growth modes?</strong>&nbsp;(p1-2)<br />One might be happier making predictions about future growth mode changes if one had a unifying explanation for the previous changes. As far as I know, we have no good idea of what was so special about those two periods. There are many&nbsp;<a href=\"http://en.wikipedia.org/wiki/Industrial_Revolution#Causes\">suggested causes of the industrial revolution</a>, but nothing uncontroversially stands out as 'twice in history' level of special.&nbsp;You might think the small number of datapoints would make this puzzle too hard. Remember however that there are quite a lot of negative datapoints - you need an explanation that&nbsp;<em>didn't happen</em>&nbsp;at all of the other times in history.&nbsp;</li>\n<li><strong>Growth of growth</strong><br />It is also interesting to compare world economic growth to the total size of the world economy. For the last few thousand years, the economy seems to have grown faster more or less in proportion to it's size (see figure below). Extrapolating such a trend would lead to an infinite economy in finite time. In fact for the thousand years until 1950 <a href=\"http://www.aiimpacts.org/historical-growth-trends\">such extrapolation</a> would place an infinite economy in the late 20th Century! The time since 1950 has been strange apparently.&nbsp;<br /><img src=\"http://images.lesswrong.com/t3_ku6_2.png?v=9f8861672d59c2fe622f594e4f04230e\" alt=\"\" width=\"652\" height=\"280\" /><br />(Figure from <a href=\"http://www.aiimpacts.org/historical-growth-trends\">here</a>)</li>\n<li><strong>Early AI programs mentioned in the book</strong>&nbsp;(p5-6)<br />You can see them in action: <a href=\"https://www.youtube.com/watch?v=QAJz4YKUwqw\">SHRDLU</a>, <a href=\"https://www.youtube.com/watch?v=qXdn6ynwpiI\">Shakey</a>, <a href=\"http://ai-su13.artifice.cc/gps.html\">General Problem Solver</a> (not quite in action), <a href=\"http://psych.fullerton.edu/mbirnbaum/psych101/Eliza.htm\">ELIZA</a>.</li>\n<li><strong>Later AI programs mentioned in the book</strong>&nbsp;(p6)<br /><a href=\"https://www.youtube.com/watch?v=CgG1HipAayU&amp;list=PL8nIR9RW0CkYcjsYNbDWzBG_vv1yeDXq0&amp;index=4\">Algorithmically generated Beethoven</a>, <a href=\"http://www.genetic-programming.com/inventionmachine.html\">algorithmic generation of patentable inventions</a>,&nbsp;<a href=\"http://homepages.abdn.ac.uk/wpn006/software.php\">artificial comedy</a>&nbsp;(requires download).</li>\n<li><strong>Modern AI algorithms mentioned</strong>&nbsp;(p7-8, 14-15)&nbsp;<br /><a href=\"http://www.clarifai.com/\">Here</a> is a neural network doing image recognition. Here is&nbsp;<a href=\"https://www.youtube.com/watch?v=QRY7mEjbT8A\">artificial evolution of jumping</a>&nbsp;and of <a href=\"http://boxcar2d.com/\">toy cars</a>. Here is a&nbsp;<a href=\"http://rekognition.com/demo/face\">face detection demo</a>&nbsp;that can tell you your attractiveness&nbsp;(apparently not reliably), happiness, age, gender, and which celebrity it mistakes you for.</li>\n<li><strong>What is maximum likelihood estimation?</strong>&nbsp;(p9)<br />Bostrom points out that many types of artificial neural network can be viewed as classifiers that perform 'maximum likelihood estimation'. If you haven't come across this term before, the idea is to find the situation that would make your observations most probable. For instance, suppose a person writes to you and tells you that you have won a car. The situation that would have made this scenario most probable is the one where you have won a car, since in that case you are almost guaranteed to be told about it. Note that this doesn't imply that you should think you won a car, if someone tells you that. Being the target of a spam email might only give you a low probability of being told that you have won a car (a spam email may instead advise you of products, or tell you that you have won a boat), but spam emails are so much more common than actually winning cars that most of the time if you get such an email, you will not have won a car. If you would like a better intuition for maximum likelihood estimation, Wolfram Alpha has&nbsp;<a href=\"http://demonstrations.wolfram.com/MaximumLikelihoodEstimation/\">several</a>&nbsp;<a href=\"http://demonstrations.wolfram.com/search.html?query=maximum%20likelihood\">demonstrations</a>&nbsp;(requires free download).</li>\n<li><strong>What are hill climbing algorithms like?</strong>&nbsp;(p9)<br />The second large class of algorithms Bostrom mentions are hill climbing algorithms. The <a href=\"http://en.wikipedia.org/wiki/Hill_climbing\">idea</a> here is fairly straightforward, but if you would like a better basic intuition for what hill climbing looks like, Wolfram Alpha has a&nbsp;<a href=\"http://demonstrations.wolfram.com/HillClimbingAlgorithm/\">demonstration</a>&nbsp;to play with (requires free download).</li>\n</ol>\n<h1>In-depth investigations</h1>\n<p>If you are particularly interested in these topics, and want to do further research, these are a few plausible directions:</p>\n<ol>\n<li>How have investments into AI changed over time? <a href=\"http://intelligence.org/2014/01/28/how-big-is-ai/\">Here's</a> a start, estimating the size of the field.</li>\n<li>What does progress in AI look like in more detail? What can we infer from it? I wrote about <a href=\"http://intelligence.org/files/AlgorithmicProgress.pdf\">algorithmic improvement</a> curves before. If you are interested in plausible next steps here, ask me.</li>\n<li>What do economic models tell us about the consequences of human-level AI? <a href=\"http://hanson.gmu.edu/aigrow.pdf\">Here</a>&nbsp;<a href=\"http://agi-conf.org/2010/wp-content/uploads/2009/06/agi10singmodels2.pdf\">is</a>&nbsp;<a href=\"http://mason.gmu.edu/~gjonesb/AIandGrowth\">some</a> such thinking;&nbsp;Eliezer Yudkowsky <a href=\"http://intelligence.org/files/IEM.pdf\">has written at length about his request for more</a>.</li>\n</ol>\n<h1>How to proceed</h1>\n<p>This has been a collection of notes on the chapter. <strong>The most important part of the reading group though is discussion</strong>, which is in the comments section. I pose some questions for you there, and I invite you to add your own. Please remember that this group contains a variety of levels of expertise: if a line of discussion seems too basic or too incomprehensible, look around for one that suits you better!</p>\n<p>Next week, we will talk about what AI researchers think about human-level AI: when it will arrive, what it will be like, and what the consequences will be. To prepare, <strong>read</strong> <em>Opinions about the future of machine intelligence</em> from Chapter 1 and also&nbsp;<em><a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\">When Will AI Be Created?</a>&nbsp;</em>by Luke Muehlhauser<em>. </em>The discussion will go live at 6pm Pacific time next Monday 22 September. Sign up to be notified <a href=\"http://intelligence.us5.list-manage.com/subscribe?u=353906382677fa789a483ba9e&amp;id=28cb982f40\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "tdt83ChxnEgwwKxi6": 1, "5f5c37ee1b5cdee568cfb297": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mmZ2PaRo86pDXu8ii", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 43, "extendedScore": null, "score": 0.00012, "legacy": true, "legacyId": "27006", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This is part of a weekly reading group on Nick Bostrom's book, Superintelligence. For more information about the group, see the&nbsp;<a href=\"/lw/kw4/superintelligence_reading_group/\">announcement post</a>. For the schedule of future topics, see <a href=\"https://intelligence.org/wp-content/uploads/2014/08/Superintelligence-Readers-Guide-early-version.pdf\">MIRI's reading guide</a>.</em></p>\n<hr>\n<p>Welcome to the <a href=\"http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111\"><em>Superintelligence</em></a> reading group. This week we discuss the first section in the reading guide, <em><strong>Past developments and present capabilities</strong></em>. This section considers the behavior of the economy over very long time scales, and the recent history of artificial intelligence (henceforth, 'AI'). These two areas are excellent background if you want to think about large economic transitions caused by AI.</p>\n<p>This post summarizes the section, and offers a few relevant notes, thoughts, and ideas for further investigation. My own thoughts and questions for discussion are in the comments.</p>\n<p>There is no need to proceed in order through this post. Feel free to jump straight to the discussion. Where applicable, page numbers indicate the rough part of the chapter that is most related (not necessarily that the chapter is being cited for the specific claim).</p>\n<p><strong>Reading</strong>: <em>Foreword,</em> and <em>Growth modes</em> through <em>State of the art</em> from Chapter 1 (p1-18)</p>\n<hr>\n<h1 id=\"Summary\">Summary</h1>\n<p>Economic growth:</p>\n<ol>\n<li>Economic growth has become radically faster over the course of human history. (p1-2)</li>\n<li>This growth has been uneven rather than continuous, perhaps corresponding to the farming and industrial revolutions. (p1-2)</li>\n<li>Thus history suggests large changes in the growth rate of the economy are plausible. (p2)</li>\n<li>This makes it more plausible that human-level AI will arrive and produce unprecedented levels of economic productivity. </li>\n<li>Predictions of much faster growth rates might also suggest the arrival of machine intelligence, because it is hard to imagine humans - slow as they are - sustaining such a rapidly growing economy. (p2-3)</li>\n<li>Thus economic history suggests that rapid growth caused by AI is more plausible than you might otherwise think.</li>\n</ol>\n<p>The history of AI:</p>\n<ol>\n<li>Human-level AI has been predicted since the 1940s.&nbsp;(p3-4)</li>\n<li>Early predictions were often optimistic about when human-level AI would come, but rarely considered whether it would pose a risk. (p4-5)</li>\n<li>AI research has been through several cycles of relative popularity and unpopularity. (p5-11)</li>\n<li>By around the 1990s, 'Good Old-Fashioned Artificial Intelligence' (GOFAI) techniques based on symbol manipulation gave way to new methods such as artificial neural networks and genetic algorithms. These are widely considered more promising, in part because they are less brittle and can learn from experience more usefully. Researchers have also lately developed a better understanding of the underlying mathematical relationships between various modern approaches. (p5-11)</li>\n<li>AI is very good at playing board games. (12-13)</li>\n<li>AI is used in many applications today (e.g. hearing aids, route-finders, recommender systems, medical decision support systems, machine translation, face recognition, scheduling, the financial market). (p14-16)</li>\n<li>In general, tasks we thought were intellectually demanding (e.g. board games) have turned out to be easy to do with AI, while tasks which seem easy to us (e.g. identifying objects) have turned out to be hard. (p14)</li>\n<li>An 'optimality notion' is the combination of a rule for learning, and a rule for making decisions. Bostrom describes one of these: a kind of ideal Bayesian agent. This is impossible to actually make, but provides a useful measure for judging imperfect agents against. (p10-11)</li>\n</ol>\n<h1 id=\"Notes_on_a_few_things\">Notes on a few things</h1>\n<ol>\n<li><strong>What is 'superintelligence'?</strong> (p22 spoiler)<br>In case you are too curious about what the topic of this book is to wait until week 3, a 'superintelligence' will soon be described as <em>'any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest'</em>. Vagueness in this definition will be cleared up later.&nbsp;</li>\n<li><strong>What is 'AI'? <br></strong>In particular, how does 'AI' differ from other computer software? The line is blurry, but basically AI research seeks to replicate the useful 'cognitive' functions of human brains ('cognitive' is perhaps unclear, but for instance it doesn't have to be squishy or prevent your head from imploding). Sometimes AI research tries to copy the methods used by human brains. Other times it tries to carry out the same broad functions as a human brain, perhaps better than a human brain. <a href=\"http://www.amazon.com/Artificial-Intelligence-Modern-Approach-Edition/dp/0136042597\">Russell and Norvig</a> (p2) divide prevailing definitions of AI into four categories: 'thinking humanly', 'thinking rationally', 'acting humanly' and 'acting rationally'. For our purposes however, the distinction is probably not too important.</li>\n<li><strong>What is 'human-level' AI?&nbsp;<br></strong>We are going to talk about 'human-level' AI a lot, so it would be good to be clear on what that is. Unfortunately the term is used in various ways, and often ambiguously. So we probably can't be that clear on it, but let us at least be clear on how the term is unclear.&nbsp;<br><br>One big ambiguity is whether you are talking about a machine that can carry out tasks as well as a human&nbsp;<em>at any price</em>, or a machine that can carry out tasks as well as a human&nbsp;<em>at the price of a human</em>. These are quite different, especially in their immediate social implications.<br><br>Other ambiguities arise in how 'levels' are measured. If AI systems were to replace almost all humans in the economy, but only because they are so much cheaper - though they often do a lower quality job - are they human level?&nbsp;What exactly does the AI need to be human-level&nbsp;at? Anything you can be paid for? Anything a human is good for? Just mental tasks? Even mental tasks like daydreaming?&nbsp;Which or how many humans does the AI need to be the same level as? Note that in a sense most humans have been replaced in their jobs before (almost everyone used to work in farming), so if you use that metric for human-level AI, it was reached long ago, and perhaps farm machinery is human-level AI. This is probably not what we want to point at.<br><br>Another thing to be aware of is the diversity of mental skills. If by 'human-level' we mean a machine that is&nbsp;at least&nbsp;as good as a human at each of these skills, then in practice the first 'human-level' machine will be much better than a human on many of those skills. It may not seem 'human-level' so much as&nbsp;'very super-human'.<br><br>We could instead think of human-level as closer to 'competitive with a human' - where the machine has some super-human talents and lacks some skills humans have. This is not usually used, I think because it is hard to define in a meaningful way. There are already machines for which a company is willing to pay more than a human: in this sense a microscope might be 'super-human'. There is no reason for a machine which is equal in value to a human to have the traits we are interested in talking about here, such as agency, superior cognitive abilities or the tendency to drive humans out of work and shape the future. Thus we talk about AI which is at least as good as a human, but you should beware that the predictions made about such an entity may apply before the entity is technically 'human-level'.<br><br><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_ku6_1.png\" alt=\"\" width=\"334\" height=\"256\"><br>Example of how the first 'human-level' AI may surpass humans in many ways.<br><br>Because of these ambiguities, AI researchers are sometimes hesitant to&nbsp;use the term. e.g. in&nbsp;<a href=\"/r/discussion/lw/999/qa_with_experts_on_risks_from_ai_1/\">these</a>&nbsp;<a href=\"/r/discussion/lw/9a1/qa_with_experts_on_risks_from_ai_2/\">interviews</a>.</li>\n<li><strong>Growth modes (p1)</strong>&nbsp;<br>Robin Hanson wrote the <a href=\"http://hanson.gmu.edu/longgrow.pdf\">seminal paper</a> on this issue. Here's a figure from it, showing the step changes in growth rates. Note that both axes are logarithmic. Note also that the changes between modes don't happen overnight. According to Robin's model, we are still transitioning into the industrial era (p10 in his paper). <img src=\"http://images.lesswrong.com/t3_ku6_0.png?v=b459222bb385e9cd35f91af9b62c7d92\" alt=\"\" width=\"607\" height=\"378\"> </li>\n<li><strong>What causes these transitions between growth modes?</strong>&nbsp;(p1-2)<br>One might be happier making predictions about future growth mode changes if one had a unifying explanation for the previous changes. As far as I know, we have no good idea of what was so special about those two periods. There are many&nbsp;<a href=\"http://en.wikipedia.org/wiki/Industrial_Revolution#Causes\">suggested causes of the industrial revolution</a>, but nothing uncontroversially stands out as 'twice in history' level of special.&nbsp;You might think the small number of datapoints would make this puzzle too hard. Remember however that there are quite a lot of negative datapoints - you need an explanation that&nbsp;<em>didn't happen</em>&nbsp;at all of the other times in history.&nbsp;</li>\n<li><strong>Growth of growth</strong><br>It is also interesting to compare world economic growth to the total size of the world economy. For the last few thousand years, the economy seems to have grown faster more or less in proportion to it's size (see figure below). Extrapolating such a trend would lead to an infinite economy in finite time. In fact for the thousand years until 1950 <a href=\"http://www.aiimpacts.org/historical-growth-trends\">such extrapolation</a> would place an infinite economy in the late 20th Century! The time since 1950 has been strange apparently.&nbsp;<br><img src=\"http://images.lesswrong.com/t3_ku6_2.png?v=9f8861672d59c2fe622f594e4f04230e\" alt=\"\" width=\"652\" height=\"280\"><br>(Figure from <a href=\"http://www.aiimpacts.org/historical-growth-trends\">here</a>)</li>\n<li><strong>Early AI programs mentioned in the book</strong>&nbsp;(p5-6)<br>You can see them in action: <a href=\"https://www.youtube.com/watch?v=QAJz4YKUwqw\">SHRDLU</a>, <a href=\"https://www.youtube.com/watch?v=qXdn6ynwpiI\">Shakey</a>, <a href=\"http://ai-su13.artifice.cc/gps.html\">General Problem Solver</a> (not quite in action), <a href=\"http://psych.fullerton.edu/mbirnbaum/psych101/Eliza.htm\">ELIZA</a>.</li>\n<li><strong>Later AI programs mentioned in the book</strong>&nbsp;(p6)<br><a href=\"https://www.youtube.com/watch?v=CgG1HipAayU&amp;list=PL8nIR9RW0CkYcjsYNbDWzBG_vv1yeDXq0&amp;index=4\">Algorithmically generated Beethoven</a>, <a href=\"http://www.genetic-programming.com/inventionmachine.html\">algorithmic generation of patentable inventions</a>,&nbsp;<a href=\"http://homepages.abdn.ac.uk/wpn006/software.php\">artificial comedy</a>&nbsp;(requires download).</li>\n<li><strong>Modern AI algorithms mentioned</strong>&nbsp;(p7-8, 14-15)&nbsp;<br><a href=\"http://www.clarifai.com/\">Here</a> is a neural network doing image recognition. Here is&nbsp;<a href=\"https://www.youtube.com/watch?v=QRY7mEjbT8A\">artificial evolution of jumping</a>&nbsp;and of <a href=\"http://boxcar2d.com/\">toy cars</a>. Here is a&nbsp;<a href=\"http://rekognition.com/demo/face\">face detection demo</a>&nbsp;that can tell you your attractiveness&nbsp;(apparently not reliably), happiness, age, gender, and which celebrity it mistakes you for.</li>\n<li><strong>What is maximum likelihood estimation?</strong>&nbsp;(p9)<br>Bostrom points out that many types of artificial neural network can be viewed as classifiers that perform 'maximum likelihood estimation'. If you haven't come across this term before, the idea is to find the situation that would make your observations most probable. For instance, suppose a person writes to you and tells you that you have won a car. The situation that would have made this scenario most probable is the one where you have won a car, since in that case you are almost guaranteed to be told about it. Note that this doesn't imply that you should think you won a car, if someone tells you that. Being the target of a spam email might only give you a low probability of being told that you have won a car (a spam email may instead advise you of products, or tell you that you have won a boat), but spam emails are so much more common than actually winning cars that most of the time if you get such an email, you will not have won a car. If you would like a better intuition for maximum likelihood estimation, Wolfram Alpha has&nbsp;<a href=\"http://demonstrations.wolfram.com/MaximumLikelihoodEstimation/\">several</a>&nbsp;<a href=\"http://demonstrations.wolfram.com/search.html?query=maximum%20likelihood\">demonstrations</a>&nbsp;(requires free download).</li>\n<li><strong>What are hill climbing algorithms like?</strong>&nbsp;(p9)<br>The second large class of algorithms Bostrom mentions are hill climbing algorithms. The <a href=\"http://en.wikipedia.org/wiki/Hill_climbing\">idea</a> here is fairly straightforward, but if you would like a better basic intuition for what hill climbing looks like, Wolfram Alpha has a&nbsp;<a href=\"http://demonstrations.wolfram.com/HillClimbingAlgorithm/\">demonstration</a>&nbsp;to play with (requires free download).</li>\n</ol>\n<h1 id=\"In_depth_investigations\">In-depth investigations</h1>\n<p>If you are particularly interested in these topics, and want to do further research, these are a few plausible directions:</p>\n<ol>\n<li>How have investments into AI changed over time? <a href=\"http://intelligence.org/2014/01/28/how-big-is-ai/\">Here's</a> a start, estimating the size of the field.</li>\n<li>What does progress in AI look like in more detail? What can we infer from it? I wrote about <a href=\"http://intelligence.org/files/AlgorithmicProgress.pdf\">algorithmic improvement</a> curves before. If you are interested in plausible next steps here, ask me.</li>\n<li>What do economic models tell us about the consequences of human-level AI? <a href=\"http://hanson.gmu.edu/aigrow.pdf\">Here</a>&nbsp;<a href=\"http://agi-conf.org/2010/wp-content/uploads/2009/06/agi10singmodels2.pdf\">is</a>&nbsp;<a href=\"http://mason.gmu.edu/~gjonesb/AIandGrowth\">some</a> such thinking;&nbsp;Eliezer Yudkowsky <a href=\"http://intelligence.org/files/IEM.pdf\">has written at length about his request for more</a>.</li>\n</ol>\n<h1 id=\"How_to_proceed\">How to proceed</h1>\n<p>This has been a collection of notes on the chapter. <strong>The most important part of the reading group though is discussion</strong>, which is in the comments section. I pose some questions for you there, and I invite you to add your own. Please remember that this group contains a variety of levels of expertise: if a line of discussion seems too basic or too incomprehensible, look around for one that suits you better!</p>\n<p>Next week, we will talk about what AI researchers think about human-level AI: when it will arrive, what it will be like, and what the consequences will be. To prepare, <strong>read</strong> <em>Opinions about the future of machine intelligence</em> from Chapter 1 and also&nbsp;<em><a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\">When Will AI Be Created?</a>&nbsp;</em>by Luke Muehlhauser<em>. </em>The discussion will go live at 6pm Pacific time next Monday 22 September. Sign up to be notified <a href=\"http://intelligence.us5.list-manage.com/subscribe?u=353906382677fa789a483ba9e&amp;id=28cb982f40\">here</a>.</p>", "sections": [{"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "Notes on a few things", "anchor": "Notes_on_a_few_things", "level": 1}, {"title": "In-depth investigations", "anchor": "In_depth_investigations", "level": 1}, {"title": "How to proceed", "anchor": "How_to_proceed", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "233 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 233, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QDmzDZ9CEHrKQdvcn", "okmpRuKjhG9dvDh3Z", "xoxZdRtpyRnXmhher"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-16T03:14:27.316Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Tortoises", "slug": "meetup-urbana-champaign-tortoises", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jack_LaSota", "createdAt": "2014-09-07T15:51:30.606Z", "isAdmin": false, "displayName": "Jack_LaSota"}, "userId": "Bj6zTrdwXRLSwp4xD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QpYemK6Zw2psp4sKy/meetup-urbana-champaign-tortoises", "pageUrlRelative": "/posts/QpYemK6Zw2psp4sKy/meetup-urbana-champaign-tortoises", "linkUrl": "https://www.lesswrong.com/posts/QpYemK6Zw2psp4sKy/meetup-urbana-champaign-tortoises", "postedAtFormatted": "Tuesday, September 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Tortoises&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Tortoises%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQpYemK6Zw2psp4sKy%2Fmeetup-urbana-champaign-tortoises%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Tortoises%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQpYemK6Zw2psp4sKy%2Fmeetup-urbana-champaign-tortoises", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQpYemK6Zw2psp4sKy%2Fmeetup-urbana-champaign-tortoises", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14j'>Urbana-Champaign: Tortoises</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 September 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">206 S. Cedar St, 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>What kind of <a href=\"http://agentyduck.blogspot.com/2014/08/small-consistent-effort-uncharted.html\" rel=\"nofollow\">easy</a> exercises can we do every day to become a little bit more rational? How can we tell if they're working?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14j'>Urbana-Champaign: Tortoises</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QpYemK6Zw2psp4sKy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.0086006012530575e-06, "legacy": true, "legacyId": "27210", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Tortoises\">Discussion article for the meetup : <a href=\"/meetups/14j\">Urbana-Champaign: Tortoises</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 September 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">206 S. Cedar St, 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>What kind of <a href=\"http://agentyduck.blogspot.com/2014/08/small-consistent-effort-uncharted.html\" rel=\"nofollow\">easy</a> exercises can we do every day to become a little bit more rational? How can we tell if they're working?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Tortoises1\">Discussion article for the meetup : <a href=\"/meetups/14j\">Urbana-Champaign: Tortoises</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Tortoises", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Tortoises", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Tortoises", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Tortoises1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-16T13:33:37.764Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, September 16-30", "slug": "group-rationality-diary-september-16-30", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:39.095Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hMReCnYz39fRupJAa/group-rationality-diary-september-16-30", "pageUrlRelative": "/posts/hMReCnYz39fRupJAa/group-rationality-diary-september-16-30", "linkUrl": "https://www.lesswrong.com/posts/hMReCnYz39fRupJAa/group-rationality-diary-september-16-30", "postedAtFormatted": "Tuesday, September 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20September%2016-30&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20September%2016-30%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhMReCnYz39fRupJAa%2Fgroup-rationality-diary-september-16-30%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20September%2016-30%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhMReCnYz39fRupJAa%2Fgroup-rationality-diary-september-16-30", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhMReCnYz39fRupJAa%2Fgroup-rationality-diary-september-16-30", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 24.2727279663086px; text-align: justify;\">This is the public group instrumental rationality diary for September 16-30.</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 24.2727279663086px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em;\">It's a place to record and chat about it if you have done, or are actively doing, things like:&nbsp;</p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 24.2727279663086px; text-align: justify;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 24.2727279663086px; text-align: justify;\"><span style=\"line-height: 24.2727279663086px;\">Previous diary: </span><a style=\"line-height: 24.2727279663086px;\" href=\"/lw/kwi/group_rationality_diary_september_115/\">September 1-15</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 24.2727279663086px; text-align: justify;\">Next diary:&nbsp;<a href=\"/r/discussion/lw/l1z/group_rationality_diary_october_115/\">October 1-15</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 24.2727279663086px; text-align: justify;\"><a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hMReCnYz39fRupJAa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "27211", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QtTp8eFW6wzQ3FK3r", "JQPQqGMBRxaKbkETT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-16T13:39:29.213Z", "modifiedAt": null, "url": null, "title": "Any LessWrong readers at the University of Michigan?", "slug": "any-lesswrong-readers-at-the-university-of-michigan", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:34.099Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Asymmetric", "createdAt": "2010-11-28T17:46:59.113Z", "isAdmin": false, "displayName": "Asymmetric"}, "userId": "3cXSbrTN5k7oZYpyz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/44DHmfbykweFWi7oF/any-lesswrong-readers-at-the-university-of-michigan", "pageUrlRelative": "/posts/44DHmfbykweFWi7oF/any-lesswrong-readers-at-the-university-of-michigan", "linkUrl": "https://www.lesswrong.com/posts/44DHmfbykweFWi7oF/any-lesswrong-readers-at-the-university-of-michigan", "postedAtFormatted": "Tuesday, September 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Any%20LessWrong%20readers%20at%20the%20University%20of%20Michigan%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAny%20LessWrong%20readers%20at%20the%20University%20of%20Michigan%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F44DHmfbykweFWi7oF%2Fany-lesswrong-readers-at-the-university-of-michigan%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Any%20LessWrong%20readers%20at%20the%20University%20of%20Michigan%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F44DHmfbykweFWi7oF%2Fany-lesswrong-readers-at-the-university-of-michigan", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F44DHmfbykweFWi7oF%2Fany-lesswrong-readers-at-the-university-of-michigan", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 23, "htmlBody": "<p>I'm interested in gauging interest in a LessWrong group at UM -- probably a Facebook group, as opposed to an official University club.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "44DHmfbykweFWi7oF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 2.0097288733295128e-06, "legacy": true, "legacyId": "27212", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-16T16:27:46.685Z", "modifiedAt": null, "url": null, "title": "Meetup : Warsaw, next week!", "slug": "meetup-warsaw-next-week", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "LvfE3p4i6uy8Exadr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GzCua9SbQeLx8xb5K/meetup-warsaw-next-week", "pageUrlRelative": "/posts/GzCua9SbQeLx8xb5K/meetup-warsaw-next-week", "linkUrl": "https://www.lesswrong.com/posts/GzCua9SbQeLx8xb5K/meetup-warsaw-next-week", "postedAtFormatted": "Tuesday, September 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Warsaw%2C%20next%20week!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Warsaw%2C%20next%20week!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGzCua9SbQeLx8xb5K%2Fmeetup-warsaw-next-week%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Warsaw%2C%20next%20week!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGzCua9SbQeLx8xb5K%2Fmeetup-warsaw-next-week", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGzCua9SbQeLx8xb5K%2Fmeetup-warsaw-next-week", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14k'>Warsaw, next week!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 September 2014 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Warsaw</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I invite you to participate in a meetup next week; exact date will be chosen by voting here: <a href=\"http://doodle.com/2dshyg6hqbth72zvnp96sa73/admin#table.\" rel=\"nofollow\">http://doodle.com/2dshyg6hqbth72zvnp96sa73/admin#table.</a> Time and place will be posted here later.</p>\n\n<p>If you use Facebook, please join our local group for better coordination: <a href=\"https://www.facebook.com/groups/lwwarsaw/\" rel=\"nofollow\">https://www.facebook.com/groups/lwwarsaw/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14k'>Warsaw, next week!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GzCua9SbQeLx8xb5K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 2.010032854761021e-06, "legacy": true, "legacyId": "27213", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Warsaw__next_week_\">Discussion article for the meetup : <a href=\"/meetups/14k\">Warsaw, next week!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 September 2014 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Warsaw</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I invite you to participate in a meetup next week; exact date will be chosen by voting here: <a href=\"http://doodle.com/2dshyg6hqbth72zvnp96sa73/admin#table.\" rel=\"nofollow\">http://doodle.com/2dshyg6hqbth72zvnp96sa73/admin#table.</a> Time and place will be posted here later.</p>\n\n<p>If you use Facebook, please join our local group for better coordination: <a href=\"https://www.facebook.com/groups/lwwarsaw/\" rel=\"nofollow\">https://www.facebook.com/groups/lwwarsaw/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Warsaw__next_week_1\">Discussion article for the meetup : <a href=\"/meetups/14k\">Warsaw, next week!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Warsaw, next week!", "anchor": "Discussion_article_for_the_meetup___Warsaw__next_week_", "level": 1}, {"title": "Discussion article for the meetup : Warsaw, next week!", "anchor": "Discussion_article_for_the_meetup___Warsaw__next_week_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-16T21:41:10.712Z", "modifiedAt": null, "url": null, "title": "Should EA's be Superrational cooperators? ", "slug": "should-ea-s-be-superrational-cooperators", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:59.987Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DqaX4zeMp96ZYeomx/should-ea-s-be-superrational-cooperators", "pageUrlRelative": "/posts/DqaX4zeMp96ZYeomx/should-ea-s-be-superrational-cooperators", "linkUrl": "https://www.lesswrong.com/posts/DqaX4zeMp96ZYeomx/should-ea-s-be-superrational-cooperators", "postedAtFormatted": "Tuesday, September 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20EA's%20be%20Superrational%20cooperators%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20EA's%20be%20Superrational%20cooperators%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDqaX4zeMp96ZYeomx%2Fshould-ea-s-be-superrational-cooperators%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20EA's%20be%20Superrational%20cooperators%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDqaX4zeMp96ZYeomx%2Fshould-ea-s-be-superrational-cooperators", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDqaX4zeMp96ZYeomx%2Fshould-ea-s-be-superrational-cooperators", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 388, "htmlBody": "<p>Back in 2012 when visiting Leverage Research, I was amazed by the level of cooperation in daily situations I got from Mark. Mark wasn't just nice, or kind, or generous. Mark seemed to be playing a <em>different game</em> than everyone else.</p>\n<p>If someone needed X, and Mark had X, he would provide X to them. This was true for lending, but also for giving away.</p>\n<p>If there was a situation in which someone needed to direct attention to a particular topic, Mark would do it.</p>\n<p>You get the picture. Faced with prisoner dilemmas, Mark would cooperate. Faced with tragedy of the commons, Mark would cooperate. Faced with non-egalitarian distributions of resources, time or luck (which are convoluted forms of the dictator game), Mark would rearrange resources without any indexical evaluation. The action would be the same, and the consequentialist one, regardless of which side of a dispute was the Mark side.</p>\n<p>I never got over that impression. The impression that I could try to be as cooperative as my idealized fiction of Mark was.</p>\n<p>In game theoretic terms, Mark was a <strong>Coo</strong>perational agent.</p>\n<ol>\n<li><strong>Alt</strong>ruistic - MaxOther</li>\n<li><strong>Coo</strong>perational - MaxSum</li>\n<li><strong>Ind</strong>ividualist - MaxOwn</li>\n<li><strong>Equ</strong>alitarian - MinDiff</li>\n<li><strong>Com</strong>petitive - MaxDiff</li>\n<li><strong>Agg</strong>ressive - MinOther</li>\n</ol>\n<p>Under these definitions of kinds of agents used in research on game theoretical scenarios, what we call Effective Altruism would be called Effective Cooperation. The reason why we call it \"altruism\" is because even the most parochial EA's care about a set containing a minimum of 7 billion minds, where to a first approximation MaxSum &asymp; MaxOther.</p>\n<p>Locally however the distinction makes sense. In biology Altruism usually refers to a third concept, different from both the \"A\" in EA, and <strong>Alt</strong>, it means acting in such a way that Other&gt;Own without reference to maximizing or minimizing, since evolution designs <a href=\"/lesswrong.com/lw/l0/adaptationexecuters_not_fitnessmaximizers\">adaptation executors, not maximizers</a>.</p>\n<p>A globally <strong>Coo</strong>perational agent acts as a consequentialist globally. So does an <strong>Alt </strong>agent. <br /><br />The question then is,</p>\n<p><strong>How should a consequentialist act locally?</strong></p>\n<p>The mathematical response is obviously as a&nbsp;<strong>Coo</strong>. What real people do is a mix of <strong>Coo </strong>and <strong>Ind</strong>.</p>\n<p>My suggestion is that we use our undesirable yet unavoidable moral tribe distinction instinct, the one that separates Us from Them, and act always as <strong>Coo</strong>s with Effective Altruists and<em> </em>mix <strong>Coo </strong>and <strong>Ind</strong> only with non EAs. That is what Mark did.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DqaX4zeMp96ZYeomx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 2.010599161315658e-06, "legacy": true, "legacyId": "26997", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-17T04:01:44.777Z", "modifiedAt": null, "url": null, "title": "Meetup : Perth, Australia: Games night", "slug": "meetup-perth-australia-games-night", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "4r4aKXsWgDHchNwHh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uoq9Ee2Y8Lt8LmMXE/meetup-perth-australia-games-night", "pageUrlRelative": "/posts/uoq9Ee2Y8Lt8LmMXE/meetup-perth-australia-games-night", "linkUrl": "https://www.lesswrong.com/posts/uoq9Ee2Y8Lt8LmMXE/meetup-perth-australia-games-night", "postedAtFormatted": "Wednesday, September 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Perth%2C%20Australia%3A%20Games%20night&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Perth%2C%20Australia%3A%20Games%20night%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuoq9Ee2Y8Lt8LmMXE%2Fmeetup-perth-australia-games-night%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Perth%2C%20Australia%3A%20Games%20night%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuoq9Ee2Y8Lt8LmMXE%2Fmeetup-perth-australia-games-night", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuoq9Ee2Y8Lt8LmMXE%2Fmeetup-perth-australia-games-night", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 89, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14l'>Perth, Australia: Games night</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 October 2014 06:00:00PM (+0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Sync Labs, 6/663 Newcastle Street, Leederville, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come play <a href=\"https://en.wikipedia.org/wiki/Zendo_%28game%29\" rel=\"nofollow\">Zendo</a>, an inductive logic game. Rowdy will teach us, and I'm told the rules are pretty simple.</p>\n\n<p>Zendo seems cool because it can perhaps teach the skill of <a href=\"http://lesswrong.com/lw/iw/positive_bias_look_into_the_dark/\">&quot;looking into the dark&quot;</a>.</p>\n\n<p>We'll be at <a href=\"http://synclabs.com.au/\" rel=\"nofollow\">Sync Labs</a>, a coworking space in Leederville. The entrance is between Niche and Cranked. Don't trust Google Maps!</p>\n\n<p>You can RSVP here: <a href=\"http://www.meetup.com/Perth-Less-Wrong/events/207744102/\" rel=\"nofollow\">http://www.meetup.com/Perth-Less-Wrong/events/207744102/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14l'>Perth, Australia: Games night</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uoq9Ee2Y8Lt8LmMXE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "27215", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Perth__Australia__Games_night\">Discussion article for the meetup : <a href=\"/meetups/14l\">Perth, Australia: Games night</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 October 2014 06:00:00PM (+0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Sync Labs, 6/663 Newcastle Street, Leederville, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come play <a href=\"https://en.wikipedia.org/wiki/Zendo_%28game%29\" rel=\"nofollow\">Zendo</a>, an inductive logic game. Rowdy will teach us, and I'm told the rules are pretty simple.</p>\n\n<p>Zendo seems cool because it can perhaps teach the skill of <a href=\"http://lesswrong.com/lw/iw/positive_bias_look_into_the_dark/\">\"looking into the dark\"</a>.</p>\n\n<p>We'll be at <a href=\"http://synclabs.com.au/\" rel=\"nofollow\">Sync Labs</a>, a coworking space in Leederville. The entrance is between Niche and Cranked. Don't trust Google Maps!</p>\n\n<p>You can RSVP here: <a href=\"http://www.meetup.com/Perth-Less-Wrong/events/207744102/\" rel=\"nofollow\">http://www.meetup.com/Perth-Less-Wrong/events/207744102/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Perth__Australia__Games_night1\">Discussion article for the meetup : <a href=\"/meetups/14l\">Perth, Australia: Games night</a></h2>", "sections": [{"title": "Discussion article for the meetup : Perth, Australia: Games night", "anchor": "Discussion_article_for_the_meetup___Perth__Australia__Games_night", "level": 1}, {"title": "Discussion article for the meetup : Perth, Australia: Games night", "anchor": "Discussion_article_for_the_meetup___Perth__Australia__Games_night1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rmAbiEKQDpDnZzcRf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-17T13:49:35.217Z", "modifiedAt": null, "url": null, "title": "Link: How Community Feedback Shapes User Behavior", "slug": "link-how-community-feedback-shapes-user-behavior", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:34.996Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tyrrell_McAllister", "createdAt": "2009-03-05T19:59:57.157Z", "isAdmin": false, "displayName": "Tyrrell_McAllister"}, "userId": "HSANMQBsHiGrZzwTB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MotL9d4PKFHSgo7ot/link-how-community-feedback-shapes-user-behavior", "pageUrlRelative": "/posts/MotL9d4PKFHSgo7ot/link-how-community-feedback-shapes-user-behavior", "linkUrl": "https://www.lesswrong.com/posts/MotL9d4PKFHSgo7ot/link-how-community-feedback-shapes-user-behavior", "postedAtFormatted": "Wednesday, September 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20How%20Community%20Feedback%20Shapes%20User%20Behavior&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20How%20Community%20Feedback%20Shapes%20User%20Behavior%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMotL9d4PKFHSgo7ot%2Flink-how-community-feedback-shapes-user-behavior%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20How%20Community%20Feedback%20Shapes%20User%20Behavior%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMotL9d4PKFHSgo7ot%2Flink-how-community-feedback-shapes-user-behavior", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMotL9d4PKFHSgo7ot%2Flink-how-community-feedback-shapes-user-behavior", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p><a href=\"http://cs.stanford.edu/people/jure/pubs/disqus-icwsm14.pdf\">This article</a> discusses how upvotes and downvotes influence the quality of posts on online communities.&nbsp; The article claims that downvotes lead to more posts of lower quality from the downvoted commenter.</p>\n<p>From the abstract:</p>\n<p style=\"padding-left: 30px;\">Social media systems rely on user feedback and rating mechanisms for personalization, ranking, and content filtering. [...] This paper investigates how ratings on a piece of content affect its author&rsquo;s future behavior. [...] [W]e find that negative feedback leads to significant behavioral changes that are detrimental to the community.&nbsp; Not only do authors of negatively-evaluated content contribute more, but also their future posts are of lower quality, and are perceived by the community as such.&nbsp; In contrast, positive feedback does not carry similar effects, and neither encourages rewarded authors to write more, nor improves the quality of their posts.</p>\n<p>The authors of the article are Justin Cheng, Cristian Danescu-Niculescu-Mizil, and Jure Leskovec.</p>\n<p><strong>Edited to add:</strong> <span class=\"comment-author\"><span class=\"author\">NancyLebovitz already <a href=\"/lw/kxv/open_thread_september_814_2014/bbgs\">posted</a> about this study in the Open Thread from September 8-14, 2014. </span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KADf7NFiAr9DAaQxN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MotL9d4PKFHSgo7ot", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 2.0123508357753338e-06, "legacy": true, "legacyId": "27217", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-17T14:19:02.157Z", "modifiedAt": null, "url": null, "title": "What It's Like to Notice Things", "slug": "what-it-s-like-to-notice-things", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:32.634Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BrienneYudkowsky", "createdAt": "2013-05-13T00:07:08.935Z", "isAdmin": false, "displayName": "LoganStrohl"}, "userId": "uuYBzWLiixkbN3s7C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o7W3oB8SR679HGeAq/what-it-s-like-to-notice-things", "pageUrlRelative": "/posts/o7W3oB8SR679HGeAq/what-it-s-like-to-notice-things", "linkUrl": "https://www.lesswrong.com/posts/o7W3oB8SR679HGeAq/what-it-s-like-to-notice-things", "postedAtFormatted": "Wednesday, September 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20It's%20Like%20to%20Notice%20Things&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20It's%20Like%20to%20Notice%20Things%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo7W3oB8SR679HGeAq%2Fwhat-it-s-like-to-notice-things%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20It's%20Like%20to%20Notice%20Things%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo7W3oB8SR679HGeAq%2Fwhat-it-s-like-to-notice-things", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo7W3oB8SR679HGeAq%2Fwhat-it-s-like-to-notice-things", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2803, "htmlBody": "<h2>Phenomenology</h2>\n<p>Phenomenology is the study of the structures of experience and consciousness. Literally, it is the study of \"that which appears\". The first time you look at a twig sticking up out of the water, you might be curious and ask, \"What forces cause things to bend when placed in water?\" If you're a curious phenomenologist, though, you'll ask things like, \"Why does that twig in water appear as though bent? Do other things appear to bend when placed in water? Do all things placed in water appear to bend to the same degree? Are there things that do not appear to bend when placed in water? Does my perception of the bending depend on the angle or direction from which I observe the twig?\"</p>\n<p>Pehenomenology means breaking experience down to its more basic components, and being precise in our descriptions of what we actually observe, free of further speculation and assumption. A phenomenologist recognizes the difference between observing \"a six-sided cube\", and observing the three faces, at most, from which we extrapolate the rest.</p>\n<p>I consider phenomenology to be a central skill of rationality. The most obvious example: You're unlikely to generate alternative hypotheses when the confirming observation and the favored hypothesis are one and the same in your experience of experience. The importance of phenomenology to rationality goes deeper than that, though. Phenomenology trains especially fine grained introspection. The more tiny and subtle are the thoughts you're aware of, the more precise can be the control you gain over the workings of your mind, and the faster can be your cognitive reflexes.</p>\n<p>(I do not at all mean to say that you should go read Husserl and Heidegger. Despite their apparent potential for unprecedented clarity, the phenomenologists, without exception, seem to revel in obfuscation. It's probably not worth your time to wade through all of that nonsense. I've mostly read about phenomenology myself for this very reason.)</p>\n<p>I've been doing some experimental phenomenology of late.</p>\n<h2>Noticing</h2>\n<p>I've noticed that rationality, in practice, depends on noticing. Some people have told me this is basically tautological, and therefore uninteresting. But if I'm right, I think it's likely very important to know, and to train deliberately.</p>\n<p>The difference between seeing the twig as bent and seeing the twig as seeming bent may seem inane. It is not news that things that are bent tend to seem bent. Without that level of granularity in your observations, though, you may not notice that it could be possible for things to merely seem bent without being bent. When we're talking about something that may be ubiquitous to all applications of rationality, like noticing, it's worth taking a closer look at the contents of our experiences.</p>\n<p>Many people talk about \"noticing confusion\", because <a href=\"/lw/if/your_strength_as_a_rationalist/\">Eliezer's written about it</a>. Really, though, every successful application of a rationality skill begins with noticing. In particular, applied rationality is founded on noticing opportunities and obstacles. (To be clear, I'm making this up right this moment, so as far as I know it's not a generally agreed-upon thing. That goes for nearly everything in this post. I still think it's true.) You can be the most technically skilled batter in the world, and it won't help a bit if you consistently fail to notice when the ball whizzes by you--if you miss the opportunities to swing. And you're not going to run very many bases if you launch the ball straight at an opposing catcher--if you're oblivious to the obstacles.</p>\n<p>It doesn't matter how many techniques you've learned if you miss all the opportunities to apply them, and fail to notice the obstacles when they get in your way. Opportunities and obstacles are everywhere. We can only be as strong as our ability to notice the ones that will make a difference.</p>\n<p>Inspired by Whales' <a href=\"/lw/jpu/a_selfexperiment_in_training_noticing_confusion/\">self-experiment in noticing confusion</a>, I've been practicing noticing things. Not difficult or complicated things, like noticing confusion, or noticing biases. I've just been trying to get a handle on noticing, full stop. And it's been interesting.</p>\n<h2>Noticing Noticing</h2>\n<p>What does it mean to notice something, and what does it feel like?</p>\n<p>I started by checking to see what I expected it to feel like to notice that it's raining, just going from memory. I tried for a split-second prediction, to find what my brain automatically stored under \"noticing rain\". When I thought about noticing rain, I got this sort of vague impression of rainyness, which included few sensory details and was more of an overall rainy feeling. My brain tried to tell me that \"noticing rain\" meant \"being directly acquainted with rainyness\", in much the same way that it tries to tell me it's experiencing a cube when it's actually only experiencing a pattern of light and shadows I interpret as three faces.</p>\n<p>Then, I waited for rain. It didn't take long, because I'm in North Carolina for the month. (This didn't happen last time I was in North Carolina, so perhaps I just happened to choose The One Valley of Eternal Rain.)</p>\n<p>The real \"noticing rain\" turned out to be a response to the physical sensations concurrent with the first raindrop falling on my skin. I did eventually have an \"abstract rainyness feeling\", but that happened a full two seconds later. My actual experience went like this.</p>\n<p>It was cloudy and humid. This was not at the forefront of my attention, but it slowly moved in that direction as the temperature dropped. I was fairly focused on reading a book.</p>\n<p>(I'm a little baffled by the apparent gradient between \"not at all conscious of x\" and \"fully aware of x\". I don't know how that works, but I experience the difference between being a little aware of the sky being cloudy and being focused on the patterns of light in the clouds, as analogous to the difference between being very-slightly-but-not-uncomfortably warm and burning my hand on the stove.)<br /><br />My awareness of something like an \"abstract rainyness feeling\" moved further toward consciousness as the wind picked up. Suddenly--and the suddenness was an important part of the experience--I felt something like a cool, dull pin-prick on my arm. I looked at it, saw the water, and recognized it as a raindrop. Over the course of about half a second, several sensations leapt forward into full awareness: the darkness of my surroundings, the humidity in the air, the dark grey-blueness of the sky, the sound of rain on leaves like television static, the scent of ozone and damp earth, the feeling of cool humid wind on my face, and the word \"rain\" in my internal monologue.<br /><br />I think it is that sudden leaping forward of many associated sensations that I would call \"noticing rain\".</p>\n<p>After that, I felt a sort of mental step backward--though it was more like a zooming out or sliding away than a discrete step--from the sensations, and then a feeling of viewing them from the outside. There was a sensation of the potential to access other memories of times when it's rained.</p>\n<p>(Sensations of potential are fascinating to me. I noticed a few weeks ago that after memorizing a list of names and faces, I could predict in the first half second of seeing the face whether or not I'd be able to retrieve the name in the next five seconds. Before I actually retrieved the name. What??? I don't know either.)</p>\n<p>Only then did all of it resolve into the more distant and abstract \"feeling of rainyness\" that I'd predicted before. The resolution took four times as long as the simultaneous-leaping-into-consciousness-of-related-sensations that I now prefer to call \"noticing\", and ten times as long as the first-raindrop-pin-prick, which I think I'll call the \"noticing trigger\" if it turns out to be a general class of pre-noticing experiences.</p>\n<p>(\"Can you really distinguish between 200 and 500 milliseconds?\" Yes, but it's an acquired skill. I spent a block of a few minutes every day for a month, then several blocks a day for about a week, doing this <a href=\"http://www.wellcomecollection.org/tiredness/index.html\">Psychomotor Vigiliance Task</a> when I was gathering data for the <a href=\"/lw/hyr/seed_study_polyphasic_sleep_in_ten_steps/\">polyphasic sleep experiment</a>. (No, I'm sorry, to the best of my knowledge Leverage has not yet published anything on the results of this. Long story short: Everyone who wasn't already polyphasic is still not polyphasic today.) It gives you fast feedback on simple response time. I'm not sure if it's useful for anything else, but it comes in handy when taking notes on experiences that pass very quickly.)</p>\n<h2>Noticing Environmental Cues</h2>\n<p>My second experiment was in repeated noticing. This is more closely related to <a href=\"http://slatestarcodex.com/2014/06/09/constant-vigilance/\">rationality as habit cultivation</a>.</p>\n<p>Can I get better at noticing something just by practicing?</p>\n<div>I was trying to zoom in on the experience of noticing itself, so I wanted something as simple as possible. Nothing subtle, nothing psychological, and certainly nothing I might be motivated to ignore. I wanted a straightforward element of my physical environment. I'm out in the country and driving around for errands and such about once a day, so I went with \"red barn roofs\".</div>\n<p>I had an intuition that I should give myself some outward sign of having noticed, lest I not notice that I noticed, and decided to snap my fingers every time I noticed a red barn roof.</p>\n<p>On the first drive, I noticed one red barn roof. That happened when I was almost at my destination and I thought, \"Oh right, I'm supposed to be noticing red barn roofs, oops\" then started actively searching for them.</p>\n<p>Noticing a red barn roof while searching for it feels very different from noticing rain while reading a book. With the rain, it felt sort of like waking up, or like catching my name in an overheard conversation. There was a complete shift in what my brain was doing. With the barn roof, it was like I had a box with a red-barn-roof-shaped hole, and it felt like completion when a I grabbed a roof and dropped it through the hole. I was prepared for the roof, and it was a smaller change in the contents of consciousness.</p>\n<p>I noticed two on the way back, also while actively searching for them, before I started thinking about something else and became oblivious.</p>\n<p>I thought that maybe there weren't enough red barn roofs, and decided to try noticing red roofs of all sorts of buildings the next day. This, it turns out, was the correct move.</p>\n<p>On day two of red-roof-noticing, I got lots of practice. I noticed around fifteen roofs on the way to the store, and around seven on the way back. By the end, I was not searching for the roofs as intently as I had been the day before, but I was still explicitly thinking about the project. I was still aware of directing my eyes to spend extra time at the right level in my field of vision to pick up roofs. It was like waving the box around and waiting for something to fall in, while thinking about how to build boxes.</p>\n<p>I went out briefly again on day two, and on the way back, I noticed a red roof while thinking about something else entirely. Specifically, I was thinking about the possibility of moving to Uruguay, and whether I knew enough Spanish to survive. In the middle of one of those unrelated thoughts, my eyes moved over a barn roof and stayed there briefly while I had the leaping-into-consciousness experience with respect to the sensations of redness, recognizing something as shaped like a building, and feeling the impulse to snap my fingers. It was like I'd been wearing the box as a hat to free up my hands, and I'd forgotten about it. And then, with a heavy ker-thunk, the roof became my new center of attention.</p>\n<p>And oh my gosh, it was so exciting! It sounds so absurd in retrospect to have been excited about noticing a roof. But I was! It meant I'd successfully installed a new cognitive habit to run in the background. On purpose. \"Woo hoo! Yeah!\" (I literally said that.)</p>\n<p>On the third day, I noticed TOO MANY red roofs. I followed the same path to the store as before, but I noticed somewhere between twenty and thirty red roofs. I got about the same number going back, so I think I was catching nearly all the opportunities to notice red roofs. (I'd have to do it for a few days to be sure.) There was a pattern to noticing, where I'd notice-in-the-background, while thinking about something else, the first roof, and then I'd be more specifically on the lookout for a minute or two after that, before my mind wandered back to something other than roofs. I got faster over time at returning to my previous thoughts after snapping my fingers, but there were still enough noticed roofs to intrude uncomfortably upon my thoughts. It was getting annoying.</p>\n<p>So I decided to switch back to only noticing the red roofs of barns in particular.</p>\n<p>Extinction of the more general habit didn't take very long. It was over by the end of my next fifteen minute drive. For the first three times I saw a roof, I rose my hand a little to snap my fingers before reminding myself that I don't care about non-barns anymore. The next couple times I didn't raise my hand, but still forcefully reminded myself of my disinterest in my non-barns. The promotion of red roofs into consciousness got weaker with each roof, until the difference between seeing a non-red non-barn roof and a red non-barn roof was barely perceptible. That was my drive to town today.</p>\n<p>On the drive back, I noticed about ten red barn roofs. Three I noticed while thinking about how to install habits, four while thinking about the differences between designing exercises for in-person workshops and designing exercises to put in books, and three soon enough after the previous barn to probably count as \"searching for barns\".</p>\n<p>So yes, for at least some things, it seems I can get better at noticing them my &nbsp;by practicing.</p>\n<h2>What These Silly Little Experiments Are Really About</h2>\n<p>My plan is to try noticing an internal psychological phenomenon next, but still something straightforward that I wouldn't be motivated not to notice. I probably need to try a couple things to find something that works well. I might go with \"thinking the word 'tomorrow' in my internal monologue\", for example, or possibly \"wondering what my boyfriend is thinking about\". I'll probably go with something more like the first, because it is clearer, and zooms in on \"noticing things inside my head\" without the extra noise of \"noticing things that are relatively temporally indiscrete\", but the second is actually a useful thing to notice.</p>\n<p>Most of the useful things to notice are a lot less obvious than \"thinking the word 'tomorrow' in my internal monologue\". From what I've learned so far, I think that for \"wondering what my boyfriend is thinking about\", I'll need to pick out a couple of very specific, instantaneous sensations that happen when I'm curious what my boyfriend is thinking about. I expect that to be a repetition of the rain experiment, where I predict what it will feel like, then wait 'til I can gather data in real time. Once I have a specific trigger, I can repeat the red roof experiment to catch the tiny moments when I wonder what he's thinking. I might need to start with a broader category, like \"notice when I'm thinking about my boyfriend\", get used to noticing those sensations, and then reduce the set of sensations I'm watching out for to things that happen only when I'm curious what my boyfriend is thinking.</p>\n<p>After that, I imagine I'll want to practice with different kinds of actions I can take when I notice a trigger. (If you've never heard of <a href=\"http://en.wikipedia.org/wiki/Implementation_intention\">Implementation Intentions</a>, I suggest trying them out.) So far, I've used the physical action of snapping my fingers. That was originally for clarity in recognizing the noticing, but it's also a behavioral response to a trigger. I could respond with a psychological behavior instead of a physical one, like \"imagining a carrot\". A useful response to noticing that I'm curious about what my boyfriend is thinking would be \"check to see if he's busy\" and then \"say, 'What are you thinking about?'\"</p>\n<p>See, this \"noticing\" thing sounds boringly simple at first, and not worth much consideration in the art of rationality. Even in his original \"noticing confusion\" post, Eliezer really talked more about recognizing the implications of confusion than about the noticing itself.</p>\n<p>Noticing is more complicated than it seems at first, and it's easy to mix it up with responding. There's a whole sub-art to noticing, and I really think that deliberate practice is making me better at it. Responses can be hard. It's essential to make noticing as effortless as possible. Then you can break the noticing and the responding apart, and you can recognize reality even before you know what to do with it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5gcpKG2XEAZGj5DEf": 9}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o7W3oB8SR679HGeAq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 69, "extendedScore": null, "score": 0.000198, "legacy": true, "legacyId": "27218", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 69, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Phenomenology\">Phenomenology</h2>\n<p>Phenomenology is the study of the structures of experience and consciousness. Literally, it is the study of \"that which appears\". The first time you look at a twig sticking up out of the water, you might be curious and ask, \"What forces cause things to bend when placed in water?\" If you're a curious phenomenologist, though, you'll ask things like, \"Why does that twig in water appear as though bent? Do other things appear to bend when placed in water? Do all things placed in water appear to bend to the same degree? Are there things that do not appear to bend when placed in water? Does my perception of the bending depend on the angle or direction from which I observe the twig?\"</p>\n<p>Pehenomenology means breaking experience down to its more basic components, and being precise in our descriptions of what we actually observe, free of further speculation and assumption. A phenomenologist recognizes the difference between observing \"a six-sided cube\", and observing the three faces, at most, from which we extrapolate the rest.</p>\n<p>I consider phenomenology to be a central skill of rationality. The most obvious example: You're unlikely to generate alternative hypotheses when the confirming observation and the favored hypothesis are one and the same in your experience of experience. The importance of phenomenology to rationality goes deeper than that, though. Phenomenology trains especially fine grained introspection. The more tiny and subtle are the thoughts you're aware of, the more precise can be the control you gain over the workings of your mind, and the faster can be your cognitive reflexes.</p>\n<p>(I do not at all mean to say that you should go read Husserl and Heidegger. Despite their apparent potential for unprecedented clarity, the phenomenologists, without exception, seem to revel in obfuscation. It's probably not worth your time to wade through all of that nonsense. I've mostly read about phenomenology myself for this very reason.)</p>\n<p>I've been doing some experimental phenomenology of late.</p>\n<h2 id=\"Noticing\">Noticing</h2>\n<p>I've noticed that rationality, in practice, depends on noticing. Some people have told me this is basically tautological, and therefore uninteresting. But if I'm right, I think it's likely very important to know, and to train deliberately.</p>\n<p>The difference between seeing the twig as bent and seeing the twig as seeming bent may seem inane. It is not news that things that are bent tend to seem bent. Without that level of granularity in your observations, though, you may not notice that it could be possible for things to merely seem bent without being bent. When we're talking about something that may be ubiquitous to all applications of rationality, like noticing, it's worth taking a closer look at the contents of our experiences.</p>\n<p>Many people talk about \"noticing confusion\", because <a href=\"/lw/if/your_strength_as_a_rationalist/\">Eliezer's written about it</a>. Really, though, every successful application of a rationality skill begins with noticing. In particular, applied rationality is founded on noticing opportunities and obstacles. (To be clear, I'm making this up right this moment, so as far as I know it's not a generally agreed-upon thing. That goes for nearly everything in this post. I still think it's true.) You can be the most technically skilled batter in the world, and it won't help a bit if you consistently fail to notice when the ball whizzes by you--if you miss the opportunities to swing. And you're not going to run very many bases if you launch the ball straight at an opposing catcher--if you're oblivious to the obstacles.</p>\n<p>It doesn't matter how many techniques you've learned if you miss all the opportunities to apply them, and fail to notice the obstacles when they get in your way. Opportunities and obstacles are everywhere. We can only be as strong as our ability to notice the ones that will make a difference.</p>\n<p>Inspired by Whales' <a href=\"/lw/jpu/a_selfexperiment_in_training_noticing_confusion/\">self-experiment in noticing confusion</a>, I've been practicing noticing things. Not difficult or complicated things, like noticing confusion, or noticing biases. I've just been trying to get a handle on noticing, full stop. And it's been interesting.</p>\n<h2 id=\"Noticing_Noticing\">Noticing Noticing</h2>\n<p>What does it mean to notice something, and what does it feel like?</p>\n<p>I started by checking to see what I expected it to feel like to notice that it's raining, just going from memory. I tried for a split-second prediction, to find what my brain automatically stored under \"noticing rain\". When I thought about noticing rain, I got this sort of vague impression of rainyness, which included few sensory details and was more of an overall rainy feeling. My brain tried to tell me that \"noticing rain\" meant \"being directly acquainted with rainyness\", in much the same way that it tries to tell me it's experiencing a cube when it's actually only experiencing a pattern of light and shadows I interpret as three faces.</p>\n<p>Then, I waited for rain. It didn't take long, because I'm in North Carolina for the month. (This didn't happen last time I was in North Carolina, so perhaps I just happened to choose The One Valley of Eternal Rain.)</p>\n<p>The real \"noticing rain\" turned out to be a response to the physical sensations concurrent with the first raindrop falling on my skin. I did eventually have an \"abstract rainyness feeling\", but that happened a full two seconds later. My actual experience went like this.</p>\n<p>It was cloudy and humid. This was not at the forefront of my attention, but it slowly moved in that direction as the temperature dropped. I was fairly focused on reading a book.</p>\n<p>(I'm a little baffled by the apparent gradient between \"not at all conscious of x\" and \"fully aware of x\". I don't know how that works, but I experience the difference between being a little aware of the sky being cloudy and being focused on the patterns of light in the clouds, as analogous to the difference between being very-slightly-but-not-uncomfortably warm and burning my hand on the stove.)<br><br>My awareness of something like an \"abstract rainyness feeling\" moved further toward consciousness as the wind picked up. Suddenly--and the suddenness was an important part of the experience--I felt something like a cool, dull pin-prick on my arm. I looked at it, saw the water, and recognized it as a raindrop. Over the course of about half a second, several sensations leapt forward into full awareness: the darkness of my surroundings, the humidity in the air, the dark grey-blueness of the sky, the sound of rain on leaves like television static, the scent of ozone and damp earth, the feeling of cool humid wind on my face, and the word \"rain\" in my internal monologue.<br><br>I think it is that sudden leaping forward of many associated sensations that I would call \"noticing rain\".</p>\n<p>After that, I felt a sort of mental step backward--though it was more like a zooming out or sliding away than a discrete step--from the sensations, and then a feeling of viewing them from the outside. There was a sensation of the potential to access other memories of times when it's rained.</p>\n<p>(Sensations of potential are fascinating to me. I noticed a few weeks ago that after memorizing a list of names and faces, I could predict in the first half second of seeing the face whether or not I'd be able to retrieve the name in the next five seconds. Before I actually retrieved the name. What??? I don't know either.)</p>\n<p>Only then did all of it resolve into the more distant and abstract \"feeling of rainyness\" that I'd predicted before. The resolution took four times as long as the simultaneous-leaping-into-consciousness-of-related-sensations that I now prefer to call \"noticing\", and ten times as long as the first-raindrop-pin-prick, which I think I'll call the \"noticing trigger\" if it turns out to be a general class of pre-noticing experiences.</p>\n<p>(\"Can you really distinguish between 200 and 500 milliseconds?\" Yes, but it's an acquired skill. I spent a block of a few minutes every day for a month, then several blocks a day for about a week, doing this <a href=\"http://www.wellcomecollection.org/tiredness/index.html\">Psychomotor Vigiliance Task</a> when I was gathering data for the <a href=\"/lw/hyr/seed_study_polyphasic_sleep_in_ten_steps/\">polyphasic sleep experiment</a>. (No, I'm sorry, to the best of my knowledge Leverage has not yet published anything on the results of this. Long story short: Everyone who wasn't already polyphasic is still not polyphasic today.) It gives you fast feedback on simple response time. I'm not sure if it's useful for anything else, but it comes in handy when taking notes on experiences that pass very quickly.)</p>\n<h2 id=\"Noticing_Environmental_Cues\">Noticing Environmental Cues</h2>\n<p>My second experiment was in repeated noticing. This is more closely related to <a href=\"http://slatestarcodex.com/2014/06/09/constant-vigilance/\">rationality as habit cultivation</a>.</p>\n<p>Can I get better at noticing something just by practicing?</p>\n<div>I was trying to zoom in on the experience of noticing itself, so I wanted something as simple as possible. Nothing subtle, nothing psychological, and certainly nothing I might be motivated to ignore. I wanted a straightforward element of my physical environment. I'm out in the country and driving around for errands and such about once a day, so I went with \"red barn roofs\".</div>\n<p>I had an intuition that I should give myself some outward sign of having noticed, lest I not notice that I noticed, and decided to snap my fingers every time I noticed a red barn roof.</p>\n<p>On the first drive, I noticed one red barn roof. That happened when I was almost at my destination and I thought, \"Oh right, I'm supposed to be noticing red barn roofs, oops\" then started actively searching for them.</p>\n<p>Noticing a red barn roof while searching for it feels very different from noticing rain while reading a book. With the rain, it felt sort of like waking up, or like catching my name in an overheard conversation. There was a complete shift in what my brain was doing. With the barn roof, it was like I had a box with a red-barn-roof-shaped hole, and it felt like completion when a I grabbed a roof and dropped it through the hole. I was prepared for the roof, and it was a smaller change in the contents of consciousness.</p>\n<p>I noticed two on the way back, also while actively searching for them, before I started thinking about something else and became oblivious.</p>\n<p>I thought that maybe there weren't enough red barn roofs, and decided to try noticing red roofs of all sorts of buildings the next day. This, it turns out, was the correct move.</p>\n<p>On day two of red-roof-noticing, I got lots of practice. I noticed around fifteen roofs on the way to the store, and around seven on the way back. By the end, I was not searching for the roofs as intently as I had been the day before, but I was still explicitly thinking about the project. I was still aware of directing my eyes to spend extra time at the right level in my field of vision to pick up roofs. It was like waving the box around and waiting for something to fall in, while thinking about how to build boxes.</p>\n<p>I went out briefly again on day two, and on the way back, I noticed a red roof while thinking about something else entirely. Specifically, I was thinking about the possibility of moving to Uruguay, and whether I knew enough Spanish to survive. In the middle of one of those unrelated thoughts, my eyes moved over a barn roof and stayed there briefly while I had the leaping-into-consciousness experience with respect to the sensations of redness, recognizing something as shaped like a building, and feeling the impulse to snap my fingers. It was like I'd been wearing the box as a hat to free up my hands, and I'd forgotten about it. And then, with a heavy ker-thunk, the roof became my new center of attention.</p>\n<p>And oh my gosh, it was so exciting! It sounds so absurd in retrospect to have been excited about noticing a roof. But I was! It meant I'd successfully installed a new cognitive habit to run in the background. On purpose. \"Woo hoo! Yeah!\" (I literally said that.)</p>\n<p>On the third day, I noticed TOO MANY red roofs. I followed the same path to the store as before, but I noticed somewhere between twenty and thirty red roofs. I got about the same number going back, so I think I was catching nearly all the opportunities to notice red roofs. (I'd have to do it for a few days to be sure.) There was a pattern to noticing, where I'd notice-in-the-background, while thinking about something else, the first roof, and then I'd be more specifically on the lookout for a minute or two after that, before my mind wandered back to something other than roofs. I got faster over time at returning to my previous thoughts after snapping my fingers, but there were still enough noticed roofs to intrude uncomfortably upon my thoughts. It was getting annoying.</p>\n<p>So I decided to switch back to only noticing the red roofs of barns in particular.</p>\n<p>Extinction of the more general habit didn't take very long. It was over by the end of my next fifteen minute drive. For the first three times I saw a roof, I rose my hand a little to snap my fingers before reminding myself that I don't care about non-barns anymore. The next couple times I didn't raise my hand, but still forcefully reminded myself of my disinterest in my non-barns. The promotion of red roofs into consciousness got weaker with each roof, until the difference between seeing a non-red non-barn roof and a red non-barn roof was barely perceptible. That was my drive to town today.</p>\n<p>On the drive back, I noticed about ten red barn roofs. Three I noticed while thinking about how to install habits, four while thinking about the differences between designing exercises for in-person workshops and designing exercises to put in books, and three soon enough after the previous barn to probably count as \"searching for barns\".</p>\n<p>So yes, for at least some things, it seems I can get better at noticing them my &nbsp;by practicing.</p>\n<h2 id=\"What_These_Silly_Little_Experiments_Are_Really_About\">What These Silly Little Experiments Are Really About</h2>\n<p>My plan is to try noticing an internal psychological phenomenon next, but still something straightforward that I wouldn't be motivated not to notice. I probably need to try a couple things to find something that works well. I might go with \"thinking the word 'tomorrow' in my internal monologue\", for example, or possibly \"wondering what my boyfriend is thinking about\". I'll probably go with something more like the first, because it is clearer, and zooms in on \"noticing things inside my head\" without the extra noise of \"noticing things that are relatively temporally indiscrete\", but the second is actually a useful thing to notice.</p>\n<p>Most of the useful things to notice are a lot less obvious than \"thinking the word 'tomorrow' in my internal monologue\". From what I've learned so far, I think that for \"wondering what my boyfriend is thinking about\", I'll need to pick out a couple of very specific, instantaneous sensations that happen when I'm curious what my boyfriend is thinking about. I expect that to be a repetition of the rain experiment, where I predict what it will feel like, then wait 'til I can gather data in real time. Once I have a specific trigger, I can repeat the red roof experiment to catch the tiny moments when I wonder what he's thinking. I might need to start with a broader category, like \"notice when I'm thinking about my boyfriend\", get used to noticing those sensations, and then reduce the set of sensations I'm watching out for to things that happen only when I'm curious what my boyfriend is thinking.</p>\n<p>After that, I imagine I'll want to practice with different kinds of actions I can take when I notice a trigger. (If you've never heard of <a href=\"http://en.wikipedia.org/wiki/Implementation_intention\">Implementation Intentions</a>, I suggest trying them out.) So far, I've used the physical action of snapping my fingers. That was originally for clarity in recognizing the noticing, but it's also a behavioral response to a trigger. I could respond with a psychological behavior instead of a physical one, like \"imagining a carrot\". A useful response to noticing that I'm curious about what my boyfriend is thinking would be \"check to see if he's busy\" and then \"say, 'What are you thinking about?'\"</p>\n<p>See, this \"noticing\" thing sounds boringly simple at first, and not worth much consideration in the art of rationality. Even in his original \"noticing confusion\" post, Eliezer really talked more about recognizing the implications of confusion than about the noticing itself.</p>\n<p>Noticing is more complicated than it seems at first, and it's easy to mix it up with responding. There's a whole sub-art to noticing, and I really think that deliberate practice is making me better at it. Responses can be hard. It's essential to make noticing as effortless as possible. Then you can break the noticing and the responding apart, and you can recognize reality even before you know what to do with it.</p>", "sections": [{"title": "Phenomenology", "anchor": "Phenomenology", "level": 1}, {"title": "Noticing", "anchor": "Noticing", "level": 1}, {"title": "Noticing Noticing", "anchor": "Noticing_Noticing", "level": 1}, {"title": "Noticing Environmental Cues", "anchor": "Noticing_Environmental_Cues", "level": 1}, {"title": "What These Silly Little Experiments Are Really About", "anchor": "What_These_Silly_Little_Experiments_Are_Really_About", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5JDkW4MYXit2CquLs", "Mjiu8n9qyoqfY7LkF", "cMnjqk8iEzycbLBDA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-17T14:28:23.902Z", "modifiedAt": null, "url": null, "title": "Meetup : MelbLW: September Social Meetup", "slug": "meetup-melblw-september-social-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:35.071Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MelbourneLW", "createdAt": "2014-07-15T07:42:47.692Z", "isAdmin": false, "displayName": "MelbourneLW"}, "userId": "fnAEhR2GNN3t6PmFc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ewmbZFXcXbidLBjdJ/meetup-melblw-september-social-meetup", "pageUrlRelative": "/posts/ewmbZFXcXbidLBjdJ/meetup-melblw-september-social-meetup", "linkUrl": "https://www.lesswrong.com/posts/ewmbZFXcXbidLBjdJ/meetup-melblw-september-social-meetup", "postedAtFormatted": "Wednesday, September 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20MelbLW%3A%20September%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20MelbLW%3A%20September%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FewmbZFXcXbidLBjdJ%2Fmeetup-melblw-september-social-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20MelbLW%3A%20September%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FewmbZFXcXbidLBjdJ%2Fmeetup-melblw-september-social-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FewmbZFXcXbidLBjdJ%2Fmeetup-melblw-september-social-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 203, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14m'>MelbLW: September Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 September 2014 06:30:00PM (+0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Alchemist's Refuge, 328 Little Collins St, Melbourne </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>September's social meetup is scheduled for this Friday (19th September) as usual. This month, we will be returning to Alchemist's Refuge.</p>\n\n<p>Our social meetups are relaxed, informal events where we chat and often play games. The start and finish times are very loose - people will be coming and going throughout the night, so don't worry if you are coming later.</p>\n\n<p>Where? Alchemist's Refuge, 328 Little Collins St, Melbourne - near the corner of Queen St, downstairs from Games Laboratory</p>\n\n<p>When? From 6:30pm until late, Friday September 19th</p>\n\n<p>Contact? If you have any questions, just text or call me (Richard) on 0421231789</p>\n\n<p>Dinner? There are a number of take-away places nearby that deliver to Alchemist's Refuge. It is also quite likely that a group of us will go out for late night souvlakis after Refuge closes.</p>\n\n<p>Games? Alchemist's Refuge do allow board games and they also have a number that can be borrowed for a minor fee. Ask around and you'll easily find some others to join you!</p>\n\n<p>To organise similar events, please send an email to melbournelw@gmail.com</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14m'>MelbLW: September Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ewmbZFXcXbidLBjdJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.012421094742869e-06, "legacy": true, "legacyId": "27219", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___MelbLW__September_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/14m\">MelbLW: September Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 September 2014 06:30:00PM (+0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Alchemist's Refuge, 328 Little Collins St, Melbourne </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>September's social meetup is scheduled for this Friday (19th September) as usual. This month, we will be returning to Alchemist's Refuge.</p>\n\n<p>Our social meetups are relaxed, informal events where we chat and often play games. The start and finish times are very loose - people will be coming and going throughout the night, so don't worry if you are coming later.</p>\n\n<p>Where? Alchemist's Refuge, 328 Little Collins St, Melbourne - near the corner of Queen St, downstairs from Games Laboratory</p>\n\n<p>When? From 6:30pm until late, Friday September 19th</p>\n\n<p>Contact? If you have any questions, just text or call me (Richard) on 0421231789</p>\n\n<p>Dinner? There are a number of take-away places nearby that deliver to Alchemist's Refuge. It is also quite likely that a group of us will go out for late night souvlakis after Refuge closes.</p>\n\n<p>Games? Alchemist's Refuge do allow board games and they also have a number that can be borrowed for a minor fee. Ask around and you'll easily find some others to join you!</p>\n\n<p>To organise similar events, please send an email to melbournelw@gmail.com</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___MelbLW__September_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/14m\">MelbLW: September Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : MelbLW: September Social Meetup", "anchor": "Discussion_article_for_the_meetup___MelbLW__September_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : MelbLW: September Social Meetup", "anchor": "Discussion_article_for_the_meetup___MelbLW__September_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-17T17:20:57.516Z", "modifiedAt": null, "url": null, "title": "Everybody's talking about machine ethics", "slug": "everybody-s-talking-about-machine-ethics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:37.043Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sbenthall", "createdAt": "2012-12-23T03:31:10.842Z", "isAdmin": false, "displayName": "sbenthall"}, "userId": "pbnv8yAoxSjxvEZr8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JtbZ3SMYRdiyHHjYt/everybody-s-talking-about-machine-ethics", "pageUrlRelative": "/posts/JtbZ3SMYRdiyHHjYt/everybody-s-talking-about-machine-ethics", "linkUrl": "https://www.lesswrong.com/posts/JtbZ3SMYRdiyHHjYt/everybody-s-talking-about-machine-ethics", "postedAtFormatted": "Wednesday, September 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Everybody's%20talking%20about%20machine%20ethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEverybody's%20talking%20about%20machine%20ethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJtbZ3SMYRdiyHHjYt%2Feverybody-s-talking-about-machine-ethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Everybody's%20talking%20about%20machine%20ethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJtbZ3SMYRdiyHHjYt%2Feverybody-s-talking-about-machine-ethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJtbZ3SMYRdiyHHjYt%2Feverybody-s-talking-about-machine-ethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 293, "htmlBody": "<p>There is a lot of mainstream interest in machine ethics now. Here are some links to some popular articles on this topic.</p>\n<p>By Zeynep Tufecki, a professor at the I School at UNC, on <a href=\"https://medium.com/message/ferguson-is-also-a-net-neutrality-issue-6d2f3db51eb0\">Facebook's algorithmic newsfeed curation</a> and why Twitter <a href=\"https://medium.com/message/the-algorithm-giveth-but-it-also-taketh-b7efad92bc1f\">should not implement the same</a>.</p>\n<p>By danah boyd, <a href=\"https://medium.com/message/what-is-fairness-73940071840\">claiming</a> that 'tech folks' are designing systems that implement an idea of fairness that comes from neoliberal ideology.</p>\n<p>danah boyd (who spells her name with no capitalization) runs the <a href=\"http://www.datasociety.net/\">Data &amp; Society</a>, a \"think/do tank\" that aims to study this stuff. They've recently gotten <a href=\"http://www.datasociety.net/updates/featured/announcements/2014/09/macarthur-foundation-supports-two-year-data-society-initiative-around-intelligent-systems/\">MacArthur Foundation funding</a> for studying the ethical and political impact of intelligent systems.&nbsp;</p>\n<p>A few observations:</p>\n<p>First, there is no mention of superintelligence or recursively self-modifying anything. These scholars are interested in how, in the near future, the already comparatively powerful machines have moral and political impact on the world.</p>\n<p>Second, these groups are quite bad at thinking in a formal or mechanically implementable way about ethics. They mainly seem to recapitulate the same tired tropes that have been resonating through academia for literally decades. On the contrary, mathematical formulation of ethical positions appears to be ya'll's specialty.</p>\n<p>Third, however much the one-true-morality may be indeterminate or presently unknowable, progress towards implementable descriptions of various plausible moral positions could at least be incremental steps forward towards an understanding of how to achieve something better. Considering a slow take-off possible future, iterative testing and design of ethical machines with high computational power seems like low-hanging fruit that could only better inform longer-term futurist thought.</p>\n<p>Personally, I try to do work in this area and find the lack of serious formal work in this area deeply disappointing. This post is a combination heads up and request to step up your game. It's go time.</p>\n<p>&nbsp;</p>\n<p>Sebastian Benthall</p>\n<p>PhD Candidate</p>\n<p>UC Berkeley School of Infromation</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JtbZ3SMYRdiyHHjYt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 24, "extendedScore": null, "score": 2.012733527437954e-06, "legacy": true, "legacyId": "27220", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-17T18:11:10.505Z", "modifiedAt": "2021-08-12T19:35:37.656Z", "url": null, "title": "Simulate and Defer To More Rational Selves", "slug": "simulate-and-defer-to-more-rational-selves", "viewCount": null, "lastCommentedAt": "2020-06-07T19:13:13.311Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "BrienneYudkowsky", "user": {"username": "BrienneYudkowsky", "createdAt": "2013-05-13T00:07:08.935Z", "isAdmin": false, "displayName": "LoganStrohl"}, "userId": "uuYBzWLiixkbN3s7C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gBnSRErajRtvhMnDr/simulate-and-defer-to-more-rational-selves", "pageUrlRelative": "/posts/gBnSRErajRtvhMnDr/simulate-and-defer-to-more-rational-selves", "linkUrl": "https://www.lesswrong.com/posts/gBnSRErajRtvhMnDr/simulate-and-defer-to-more-rational-selves", "postedAtFormatted": "Wednesday, September 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Simulate%20and%20Defer%20To%20More%20Rational%20Selves&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASimulate%20and%20Defer%20To%20More%20Rational%20Selves%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgBnSRErajRtvhMnDr%2Fsimulate-and-defer-to-more-rational-selves%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Simulate%20and%20Defer%20To%20More%20Rational%20Selves%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgBnSRErajRtvhMnDr%2Fsimulate-and-defer-to-more-rational-selves", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgBnSRErajRtvhMnDr%2Fsimulate-and-defer-to-more-rational-selves", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1532, "htmlBody": "<p>I sometimes let imaginary versions of myself make decisions for me.</p><p>I first started doing this after a friend told me (something along the lines of) this story. When they first became executive director of their organization, they suddenly had many more decisions to deal with per day than ever before. \"Should we hire this person?\" \"Should I go buy more coffee for the coffee machine, or wait for someone else deal with it?\" \"How many participants should attend our first event?\" \"When can I schedule time to plan the fund drive?\" </p><p>I'm making up these examples myself, but I'm sure you, too, can imagine how leading a brand new organization might involve a constant assault on the parts of your brain responsible for making decisions. They found it <em>exhausting</em>, and by the time they got home at the end of the day, a question like, \"Would you rather we have peas or green beans with dinner?\" often felt like the last straw. \"I don't care about the stupid vegetables, just give me food and don't make me decide <em>any more things!</em>\"</p><p>They were rescued by the following technique. When faced with a decision, they'd imagine \"the Executive Director of the organization\", and ask themselves, \"What would 'the Executive Director of the organization' do?\" Instead of making a decision, they'd make a <em>prediction</em> about the actions of that other person. Then, they'd just do whatever that person would do!</p><p>In my friend's case, they were trying to reduce decision fatigue. When I started trying it out myself, I was after a cure for something slightly different.</p><p>Imagine you're about to go bungee jumping off a high cliff. You know it's perfectly safe, and all you have to do is take a step forward, just like you've done every single time you've ever walked. But something is stopping you. The decision to step off the ledge is entirely yours, and you know you want to do it because this is why you're here. Yet here you are, still standing on the ledge. </p><p>You're scared. There's a battle happening in your brain. Part of you is going, \"Just jump, it's easy, just do it!\", while another part--the part in charge of your legs, apparently--is going, \"NOPE. Nope nope nope nope NOPE.\" And you have this strange thought: \"I wish someone would just push me so I don't have to decide.\"</p><p>Maybe you've been bungee jumping, and this is not at all how you responded to it. But I hope (for the sake of communication) that you've experienced this sensation in other contexts. Maybe when you wanted to tell someone that you loved them, but the phrase hovered just behind your lips, and you couldn't get it out. You almost wished it would tumble out of your mouth accidentally. \"Just say it,\" you thought to yourself, and remained silent. For some reason, you were terrified of the decision, and inaction felt more like not deciding.</p><p>When I heard this story from my friend, <a href=\"http://agentyduck.blogspot.com/2014/02/lobs-theorem-cured-my-social-anxiety.html\">I had social anxiety</a>. I didn't have way more decisions than I knew how to handle, but I did find certain decisions terrifying, and was often paralyzed by them. For example, this always happened if someone I liked, respected, and wanted to interact with more asked to meet with them. It was pretty obvious to me that it was a good idea to say yes, but I'd agonize over the email endlessly instead of simply typing \"yes\" and hitting \"send\".</p><p>So here's what it looked like when I applied the technique. I'd be invited to a party. I'd feel paralyzing fear, and a sense of impending doom as I noticed that I likely believed going to the party was the right decision. Then, as soon as I felt that doom, I'd take a mental step backward and <em>not</em> try to force myself to decide. Instead, I'd imagine a version of myself <em>who wasn't scared</em>, and I'd <em>predict</em> what she'd do. If the party really <em>wasn't</em> a great idea, either because she didn't consider it worth my time or because she didn't actually anticipate me having any fun, she'd decide not to go. Otherwise, she'd decide to go. <em>I</em> would not decide. I'd just run my simulation of her, and see what she had to say. It was easy for her to think clearly about the decision, because she wasn't scared. And then I'd just defer to her.</p><p>Recently, I've noticed that there are all sorts of circumstances under which it helps to predict the decisions of a version of myself who doesn't have my current obstacle to rational decision making. Whenever I'm having a hard time thinking clearly about something because I'm angry, or tired, or scared, I can call upon imaginary Rational Brienne to see if she can do any better.</p><p>Example: I get depressed when I don't get enough sunlight. I was working inside where it was dark, and Eliezer noticed that I'd seemed depressed lately. So he told me he thought I should work outside instead. I was indeed a bit down and irritable, so my immediate response was to feel angry--that I'd been interrupted, that he was nagging me about getting sunlight <em>again</em>, and that I have this sunlight problem in the first place. </p><p>I started to argue with him, but then I stopped. I stopped because I'd noticed something. In addition to anger, I felt something like confusion. More complicated and specific than confusion, though. It's the feeling I get when I'm playing through familiar motions that have tended to lead to disutility. Like when you're watching a horror movie and the main character says, \"Let's split up!\" and you feel like, \"Ugh, not this again. Listen, you're in a horror movie. If you split up, <em>you will die</em>. It happens every time.\" A familiar twinge of something being not quite right.</p><p>But even though I noticed the feeling, I couldn't get a handle on it. Recognizing that I really should make the decision to go outside instead of arguing--it was just too much for me. I was angry, and that severely impedes my introspective vision. And I knew that. I knew that familiar not-quite-right feeling meant something was preventing me from applying some of my rationality skills. </p><p>So, as I'd previously decided to do in situations like this, I called upon my simulation of non-angry Brienne. </p><p>She immediately got up and went outside.</p><p>To her, it was extremely obviously the right thing to do. So I just deferred to her (which I'd also previously decided to do in situations like this, and I knew it would only work in the future if I did it now too, ain't timeless decision theory great). I stopped arguing, got up, and went outside. </p><p>I was still <em>pissed</em>, mind you. I even felt myself rationalizing that I was doing it because going outside despite Eliezer being <em>wrong wrong wrong</em> is easier than arguing with him, and arguing with him isn't worth the effort. And then I told him as much over chat. (But not the \"rationalizing\" part; I wasn't fully conscious of that yet.)</p><p>But <em>I went outside</em>, right away, instead of wasting a bunch of time and effort first. My internal state was still in disarray, but I took the correct external actions. </p><p>This has happened a few times now. I'm still getting the hang of it, but it's working.</p><p>Imaginary Rational Brienne isn't magic. Her only available skills are the ones I have in fact picked up, so anything I've not learned, she can't implement. She still makes mistakes. </p><p>Her special strength is <em>constancy</em>. </p><p>In real life, all kinds of things limit my access to my own skills. In fact, the times when I most need a skill will very likely be the times when I find it hardest to access. For example, it's more important to consider the opposite when I'm really invested in believing something than when I'm not invested at all, but it's much harder to actually carry out the mental motion of \"considering the opposite\" when all the cognitive momentum is moving toward arguing single-mindedly for my favored belief.</p><p>The advantage of Rational Brienne (or, really, the Rational Briennes, because so far I've always ended up simulating a version of myself that's exactly the same except lacking whatever particular obstacle is relevant at the time) is that her access doesn't vary by situation. She can always use all of my tools all of the time.</p><p>I've been trying to figure out this constancy thing for quite a while. What do I do when I call upon my art as a rationalist, and just get a 404 Not Found? Turns out, \"trying harder\" doesn't do the trick. \"No, really, I don't care that I'm scared, I'm going to think clearly about this. Here I go. I mean it this time.\" It seldom works.</p><p>I hope that it will one day. I would rather not have to rely on tricks like this. I hope I'll eventually just be able to go straight from noticing dissonance to re-orienting my whole mind so it's in line with the truth and with whatever I need to reach my goals. Or, you know, not experiencing the dissonance in the first place because I'm already doing everything right.</p><p>In the mean time, this trick seems pretty powerful.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5uHdFgR938LGGxMKQ": 3, "KoXbd2HmbdRfqLngk": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gBnSRErajRtvhMnDr", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 155, "baseScore": 203, "extendedScore": null, "score": 0.00058, "legacy": true, "legacyId": "27142", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 203, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 115, "af": false, "version": "1.2.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2014-09-17T18:11:10.505Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-17T18:56:52.273Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington, D.C.: Mini Talks", "slug": "meetup-washington-d-c-mini-talks", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinZ", "createdAt": "2009-07-08T20:34:05.168Z", "isAdmin": false, "displayName": "RobinZ"}, "userId": "eTMojvi4f2z3pDfsc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fTTjaQXJuQwRCw3rS/meetup-washington-d-c-mini-talks", "pageUrlRelative": "/posts/fTTjaQXJuQwRCw3rS/meetup-washington-d-c-mini-talks", "linkUrl": "https://www.lesswrong.com/posts/fTTjaQXJuQwRCw3rS/meetup-washington-d-c-mini-talks", "postedAtFormatted": "Wednesday, September 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%2C%20D.C.%3A%20Mini%20Talks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%2C%20D.C.%3A%20Mini%20Talks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfTTjaQXJuQwRCw3rS%2Fmeetup-washington-d-c-mini-talks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%2C%20D.C.%3A%20Mini%20Talks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfTTjaQXJuQwRCw3rS%2Fmeetup-washington-d-c-mini-talks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfTTjaQXJuQwRCw3rS%2Fmeetup-washington-d-c-mini-talks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 165, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14n'>Washington, D.C.: Mini Talks</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 September 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting in the Kogod Courtyard of the National Portrait Gallery (8th and F Sts or 8th and G Sts NW, go straight past the information desk from either entrance) to take turns presenting short (~10-20 minute) lectures on random topics. As before, the period from 3:00 to 3:30 will be reserved for congregating; the talks will run as long as people are interested or until the museum closes at 7:00 p.m., whichever comes first.\n(If you need to show up late or leave early, that's fine - feel free to let us know if you want to give a talk at a particular time.)</p>\n\n<p>Upcoming meetups:</p>\n\n<ul>\n<li>Sept. 28: Book Swap</li>\n<li>Oct. 5: Fun &amp; Games</li>\n<li>Oct. 12: TBA</li>\n<li>Oct. 19: Mini Talks</li>\n<li>Oct. 26: TBA</li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14n'>Washington, D.C.: Mini Talks</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fTTjaQXJuQwRCw3rS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.0129072211261152e-06, "legacy": true, "legacyId": "27221", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington__D_C___Mini_Talks\">Discussion article for the meetup : <a href=\"/meetups/14n\">Washington, D.C.: Mini Talks</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 September 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting in the Kogod Courtyard of the National Portrait Gallery (8th and F Sts or 8th and G Sts NW, go straight past the information desk from either entrance) to take turns presenting short (~10-20 minute) lectures on random topics. As before, the period from 3:00 to 3:30 will be reserved for congregating; the talks will run as long as people are interested or until the museum closes at 7:00 p.m., whichever comes first.\n(If you need to show up late or leave early, that's fine - feel free to let us know if you want to give a talk at a particular time.)</p>\n\n<p>Upcoming meetups:</p>\n\n<ul>\n<li>Sept. 28: Book Swap</li>\n<li>Oct. 5: Fun &amp; Games</li>\n<li>Oct. 12: TBA</li>\n<li>Oct. 19: Mini Talks</li>\n<li>Oct. 26: TBA</li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington__D_C___Mini_Talks1\">Discussion article for the meetup : <a href=\"/meetups/14n\">Washington, D.C.: Mini Talks</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington, D.C.: Mini Talks", "anchor": "Discussion_article_for_the_meetup___Washington__D_C___Mini_Talks", "level": 1}, {"title": "Discussion article for the meetup : Washington, D.C.: Mini Talks", "anchor": "Discussion_article_for_the_meetup___Washington__D_C___Mini_Talks1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-18T02:55:19.303Z", "modifiedAt": "2020-01-14T04:21:16.103Z", "url": null, "title": "Petrov Day is September 26", "slug": "petrov-day-is-september-26", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:38.513Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "jimrandomh", "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fr8MEigHzJeFQkctL/petrov-day-is-september-26", "pageUrlRelative": "/posts/fr8MEigHzJeFQkctL/petrov-day-is-september-26", "linkUrl": "https://www.lesswrong.com/posts/fr8MEigHzJeFQkctL/petrov-day-is-september-26", "postedAtFormatted": "Thursday, September 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Petrov%20Day%20is%20September%2026&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APetrov%20Day%20is%20September%2026%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffr8MEigHzJeFQkctL%2Fpetrov-day-is-september-26%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Petrov%20Day%20is%20September%2026%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffr8MEigHzJeFQkctL%2Fpetrov-day-is-september-26", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffr8MEigHzJeFQkctL%2Fpetrov-day-is-september-26", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 258, "htmlBody": "<p>On September 26th, 1983, the world was nearly destroyed by nuclear war. That day is Petrov Day, named for the man who averted it. Petrov Day is now a yearly event on September 26 commemorating the anniversary of the Petrov incident. Last year, Citadel, the Boston-area rationalist house, performed a ritual on Petrov day. We will be doing it again - and have published a revised version, for anyone else who wants to have a Petrov Day celebration themselves.</p>\n<p>The purpose of the ritual is to make catastrophic and existential risk emotionally salient, by putting it into historical context and providing positive and negative examples of how it has been handled. This is not for the faint of heart and not for the uninitiated; it is aimed at those who already know what catastrophic and existential risk is, have some background knowledge of what those risks are, and believe (at least on an abstract level) that preventing those risks from coming to pass is important.</p>\n<p>Petrov Day is designed for groups of 5-10 people, and consists of a series of readings and symbolic actions which people take turns doing. It is easy to organize; you'll need a few simple props (candles and a candle-holder) and a printout of the program for each person, but other than that no preparation is necessary.</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://petrovday.com/downloads/PetrovDay-OrganizerGuide.pdf\">Organizer guide and program (for one-sided printing)</a>&nbsp;(PDF)<br /><a href=\"http://petrovday.com/downloads/PetrovDay-DoubleSidedBooklet.pdf\">Program for two-sided print and fold</a>&nbsp;(PDF)</p>\n<p>There will be a Petrov Day ritual hosted at Citadel (Boston area) and at Highgarden (New York area). If you live somewhere else, consider running one yourself!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2i3w84KCkqZzpnQ4d": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fr8MEigHzJeFQkctL", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 32, "extendedScore": null, "score": 0.000142, "legacy": true, "legacyId": "27222", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-18T16:57:38.372Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow Meetup: Unusual as usual", "slug": "meetup-moscow-meetup-unusual-as-usual", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexander230", "createdAt": "2014-08-27T08:55:16.153Z", "isAdmin": false, "displayName": "Alexander230"}, "userId": "xqoKSJayCCtP5juLh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GWovuZsMNyuXXE2PS/meetup-moscow-meetup-unusual-as-usual", "pageUrlRelative": "/posts/GWovuZsMNyuXXE2PS/meetup-moscow-meetup-unusual-as-usual", "linkUrl": "https://www.lesswrong.com/posts/GWovuZsMNyuXXE2PS/meetup-moscow-meetup-unusual-as-usual", "postedAtFormatted": "Thursday, September 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%20Meetup%3A%20Unusual%20as%20usual&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%20Meetup%3A%20Unusual%20as%20usual%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGWovuZsMNyuXXE2PS%2Fmeetup-moscow-meetup-unusual-as-usual%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%20Meetup%3A%20Unusual%20as%20usual%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGWovuZsMNyuXXE2PS%2Fmeetup-moscow-meetup-unusual-as-usual", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGWovuZsMNyuXXE2PS%2Fmeetup-moscow-meetup-unusual-as-usual", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14o'>Moscow Meetup: Unusual as usual</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 September 2014 02:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Here's our plan:</p>\n\n<ul>\n<li>\"Calibration with topics\" excercise.</li>\n<li>A talk about consequences of a belief.</li>\n<li>A talk about habits.</li>\n<li>A talk about one Cognitive Bias.</li>\n<li>A talk about Shelling points and Shelling fences.</li>\n<li>A game of Fallacymania at the afterparty.</li>\n</ul>\n\n<p>Details and schedule:</p>\n\n<p><a href=\"https://lesswrong-ru.hackpad.com/-28--1SH4T1crrv6\" rel=\"nofollow\">https://lesswrong-ru.hackpad.com/-28--1SH4T1crrv6</a></p>\n\n<p>Yudcoins, positive reinforcement and pizza will all be present. If you've been to our meetups, you know what I'm talking about, and if you didn't, the best way to find out is to come and see for yourself.</p>\n\n<p>Info for newcomers: We gather in the Yandex office, you need the first revolving door under the archway. Here is a guide how to get there:</p>\n\n<p><a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">http://company.yandex.ru/contacts/redrose/</a></p>\n\n<p>Call Slava at +7(926)313-96-42 if you're late. We start at 14:00 and stay until at least 19-20. Please pay attention that we only gather near the entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14o'>Moscow Meetup: Unusual as usual</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GWovuZsMNyuXXE2PS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.01530177830483e-06, "legacy": true, "legacyId": "27223", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow_Meetup__Unusual_as_usual\">Discussion article for the meetup : <a href=\"/meetups/14o\">Moscow Meetup: Unusual as usual</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 September 2014 02:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Here's our plan:</p>\n\n<ul>\n<li>\"Calibration with topics\" excercise.</li>\n<li>A talk about consequences of a belief.</li>\n<li>A talk about habits.</li>\n<li>A talk about one Cognitive Bias.</li>\n<li>A talk about Shelling points and Shelling fences.</li>\n<li>A game of Fallacymania at the afterparty.</li>\n</ul>\n\n<p>Details and schedule:</p>\n\n<p><a href=\"https://lesswrong-ru.hackpad.com/-28--1SH4T1crrv6\" rel=\"nofollow\">https://lesswrong-ru.hackpad.com/-28--1SH4T1crrv6</a></p>\n\n<p>Yudcoins, positive reinforcement and pizza will all be present. If you've been to our meetups, you know what I'm talking about, and if you didn't, the best way to find out is to come and see for yourself.</p>\n\n<p>Info for newcomers: We gather in the Yandex office, you need the first revolving door under the archway. Here is a guide how to get there:</p>\n\n<p><a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">http://company.yandex.ru/contacts/redrose/</a></p>\n\n<p>Call Slava at +7(926)313-96-42 if you're late. We start at 14:00 and stay until at least 19-20. Please pay attention that we only gather near the entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow_Meetup__Unusual_as_usual1\">Discussion article for the meetup : <a href=\"/meetups/14o\">Moscow Meetup: Unusual as usual</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow Meetup: Unusual as usual", "anchor": "Discussion_article_for_the_meetup___Moscow_Meetup__Unusual_as_usual", "level": 1}, {"title": "Discussion article for the meetup : Moscow Meetup: Unusual as usual", "anchor": "Discussion_article_for_the_meetup___Moscow_Meetup__Unusual_as_usual1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-18T18:37:14.708Z", "modifiedAt": null, "url": null, "title": "Link: The trap of \"optimal conditions\"", "slug": "link-the-trap-of-optimal-conditions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:35.683Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sfbrEoEHTAqWgDTA4/link-the-trap-of-optimal-conditions", "pageUrlRelative": "/posts/sfbrEoEHTAqWgDTA4/link-the-trap-of-optimal-conditions", "linkUrl": "https://www.lesswrong.com/posts/sfbrEoEHTAqWgDTA4/link-the-trap-of-optimal-conditions", "postedAtFormatted": "Thursday, September 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20The%20trap%20of%20%22optimal%20conditions%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20The%20trap%20of%20%22optimal%20conditions%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsfbrEoEHTAqWgDTA4%2Flink-the-trap-of-optimal-conditions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20The%20trap%20of%20%22optimal%20conditions%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsfbrEoEHTAqWgDTA4%2Flink-the-trap-of-optimal-conditions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsfbrEoEHTAqWgDTA4%2Flink-the-trap-of-optimal-conditions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<p>\"the next time you&rsquo;re stopping yourself from trying something because the conditions are not optimal, remember that those optimal conditions may not have been the reason it worked. They may not be the cause. They may not even be correlated. They may just be a myth you&rsquo;ve bought into or sold yourself that limits you from breaking out and exceeding your expectations.\"</p>\n<p>More at:</p>\n<p>http://goodmenproject.com/ethics-values/1-huge-way-limit-break-fiff</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sfbrEoEHTAqWgDTA4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 14, "extendedScore": null, "score": 2.0154825678900304e-06, "legacy": true, "legacyId": "27224", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-18T22:33:31.750Z", "modifiedAt": null, "url": null, "title": "Friendliness in Natural Intelligences", "slug": "friendliness-in-natural-intelligences-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:35.231Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Slider", "createdAt": "2012-04-02T11:54:00.388Z", "isAdmin": false, "displayName": "Slider"}, "userId": "k8CWenAPWgEBknCH9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ESNypvr4Yn3DyLAyQ/friendliness-in-natural-intelligences-0", "pageUrlRelative": "/posts/ESNypvr4Yn3DyLAyQ/friendliness-in-natural-intelligences-0", "linkUrl": "https://www.lesswrong.com/posts/ESNypvr4Yn3DyLAyQ/friendliness-in-natural-intelligences-0", "postedAtFormatted": "Thursday, September 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Friendliness%20in%20Natural%20Intelligences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFriendliness%20in%20Natural%20Intelligences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FESNypvr4Yn3DyLAyQ%2Ffriendliness-in-natural-intelligences-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Friendliness%20in%20Natural%20Intelligences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FESNypvr4Yn3DyLAyQ%2Ffriendliness-in-natural-intelligences-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FESNypvr4Yn3DyLAyQ%2Ffriendliness-in-natural-intelligences-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1583, "htmlBody": "<p>The challenge of friendliness in Artificial Intelligence is to ensure how a general intelligence will be of utility instead of being destructive or pathologically indifferent to the values of existing individuals or aims and goals of their creation. The current provision of computer science is likely to yield bugs and way too technical and inflexible guidelines of action. It is known to be inadequate to handle the job sufficiently. However the challenge of friendliness is also faced by natural intelligences, those that are not designed by an intelligence but molded into being by natural selection.</p>\n<p>We know that natural intelligences do the job adequately enough that we do not think that natural intelligence unfriendliness is a significant existential threat. Like plants do solar energy capturing way more efficently and maybe utilising quantum effects that humans can't harness, we know that natural intelligences are using friendliness technology that is of higher caliber that we can build into machines. However as we progress this technology maybe lacking dangerously behind and we need to be able to apply it to hardware in addition to wetware and potentially boost it to new levels.</p>\n<p>The earliest concrete example of a natural intelligence being controlled for friendliness I can think of is Socrates. He was charged for \"corruption of the heart of the societys youngters\". He defended that his stance of questioning everything was without fault. He was however found quilty even thought the trial could be identified with faults. The jury might have been politically motivated or persuaded and the citizens might have expected the results to not be taken seriously. While Socrates was given a very real possibility of escaping imprisonment and capital punishment he did not circumvent his society operation. In fact he was obidient enough that he acted as his own executioner drinking the poison himself. Because of the kind of farce his teachers death had been Plato lost hope for the principles that lead to such an absurd result him becoming skeptical of democrasy.</p>\n<p>However if the situation would have been about a artificial intelligence a lot of things went very right. The intelligences society became scared of him and asked it to die. There was dialog about how the deciders were ignorant and stupid and that nothing questionable had been done. However ultimately when issues of miscommunications had been cleared and the society insisted upon its expression of will instead of circumventing the intervention the intelligence pulled its own plug voluntarily. Therefore Socrates was propably the first friendly (natural) intelligence.</p>\n<p>The mechanism used in this case was that of a juridical system. That is a human society recognises that certain acts and individuals are worth restraining for the danger that they pose to the common good. A common method is incarcenation and the threat of it. That is certain bad acts can be tolerated in the wild and corrective action can then be employed. When there is reason to expect bad acts or no reason to expect good acts individuals can be restricted in never being able to act in the first place. Whether a criminal is released early can depend on whether there is reason to expect not to be a repeat offender. That is understanding how an agent acts makes it easier to grant operating priviledges. Such hearings are very analogous to a gatekeeper and a AI in a AI-boxing situation.</p>\n<p>However when a new human is created it is not assumed hostile until proven friendly. Rather humans are born innocent but powerless. A fully educated and socialised intelligence is assigned for multiple year observation and control period. These so called \"parents\" have a very wide freedom on programming principles. However human psychology also has peroid of \"peer guidedness\" where the opinion of peers becomes important. When a youngter grows his thinking is constantly being monitored and things like time of onset of speech are monitored with interest. They also gain guidance on very trivial thinking skills. While this has culture passing effect it also keeps the parent very updated on what is the mental status of the child. Never is a child allowed to grow or reason extended amounts of time isolated. Thus the task of evaluating whether an unknown individual is friendly or not is not encountered. There is never a need to turing-test that a child \"works\". There is always a maintainer and it has the equivalent of psychological growth logs.</p>\n<p>However despite all these measures we know that small children can be cruel and have little empathy. However instead of shelving them as rejects we either accomodate them with an environment that minimises the harm or direct them to a more responcible path. When a child ask a question on how they should approach a particular kind of situation this can be challenging for the parent to answer. The parent might also resort to giving a best-effort answer that might not be entirely satisfactory or even wrong advice may be given. However children have dialog with their parents and other peers.</p>\n<p>An interesting question is does parenting break down if the child is intellectually too developed compared to the parent or parenting environment? It's also worth noting that children are not equipped with a \"constitution of morality\". Some things they infer from experience. Some ethical rules are thougth them explicitly. They learn to apply the rules and interpret them in different situations. Some rules might be contradictory and some moral authorities trusted more.</p>\n<p>Beoynd the individual level groups of people have an mechanism of acccepting other groups. This doesn't always happen without conditions. However here things seem to work much less efficently. If two groups of people differ in values enough they might start a war of ideology against each other. This kind of war usually concludes with physical action instead of arguments. Suppression of Nazi Germany can be seen as friendliness immune reaction. Normally divergent values and issues having countries wanted and could unite against a different set of values that was tried to be imposed by force. However the success Nazis had can debatably be attributed for a lousy conclusion of world war I. The effort extended to build peace varies and contests with other values.</p>\n<p>Friendliness migth also have an important component that it is relative to a set of values. A society will support the upring of certain kinds of children with the suppression of certain other kinds. USSR had officers that's sole job was to protect that things were going according to party line. At this point we have trouble getting a computer to follow anyones values. However it might be important to ask \"friendly to whom?\". The exploration of friendliness is also an exploration in hostility. We want to be hostile towards UFAIs. It would be awful for a AI to be friendly only towards it's inventor, or only towards it's company. However we have been hostile to neardentals. Was that wrong? Would it be a signficant loss to developed sentience if AIs were less than friendly to humans?</p>\n<p>If we ask our grandgrandgrandparents on how we should conduct things they might give a different version than we have. It's expectable that our children are capable of going beyond our morality. Ensuring that a societys values are never violated would be to freeze them in time indefinately. In this way there can be danger in developing a too friendly AI. For that AI could never be truly superhuman. In a way if my child asks me a morally challenging question and I change my opinion about it by the result of that conversation it might be a friendliness failure. Instead of imparting values I receive them with the values causal history being in the inside of a young head instead of a cultural heritage of a longlived civilization.</p>\n<p>As a civilizaton we have mapped a variety of thoughts and psyche- and organizational strucutres on how they work. The thought space on how an AI might think is poorly mapped. However we are spreading our understandig on cognitive diversity learning about how austistic persons think as well as dolphins. We can establish things liek that some savants are really good with dates and that askingn about dates from that kind of person is more realiable than an ordinary person. To be able to use AI thinking we need to understand what AI thought is. Up to now we have not needed to study in detail how humans think. We can just adapt to the way they do without attending to how it works. But in similar that we need to know the structure of a particle accelerator to be able to say that it provides information about particle behaviour we need to know why it would make sense to take what an AI says seriously. The challenge would be the same if we were asked to listen seriously to a natural intelligence from a foreign culture. Thus the enemy is inferential distance itself rather than the resultant thought processes. For we know that we can create things we don't understand. Thus it's important to understand that doing things you don't understand is a recipe for disaster. And we must not fool ourself that we understand what a machine thinking would be. Only once we have convinced our fellow natural intelligences that we know what we are doing can it make sense to listen to our creations. Socrates could not explain himself so his effect on others was unsafe. If you need to influence others you need to explain why you are doing so.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ESNypvr4Yn3DyLAyQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -7, "extendedScore": null, "score": 2.015911551112375e-06, "legacy": true, "legacyId": "27226", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-18T22:33:52.817Z", "modifiedAt": null, "url": null, "title": "Friendliness in Natural Intelligences", "slug": "friendliness-in-natural-intelligences", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Slider", "createdAt": "2012-04-02T11:54:00.388Z", "isAdmin": false, "displayName": "Slider"}, "userId": "k8CWenAPWgEBknCH9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BHMeaBrAvCEotNSc8/friendliness-in-natural-intelligences", "pageUrlRelative": "/posts/BHMeaBrAvCEotNSc8/friendliness-in-natural-intelligences", "linkUrl": "https://www.lesswrong.com/posts/BHMeaBrAvCEotNSc8/friendliness-in-natural-intelligences", "postedAtFormatted": "Thursday, September 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Friendliness%20in%20Natural%20Intelligences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFriendliness%20in%20Natural%20Intelligences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHMeaBrAvCEotNSc8%2Ffriendliness-in-natural-intelligences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Friendliness%20in%20Natural%20Intelligences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHMeaBrAvCEotNSc8%2Ffriendliness-in-natural-intelligences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHMeaBrAvCEotNSc8%2Ffriendliness-in-natural-intelligences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1583, "htmlBody": "<p>The challenge of friendliness in Artificial Intelligence is to ensure how a general intelligence will be of utility instead of being destructive or pathologically indifferent to the values of existing individuals or aims and goals of their creation. The current provision of computer science is likely to yield bugs and way too technical and inflexible guidelines of action. It is known to be inadequate to handle the job sufficiently. However the challenge of friendliness is also faced by natural intelligences, those that are not designed by an intelligence but molded into being by natural selection.</p>\n<p>We know that natural intelligences do the job adequately enough that we do not think that natural intelligence unfriendliness is a significant existential threat. Like plants do solar energy capturing way more efficently and maybe utilising quantum effects that humans can't harness, we know that natural intelligences are using friendliness technology that is of higher caliber that we can build into machines. However as we progress this technology maybe lacking dangerously behind and we need to be able to apply it to hardware in addition to wetware and potentially boost it to new levels.</p>\n<p>The earliest concrete example of a natural intelligence being controlled for friendliness I can think of is Socrates. He was charged for \"corruption of the heart of the societys youngters\". He defended that his stance of questioning everything was without fault. He was however found quilty even thought the trial could be identified with faults. The jury might have been politically motivated or persuaded and the citizens might have expected the results to not be taken seriously. While Socrates was given a very real possibility of escaping imprisonment and capital punishment he did not circumvent his society operation. In fact he was obidient enough that he acted as his own executioner drinking the poison himself. Because of the kind of farce his teachers death had been Plato lost hope for the principles that lead to such an absurd result him becoming skeptical of democrasy.</p>\n<p>However if the situation would have been about a artificial intelligence a lot of things went very right. The intelligences society became scared of him and asked it to die. There was dialog about how the deciders were ignorant and stupid and that nothing questionable had been done. However ultimately when issues of miscommunications had been cleared and the society insisted upon its expression of will instead of circumventing the intervention the intelligence pulled its own plug voluntarily. Therefore Socrates was propably the first friendly (natural) intelligence.</p>\n<p>The mechanism used in this case was that of a juridical system. That is a human society recognises that certain acts and individuals are worth restraining for the danger that they pose to the common good. A common method is incarcenation and the threat of it. That is certain bad acts can be tolerated in the wild and corrective action can then be employed. When there is reason to expect bad acts or no reason to expect good acts individuals can be restricted in never being able to act in the first place. Whether a criminal is released early can depend on whether there is reason to expect not to be a repeat offender. That is understanding how an agent acts makes it easier to grant operating priviledges. Such hearings are very analogous to a gatekeeper and a AI in a AI-boxing situation.</p>\n<p>However when a new human is created it is not assumed hostile until proven friendly. Rather humans are born innocent but powerless. A fully educated and socialised intelligence is assigned for multiple year observation and control period. These so called \"parents\" have a very wide freedom on programming principles. However human psychology also has peroid of \"peer guidedness\" where the opinion of peers becomes important. When a youngter grows his thinking is constantly being monitored and things like time of onset of speech are monitored with interest. They also gain guidance on very trivial thinking skills. While this has culture passing effect it also keeps the parent very updated on what is the mental status of the child. Never is a child allowed to grow or reason extended amounts of time isolated. Thus the task of evaluating whether an unknown individual is friendly or not is not encountered. There is never a need to turing-test that a child \"works\". There is always a maintainer and it has the equivalent of psychological growth logs.</p>\n<p>However despite all these measures we know that small children can be cruel and have little empathy. However instead of shelving them as rejects we either accomodate them with an environment that minimises the harm or direct them to a more responcible path. When a child ask a question on how they should approach a particular kind of situation this can be challenging for the parent to answer. The parent might also resort to giving a best-effort answer that might not be entirely satisfactory or even wrong advice may be given. However children have dialog with their parents and other peers.</p>\n<p>An interesting question is does parenting break down if the child is intellectually too developed compared to the parent or parenting environment? It's also worth noting that children are not equipped with a \"constitution of morality\". Some things they infer from experience. Some ethical rules are thougth them explicitly. They learn to apply the rules and interpret them in different situations. Some rules might be contradictory and some moral authorities trusted more.</p>\n<p>Beoynd the individual level groups of people have an mechanism of acccepting other groups. This doesn't always happen without conditions. However here things seem to work much less efficently. If two groups of people differ in values enough they might start a war of ideology against each other. This kind of war usually concludes with physical action instead of arguments. Suppression of Nazi Germany can be seen as friendliness immune reaction. Normally divergent values and issues having countries wanted and could unite against a different set of values that was tried to be imposed by force. However the success Nazis had can debatably be attributed for a lousy conclusion of world war I. The effort extended to build peace varies and contests with other values.</p>\n<p>Friendliness migth also have an important component that it is relative to a set of values. A society will support the upring of certain kinds of children with the suppression of certain other kinds. USSR had officers that's sole job was to protect that things were going according to party line. At this point we have trouble getting a computer to follow anyones values. However it might be important to ask \"friendly to whom?\". The exploration of friendliness is also an exploration in hostility. We want to be hostile towards UFAIs. It would be awful for a AI to be friendly only towards it's inventor, or only towards it's company. However we have been hostile to neardentals. Was that wrong? Would it be a signficant loss to developed sentience if AIs were less than friendly to humans?</p>\n<p>If we ask our grandgrandgrandparents on how we should conduct things they might give a different version than we have. It's expectable that our children are capable of going beyond our morality. Ensuring that a societys values are never violated would be to freeze them in time indefinately. In this way there can be danger in developing a too friendly AI. For that AI could never be truly superhuman. In a way if my child asks me a morally challenging question and I change my opinion about it by the result of that conversation it might be a friendliness failure. Instead of imparting values I receive them with the values causal history being in the inside of a young head instead of a cultural heritage of a longlived civilization.</p>\n<p>As a civilizaton we have mapped a variety of thoughts and psyche- and organizational strucutres on how they work. The thought space on how an AI might think is poorly mapped. However we are spreading our understandig on cognitive diversity learning about how austistic persons think as well as dolphins. We can establish things liek that some savants are really good with dates and that askingn about dates from that kind of person is more realiable than an ordinary person. To be able to use AI thinking we need to understand what AI thought is. Up to now we have not needed to study in detail how humans think. We can just adapt to the way they do without attending to how it works. But in similar that we need to know the structure of a particle accelerator to be able to say that it provides information about particle behaviour we need to know why it would make sense to take what an AI says seriously. The challenge would be the same if we were asked to listen seriously to a natural intelligence from a foreign culture. Thus the enemy is inferential distance itself rather than the resultant thought processes. For we know that we can create things we don't understand. Thus it's important to understand that doing things you don't understand is a recipe for disaster. And we must not fool ourself that we understand what a machine thinking would be. Only once we have convinced our fellow natural intelligences that we know what we are doing can it make sense to listen to our creations. Socrates could not explain himself so his effect on others was unsafe. If you need to influence others you need to explain why you are doing so.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BHMeaBrAvCEotNSc8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "27227", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-19T13:01:41.032Z", "modifiedAt": null, "url": null, "title": "A proof of L\u00f6b's theorem in Haskell", "slug": "a-proof-of-loeb-s-theorem-in-haskell", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:36.000Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jshdZw3xofq9wgE7T/a-proof-of-loeb-s-theorem-in-haskell", "pageUrlRelative": "/posts/jshdZw3xofq9wgE7T/a-proof-of-loeb-s-theorem-in-haskell", "linkUrl": "https://www.lesswrong.com/posts/jshdZw3xofq9wgE7T/a-proof-of-loeb-s-theorem-in-haskell", "postedAtFormatted": "Friday, September 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20proof%20of%20L%C3%B6b's%20theorem%20in%20Haskell&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20proof%20of%20L%C3%B6b's%20theorem%20in%20Haskell%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjshdZw3xofq9wgE7T%2Fa-proof-of-loeb-s-theorem-in-haskell%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20proof%20of%20L%C3%B6b's%20theorem%20in%20Haskell%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjshdZw3xofq9wgE7T%2Fa-proof-of-loeb-s-theorem-in-haskell", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjshdZw3xofq9wgE7T%2Fa-proof-of-loeb-s-theorem-in-haskell", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1008, "htmlBody": "<p>I'm not sure if this post is very on-topic for LW, but we have many folks who understand&nbsp;<a href=\"http://www.haskell.org/haskellwiki/Haskell\">Haskell</a>&nbsp;and many folks who are interested in&nbsp;<a href=\"http://en.wikipedia.org/wiki/L%C3%B6b's_theorem\">L&ouml;b's theorem</a>&nbsp;(see e.g. Eliezer's&nbsp;<a href=\"/lw/t6/the_cartoon_guide_to_l%C3%83%C6%92%C3%86%E2%80%99%C3%83%E2%80%A0%C3%A2%E2%82%AC%E2%84%A2%C3%83%C6%92%C3%A2%E2%82%AC%C5%A1%C3%83%E2%80%9A%C3%82%C2%B6bs_theorem/\">picture proof</a>), so I thought why not post it here? If no one likes it, I can always just move it to my own blog.</p>\n<p>A few days ago I stumbled across&nbsp;<a href=\"http://blog.sigfpe.com/2006/11/from-l-theorem-to-spreadsheet.html\">a post by Dan Piponi</a>, claiming to show a Haskell&nbsp;implementation of something similar to L&ouml;b's theorem. Unfortunately his code had a couple flaws. It was circular and relied on Haskell's laziness, and it used an assumption that doesn't actually hold in logic (see the second comment by Ashley Yakeley there). So I started to wonder, what would it take to code up an actual proof? Wikipedia <a href=\"http://en.wikipedia.org/wiki/L%C3%B6b's_theorem#Proof_of_L.C3.B6b.27s_theorem\">spells out the steps</a> very nicely, so it seemed to be just a matter of programming.</p>\n<p>Well, it turned out to be harder than I thought.</p>\n<p>One problem is that Haskell <a href=\"http://programmers.stackexchange.com/questions/177967/why-doesnt-haskell-have-type-level-lambda-abstractions\">has no type-level lambdas</a>, which are the most obvious&nbsp;way (by&nbsp;<a href=\"http://homepages.inf.ed.ac.uk/wadler/papers/propositions-as-types/propositions-as-types.pdf\">Curry-Howard</a>)&nbsp;to represent formulas with propositional variables. These are very useful for proving stuff in general, and L&ouml;b's theorem uses them to build fixpoints by the <a href=\"http://en.wikipedia.org/wiki/Diagonal_lemma\">diagonal lemma</a>.</p>\n<p>The other problem is that Haskell is <a href=\"http://en.wikipedia.org/wiki/Turing_completeness\">Turing complete</a>, which means it can't really be used for proof checking, because a non-terminating program <a href=\"http://en.wikibooks.org/wiki/Haskell/The_Curry-Howard_isomorphism#The_problem_with_.E2.8A.A5\">can be viewed</a> as the proof of any sentence. Several people have told me that&nbsp;<a href=\"http://en.wikipedia.org/wiki/Agda_(programming_language)\">Agda</a> or <a href=\"http://www.idris-lang.org/\">Idris</a>&nbsp;might be better choices in this regard. Ultimately I decided to use Haskell after all, because that way the post will be understandable to a wider audience. It's easy enough to convince yourself by looking at the code that it is in fact total, and transliterate it into a total language if needed. (That way you can also use the nice type-level lambdas and fixpoints, instead of just postulating one particular fixpoint as I did in Haskell.)</p>\n<p>But the biggest problem for me was that the Web didn't seem to have any good explanations for the thing I wanted to do! At first it seems like modal proofs and Haskell-like languages should be a match made in heaven, but in reality it's full of subtle issues that no one has written down, as far as I know. So I'd like this post to serve as a reference, an example approach that avoids all difficulties and just works.</p>\n<p>LW user lmm has <a href=\"/lw/kzq/open_thread_september_1521_2014/bbqr\">helped me a lot</a> with understanding the issues involved, and wrote a&nbsp;candidate implementation in Scala. The good folks on <a href=\"http://www.reddit.com/r/haskell/comments/2gnsfg/using_haskell_to_study_provability_logic/\">/r/haskell</a>&nbsp;were also very helpful, especially <a href=\"http://gelisam.blogspot.ch/\">Samuel G&eacute;lineau</a>&nbsp;who suggested a nice <a href=\"http://www.reddit.com/r/haskell/comments/2gnsfg/using_haskell_to_study_provability_logic/cklhfgw\">partial implementation</a> in Agda, which I then converted into the Haskell version below.</p>\n<p>To play with it online, you can copy the whole bunch of code, then go to&nbsp;<a href=\"http://www.compileonline.com/compile_haskell_online.php\">CompileOnline</a>&nbsp;and paste it in the edit box on the left, replacing what's already there. Then click \"Compile &amp; Execute\" in the top left. If it compiles without errors, that means everything is right with the world, so you can change something and try again. (I hate people who write about programming and don't make it easy to try out their code!) Here we go:</p>\n<pre>main = return ()<br />\n-- Assumptions<br />\ndata Theorem a<br />\nlogic1 = undefined :: Theorem (a -&gt; b) -&gt; Theorem a -&gt; Theorem b\nlogic2 = undefined :: Theorem (a -&gt; b) -&gt; Theorem (b -&gt; c) -&gt; Theorem (a -&gt; c)\nlogic3 = undefined :: Theorem (a -&gt; b -&gt; c) -&gt; Theorem (a -&gt; b) -&gt; Theorem (a -&gt; c)<br />\ndata Provable a<br />\nrule1 = undefined :: Theorem a -&gt; Theorem (Provable a)\nrule2 = undefined :: Theorem (Provable a -&gt; Provable (Provable a))\nrule3 = undefined :: Theorem (Provable (a -&gt; b) -&gt; Provable a -&gt; Provable b)<br />\ndata P<br />\npremise = undefined :: Theorem (Provable P -&gt; P)<br />\ndata Psi<br />\npsi1 = undefined :: Theorem (Psi -&gt; (Provable Psi -&gt; P))\npsi2 = undefined :: Theorem ((Provable Psi -&gt; P) -&gt; Psi)<br />\n-- Proof<br />\nstep3 :: Theorem (Psi -&gt; Provable Psi -&gt; P)\nstep3 = psi1<br />\nstep4 :: Theorem (Provable (Psi -&gt; Provable Psi -&gt; P))\nstep4 = rule1 step3<br />\nstep5 :: Theorem (Provable Psi -&gt; Provable (Provable Psi -&gt; P))\nstep5 = logic1 rule3 step4<br />\nstep6 :: Theorem (Provable (Provable Psi -&gt; P) -&gt; Provable (Provable Psi) -&gt; Provable P)\nstep6 = rule3<br />\nstep7 :: Theorem (Provable Psi -&gt; Provable (Provable Psi) -&gt; Provable P)\nstep7 = logic2 step5 step6<br />\nstep8 :: Theorem (Provable Psi -&gt; Provable (Provable Psi))\nstep8 = rule2<br />\nstep9 :: Theorem (Provable Psi -&gt; Provable P)\nstep9 = logic3 step7 step8<br />\nstep10 :: Theorem (Provable Psi -&gt; P)\nstep10 = logic2 step9 premise<br />\nstep11 :: Theorem ((Provable Psi -&gt; P) -&gt; Psi)\nstep11 = psi2<br />\nstep12 :: Theorem Psi\nstep12 = logic1 step11 step10<br />\nstep13 :: Theorem (Provable Psi)\nstep13 = rule1 step12<br />\nstep14 :: Theorem P\nstep14 = logic1 step10 step13<br />\n-- All the steps squished together<br />\nlemma :: Theorem (Provable Psi -&gt; P)\nlemma = logic2 (logic3 (logic2 (logic1 rule3 (rule1 psi1)) rule3) rule2) premise<br />\ntheorem :: Theorem P\ntheorem = logic1 lemma (rule1 (logic1 psi2 lemma))</pre>\n<p>To make sense of the code, you should interpret the type constructor Theorem as the symbol&nbsp;\u22a2 from the <a href=\"http://en.wikipedia.org/wiki/L%C3%B6b's_theorem#Proof_of_L.C3.B6b.27s_theorem\">Wikipedia proof</a>, and Provable as the symbol&nbsp;\u2610. All the assumptions have value \"undefined\" because we don't care about their computational content, only their types. The assumptions logic1..3 give just enough propositional logic for the proof to work, while rule1..3 are direct translations of the three rules from Wikipedia. The assumptions psi1 and psi2 describe the specific fixpoint used in the proof, because adding general fixpoint machinery would make the code much more complicated. The types P and Psi, of course, correspond to sentences P and&nbsp;&Psi;, and \"premise\" is the premise of the whole theorem, that is, \u22a2(\u2610P&rarr;P). The conclusion \u22a2P can be seen in the type of step14.</p>\n<p>As for the \"squished\" version, I guess I wrote it just to satisfy my refactoring urge. I don't recommend anyone to try reading that, except maybe to marvel at the complexity :-)</p>\n<p>EDIT: in addition to the <a href=\"http://www.reddit.com/r/haskell/comments/2gnsfg/using_haskell_to_study_provability_logic/\">previous Reddit thread</a>, there's now a <a href=\"http://www.reddit.com/r/haskell/comments/2gy4la/a_proof_of_l%C3%B6bs_theorem_in_haskell/\">new Reddit thread</a> about this post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HFou6RHqFagkyrKkW": 1, "6nS8oYmSMuFMaiowF": 1, "GY5kPPpCoyt9fnTMn": 2, "FJ3MGb684F88BoN2o": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jshdZw3xofq9wgE7T", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 52, "extendedScore": null, "score": 0.000126, "legacy": true, "legacyId": "27229", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ALCnqX6Xx8bpFMZq3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-19T16:43:19.602Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-32", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rWBZAB85996yAmveq/weekly-lw-meetups-32", "pageUrlRelative": "/posts/rWBZAB85996yAmveq/weekly-lw-meetups-32", "linkUrl": "https://www.lesswrong.com/posts/rWBZAB85996yAmveq/weekly-lw-meetups-32", "postedAtFormatted": "Friday, September 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrWBZAB85996yAmveq%2Fweekly-lw-meetups-32%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrWBZAB85996yAmveq%2Fweekly-lw-meetups-32", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrWBZAB85996yAmveq%2Fweekly-lw-meetups-32", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 608, "htmlBody": "<p><strong>This summary was posted to LW Main on September 12th. The following week's summary is <a href=\"/lw/l0f/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/147\">Copenhagen - September: This Wavefunction Has Uncollapsed:&nbsp;<span class=\"date\">13 September 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/14f\">Frankfurt: How to improve your life:&nbsp;<span class=\"date\">28 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/zr\">Houston, TX:&nbsp;<span class=\"date\">13 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/146\">[Perth] Sunday lunch:&nbsp;<span class=\"date\">21 September 2014 12:00PM</span></a></li>\n<li><a href=\"/meetups/14c\">Portland Teachable Skills Discussion:&nbsp;<span class=\"date\">20 September 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/14d\">Tel Aviv: Rational Career Development:&nbsp;<span class=\"date\">18 September 2014 08:33PM</span></a></li>\n<li><a href=\"/meetups/13b\">Utrecht: Debiasing techniques:&nbsp;<span class=\"date\">21 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13u\">Utrecht: Effective Altruism and Politics:&nbsp;<span class=\"date\">05 October 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13v\">Utrecht: Artificial Intelligence:&nbsp;<span class=\"date\">19 October 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13w\">Utrecht: Climate Change:&nbsp;<span class=\"date\">02 November 2014 03:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">13 September 2025 01:30PM</span></a></li>\n<li><a href=\"/meetups/149\">Brussels - September meetup:&nbsp;<span class=\"date\">13 September 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/14a\">[Cambridge MA] Passive Investing and Financial Independence:&nbsp;<span class=\"date\">21 September 2014 03:30PM</span></a></li>\n<li><a href=\"/meetups/14b\">[Cambridge MA] Social Skills:&nbsp;<span class=\"date\">24 September 2014 03:30PM</span></a></li>\n<li><a href=\"/meetups/13m\">Canberra: Akrasia-busters!:&nbsp;<span class=\"date\">13 September 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/148\">London social meetup:&nbsp;<span class=\"date\">14 September 2014 02:00AM</span></a></li>\n<li><a href=\"/meetups/140\">Moscow Meetup: Codename Felix:&nbsp;<span class=\"date\">14 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/142\">Sydney Meetup - September:&nbsp;<span class=\"date\">24 September 2014 06:30PM</span></a></li>\n<li><a href=\"/meetups/143\">Vienna - Superintelligence:&nbsp;<span class=\"date\">27 September 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/14e\">Washington D.C.: Parkour (Backup: Task Management):&nbsp;<span class=\"date\">14 September 2014 03:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Buffalo.2C_NY\">Buffalo</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Moscow.2C_Russia\">Moscow</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong>&nbsp;</strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Sydney\">Sydney</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> </strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tel_Aviv.2C_Israel\">Tel Aviv</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Warsaw.2C_Poland\">Warsaw</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rWBZAB85996yAmveq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 2.0178922152934656e-06, "legacy": true, "legacyId": "27184", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dMs7ugrhvvBAqvYgt", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-19T22:18:39.041Z", "modifiedAt": null, "url": null, "title": "Link: quotas-microaggression-and-meritocracy", "slug": "link-quotas-microaggression-and-meritocracy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:39.291Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Lexico", "createdAt": "2014-07-22T03:30:25.531Z", "isAdmin": false, "displayName": "Lexico"}, "userId": "m2gxgmujnsASzQmZg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N9ZWcqtopKqxHhbEB/link-quotas-microaggression-and-meritocracy", "pageUrlRelative": "/posts/N9ZWcqtopKqxHhbEB/link-quotas-microaggression-and-meritocracy", "linkUrl": "https://www.lesswrong.com/posts/N9ZWcqtopKqxHhbEB/link-quotas-microaggression-and-meritocracy", "postedAtFormatted": "Friday, September 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20quotas-microaggression-and-meritocracy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20quotas-microaggression-and-meritocracy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN9ZWcqtopKqxHhbEB%2Flink-quotas-microaggression-and-meritocracy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20quotas-microaggression-and-meritocracy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN9ZWcqtopKqxHhbEB%2Flink-quotas-microaggression-and-meritocracy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN9ZWcqtopKqxHhbEB%2Flink-quotas-microaggression-and-meritocracy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 138, "htmlBody": "<p>&nbsp;</p>\n<p>I remember seeing a talk of the concept of privilege show up in the discussion thread on <a href=\"/r/discussion/lw/kzn/what_are_your_contrarian_views/\">contrarian views</a>.</p>\n<p>Some discussion got started from \"Feminism is a good thing. Privilege is real.\"</p>\n<p>This is an article that presents some of those ideas in a way that might be approachable for LW.</p>\n<p>http://curt-rice.com/quotas-microaggression-and-meritocracy/</p>\n<p>One of the ideas I take out of this is that these issues can be examined as the result of unconscious cognitive bias. IE sexism isn't the result of any conscious thought, but can be the result as a failure mode where we don't rationality correctly in these social situations.</p>\n<p>Of course a broad view of these issues exist, and many people have different ways of looking at these issues, but I think it would be good to focus on the case presented in this article rather than your other associations.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N9ZWcqtopKqxHhbEB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": -4, "extendedScore": null, "score": -1.2e-05, "legacy": true, "legacyId": "27232", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 164, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vsGJTr4bNhA4MutMs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-20T08:55:31.119Z", "modifiedAt": null, "url": null, "title": "[question] What edutainment apps do you recommend?", "slug": "question-what-edutainment-apps-do-you-recommend", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:36.182Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zwtCtvHkgcTsuBxfn/question-what-edutainment-apps-do-you-recommend", "pageUrlRelative": "/posts/zwtCtvHkgcTsuBxfn/question-what-edutainment-apps-do-you-recommend", "linkUrl": "https://www.lesswrong.com/posts/zwtCtvHkgcTsuBxfn/question-what-edutainment-apps-do-you-recommend", "postedAtFormatted": "Saturday, September 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bquestion%5D%20What%20edutainment%20apps%20do%20you%20recommend%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bquestion%5D%20What%20edutainment%20apps%20do%20you%20recommend%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzwtCtvHkgcTsuBxfn%2Fquestion-what-edutainment-apps-do-you-recommend%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bquestion%5D%20What%20edutainment%20apps%20do%20you%20recommend%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzwtCtvHkgcTsuBxfn%2Fquestion-what-edutainment-apps-do-you-recommend", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzwtCtvHkgcTsuBxfn%2Fquestion-what-edutainment-apps-do-you-recommend", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p><strong>Follow up to: <a href=\"/r/discussion/lw/dhn/rationality_games_apps_brainstorming/\">Rationality Games Apps</a></strong></p>\n<p><strong>In the spirit of: <a href=\"/lw/ilw/games_for_rationalists/\">Games for rationalists</a></strong></p>\n<div>My son (10) wants a smartphone and I reasonably expect that he wants to and will play games with it. He appears to be the right age to use it. I don't want to prevent him from playing games nor do I think that possible or helpful. But I'd like to suggest and promote a few apps and games that *are* helpful or from which he can learn something.&nbsp;</div>\n<p>Obvious candidates are&nbsp;</p>\n<p>\n<ul>\n<li><a href=\"http://www.dragonboxapp.com/\">Dragon Box</a></li>\n<li><a href=\"https://habitrpg.com/static/front\">HabitRPG</a></li>\n<li><a href=\"http://ankisrs.net/\">Anki App</a></li>\n</ul>\n</p>\n<p>There are lots of low profile apps <a href=\"https://play.google.com/store/apps/category/EDUCATION/collection/topselling_free\">filed under learning in the app stores</a> but most of this is crap and it takes lots of time to explore these.&nbsp;</p>\n<p>I also found some recommendation for <a href=\"http://www.teachthought.com/technology/the-best-education-apps-for-android/\">learning with Android apps</a> and will point my son to these.&nbsp;</p>\n<p>I'd like to hear what apps do you or yours children use. Which apps and esp. games do you recommend for future rationalists?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zwtCtvHkgcTsuBxfn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 2.0196620436593766e-06, "legacy": true, "legacyId": "27233", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Follow_up_to__Rationality_Games_Apps\">Follow up to: <a href=\"/r/discussion/lw/dhn/rationality_games_apps_brainstorming/\">Rationality Games Apps</a></strong></p>\n<p><strong id=\"In_the_spirit_of__Games_for_rationalists\">In the spirit of: <a href=\"/lw/ilw/games_for_rationalists/\">Games for rationalists</a></strong></p>\n<div>My son (10) wants a smartphone and I reasonably expect that he wants to and will play games with it. He appears to be the right age to use it. I don't want to prevent him from playing games nor do I think that possible or helpful. But I'd like to suggest and promote a few apps and games that *are* helpful or from which he can learn something.&nbsp;</div>\n<p>Obvious candidates are&nbsp;</p>\n<p>\n</p><ul>\n<li><a href=\"http://www.dragonboxapp.com/\">Dragon Box</a></li>\n<li><a href=\"https://habitrpg.com/static/front\">HabitRPG</a></li>\n<li><a href=\"http://ankisrs.net/\">Anki App</a></li>\n</ul>\n<p></p>\n<p>There are lots of low profile apps <a href=\"https://play.google.com/store/apps/category/EDUCATION/collection/topselling_free\">filed under learning in the app stores</a> but most of this is crap and it takes lots of time to explore these.&nbsp;</p>\n<p>I also found some recommendation for <a href=\"http://www.teachthought.com/technology/the-best-education-apps-for-android/\">learning with Android apps</a> and will point my son to these.&nbsp;</p>\n<p>I'd like to hear what apps do you or yours children use. Which apps and esp. games do you recommend for future rationalists?</p>", "sections": [{"title": "Follow up to: Rationality Games Apps", "anchor": "Follow_up_to__Rationality_Games_Apps", "level": 1}, {"title": "In the spirit of: Games for rationalists", "anchor": "In_the_spirit_of__Games_for_rationalists", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CNZQesEh6Ts5G2r7b", "toFESsZBQEZ4wq9od"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-20T12:09:50.492Z", "modifiedAt": null, "url": null, "title": "Discussion of \"What are your contrarian views?\"", "slug": "discussion-of-what-are-your-contrarian-views", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:00.402Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Metus", "createdAt": "2011-01-23T21:54:34.357Z", "isAdmin": false, "displayName": "Metus"}, "userId": "mNQ4fSvro7LYgrii4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KZgj5QJx57RGdgu46/discussion-of-what-are-your-contrarian-views", "pageUrlRelative": "/posts/KZgj5QJx57RGdgu46/discussion-of-what-are-your-contrarian-views", "linkUrl": "https://www.lesswrong.com/posts/KZgj5QJx57RGdgu46/discussion-of-what-are-your-contrarian-views", "postedAtFormatted": "Saturday, September 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Discussion%20of%20%22What%20are%20your%20contrarian%20views%3F%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADiscussion%20of%20%22What%20are%20your%20contrarian%20views%3F%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZgj5QJx57RGdgu46%2Fdiscussion-of-what-are-your-contrarian-views%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Discussion%20of%20%22What%20are%20your%20contrarian%20views%3F%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZgj5QJx57RGdgu46%2Fdiscussion-of-what-are-your-contrarian-views", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZgj5QJx57RGdgu46%2Fdiscussion-of-what-are-your-contrarian-views", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<p>I'd like to use this thread to review the \"What are your contrarian views?\" thread as the meta discussion there was drowned out by the intended content I feel. What can be done better with the voting system? Should threads like these be a regular occurence? What have you specifically learned from that thread? Did you like it at all?</p>\n<p>&nbsp;</p>\n<p>Usual voting rules apply.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6Qic6PwwBycopJFNN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KZgj5QJx57RGdgu46", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 2.0200161283766995e-06, "legacy": true, "legacyId": "27234", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-20T14:08:04.596Z", "modifiedAt": null, "url": null, "title": "Street action \"Stop existential risks!\", Union square, San Francisco, September 27, 2014 at 2:00 PM", "slug": "street-action-stop-existential-risks-union-square-san", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:38.286Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "turchin", "createdAt": "2010-02-03T20:22:54.095Z", "isAdmin": false, "displayName": "turchin"}, "userId": "2kDfHyTEpYCoa2SRq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E6vdrYTHGYDuTw7BG/street-action-stop-existential-risks-union-square-san", "pageUrlRelative": "/posts/E6vdrYTHGYDuTw7BG/street-action-stop-existential-risks-union-square-san", "linkUrl": "https://www.lesswrong.com/posts/E6vdrYTHGYDuTw7BG/street-action-stop-existential-risks-union-square-san", "postedAtFormatted": "Saturday, September 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Street%20action%20%22Stop%20existential%20risks!%22%2C%20Union%20square%2C%20San%20Francisco%2C%20September%2027%2C%202014%20at%202%3A00%20PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStreet%20action%20%22Stop%20existential%20risks!%22%2C%20Union%20square%2C%20San%20Francisco%2C%20September%2027%2C%202014%20at%202%3A00%20PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE6vdrYTHGYDuTw7BG%2Fstreet-action-stop-existential-risks-union-square-san%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Street%20action%20%22Stop%20existential%20risks!%22%2C%20Union%20square%2C%20San%20Francisco%2C%20September%2027%2C%202014%20at%202%3A00%20PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE6vdrYTHGYDuTw7BG%2Fstreet-action-stop-existential-risks-union-square-san", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE6vdrYTHGYDuTw7BG%2Fstreet-action-stop-existential-risks-union-square-san", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 196, "htmlBody": "<p>Existential risks are the risks of human extinction. A global catastrophe will happen most likely because of the new technologies such as biotech, nanotech, and AI, along with several other risks: runaway global warming, and nuclear war. Sir Martin Rees estimates these risks to have a fifty percent probability in the 21<sup>st</sup> century.</p>\n<p>We must raise the awareness of impending doom and make the first ever street action against the possibility of human extinction. Our efforts could help to prevent these global catastrophes from taking place. I suggest we meet in <strong style=\"mso-bidi-font-weight: normal;\">Union square, San Francisco, September 27, 2014 at 2:00 PM </strong>in order to make a short and intense photo session with the following slogans:</p>\n<p>Stop Existential Risks!</p>\n<p>No Human Extinction!</p>\n<p>AI must be Friendly!</p>\n<p>No Doomsday Weapons!</p>\n<p>Ebola must die!</p>\n<p>Prevent Global Catastrophe!</p>\n<p><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>180</o:Words> <o:Characters>1026</o:Characters> <o:Company>rtd</o:Company> <o:Lines>8</o:Lines> <o:Paragraphs>2</o:Paragraphs> <o:CharactersWithSpaces>1204</o:CharactersWithSpaces> <o:Version>14.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"\u041e\u0431\u044b\u0447\u043d\u0430\u044f \u0442\u0430\u0431\u043b\u0438\u0446\u0430\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin-top:0cm; mso-para-margin-right:0cm; mso-para-margin-bottom:8.0pt; mso-para-margin-left:0cm; line-height:200%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:Calibri; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-fareast-language:EN-US;} --> <!--[endif] --> <!--StartFragment--> <!--EndFragment--></p>\n<p>These slogans will be printed in advance, but more banners are welcome. I have previous experience with organizing actions for immortality and funding of life extension near Googleplex, the White house in DC, and Burning Man, and I know this street action, taking place on <strong style=\"mso-bidi-font-weight: normal;\">September 27<sup>th</sup></strong>,<span style=\"mso-spacerun: yes;\">&nbsp; </span>is both legal and a fun way to express our points of view.</p>\n<p>Organized by Alexey Turchin and Longevity Party.</p>\n<p>&nbsp;</p>\n<p>Update: Photos from the action.</p>\n<p><img src=\"http://img-fotki.yandex.ru/get/6736/10149979.4/0_b7a78_3c3f4bab_orig\" alt=\"\" width=\"800\" height=\"600\" /></p>\n<p>&nbsp;</p>\n<p><img src=\"http://img-fotki.yandex.ru/get/6743/10149979.4/0_b7a79_52600752_orig\" alt=\"\" width=\"800\" height=\"600\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E6vdrYTHGYDuTw7BG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": -17, "extendedScore": null, "score": -7.4e-05, "legacy": true, "legacyId": "27235", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-20T15:02:40.692Z", "modifiedAt": null, "url": null, "title": "LessWrong's attitude towards AI research", "slug": "lesswrong-s-attitude-towards-ai-research", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:38.024Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Florian_Dietz", "createdAt": "2014-09-01T13:22:59.032Z", "isAdmin": false, "displayName": "Florian_Dietz"}, "userId": "AuDdqaZ28udg4y2GP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T9Q5XB9NYDjB9QBoB/lesswrong-s-attitude-towards-ai-research", "pageUrlRelative": "/posts/T9Q5XB9NYDjB9QBoB/lesswrong-s-attitude-towards-ai-research", "linkUrl": "https://www.lesswrong.com/posts/T9Q5XB9NYDjB9QBoB/lesswrong-s-attitude-towards-ai-research", "postedAtFormatted": "Saturday, September 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong's%20attitude%20towards%20AI%20research&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong's%20attitude%20towards%20AI%20research%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT9Q5XB9NYDjB9QBoB%2Flesswrong-s-attitude-towards-ai-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong's%20attitude%20towards%20AI%20research%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT9Q5XB9NYDjB9QBoB%2Flesswrong-s-attitude-towards-ai-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT9Q5XB9NYDjB9QBoB%2Flesswrong-s-attitude-towards-ai-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 210, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">AI friendliness is an important goal and it would be insanely dangerous to build an AI without researching this issue first. I think this is pretty much the consensus view, and that is perfectly sensible.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">However, I believe that we are making the wrong inferences from this.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">The straightforward inference is \"we should ensure that we completely understand AI friendliness before starting to build an AI\". This leads to a strongly negative view of AI researchers and scares them away. But unfortunately reality isn't that simple. The goal isn't \"build a friendly AI\", but \"make sure that whoever builds the first AI makes it friendly\".</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">It seems to me that it is vastly more likely that the first AI will be built by a large company, or as a large government project, than by a group of university researchers, who just don't have the funding for that.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">I therefore think that we should try to take a more pragmatic approach. The way to do this would be to focus more on outreach and less on research. It won't do anyone any good if we find the perfect formula for AI friendliness on the same day that someone who has never heard of AI friendliness before finishes his paperclip maximizer.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">What is your opinion on this?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T9Q5XB9NYDjB9QBoB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 2.0203311542628963e-06, "legacy": true, "legacyId": "27238", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-20T15:39:07.903Z", "modifiedAt": null, "url": null, "title": "You\u2019re Entitled to Everyone\u2019s Opinion", "slug": "you-re-entitled-to-everyone-s-opinion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:32.661Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "satt", "createdAt": "2010-07-07T08:36:57.786Z", "isAdmin": false, "displayName": "satt"}, "userId": "iWyTE3Rqvf5rpA3B5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CL9KMiuiff4Y4CqFE/you-re-entitled-to-everyone-s-opinion", "pageUrlRelative": "/posts/CL9KMiuiff4Y4CqFE/you-re-entitled-to-everyone-s-opinion", "linkUrl": "https://www.lesswrong.com/posts/CL9KMiuiff4Y4CqFE/you-re-entitled-to-everyone-s-opinion", "postedAtFormatted": "Saturday, September 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You%E2%80%99re%20Entitled%20to%20Everyone%E2%80%99s%20Opinion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou%E2%80%99re%20Entitled%20to%20Everyone%E2%80%99s%20Opinion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCL9KMiuiff4Y4CqFE%2Fyou-re-entitled-to-everyone-s-opinion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You%E2%80%99re%20Entitled%20to%20Everyone%E2%80%99s%20Opinion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCL9KMiuiff4Y4CqFE%2Fyou-re-entitled-to-everyone-s-opinion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCL9KMiuiff4Y4CqFE%2Fyou-re-entitled-to-everyone-s-opinion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1137, "htmlBody": "<p>Over the past year, I've noticed a topic where Less Wrong might have a blind spot: public opinion. Since last September I've had (or butted into) five conversations here where someone's written something which made me think, \"you wouldn't be saying that if you'd looked up surveys where people were actually asked about this\". The following list includes six findings I've brought up in those LW threads. All of the findings come from surveys of public opinion in the United States, though some of the results are so obvious that polls scarcely seem necessary to establish their truth.</p>\n<ol>\n<li>The public's view of the harms and benefits from scientific research has consistently become more pessimistic since the National Science Foundation began its surveys in 1979. (In the wake of repeated misconduct scandals, and controversies like those over vaccination, global warming, fluoridation, animal research, stem cells, and genetic modification, people consider scientists less objective and less trustworthy.)</li>\n<li>Most adults identify as neither Republican nor Democrat. (Although the public is far from apolitical, lots of people are unhappy with how politics currently works, and also recognize that their beliefs align imperfectly with the simplistic left-right axis. This dissuades them from identifying with mainstream parties.)</li>\n<li>Adults under 30 are less likely to believe that abortion should be illegal than the middle-aged. (Younger adults tend to be more socially liberal in general than their parents' generation.)</li>\n<li>In the 1960s, those under 30 were less likely than the middle-aged to think the US made a mistake in sending troops to fight in Vietnam. (The under-30s were more likely to be students and/or highly educated, and more educated people were less likely to think sending troops to Vietnam was a mistake.)</li>\n<li>The Harris Survey asked, in November 1969, \"as far as their objectives are concerned, do you sympathize with the goals of the people who are demonstrating, marching, and protesting against the war in Vietnam, or do you disagree with their goals?\" Most respondents aged 50+ sympathized with the protesters' goals, whereas only 28% of under-35s did. (Despite the specific wording of the question, the younger respondents worried that the protests reflected badly on their demographic, whereas older respondents were more often glad to see their own dissent voiced.)</li>\n<li>A 2002 survey found that about 90% of adult smokers agreed with the statement, \"If you had to do it over again, you would not have started smoking.\" (While most smokers derive enjoyment from smoking, many weight smoking's negative consequences strongly enough that they'd rather not smoke; they continue smoking because of habit or addiction.)</li>\n</ol>\n<p><a id=\"more\"></a> If you've read Eliezer's \"<a href=\"/lw/im/hindsight_devalues_science/\">Hindsight Devalues Science</a>\", you're probably starting to feel d&eacute;j&agrave; vu, and might have guessed that I'm bluffing you to make a point. If so, well done &mdash; you're quite correct! But before you assume I'm about to repeat Eliezer's trick and stop there, read the other half of my list:</p>\n<ol>\n<li>The public's view of the harms and benefits from scientific research has remained about the same since the National Science Foundation began its surveys in 1979. (Despite the last 35 years of scientific scandals and controversies, people appreciate the technological advances science brings, and think scientists come off well compared to other professions.)</li>\n<li>Most adults identify as Republican or Democrat. (Although many are dissatisfied with contemporary politics, most voters' political views are nonetheless represented better by one mainstream party than the other, and people find it logical to give more power to the party which supports more of one's preferred policies.)</li>\n<li>Adults under 30 are more likely to believe that abortion should be illegal than the middle-aged. (Younger adults, being less reflective in general, tend to have more extreme political beliefs than the middle-aged.)</li>\n<li>In the 1960s, those under 30 were more likely than the middle-aged to think the US made a mistake in sending troops to fight in Vietnam. (The under-30s were more likely to be students and/or highly educated, and more educated people were more likely to think sending troops to Vietnam was a mistake.) <!-- (Young people were, on average, more pacifist than older people.) --></li>\n<li>The Harris Survey asked, in November 1969, \"as far as their objectives are concerned, do you sympathize with the goals of the people who are demonstrating, marching, and protesting against the war in Vietnam, or do you disagree with their goals?\" Most respondents aged 35 or less sympathized with the protesters' goals, whereas only 28% of those aged 50+ did. (Younger people were more anti-authoritarian, which translated to more sympathy for protesters regardless of their goals.)</li>\n<li>A 2002 survey found that about 90% of adult smokers disagreed with the statement, \"If you had to do it over again, you would not have started smoking.\" (Whatever the other consequences, smokers derive much pleasure from the experience of smoking, and even an addict who suffers major harm could justify their addiction as the result of a rational decision.)</li>\n</ol>\n<p id=\"#back\">Here's my twist on Eliezer's twist. It is technically true that the list includes six true findings, but the complete list has 12 items, so half of the statements are false. I made up the false statements as fake variations on the true findings, concocted parenthetical rationalizations for them, and randomly mixed the false claims with the true. I expect a lot of people reading this would, after seeing the full list, have a hard time sorting the true from the false without looking at <a href=\"#sources\">the data</a> &mdash; including the people who nodded along in agreement with the first half of the list.</p>\n<p>Totally spurious beliefs about public opinion can have a ring of plausibility, especially because it's easy to invent sensible-sounding reasons why they ought to be correct. The <a href=\"https://en.wikipedia.org/wiki/Availability_heuristic\">availability heuristic</a> presumably plays a role too, with people inferring the state of public opinion from what their friends &amp; acquaintances think, not accounting for how unrepresentative their social network is. In any event, people's opinions of public opinion are often wrong, and it's worth taking a couple of minutes to look for Gallup poll results and the like online before commenting on public opinion.</p>\n<hr />\n<p id=\"#sources\"><small> <strong>Sources.</strong> On perceptions of whether the benefits of science outweigh its harmful results, see <a href=\"http://www.nsf.gov/statistics/seind12/c7/fig07-11.gif\">figure 7-11</a> from <a href=\"http://www.nsf.gov/statistics/seind12/c7/c7s3.htm\">chapter 7</a> of the National Science Foundation's \"Science and Engineering Indicators 2012\". On party affiliation, see <a href=\"http://www.gallup.com/poll/15370/party-affiliation.aspx\">the polls Gallup runs</a> every month. On abortion views, <a href=\"http://www.people-press.org/2012/04/25/more-support-for-gun-rights-gay-marriage-than-in-2008-or-2004/4-25-12-9/\">Pew Research Center</a> has statistics for 2007 through 2012. For a breakdown of beliefs about the Vietnam War by age, consult Hazel Erskine's 1970 article \"<a href=\"http://www.jstor.org/stable/2747894\">The Polls: Is War a Mistake?</a>\" (<cite>Public Opinion Quarterly</cite>, <em>34</em>(1), 134&ndash;150); Jim Miller <a href=\"http://www.seanet.com/~jimxc/Politics/Mistakes/Vietnam_support.html\">extends the data</a> to May 1971, but presents them somewhat differently. On smokers' regrets, see Geoffrey T. Fong et al.'s 2004 article \"<a href=\"http://www.davidhammond.ca/Old%20Website/Publication%20new/2004%20ITC%20Regret%20-%20NTR%20%28Fong%29.pdf\">The near-universal experience of regret among smokers in four countries: Findings from the International Tobacco Control Policy Evaluation Survey</a>\" (<cite>Nicotine &amp; Tobacco Research</cite>, <em>6</em>(S3), S341&ndash;S351). <a href=\"#back\">\u21a9</a> </small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"gHCNhqxuJq2bZ2akb": 1, "FkzScn5byCs9PxGsA": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CL9KMiuiff4Y4CqFE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 33, "extendedScore": null, "score": 2.020397608388935e-06, "legacy": true, "legacyId": "27239", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WnheMGAka4fL99eae"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-20T18:40:25.528Z", "modifiedAt": null, "url": null, "title": "An introduction to Newcomblike problems", "slug": "an-introduction-to-newcomblike-problems", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:36.485Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wk7WmrN4FeNmyepXm/an-introduction-to-newcomblike-problems", "pageUrlRelative": "/posts/wk7WmrN4FeNmyepXm/an-introduction-to-newcomblike-problems", "linkUrl": "https://www.lesswrong.com/posts/wk7WmrN4FeNmyepXm/an-introduction-to-newcomblike-problems", "postedAtFormatted": "Saturday, September 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20introduction%20to%20Newcomblike%20problems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20introduction%20to%20Newcomblike%20problems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwk7WmrN4FeNmyepXm%2Fan-introduction-to-newcomblike-problems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20introduction%20to%20Newcomblike%20problems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwk7WmrN4FeNmyepXm%2Fan-introduction-to-newcomblike-problems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwk7WmrN4FeNmyepXm%2Fan-introduction-to-newcomblike-problems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2170, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><em>This is&nbsp;<span style=\"color: #8a8a8b;\"><a href=\"http://mindingourway.com/intro-to-newcomblike-problems/\">crossposted</a></span>&nbsp;from&nbsp;<span style=\"color: #8a8a8b;\"><a href=\"http://mindingourway.com\">my new blog</a></span>, following up on my <a href=\"/lw/kz9/causal_decision_theory_is_unsatisfactory/\">previous post</a>. It introduces the original \"Newcomb's problem\" and discusses the motivation behind twoboxing and the reasons why CDT fails. content is probably review for most LessWrongers, later posts in the sequence may be of more interest.</em></p>\n<hr />\n<p><a href=\"/lw/kz9/causal_decision_theory_is_unsatisfactory/\">Last time</a> I introduced causal decision theory (CDT) and showed how it has unsatisfactory behavior on \"Newcomblike problems\". Today, we'll explore Newcomblike problems in a bit more depth, starting with William Newcomb's original problem.</p>\n<h2>The Problem</h2>\n<p>Once upon a time there was a strange alien named &Omega; who is very very good at predicting humans. There is this one game that &Omega; likes to play with humans, and &Omega; has played it thousands of times without ever making a mistake. The game works as follows:</p>\n<p>First, &Omega; observes the human for a while and collects lots of information about the human. Then, &Omega; makes a decision based on how &Omega; predicts the human will react in the upcoming game. Finally, &Omega; presents the human with two boxes.</p>\n<p>The first box is blue, transparent, and contains $1000. The second box is red and opaque.</p>\n<blockquote>\n<p>You may take either the red box alone, or both boxes,</p>\n</blockquote>\n<p>&Omega; informs the human. (These are magical boxes where if you decide to take only the red one then the blue one, and the $1000 within, will disappear.)</p>\n<blockquote>\n<p>If I predicted that you would take only the red box, then I filled it with $1,000,000. Otherwise, I left it empty. I have already made my choice,</p>\n</blockquote>\n<p>&Omega; concludes, before turning around and walking away.</p>\n<p>You may take either only the red box, or both boxes. (If you try something clever, like taking the red box while a friend takes a blue box, then the red box is filled with hornets. Lots and lots of hornets.) What do you do?</p>\n<p><a id=\"more\"></a></p>\n<h2>The Dilemma</h2>\n<p>Should you take one box or two boxes?</p>\n<p>If you take only the red box, then you're <em>necessarily</em> leaving $1,000 on the table.</p>\n<p>But if you take both boxes, then &Omega; predicted that, and you miss out at a chance of $1,000,000.</p>\n<h2>Recap</h2>\n<p>Now, as you'll remember, we're discussing <em>decision theories</em>, algorithms that prescribe actions in games like these. Our motivation for studying decision theories is manyfold. For one thing, people who <em>don't</em> have tools for making good decisions can often become confused (remember the people who <a href=\"http://www.thedailybeast.com/articles/2013/07/12/your-future-is-in-the-palm-of-your-surgeon-s-hand.html\">got palm surgery to change their fortunes</a>).</p>\n<p>This is not just a problem for irrational or undereducated people: it's easy to trust yourself to make the right choices most of the time, because human heuristics and intuitions are usually pretty good. But what happens in the strange edge-case scenarios? What do you do when you encounter problems where your intuitions conflict? In these cases, it's important to know <em>what it means</em> to make a good choice.</p>\n<p><em>My</em> motivation for studying decision theory is that in order to construct an artificial intelligence you need a pretty dang good understanding of what sort of decision algorithms perform well.</p>\n<p>Last post, we explored the standard decision theory used by modern philosophers to encode rational decision making. I'm <em>eventually</em> going to use this series of posts to explain why our current knowledge of decision theory (and CDT in particular) are completely inadequate for use in any sufficiently intelligent self-modifying agent.</p>\n<p>But before we go there, let's see how causal decision theory reacts to Newcomb's problem.</p>\n<h2>The Choice</h2>\n<p>Let's analyze this problem using causal decision theory from yesterday. Roughly, the causal decision theorist reasons as follows:</p>\n<blockquote>\n<p>&Omega; offers me the boxes after making its decision. Now, either &Omega; has filled the red box or &Omega; has not filled the red box. The decision has <em>already been made</em>. If &Omega; filled the red box, then I had better take both boxes (and get a thousand and a million dollars). If &Omega; didn't fill the red box, then I had better take both boxes so that I at least get a thousand dollars. <em>No matter what</em> &Omega; chose, I had better take both boxes.</p>\n</blockquote>\n<p>And so, someone reasoning using causal decision theory takes both boxes (and, because &Omega; is a very good predictor, they walk away with $1000).</p>\n<p>Let's walk through that reasoning in slow-mo, using causal graphs. The causal graph for this problem looks like this:</p>\n<p style=\"text-align: center\"><img src=\"http://mindingourway.com/content/images/2014/Sep/newcomb-problem.png\" alt=\"\" /></p>\n<p>With the nodes defined as follows:</p>\n<ul>\n<li><code>You (yesterday)</code> is the algorithm implementing you yesterday. In this simplified setting, we assuem that its value determines the contents of <code>You (today)</code>.</li>\n<li><code>&Omega;</code> is a function that observes you yesterday and decides whether to put $1,000,000 into the red box. Its value is either <em>filled</em> or <em>empty</em>.</li>\n<li><code>You (today)</code> is your decision algorithm. It must output either <em>onebox</em> or <em>twobox</em>.</li>\n<li><code>$</code> is $1,000,000 if <code>&Omega;</code>=<em>filled</em> plus $1,000 if <code>You (today)</code>=<em>twobox</em>.</li>\n</ul>\n<p>We must decide whether to output <em>onebox</em> (take only the red box) or <em>twobox</em> (take both boxes). Given some expecation p that <code>&Omega;</code>=<em>filled</em>, causal decision theory reasons as follows:</p>\n<ol>\n<li>The decision node is <code>You (today)</code>.</li>\n<li>The available actions are <em>onebox</em> and <em>twobox</em>.</li>\n<li>The utility node is <code>$</code>.</li>\n<li>Set <code>You (today)</code>=<code>const onebox</code> \n<ul>\n<li><code>$</code> = 1,000,000p</li>\n</ul>\n</li>\n<li>Set <code>You (today)</code>=<code>const twobox</code> \n<ul>\n<li><code>$</code> = 1,000,000p + 1,000</li>\n</ul>\n</li>\n</ol>\n<p>Thus, <em>no matter what probability p is</em>, CDT takes both boxes, because the action <em>twobox</em> results in an extra $1000.</p>\n<h2>The Excuse</h2>\n<p>This is, of course, the wrong answer.</p>\n<p>Because &Omega; is a really good predictor, and because &Omega; only gives a million dollars to people who take one box, if you want a million dollars then you had better only take one box.</p>\n<p>Where did CDT go wrong?</p>\n<p>Causal decision theorists sometimes argue that <em>nothing</em> went wrong. It is \"rational\" to take both boxes, because otherwise you're leaving $1,000 on the table. Reasoning about how you have to take one box so that that box will be filled is nonsense, because by the time you're making your decision, &Omega; has already left. (This argument can be found at <a href=\"http://books.google.com/books?id=LYTMhPzCUxYC&amp;q=rachel#v=onepage&amp;q&amp;f=false\">Foundations of Causal Decision Theory, p152</a>.)</p>\n<p>Of course, these people will agree that the causal decision theorists walk away with less money. But they'll tell you that this is not their fault: they are making the \"rational\" choice, and &Omega; has decided to punish people who are acting \"rationally\".</p>\n<h2>The Counter</h2>\n<p>There is some merit to this complaint. <em>No matter how you make decisions</em>, there is at least one decision problem where you do poorly and everybody else does well. To illustrate, consider the following game: every player submits a computer program that has to choose whether to press the green button or the blue button (by outputting either <code>green</code> or <code>blue</code>). Then I check which button your program is going to press. Then I give $10 to everyone whose program presses the opposite button.</p>\n<p>Clearly, many people can win $10 in this game. Also clearly, you cannot. In this game, no matter how you choose, you lose. I'm punishing <em>your decision algorithm</em> specifically.</p>\n<p>I'm sympathetic to the claim that there are games where the host punishes you unfairly. However, Newcomb's problem is <em>not</em> unfair in this sense.</p>\n<p>&Omega; is not punishing <em>people who act like you</em>, &Omega; is punishing <em>people who take two boxes</em>. (Well, I mean, &Omega; is giving $1,000 to those people, but &Omega; is withholding $1,000,000 that it gives to oneboxers, so I'll keep calling it \"punishment\").</p>\n<p>You <em>can't</em> win at the bluegreen game above. You <em>can</em> win on Newcomb's problem. All you have to do is <em>only take one box</em>.</p>\n<p>Causal decision theorists claim that it's not <em>rational</em> to take one box, because that leaves $1,000 on the table even though your choice can no longer affect the outcome. They claim that &Omega; is punishing people who act reasonably. They can't just <em>decide</em> to stop being reasonable just because &Omega; is rewarding unreasonable behavior in this case.</p>\n<p>To which I say: fine. But pretend I wasn't motivated by adherence to some archaic definition of \"reasonableness\" which clearly fails systematically in this scenario, but that I was <em>instead</em> motivated by a desire to succeed. <em>Then</em> how should I make decisions? Because in these scenarios, I clearly should not use causal decision theory.</p>\n<h2>The Failure</h2>\n<p>In fact, I can go further. I can <em>point</em> to the place where CDT goes wrong. CDT goes wrong when it reasons about the outcome of its action \"no matter what the probability p that &Omega; fills the box\".</p>\n<p><em>&Omega;'s choice to fill the box depends upon your decision algorithm.</em></p>\n<p>If your algorithm chooses to take one box, then the probability that <code>&Omega;</code>=<em>filled</em> is high. If your algorithm chooses to take two boxes, then the probability that <code>&Omega;</code>=<em>filled</em> is low.</p>\n<p>CDT's reasoning neglects this fact, because <code>&Omega;</code> is causally disconnected from <code>You (today)</code>. CDT reasons that its choice can no longer affect &Omega;, but then <em>mistakenly</em> assumes that this means the probability of <code>&Omega;</code>=<em>filled</em> is independent from <code>You (today)</code>.</p>\n<p>&Omega; makes its decision based on knowledge of what <em>the algorithm</em> inside <code>You (today)</code> looks like. When CDT reasons about what would happen if it took one box, it considers what the world would look like if (counterfactually) <code>You (today)</code> was filled with an algorithm that, <em>instead of being CDT</em>, always returned <code>onebox</code>. CDT changes the contents of <code>You (today)</code> and nothing else, and sees what would happen. But this is a bad counterfactual! If, instead of implementing CDT, you implemented an algorithm that always took one box, <em>then &Omega; would act differently</em>.</p>\n<p>CDT assumes it can change <code>You (today)</code> without affecting <code>&Omega;</code>. But because &Omega;'s choice depends upon the <em>contents</em> of <code>You (today)</code>, CDT's method of constructing counterfactuals destroys some of the structure of the scenario (namely, the connection between &Omega;'s choice and your algorithm).</p>\n<p>In fact, take a look at the graph for Newcomb's problem and compare it to the graph for the Mirror Token Trade from last post:</p>\n<p style=\"text-align: center\"><img src=\"http://mindingourway.com/content/images/2014/Sep/token-mirror.png\" alt=\"\" /></p>\n<p>These are the same graph, and CDT is making the same error. It's assuming that everything which is <em>causally disconnected</em> from it is <em>independent</em> of it.</p>\n<p>In both Newcomb's original problem and in the Mirror Token Trade, this assumption is violated. &Omega;'s choice is <em>logically</em> connected to your choice, even though your choice is causally disconnected from Omega's.</p>\n<p>In any scenario where there are logical non-causal connections between your action node and other nodes, CDT's method of counterfactual reasoning can fail.</p>\n<h2>The Solution</h2>\n<p>The solution, of course, is to respect the logical connection between &Omega;'s choice and your own. I would take one box, because in this game, &Omega; gives $1,000,000 to people who use the strategy \"take one box\" and $1,000 to people who use the strategy \"take two boxes\".</p>\n<p>There are numerous intuition pumps by which you can arrive at this solution. For example, you could notice that you would like to precommit now to take only one box, and then \"implement\" that precommitment (by becoming, now, the type of person who only takes one box).</p>\n<p>Alternatively, you can imagine that &Omega; gets its impeccable predictions by simulating people. Then, when you find yourself facing down &Omega;, you can't be sure whether you're in the simulation or reality.</p>\n<p>It doesn't matter which intuition pump you use. It matters <em>which choice you take</em>: and CDT takes the wrong one.</p>\n<p>We do, in fact, have alternative decision theories that perform well in Newcomb's problem (some better than others), but we'll get to that later. For now, we're going to keep exaimining CDT.</p>\n<h2>Why I care</h2>\n<p>Both the Mirror Token Trade and Newcomb's Problem share a similar structure: there is another agent in the environment that knows how you reason.</p>\n<p>We can generalize this to a whole <em>class</em> of problems, known as \"Newcomblike problems\".</p>\n<p>In the Mirror Token Trade, the other agent is a copy of you who acts the same way you do. If you ever find yourself playing a token trade against a copy of yourself, you had better trade your token, even if you're completely selfish.</p>\n<p>In Newcomb's original problem, &Omega; knows how you act, and uses this to decide whether or not to give you a million dollars. If you want the million, you had better take one box.</p>\n<p>These problems may seem like weird edge cases. After all, in the Mirror Token Trade we assume that you are perfectly deterministic and that you can be copied. And in Newcomb's problem, we presume the existence of a powerful alien capable of perfectly predicting your action.</p>\n<p>It's a chaotic world, and perfect prediction of a human is somewhere between \"pretty dang hard\" and \"downright impossible\".</p>\n<p>So fine: CDT fails systematically on Newcomblike problems. But is that so bad? We're pretty unlikely to meet &Omega; anytime soon. Failure on Newcomblike problems may be a flaw, but if CDT works everywhere <em>except</em> on crazy scenarios like Newcomb's problem then it's hardly a fatal flaw.</p>\n<p>But while these two example problems are simple scenarios where the other agents are \"perfect\" copies or \"perfect\" predictors, there are many more feasible Newcomblike scenarios.</p>\n<p><em>Any</em> scenario where another agent has knowledge about your decision algorithm (even if that knowledge is imperfect, even if they lack the capability to simulate you) is a Newcomblike problem.</p>\n<p>In fact, in the next post (seriously, I'm going to get to my original point soon, I promise) I'll argue that <em>Newcomblike problems are the norm</em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb28e": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wk7WmrN4FeNmyepXm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 23, "extendedScore": null, "score": 2.0207281615503935e-06, "legacy": true, "legacyId": "27241", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><em>This is&nbsp;<span style=\"color: #8a8a8b;\"><a href=\"http://mindingourway.com/intro-to-newcomblike-problems/\">crossposted</a></span>&nbsp;from&nbsp;<span style=\"color: #8a8a8b;\"><a href=\"http://mindingourway.com\">my new blog</a></span>, following up on my <a href=\"/lw/kz9/causal_decision_theory_is_unsatisfactory/\">previous post</a>. It introduces the original \"Newcomb's problem\" and discusses the motivation behind twoboxing and the reasons why CDT fails. content is probably review for most LessWrongers, later posts in the sequence may be of more interest.</em></p>\n<hr>\n<p><a href=\"/lw/kz9/causal_decision_theory_is_unsatisfactory/\">Last time</a> I introduced causal decision theory (CDT) and showed how it has unsatisfactory behavior on \"Newcomblike problems\". Today, we'll explore Newcomblike problems in a bit more depth, starting with William Newcomb's original problem.</p>\n<h2 id=\"The_Problem\">The Problem</h2>\n<p>Once upon a time there was a strange alien named \u03a9 who is very very good at predicting humans. There is this one game that \u03a9 likes to play with humans, and \u03a9 has played it thousands of times without ever making a mistake. The game works as follows:</p>\n<p>First, \u03a9 observes the human for a while and collects lots of information about the human. Then, \u03a9 makes a decision based on how \u03a9 predicts the human will react in the upcoming game. Finally, \u03a9 presents the human with two boxes.</p>\n<p>The first box is blue, transparent, and contains $1000. The second box is red and opaque.</p>\n<blockquote>\n<p>You may take either the red box alone, or both boxes,</p>\n</blockquote>\n<p>\u03a9 informs the human. (These are magical boxes where if you decide to take only the red one then the blue one, and the $1000 within, will disappear.)</p>\n<blockquote>\n<p>If I predicted that you would take only the red box, then I filled it with $1,000,000. Otherwise, I left it empty. I have already made my choice,</p>\n</blockquote>\n<p>\u03a9 concludes, before turning around and walking away.</p>\n<p>You may take either only the red box, or both boxes. (If you try something clever, like taking the red box while a friend takes a blue box, then the red box is filled with hornets. Lots and lots of hornets.) What do you do?</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"The_Dilemma\">The Dilemma</h2>\n<p>Should you take one box or two boxes?</p>\n<p>If you take only the red box, then you're <em>necessarily</em> leaving $1,000 on the table.</p>\n<p>But if you take both boxes, then \u03a9 predicted that, and you miss out at a chance of $1,000,000.</p>\n<h2 id=\"Recap\">Recap</h2>\n<p>Now, as you'll remember, we're discussing <em>decision theories</em>, algorithms that prescribe actions in games like these. Our motivation for studying decision theories is manyfold. For one thing, people who <em>don't</em> have tools for making good decisions can often become confused (remember the people who <a href=\"http://www.thedailybeast.com/articles/2013/07/12/your-future-is-in-the-palm-of-your-surgeon-s-hand.html\">got palm surgery to change their fortunes</a>).</p>\n<p>This is not just a problem for irrational or undereducated people: it's easy to trust yourself to make the right choices most of the time, because human heuristics and intuitions are usually pretty good. But what happens in the strange edge-case scenarios? What do you do when you encounter problems where your intuitions conflict? In these cases, it's important to know <em>what it means</em> to make a good choice.</p>\n<p><em>My</em> motivation for studying decision theory is that in order to construct an artificial intelligence you need a pretty dang good understanding of what sort of decision algorithms perform well.</p>\n<p>Last post, we explored the standard decision theory used by modern philosophers to encode rational decision making. I'm <em>eventually</em> going to use this series of posts to explain why our current knowledge of decision theory (and CDT in particular) are completely inadequate for use in any sufficiently intelligent self-modifying agent.</p>\n<p>But before we go there, let's see how causal decision theory reacts to Newcomb's problem.</p>\n<h2 id=\"The_Choice\">The Choice</h2>\n<p>Let's analyze this problem using causal decision theory from yesterday. Roughly, the causal decision theorist reasons as follows:</p>\n<blockquote>\n<p>\u03a9 offers me the boxes after making its decision. Now, either \u03a9 has filled the red box or \u03a9 has not filled the red box. The decision has <em>already been made</em>. If \u03a9 filled the red box, then I had better take both boxes (and get a thousand and a million dollars). If \u03a9 didn't fill the red box, then I had better take both boxes so that I at least get a thousand dollars. <em>No matter what</em> \u03a9 chose, I had better take both boxes.</p>\n</blockquote>\n<p>And so, someone reasoning using causal decision theory takes both boxes (and, because \u03a9 is a very good predictor, they walk away with $1000).</p>\n<p>Let's walk through that reasoning in slow-mo, using causal graphs. The causal graph for this problem looks like this:</p>\n<p style=\"text-align: center\"><img src=\"http://mindingourway.com/content/images/2014/Sep/newcomb-problem.png\" alt=\"\"></p>\n<p>With the nodes defined as follows:</p>\n<ul>\n<li><code>You (yesterday)</code> is the algorithm implementing you yesterday. In this simplified setting, we assuem that its value determines the contents of <code>You (today)</code>.</li>\n<li><code>\u03a9</code> is a function that observes you yesterday and decides whether to put $1,000,000 into the red box. Its value is either <em>filled</em> or <em>empty</em>.</li>\n<li><code>You (today)</code> is your decision algorithm. It must output either <em>onebox</em> or <em>twobox</em>.</li>\n<li><code>$</code> is $1,000,000 if <code>\u03a9</code>=<em>filled</em> plus $1,000 if <code>You (today)</code>=<em>twobox</em>.</li>\n</ul>\n<p>We must decide whether to output <em>onebox</em> (take only the red box) or <em>twobox</em> (take both boxes). Given some expecation p that <code>\u03a9</code>=<em>filled</em>, causal decision theory reasons as follows:</p>\n<ol>\n<li>The decision node is <code>You (today)</code>.</li>\n<li>The available actions are <em>onebox</em> and <em>twobox</em>.</li>\n<li>The utility node is <code>$</code>.</li>\n<li>Set <code>You (today)</code>=<code>const onebox</code> \n<ul>\n<li><code>$</code> = 1,000,000p</li>\n</ul>\n</li>\n<li>Set <code>You (today)</code>=<code>const twobox</code> \n<ul>\n<li><code>$</code> = 1,000,000p + 1,000</li>\n</ul>\n</li>\n</ol>\n<p>Thus, <em>no matter what probability p is</em>, CDT takes both boxes, because the action <em>twobox</em> results in an extra $1000.</p>\n<h2 id=\"The_Excuse\">The Excuse</h2>\n<p>This is, of course, the wrong answer.</p>\n<p>Because \u03a9 is a really good predictor, and because \u03a9 only gives a million dollars to people who take one box, if you want a million dollars then you had better only take one box.</p>\n<p>Where did CDT go wrong?</p>\n<p>Causal decision theorists sometimes argue that <em>nothing</em> went wrong. It is \"rational\" to take both boxes, because otherwise you're leaving $1,000 on the table. Reasoning about how you have to take one box so that that box will be filled is nonsense, because by the time you're making your decision, \u03a9 has already left. (This argument can be found at <a href=\"http://books.google.com/books?id=LYTMhPzCUxYC&amp;q=rachel#v=onepage&amp;q&amp;f=false\">Foundations of Causal Decision Theory, p152</a>.)</p>\n<p>Of course, these people will agree that the causal decision theorists walk away with less money. But they'll tell you that this is not their fault: they are making the \"rational\" choice, and \u03a9 has decided to punish people who are acting \"rationally\".</p>\n<h2 id=\"The_Counter\">The Counter</h2>\n<p>There is some merit to this complaint. <em>No matter how you make decisions</em>, there is at least one decision problem where you do poorly and everybody else does well. To illustrate, consider the following game: every player submits a computer program that has to choose whether to press the green button or the blue button (by outputting either <code>green</code> or <code>blue</code>). Then I check which button your program is going to press. Then I give $10 to everyone whose program presses the opposite button.</p>\n<p>Clearly, many people can win $10 in this game. Also clearly, you cannot. In this game, no matter how you choose, you lose. I'm punishing <em>your decision algorithm</em> specifically.</p>\n<p>I'm sympathetic to the claim that there are games where the host punishes you unfairly. However, Newcomb's problem is <em>not</em> unfair in this sense.</p>\n<p>\u03a9 is not punishing <em>people who act like you</em>, \u03a9 is punishing <em>people who take two boxes</em>. (Well, I mean, \u03a9 is giving $1,000 to those people, but \u03a9 is withholding $1,000,000 that it gives to oneboxers, so I'll keep calling it \"punishment\").</p>\n<p>You <em>can't</em> win at the bluegreen game above. You <em>can</em> win on Newcomb's problem. All you have to do is <em>only take one box</em>.</p>\n<p>Causal decision theorists claim that it's not <em>rational</em> to take one box, because that leaves $1,000 on the table even though your choice can no longer affect the outcome. They claim that \u03a9 is punishing people who act reasonably. They can't just <em>decide</em> to stop being reasonable just because \u03a9 is rewarding unreasonable behavior in this case.</p>\n<p>To which I say: fine. But pretend I wasn't motivated by adherence to some archaic definition of \"reasonableness\" which clearly fails systematically in this scenario, but that I was <em>instead</em> motivated by a desire to succeed. <em>Then</em> how should I make decisions? Because in these scenarios, I clearly should not use causal decision theory.</p>\n<h2 id=\"The_Failure\">The Failure</h2>\n<p>In fact, I can go further. I can <em>point</em> to the place where CDT goes wrong. CDT goes wrong when it reasons about the outcome of its action \"no matter what the probability p that \u03a9 fills the box\".</p>\n<p><em>\u03a9's choice to fill the box depends upon your decision algorithm.</em></p>\n<p>If your algorithm chooses to take one box, then the probability that <code>\u03a9</code>=<em>filled</em> is high. If your algorithm chooses to take two boxes, then the probability that <code>\u03a9</code>=<em>filled</em> is low.</p>\n<p>CDT's reasoning neglects this fact, because <code>\u03a9</code> is causally disconnected from <code>You (today)</code>. CDT reasons that its choice can no longer affect \u03a9, but then <em>mistakenly</em> assumes that this means the probability of <code>\u03a9</code>=<em>filled</em> is independent from <code>You (today)</code>.</p>\n<p>\u03a9 makes its decision based on knowledge of what <em>the algorithm</em> inside <code>You (today)</code> looks like. When CDT reasons about what would happen if it took one box, it considers what the world would look like if (counterfactually) <code>You (today)</code> was filled with an algorithm that, <em>instead of being CDT</em>, always returned <code>onebox</code>. CDT changes the contents of <code>You (today)</code> and nothing else, and sees what would happen. But this is a bad counterfactual! If, instead of implementing CDT, you implemented an algorithm that always took one box, <em>then \u03a9 would act differently</em>.</p>\n<p>CDT assumes it can change <code>You (today)</code> without affecting <code>\u03a9</code>. But because \u03a9's choice depends upon the <em>contents</em> of <code>You (today)</code>, CDT's method of constructing counterfactuals destroys some of the structure of the scenario (namely, the connection between \u03a9's choice and your algorithm).</p>\n<p>In fact, take a look at the graph for Newcomb's problem and compare it to the graph for the Mirror Token Trade from last post:</p>\n<p style=\"text-align: center\"><img src=\"http://mindingourway.com/content/images/2014/Sep/token-mirror.png\" alt=\"\"></p>\n<p>These are the same graph, and CDT is making the same error. It's assuming that everything which is <em>causally disconnected</em> from it is <em>independent</em> of it.</p>\n<p>In both Newcomb's original problem and in the Mirror Token Trade, this assumption is violated. \u03a9's choice is <em>logically</em> connected to your choice, even though your choice is causally disconnected from Omega's.</p>\n<p>In any scenario where there are logical non-causal connections between your action node and other nodes, CDT's method of counterfactual reasoning can fail.</p>\n<h2 id=\"The_Solution\">The Solution</h2>\n<p>The solution, of course, is to respect the logical connection between \u03a9's choice and your own. I would take one box, because in this game, \u03a9 gives $1,000,000 to people who use the strategy \"take one box\" and $1,000 to people who use the strategy \"take two boxes\".</p>\n<p>There are numerous intuition pumps by which you can arrive at this solution. For example, you could notice that you would like to precommit now to take only one box, and then \"implement\" that precommitment (by becoming, now, the type of person who only takes one box).</p>\n<p>Alternatively, you can imagine that \u03a9 gets its impeccable predictions by simulating people. Then, when you find yourself facing down \u03a9, you can't be sure whether you're in the simulation or reality.</p>\n<p>It doesn't matter which intuition pump you use. It matters <em>which choice you take</em>: and CDT takes the wrong one.</p>\n<p>We do, in fact, have alternative decision theories that perform well in Newcomb's problem (some better than others), but we'll get to that later. For now, we're going to keep exaimining CDT.</p>\n<h2 id=\"Why_I_care\">Why I care</h2>\n<p>Both the Mirror Token Trade and Newcomb's Problem share a similar structure: there is another agent in the environment that knows how you reason.</p>\n<p>We can generalize this to a whole <em>class</em> of problems, known as \"Newcomblike problems\".</p>\n<p>In the Mirror Token Trade, the other agent is a copy of you who acts the same way you do. If you ever find yourself playing a token trade against a copy of yourself, you had better trade your token, even if you're completely selfish.</p>\n<p>In Newcomb's original problem, \u03a9 knows how you act, and uses this to decide whether or not to give you a million dollars. If you want the million, you had better take one box.</p>\n<p>These problems may seem like weird edge cases. After all, in the Mirror Token Trade we assume that you are perfectly deterministic and that you can be copied. And in Newcomb's problem, we presume the existence of a powerful alien capable of perfectly predicting your action.</p>\n<p>It's a chaotic world, and perfect prediction of a human is somewhere between \"pretty dang hard\" and \"downright impossible\".</p>\n<p>So fine: CDT fails systematically on Newcomblike problems. But is that so bad? We're pretty unlikely to meet \u03a9 anytime soon. Failure on Newcomblike problems may be a flaw, but if CDT works everywhere <em>except</em> on crazy scenarios like Newcomb's problem then it's hardly a fatal flaw.</p>\n<p>But while these two example problems are simple scenarios where the other agents are \"perfect\" copies or \"perfect\" predictors, there are many more feasible Newcomblike scenarios.</p>\n<p><em>Any</em> scenario where another agent has knowledge about your decision algorithm (even if that knowledge is imperfect, even if they lack the capability to simulate you) is a Newcomblike problem.</p>\n<p>In fact, in the next post (seriously, I'm going to get to my original point soon, I promise) I'll argue that <em>Newcomblike problems are the norm</em>.</p>", "sections": [{"title": "The Problem", "anchor": "The_Problem", "level": 1}, {"title": "The Dilemma", "anchor": "The_Dilemma", "level": 1}, {"title": "Recap", "anchor": "Recap", "level": 1}, {"title": "The Choice", "anchor": "The_Choice", "level": 1}, {"title": "The Excuse", "anchor": "The_Excuse", "level": 1}, {"title": "The Counter", "anchor": "The_Counter", "level": 1}, {"title": "The Failure", "anchor": "The_Failure", "level": 1}, {"title": "The Solution", "anchor": "The_Solution", "level": 1}, {"title": "Why I care", "anchor": "Why_I_care", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5oBw9T5y5DLpJZL63"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-20T20:00:16.749Z", "modifiedAt": null, "url": null, "title": "Meetup : Stanford THINK starts weekly meetups this Sunday", "slug": "meetup-stanford-think-starts-weekly-meetups-this-sunday", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KPier", "createdAt": "2011-05-29T20:37:16.788Z", "isAdmin": false, "displayName": "KPier"}, "userId": "LNNvCDMS2RvA4jZAG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N53zovMd3QyxbFKwX/meetup-stanford-think-starts-weekly-meetups-this-sunday", "pageUrlRelative": "/posts/N53zovMd3QyxbFKwX/meetup-stanford-think-starts-weekly-meetups-this-sunday", "linkUrl": "https://www.lesswrong.com/posts/N53zovMd3QyxbFKwX/meetup-stanford-think-starts-weekly-meetups-this-sunday", "postedAtFormatted": "Saturday, September 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Stanford%20THINK%20starts%20weekly%20meetups%20this%20Sunday&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Stanford%20THINK%20starts%20weekly%20meetups%20this%20Sunday%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN53zovMd3QyxbFKwX%2Fmeetup-stanford-think-starts-weekly-meetups-this-sunday%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Stanford%20THINK%20starts%20weekly%20meetups%20this%20Sunday%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN53zovMd3QyxbFKwX%2Fmeetup-stanford-think-starts-weekly-meetups-this-sunday", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN53zovMd3QyxbFKwX%2Fmeetup-stanford-think-starts-weekly-meetups-this-sunday", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14p'>Stanford THINK starts weekly meetups this Sunday</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 September 2014 04:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Old Union rm 121, Stanford University, Palo Alto</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>THINK is an effective altruism/rationality group that meets during the school year at Stanford. This is our third year. Most attendees are Stanford students, but anyone is welcome.</p>\n\n<p>This week's discussion topic is rationality habits (from CFAR's checklist); we usually have a few games and structures activities and then allow them to dissolve into unstructured discussion.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14p'>Stanford THINK starts weekly meetups this Sunday</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N53zovMd3QyxbFKwX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 2.020873789193259e-06, "legacy": true, "legacyId": "27242", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Stanford_THINK_starts_weekly_meetups_this_Sunday\">Discussion article for the meetup : <a href=\"/meetups/14p\">Stanford THINK starts weekly meetups this Sunday</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 September 2014 04:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Old Union rm 121, Stanford University, Palo Alto</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>THINK is an effective altruism/rationality group that meets during the school year at Stanford. This is our third year. Most attendees are Stanford students, but anyone is welcome.</p>\n\n<p>This week's discussion topic is rationality habits (from CFAR's checklist); we usually have a few games and structures activities and then allow them to dissolve into unstructured discussion.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Stanford_THINK_starts_weekly_meetups_this_Sunday1\">Discussion article for the meetup : <a href=\"/meetups/14p\">Stanford THINK starts weekly meetups this Sunday</a></h2>", "sections": [{"title": "Discussion article for the meetup : Stanford THINK starts weekly meetups this Sunday", "anchor": "Discussion_article_for_the_meetup___Stanford_THINK_starts_weekly_meetups_this_Sunday", "level": 1}, {"title": "Discussion article for the meetup : Stanford THINK starts weekly meetups this Sunday", "anchor": "Discussion_article_for_the_meetup___Stanford_THINK_starts_weekly_meetups_this_Sunday1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-20T20:25:36.802Z", "modifiedAt": null, "url": null, "title": "In order to greatly reduce X-risk, design self-replicating spacecraft without AGI", "slug": "in-order-to-greatly-reduce-x-risk-design-self-replicating", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:34.891Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PN8of23agW7QdsHtM/in-order-to-greatly-reduce-x-risk-design-self-replicating", "pageUrlRelative": "/posts/PN8of23agW7QdsHtM/in-order-to-greatly-reduce-x-risk-design-self-replicating", "linkUrl": "https://www.lesswrong.com/posts/PN8of23agW7QdsHtM/in-order-to-greatly-reduce-x-risk-design-self-replicating", "postedAtFormatted": "Saturday, September 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20order%20to%20greatly%20reduce%20X-risk%2C%20design%20self-replicating%20spacecraft%20without%20AGI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20order%20to%20greatly%20reduce%20X-risk%2C%20design%20self-replicating%20spacecraft%20without%20AGI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPN8of23agW7QdsHtM%2Fin-order-to-greatly-reduce-x-risk-design-self-replicating%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20order%20to%20greatly%20reduce%20X-risk%2C%20design%20self-replicating%20spacecraft%20without%20AGI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPN8of23agW7QdsHtM%2Fin-order-to-greatly-reduce-x-risk-design-self-replicating", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPN8of23agW7QdsHtM%2Fin-order-to-greatly-reduce-x-risk-design-self-replicating", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1770, "htmlBody": "<p><strong>tl/dr:</strong> If we'd build a working self-replicating spacecraft, that'd prove we're past the Great Filter. Therefore, certainty we can do that would eliminate much existential risk. It is a potentially highly visible project that gives publicity to reasons not to include AGI. Therefore, serious design work on a self-replicating spacecraft should have a high priority.</p>\n<p>I'm assuming you've read <a href=\"/lw/kvk/the_octopus_the_dolphin_and_us_a_great_filter_tale/\">Stuart_Armstrong's excellent recent article</a> on the Great Filter. In the discussion thread for that, <a href=\"/lw/kvk/the_octopus_the_dolphin_and_us_a_great_filter_tale/bau1\">RussellThor observed</a>:</p>\n<blockquote>\n<p>if we make a simple replicator and have it successfully reach another solar system (with possibly habitable planets) then that would seem to demonstrate that the filter is behind us.</p>\n</blockquote>\n<p>If that is obvious to you, skip to the next subheading.</p>\n<p>The evolution from intelligent spacefaring species to producer of <a href=\"http://en.wikipedia.org/wiki/Self-replicating_spacecraft\">self-replicating spacecraft</a> (henceforth SRS, used in the plural) is inevitable, if SRS are possible. This is simply because the matter and negentropy available in the wider universe is a staggeringly vast resource of staggering value. Even species who are unlikely to ever visit and colonize other stars in the form that evolution gave them (this includes us) can make use of these resources. For example, if we could build on (or out of) empty planets supercomputers that receive computation tasks by laser beam and output results the same way, we would be economically compelled to do so simply because those supercomputers could handle computational tasks that no computer on Earth could complete in less than the time it takes that laser beam to travel forth and back. That supercomputer would not need to run even a weak AI to be worth more than the cost of sending the probe that builds it.</p>\n<p>Without a doubt there are countless more possible uses for these, shall we say, exoresources. If <a href=\"http://en.wikipedia.org/wiki/Dyson_sphere\">Dyson bubbles</a> or <a href=\"http://en.wikipedia.org/wiki/Mind_uploading\">mind uploads</a> or multistellar <a href=\"http://en.wikipedia.org/wiki/Astronomical_interferometer#Labeyrie.27s_hypertelescope\">hypertelescopes</a> or <a href=\"http://en.wikipedia.org/wiki/Terraforming\">terraforming</a> are possible, each of these alone create another huge incentive to build SRS. Even mere self-replicating refineries that break up planets into more readily accessible resources for future generations to draw from would be an excellent investment. But the obvious existence of this supercomputer incentive is already reason enough to do it.</p>\n<p>All the Great Filter debate boils down to the question of how improbable our existence really is. If we're probable, many intelligent species capable of very basic space travel should exist. If we're not, they shouldn't. We know there doesn't appear to be any species inside a large fraction of our light cone so capable of space travel it has sent out SRS. So the only way we could be probable is if there's a Great Filter ahead of us, stopping us (and everyone else capable of basic space travel) from becoming the kind of species that sends out SRS. If we became such a species, we'd know we're past the Filter and while we still wouldn't know how improbable which of the conditions that allowed for our existence was, we'd know that when putting them all together, they multiply into some very small probability of our existence, and a very small probability of any comparable species existing in a large section of our light cone.</p>\n<p>LW users generally seem to think SRS are doable and that means we're quite improbable, i.e. the Filter is behind us. But lots of people are less sure, and even more people haven't thought about it. The original formulation of the <a href=\"http://en.wikipedia.org/wiki/Drake_equation\">Drake equation</a> included a lifespan of civilizations partly to account for the intuition that a Great Filter type event could be coming in the future. We could be more sure than we are now, and make a lot of people much more sure than they are now, about our position in reference to that Filter. And that'd have some interesting consequences.</p>\n<h2>How knowing we're past the Great Filter reduces X-risk</h2>\n<p>The single largest X-risk we've successfully eliminated is the impact of an asteroid large enough to destroy us entirely. And we didn't do that by moving any asteroids; we simply mapped all of the big ones. We now know there's no asteroid that is both large enough to kill us off and coming soon enough that we can't do anything about it. Hindsight bias tells us this was never a big threat - but look ten years back and you'll find The Big Asteroid on every list of global catastrophic risks, usually near the top. We eliminated that risk simply by observation and deduction, <em>by finding out it did not exist rather than removing it</em>.</p>\n<p>Obviously a working SRS that gives humanity outposts in other solar systems would reduce most types of X-risk. But even just knowing we could build one should decrease confidence in the ability of X-risks to take us out entirely. After all, if <a href=\"http://www.nickbostrom.com/extraterrestrial.pdf\">as Bostrom argues</a>, the possibility that the Filter is ahead of is increases the probability of <em>any </em>X-risk, the knowledge that it is not ahead of us has to be evidence <em>against all of them</em> except those that could kill a <a href=\"http://en.wikipedia.org/wiki/Kardashev_scale#Energy_development\">Type 3 civilization</a>. And if, as Bostrom says in that same paper, finding life elsewhere that is closer to our stage of development is  worse news than finding life further from it, to increase the distance  between us and either type of life decreases the badness of the  existence of either.</p>\n<p>Of course we'd only be certain if we had actually built and sent such a spacecraft. But in order to gain confidence we're past the filter, and to gain a greater lead to life possibly discovered elsewhere, a design that is agreed to be workable would go most of the way. If it is clear enough that someone with enough capital could claim incredible gains by doing that, we can be sure enough someone eventually (e.g. Elon Musk after SpaceX's IPO around 2035) will do that, giving high confidence we've passed the filter.</p>\n<p>I'm not sure what would happen if we could say (with more confidence than currently) that we're probably the species that's furthest ahead at least in this galaxy. But if that's true, I don't just want to believe it, I want everyone else to believe it too, because it seems like a fairly important fact. And an SRS design would help do that.</p>\n<p>We'd be more sure we're becoming a Type 3 civilization, so we should then begin to think about what type of risk could kill that, and UFAI would probably be more pronounced on that list than it is on the current geocentric ones.</p>\n<p>What if we find out SRS are impossible at our pre-AGI level of technology? We still wouldn't know if an AI could do it. But even knowing our own inability would be very useful information, especially about the dangerousness of vatrious types of X-risk.</p>\n<h2>How easily this X-risk reducing knowledge can be attained<br /></h2>\n<p><a rel=\"nofollow\" href=\"http://www.sciencedirect.com/science/article/pii/S0094576513001148\">Armstrong and Sandberg</a> claim the feasibility of self-replicating spacecraft has been a settled matter since the Freitag design of 1980. But <a rel=\"nofollow\" href=\"http://www.rfreitas.com/Astro/ReproJBISJuly1980.htm\">that paper</a>, while impressively detailed and a great read, glosses over the exact computing abilities such a system would need, does not mention hardening against interstellar radiation, assumes fusion drives and probably has a bunch of other problems that I'm not qualified to discover. I haven't looked at all the papers that cite it (yet), but the ones I've seen seem to agree self-replicating spacecraft are plausible. <a href=\"http://intelligence.org/2014/03/02/anders-sandberg/\">Sandberg has some good research questions</a> that I agree need to be answered, but never seems to waver from his assumption that SRS are basically possible, although he's aware of the gaps in knowledge that preclude such an assumption from being safe.</p>\n<p>There are certainly some questions that I'm not sure we can answer. For example:</p>\n<ol>\n<li>Can we build fission-powered spacecraft (let alone more speculative designs) that will survive the interstellar environment for decades or centuries?</li>\n<li>How can we be certain to avoid mutations that grow outside of our control, and eventually devour Earth?</li>\n<li>Can communication between SRS and colonies, especially software updates, be made secure enough?</li>\n<li>Can a finite number of probe designs (to be included on any of them) provide a vehicle for every type of journey we'd want the SRS network to make?</li>\n<li>Can a fiinite number of colony designs provide a blueprint for every source of matter and negentropy we'd want to develop?</li>\n<li>What is the ethical way to treat any life the SRS network might encounter?</li>\n</ol>\n<p>But all of these except for the last one, and Sandberg's questions, are engineering questions and those tend to be answerable. If not, remember, we don't need to have a functioning SRS to manage X-risk, any reduction of uncertainty around their feasibility already helps. And again, <em>the only design I could find that gives any detail at all is from a single guy writing in 1980</em>. If we merely do better than he did (find or rule out a few of the remaining obstacles), we already help ascertain our level of X-risk. Compare the asteroid detection analogy: We couldn't be certain that we wouldn't be hit by an asteroid until we looked at all of them, but getting started with part of the search space was a very valuable thing to do anyway.</p>\n<p>Freitag and others use to assume SRS should be run by some type of AGI. Sandberg says SRS without AGI, with what he calls \"lower order intelligence\", \"might be adequate\". I disagree with both assessments, and with Sandberg's giving this question less priority than, say, study of mass drivers. Given the issues of AGI safety, a probe that works without AGI should be distinctly preferable. And (unlike an intelligent one) its computational components can be designed right now, down to the decision tree it should follow. While at it, and in order to use the publicity such a project might generate, give an argument for this design choice that highlights the AGI safety issues. A scenario where a self-replicating computer planet out there decides for itself should serve to highlight the dangers of AGI far more viscerally than conventional \"self-aware desktop box\" scenarios.</p>\n<p>If we're not looking for an optimal design, but the bare minimum necessary to know we're past the filter, that gives us somewhat relaxed design constraints. This probe wouldn't necessarily need to travel at a significant fraction of light speed, and its first generation wouldn't need to be capable of journeys beyond, say, five parsec. It does have to be capable of interstellar travel, and of progressing to intergalactic travel at some point, say when it finds all nearby star systems to contain copies of itself. A non-interstellar probe fit to begin the self-replication process on a planet like Jupiter, refining resources and building launch facilities there, would be a necessary first step.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PN8of23agW7QdsHtM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 3, "extendedScore": null, "score": 2.020919994570207e-06, "legacy": true, "legacyId": "27185", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>tl/dr:</strong> If we'd build a working self-replicating spacecraft, that'd prove we're past the Great Filter. Therefore, certainty we can do that would eliminate much existential risk. It is a potentially highly visible project that gives publicity to reasons not to include AGI. Therefore, serious design work on a self-replicating spacecraft should have a high priority.</p>\n<p>I'm assuming you've read <a href=\"/lw/kvk/the_octopus_the_dolphin_and_us_a_great_filter_tale/\">Stuart_Armstrong's excellent recent article</a> on the Great Filter. In the discussion thread for that, <a href=\"/lw/kvk/the_octopus_the_dolphin_and_us_a_great_filter_tale/bau1\">RussellThor observed</a>:</p>\n<blockquote>\n<p>if we make a simple replicator and have it successfully reach another solar system (with possibly habitable planets) then that would seem to demonstrate that the filter is behind us.</p>\n</blockquote>\n<p>If that is obvious to you, skip to the next subheading.</p>\n<p>The evolution from intelligent spacefaring species to producer of <a href=\"http://en.wikipedia.org/wiki/Self-replicating_spacecraft\">self-replicating spacecraft</a> (henceforth SRS, used in the plural) is inevitable, if SRS are possible. This is simply because the matter and negentropy available in the wider universe is a staggeringly vast resource of staggering value. Even species who are unlikely to ever visit and colonize other stars in the form that evolution gave them (this includes us) can make use of these resources. For example, if we could build on (or out of) empty planets supercomputers that receive computation tasks by laser beam and output results the same way, we would be economically compelled to do so simply because those supercomputers could handle computational tasks that no computer on Earth could complete in less than the time it takes that laser beam to travel forth and back. That supercomputer would not need to run even a weak AI to be worth more than the cost of sending the probe that builds it.</p>\n<p>Without a doubt there are countless more possible uses for these, shall we say, exoresources. If <a href=\"http://en.wikipedia.org/wiki/Dyson_sphere\">Dyson bubbles</a> or <a href=\"http://en.wikipedia.org/wiki/Mind_uploading\">mind uploads</a> or multistellar <a href=\"http://en.wikipedia.org/wiki/Astronomical_interferometer#Labeyrie.27s_hypertelescope\">hypertelescopes</a> or <a href=\"http://en.wikipedia.org/wiki/Terraforming\">terraforming</a> are possible, each of these alone create another huge incentive to build SRS. Even mere self-replicating refineries that break up planets into more readily accessible resources for future generations to draw from would be an excellent investment. But the obvious existence of this supercomputer incentive is already reason enough to do it.</p>\n<p>All the Great Filter debate boils down to the question of how improbable our existence really is. If we're probable, many intelligent species capable of very basic space travel should exist. If we're not, they shouldn't. We know there doesn't appear to be any species inside a large fraction of our light cone so capable of space travel it has sent out SRS. So the only way we could be probable is if there's a Great Filter ahead of us, stopping us (and everyone else capable of basic space travel) from becoming the kind of species that sends out SRS. If we became such a species, we'd know we're past the Filter and while we still wouldn't know how improbable which of the conditions that allowed for our existence was, we'd know that when putting them all together, they multiply into some very small probability of our existence, and a very small probability of any comparable species existing in a large section of our light cone.</p>\n<p>LW users generally seem to think SRS are doable and that means we're quite improbable, i.e. the Filter is behind us. But lots of people are less sure, and even more people haven't thought about it. The original formulation of the <a href=\"http://en.wikipedia.org/wiki/Drake_equation\">Drake equation</a> included a lifespan of civilizations partly to account for the intuition that a Great Filter type event could be coming in the future. We could be more sure than we are now, and make a lot of people much more sure than they are now, about our position in reference to that Filter. And that'd have some interesting consequences.</p>\n<h2 id=\"How_knowing_we_re_past_the_Great_Filter_reduces_X_risk\">How knowing we're past the Great Filter reduces X-risk</h2>\n<p>The single largest X-risk we've successfully eliminated is the impact of an asteroid large enough to destroy us entirely. And we didn't do that by moving any asteroids; we simply mapped all of the big ones. We now know there's no asteroid that is both large enough to kill us off and coming soon enough that we can't do anything about it. Hindsight bias tells us this was never a big threat - but look ten years back and you'll find The Big Asteroid on every list of global catastrophic risks, usually near the top. We eliminated that risk simply by observation and deduction, <em>by finding out it did not exist rather than removing it</em>.</p>\n<p>Obviously a working SRS that gives humanity outposts in other solar systems would reduce most types of X-risk. But even just knowing we could build one should decrease confidence in the ability of X-risks to take us out entirely. After all, if <a href=\"http://www.nickbostrom.com/extraterrestrial.pdf\">as Bostrom argues</a>, the possibility that the Filter is ahead of is increases the probability of <em>any </em>X-risk, the knowledge that it is not ahead of us has to be evidence <em>against all of them</em> except those that could kill a <a href=\"http://en.wikipedia.org/wiki/Kardashev_scale#Energy_development\">Type 3 civilization</a>. And if, as Bostrom says in that same paper, finding life elsewhere that is closer to our stage of development is  worse news than finding life further from it, to increase the distance  between us and either type of life decreases the badness of the  existence of either.</p>\n<p>Of course we'd only be certain if we had actually built and sent such a spacecraft. But in order to gain confidence we're past the filter, and to gain a greater lead to life possibly discovered elsewhere, a design that is agreed to be workable would go most of the way. If it is clear enough that someone with enough capital could claim incredible gains by doing that, we can be sure enough someone eventually (e.g. Elon Musk after SpaceX's IPO around 2035) will do that, giving high confidence we've passed the filter.</p>\n<p>I'm not sure what would happen if we could say (with more confidence than currently) that we're probably the species that's furthest ahead at least in this galaxy. But if that's true, I don't just want to believe it, I want everyone else to believe it too, because it seems like a fairly important fact. And an SRS design would help do that.</p>\n<p>We'd be more sure we're becoming a Type 3 civilization, so we should then begin to think about what type of risk could kill that, and UFAI would probably be more pronounced on that list than it is on the current geocentric ones.</p>\n<p>What if we find out SRS are impossible at our pre-AGI level of technology? We still wouldn't know if an AI could do it. But even knowing our own inability would be very useful information, especially about the dangerousness of vatrious types of X-risk.</p>\n<h2 id=\"How_easily_this_X_risk_reducing_knowledge_can_be_attained\">How easily this X-risk reducing knowledge can be attained<br></h2>\n<p><a rel=\"nofollow\" href=\"http://www.sciencedirect.com/science/article/pii/S0094576513001148\">Armstrong and Sandberg</a> claim the feasibility of self-replicating spacecraft has been a settled matter since the Freitag design of 1980. But <a rel=\"nofollow\" href=\"http://www.rfreitas.com/Astro/ReproJBISJuly1980.htm\">that paper</a>, while impressively detailed and a great read, glosses over the exact computing abilities such a system would need, does not mention hardening against interstellar radiation, assumes fusion drives and probably has a bunch of other problems that I'm not qualified to discover. I haven't looked at all the papers that cite it (yet), but the ones I've seen seem to agree self-replicating spacecraft are plausible. <a href=\"http://intelligence.org/2014/03/02/anders-sandberg/\">Sandberg has some good research questions</a> that I agree need to be answered, but never seems to waver from his assumption that SRS are basically possible, although he's aware of the gaps in knowledge that preclude such an assumption from being safe.</p>\n<p>There are certainly some questions that I'm not sure we can answer. For example:</p>\n<ol>\n<li>Can we build fission-powered spacecraft (let alone more speculative designs) that will survive the interstellar environment for decades or centuries?</li>\n<li>How can we be certain to avoid mutations that grow outside of our control, and eventually devour Earth?</li>\n<li>Can communication between SRS and colonies, especially software updates, be made secure enough?</li>\n<li>Can a finite number of probe designs (to be included on any of them) provide a vehicle for every type of journey we'd want the SRS network to make?</li>\n<li>Can a fiinite number of colony designs provide a blueprint for every source of matter and negentropy we'd want to develop?</li>\n<li>What is the ethical way to treat any life the SRS network might encounter?</li>\n</ol>\n<p>But all of these except for the last one, and Sandberg's questions, are engineering questions and those tend to be answerable. If not, remember, we don't need to have a functioning SRS to manage X-risk, any reduction of uncertainty around their feasibility already helps. And again, <em>the only design I could find that gives any detail at all is from a single guy writing in 1980</em>. If we merely do better than he did (find or rule out a few of the remaining obstacles), we already help ascertain our level of X-risk. Compare the asteroid detection analogy: We couldn't be certain that we wouldn't be hit by an asteroid until we looked at all of them, but getting started with part of the search space was a very valuable thing to do anyway.</p>\n<p>Freitag and others use to assume SRS should be run by some type of AGI. Sandberg says SRS without AGI, with what he calls \"lower order intelligence\", \"might be adequate\". I disagree with both assessments, and with Sandberg's giving this question less priority than, say, study of mass drivers. Given the issues of AGI safety, a probe that works without AGI should be distinctly preferable. And (unlike an intelligent one) its computational components can be designed right now, down to the decision tree it should follow. While at it, and in order to use the publicity such a project might generate, give an argument for this design choice that highlights the AGI safety issues. A scenario where a self-replicating computer planet out there decides for itself should serve to highlight the dangers of AGI far more viscerally than conventional \"self-aware desktop box\" scenarios.</p>\n<p>If we're not looking for an optimal design, but the bare minimum necessary to know we're past the filter, that gives us somewhat relaxed design constraints. This probe wouldn't necessarily need to travel at a significant fraction of light speed, and its first generation wouldn't need to be capable of journeys beyond, say, five parsec. It does have to be capable of interstellar travel, and of progressing to intergalactic travel at some point, say when it finds all nearby star systems to contain copies of itself. A non-interstellar probe fit to begin the self-replication process on a planet like Jupiter, refining resources and building launch facilities there, would be a necessary first step.</p>", "sections": [{"title": "How knowing we're past the Great Filter reduces X-risk", "anchor": "How_knowing_we_re_past_the_Great_Filter_reduces_X_risk", "level": 1}, {"title": "How easily this X-risk reducing knowledge can be attained", "anchor": "How_easily_this_X_risk_reducing_knowledge_can_be_attained", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "37 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GMqZ2ofMnxwhoa7fD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-21T11:50:44.225Z", "modifiedAt": null, "url": null, "title": "Meetup : Copenhagen September Social Meetup - Botanisk Have", "slug": "meetup-copenhagen-september-social-meetup-botanisk-have", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:38.727Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ruby", "createdAt": "2014-04-03T03:38:23.914Z", "isAdmin": true, "displayName": "Ruby"}, "userId": "qgdGA4ZEyW7zNdK84", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S3CxRmF2SD7FtGJJa/meetup-copenhagen-september-social-meetup-botanisk-have", "pageUrlRelative": "/posts/S3CxRmF2SD7FtGJJa/meetup-copenhagen-september-social-meetup-botanisk-have", "linkUrl": "https://www.lesswrong.com/posts/S3CxRmF2SD7FtGJJa/meetup-copenhagen-september-social-meetup-botanisk-have", "postedAtFormatted": "Sunday, September 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Copenhagen%20September%20Social%20Meetup%20-%20Botanisk%20Have&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Copenhagen%20September%20Social%20Meetup%20-%20Botanisk%20Have%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS3CxRmF2SD7FtGJJa%2Fmeetup-copenhagen-september-social-meetup-botanisk-have%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Copenhagen%20September%20Social%20Meetup%20-%20Botanisk%20Have%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS3CxRmF2SD7FtGJJa%2Fmeetup-copenhagen-september-social-meetup-botanisk-have", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS3CxRmF2SD7FtGJJa%2Fmeetup-copenhagen-september-social-meetup-botanisk-have", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14q'>Copenhagen September Social Meetup - Botanisk Have</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 September 2014 02:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Gothersage 128, Copenhagen</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>G'day all,\nSorry for the delayed announcement, but I'd like to suggest we all hang out at the Botanical Gardens on Saturday.</p>\n\n<p>Meet just inside the main gate, Gothersgade 128 - we'll wait there from 2:30pm-3:00pm. Call 2247-8373 to find us.</p>\n\n<p>Bring food and drinks, picnic blanket.</p>\n\n<p>I imagine we'll do a mixture of walking around the gardens as well as relaxing in one spot. Gardens close at 6:00pm and we might continue on elsewhere afterwards too.</p>\n\n<p>Cheers</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14q'>Copenhagen September Social Meetup - Botanisk Have</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S3CxRmF2SD7FtGJJa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.0226085489922186e-06, "legacy": true, "legacyId": "27243", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Copenhagen_September_Social_Meetup___Botanisk_Have\">Discussion article for the meetup : <a href=\"/meetups/14q\">Copenhagen September Social Meetup - Botanisk Have</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 September 2014 02:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Gothersage 128, Copenhagen</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>G'day all,\nSorry for the delayed announcement, but I'd like to suggest we all hang out at the Botanical Gardens on Saturday.</p>\n\n<p>Meet just inside the main gate, Gothersgade 128 - we'll wait there from 2:30pm-3:00pm. Call 2247-8373 to find us.</p>\n\n<p>Bring food and drinks, picnic blanket.</p>\n\n<p>I imagine we'll do a mixture of walking around the gardens as well as relaxing in one spot. Gardens close at 6:00pm and we might continue on elsewhere afterwards too.</p>\n\n<p>Cheers</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Copenhagen_September_Social_Meetup___Botanisk_Have1\">Discussion article for the meetup : <a href=\"/meetups/14q\">Copenhagen September Social Meetup - Botanisk Have</a></h2>", "sections": [{"title": "Discussion article for the meetup : Copenhagen September Social Meetup - Botanisk Have", "anchor": "Discussion_article_for_the_meetup___Copenhagen_September_Social_Meetup___Botanisk_Have", "level": 1}, {"title": "Discussion article for the meetup : Copenhagen September Social Meetup - Botanisk Have", "anchor": "Discussion_article_for_the_meetup___Copenhagen_September_Social_Meetup___Botanisk_Have1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-21T16:07:41.241Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA\u2014Inflation of Terminology", "slug": "meetup-west-la-inflation-of-terminology", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZMXeGWztuWDDxdSce/meetup-west-la-inflation-of-terminology", "pageUrlRelative": "/posts/ZMXeGWztuWDDxdSce/meetup-west-la-inflation-of-terminology", "linkUrl": "https://www.lesswrong.com/posts/ZMXeGWztuWDDxdSce/meetup-west-la-inflation-of-terminology", "postedAtFormatted": "Sunday, September 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%E2%80%94Inflation%20of%20Terminology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%E2%80%94Inflation%20of%20Terminology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZMXeGWztuWDDxdSce%2Fmeetup-west-la-inflation-of-terminology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%E2%80%94Inflation%20of%20Terminology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZMXeGWztuWDDxdSce%2Fmeetup-west-la-inflation-of-terminology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZMXeGWztuWDDxdSce%2Fmeetup-west-la-inflation-of-terminology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14r'>West LA\u2014Inflation of Terminology</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 September 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to Find Us</strong>: Go into <a href=\"https://www.google.com/maps/place/Del+Taco/@34.047464,-118.443286,974m/data=!3m2!1e3!4b1!4m2!3m1!1s0x80c2bb777277de93:0x99f58a53b04bb89c?hl=en\" rel=\"nofollow\">this</a> Del Taco. We will be in the back room if possible.</p>\n\n<p><strong>Parking</strong> is free in the lot out front or on the street nearby.</p>\n\n<p><strong>Discussion</strong>: A problem for people trying to be <a href=\"http://lesswrong.com/lw/ic/the_virtue_of_narrowness/\">precise</a> is that words tend to take on additional meanings over time, and seldom lose meanings. It is good policy to avoid habits that promote this tendency. We will discuss why this is so, and how to implement this policy in daily conversation.</p>\n\n<p><strong>Recommended Reading</strong>:</p>\n\n<ul>\n<li><strong><a href=\"http://lesswrong.com/lw/coo/avoid_inflationary_use_of_terms/\">Avoid Inflationary Use of Terms</a></strong></li>\n<li><a href=\"http://lesswrong.com/lw/nj/similarity_clusters/\">Similarity Clusters</a></li>\n<li><a href=\"http://lesswrong.com/lw/ny/sneaking_in_connotations/\">Sneaking in Connotations</a></li>\n<li><strong><a href=\"http://squid314.livejournal.com/323694.html\" rel=\"nofollow\">The Worst Argument in the World</a></strong></li>\n</ul>\n\n<p><em>No prior exposure to Less Wrong is required</em>; this will be generally accessible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14r'>West LA\u2014Inflation of Terminology</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZMXeGWztuWDDxdSce", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.0230779826974763e-06, "legacy": true, "legacyId": "27244", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Inflation_of_Terminology\">Discussion article for the meetup : <a href=\"/meetups/14r\">West LA\u2014Inflation of Terminology</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 September 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to Find Us</strong>: Go into <a href=\"https://www.google.com/maps/place/Del+Taco/@34.047464,-118.443286,974m/data=!3m2!1e3!4b1!4m2!3m1!1s0x80c2bb777277de93:0x99f58a53b04bb89c?hl=en\" rel=\"nofollow\">this</a> Del Taco. We will be in the back room if possible.</p>\n\n<p><strong>Parking</strong> is free in the lot out front or on the street nearby.</p>\n\n<p><strong>Discussion</strong>: A problem for people trying to be <a href=\"http://lesswrong.com/lw/ic/the_virtue_of_narrowness/\">precise</a> is that words tend to take on additional meanings over time, and seldom lose meanings. It is good policy to avoid habits that promote this tendency. We will discuss why this is so, and how to implement this policy in daily conversation.</p>\n\n<p><strong>Recommended Reading</strong>:</p>\n\n<ul>\n<li><strong><a href=\"http://lesswrong.com/lw/coo/avoid_inflationary_use_of_terms/\">Avoid Inflationary Use of Terms</a></strong></li>\n<li><a href=\"http://lesswrong.com/lw/nj/similarity_clusters/\">Similarity Clusters</a></li>\n<li><a href=\"http://lesswrong.com/lw/ny/sneaking_in_connotations/\">Sneaking in Connotations</a></li>\n<li><strong><a href=\"http://squid314.livejournal.com/323694.html\" rel=\"nofollow\">The Worst Argument in the World</a></strong></li>\n</ul>\n\n<p><em>No prior exposure to Less Wrong is required</em>; this will be generally accessible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Inflation_of_Terminology1\">Discussion article for the meetup : <a href=\"/meetups/14r\">West LA\u2014Inflation of Terminology</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA\u2014Inflation of Terminology", "anchor": "Discussion_article_for_the_meetup___West_LA_Inflation_of_Terminology", "level": 1}, {"title": "Discussion article for the meetup : West LA\u2014Inflation of Terminology", "anchor": "Discussion_article_for_the_meetup___West_LA_Inflation_of_Terminology1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yDfxTj9TKYsYiWH5o", "LqfnkpmK2EBcwbAwA", "jMTbQj9XB5ah2maup", "yuKaWPRTxZoov4z8K"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-21T20:07:40.494Z", "modifiedAt": null, "url": null, "title": "Meetup : Czech's first Meetup Prague", "slug": "meetup-czech-s-first-meetup-prague-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:59.339Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kotrfa", "createdAt": "2014-01-09T19:06:28.770Z", "isAdmin": false, "displayName": "kotrfa"}, "userId": "PNgajXg435nt8gCWo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qPg96rujbCBrcBQsd/meetup-czech-s-first-meetup-prague-0", "pageUrlRelative": "/posts/qPg96rujbCBrcBQsd/meetup-czech-s-first-meetup-prague-0", "linkUrl": "https://www.lesswrong.com/posts/qPg96rujbCBrcBQsd/meetup-czech-s-first-meetup-prague-0", "postedAtFormatted": "Sunday, September 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Czech's%20first%20Meetup%20Prague&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Czech's%20first%20Meetup%20Prague%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqPg96rujbCBrcBQsd%2Fmeetup-czech-s-first-meetup-prague-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Czech's%20first%20Meetup%20Prague%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqPg96rujbCBrcBQsd%2Fmeetup-czech-s-first-meetup-prague-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqPg96rujbCBrcBQsd%2Fmeetup-czech-s-first-meetup-prague-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 144, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14s'>Czech's first Meetup Prague</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 September 2014 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">V\u00e1clavsk\u00e9 n\u00e1m\u011bst\u00ed 778/14, Praha 110 00</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello,\nthis is going to be first meetup I know about in Czech Republic, specially in Prague. Since I don't know any other rationalists here, I'll be waiting in Dobr\u00e1 \u010cajovna s.r.o. (tearoom) for at least one hour. Look for young, tall, skinny, brown curly haired guy.</p>\n\n<p>Please contact me if you are interested in a meeting, but the time just doesn't fit you - it will be at least some sign that someone is interested.</p>\n\n<p>Feel free to ask if you have any questions.</p>\n\n<p>I decided to try to make a meetup from time to time to see if someone will ever show up. It's a pity (and shame) that there are no meetups here.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14s'>Czech's first Meetup Prague</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qPg96rujbCBrcBQsd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.0235166005889114e-06, "legacy": true, "legacyId": "27245", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Czech_s_first_Meetup_Prague\">Discussion article for the meetup : <a href=\"/meetups/14s\">Czech's first Meetup Prague</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 September 2014 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">V\u00e1clavsk\u00e9 n\u00e1m\u011bst\u00ed 778/14, Praha 110 00</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello,\nthis is going to be first meetup I know about in Czech Republic, specially in Prague. Since I don't know any other rationalists here, I'll be waiting in Dobr\u00e1 \u010cajovna s.r.o. (tearoom) for at least one hour. Look for young, tall, skinny, brown curly haired guy.</p>\n\n<p>Please contact me if you are interested in a meeting, but the time just doesn't fit you - it will be at least some sign that someone is interested.</p>\n\n<p>Feel free to ask if you have any questions.</p>\n\n<p>I decided to try to make a meetup from time to time to see if someone will ever show up. It's a pity (and shame) that there are no meetups here.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Czech_s_first_Meetup_Prague1\">Discussion article for the meetup : <a href=\"/meetups/14s\">Czech's first Meetup Prague</a></h2>", "sections": [{"title": "Discussion article for the meetup : Czech's first Meetup Prague", "anchor": "Discussion_article_for_the_meetup___Czech_s_first_Meetup_Prague", "level": 1}, {"title": "Discussion article for the meetup : Czech's first Meetup Prague", "anchor": "Discussion_article_for_the_meetup___Czech_s_first_Meetup_Prague1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-22T05:59:24.963Z", "modifiedAt": null, "url": null, "title": "Open thread, September 22-28, 2014", "slug": "open-thread-september-22-28-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:51.834Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M4Pw3SbQbzmcMBLwo/open-thread-september-22-28-2014", "pageUrlRelative": "/posts/M4Pw3SbQbzmcMBLwo/open-thread-september-22-28-2014", "linkUrl": "https://www.lesswrong.com/posts/M4Pw3SbQbzmcMBLwo/open-thread-september-22-28-2014", "postedAtFormatted": "Monday, September 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20September%2022-28%2C%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20September%2022-28%2C%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM4Pw3SbQbzmcMBLwo%2Fopen-thread-september-22-28-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20September%2022-28%2C%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM4Pw3SbQbzmcMBLwo%2Fopen-thread-september-22-28-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM4Pw3SbQbzmcMBLwo%2Fopen-thread-september-22-28-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19.5px;\"><span style=\"line-height: 19px; font-weight: bold;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19.5px;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19.5px;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19.5px;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one. (<em>Immediately</em>&nbsp;before; refresh the list-of-threads page before posting.)<br /></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19.5px;\"><span style=\"line-height: 19px;\">3.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should be posted in Discussion, and not Main.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19.5px;\"><span style=\"line-height: 19px;\">4.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should start on Monday, and end on Sunday.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M4Pw3SbQbzmcMBLwo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "27246", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 216, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-22T11:24:24.932Z", "modifiedAt": null, "url": null, "title": "CEV: coherence versus extrapolation", "slug": "cev-coherence-versus-extrapolation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:57.818Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RGd85AErgmXmAMKw5/cev-coherence-versus-extrapolation", "pageUrlRelative": "/posts/RGd85AErgmXmAMKw5/cev-coherence-versus-extrapolation", "linkUrl": "https://www.lesswrong.com/posts/RGd85AErgmXmAMKw5/cev-coherence-versus-extrapolation", "postedAtFormatted": "Monday, September 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20CEV%3A%20coherence%20versus%20extrapolation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACEV%3A%20coherence%20versus%20extrapolation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRGd85AErgmXmAMKw5%2Fcev-coherence-versus-extrapolation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=CEV%3A%20coherence%20versus%20extrapolation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRGd85AErgmXmAMKw5%2Fcev-coherence-versus-extrapolation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRGd85AErgmXmAMKw5%2Fcev-coherence-versus-extrapolation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 518, "htmlBody": "<p>It's just struck me that there might be a tension between the coherence (C) and the extrapolated (E) part of <a href=\"http://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition\">CEV</a>. One reason that CEV might work is that the <a href=\"/lw/rl/the_psychological_unity_of_humankind/\">mindspace of humanity isn't that large</a> - humans are pretty close to each other, in comparison to the <a href=\"/lw/rm/the_design_space_of_mindsingeneral/\">space of</a>&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Mind_design_space\">possible minds</a>. But this is far more true in every day decisions than in large scale ones.</p>\n<p>Take a fundamentalist Christian, a total utilitarian, a strong Marxist, an extreme libertarian, and a couple more stereotypes that fit your fancy. What can their ideology tell us about their everyday activities? Well, very little. Those people could be rude, polite, arrogant, compassionate, etc... and their ideology is a very weak indication of that. Different ideologies and moral systems seem to mandate almost identical everyday and personal interactions (this is in itself very interesting, and causes me to see many systems of moralities as formal justifications of what people/society find \"moral\" anyway).</p>\n<p>But now let's more to a more distant - \"far\" - level. How will these people vote in elections? Will they donate to charity, and if so, which ones? If they were given power (via wealth or position in some political or other organisation), how are they likely to use that power? Now their ideology is much more informative. Though it's not fully determinative, we would start to question the label if their actions at this level seemed out of synch. A Marxist that donated to a Conservative party, for instance, would give us pause, and we'd want to understand the apparent contradiction.</p>\n<p>Let's move up yet another level. How would they design or change the universe if they had complete power? What is their ideal plan for the long term? At this level, we're entirely in far mode, and we would expect that their vastly divergent ideologies would be the most informative piece of information about their moral preferences. Details about their character and personalities, which loomed so large at the everyday level, will now be of far lesser relevance. This is because their large scale ideals are not tempered by reality and by human interactions, but exist in a pristine state in their minds, changing little if at all. And in almost every case, the world they imagine as their paradise will be literal hell for the others (and quite possibly for themselves).</p>\n<p>To summarise: the human mindspace is much narrower in&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">near mode</a> than in far mode.</p>\n<p>And what about CEV? Well, CEV is what we would be \"if we knew more, thought faster, were more the people we wished we were, had grown up farther together\". The \"were more the people we wished we were\" is going to be dominated by the highly divergent far mode thinking. The \"had grown up farther together\" clause attempts to mesh these divergences, but that simply obscures the difficulty involved. The more we extrapolate, the harder coherence becomes.</p>\n<p>It strikes me that there is a strong order-of-operations issue here. I'm <a href=\"/lw/hmh/mahatma_armstrong_ceved_to_death/\">not a fan</a> of CEV, but it seems it would be much better to construct, first, the coherent volition of humanity, and only then to extrapolate it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "W6QZYSNt5FgWgvbdT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RGd85AErgmXmAMKw5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 21, "extendedScore": null, "score": 2.025193649450404e-06, "legacy": true, "legacyId": "27247", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Cyj6wQLW6SeF6aGLy", "tnWRXkcDi5Tw9rzXw", "vgFvnr7FefZ3s3tHp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-22T18:21:51.450Z", "modifiedAt": null, "url": null, "title": "CEV-tropes", "slug": "cev-tropes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:38.476Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "snarles", "createdAt": "2009-06-01T03:48:38.132Z", "isAdmin": false, "displayName": "snarles"}, "userId": "YsmFaM5MdsDW8GNop", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6bFBkk3XNiTwgE8R3/cev-tropes", "pageUrlRelative": "/posts/6bFBkk3XNiTwgE8R3/cev-tropes", "linkUrl": "https://www.lesswrong.com/posts/6bFBkk3XNiTwgE8R3/cev-tropes", "postedAtFormatted": "Monday, September 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20CEV-tropes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACEV-tropes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bFBkk3XNiTwgE8R3%2Fcev-tropes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=CEV-tropes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bFBkk3XNiTwgE8R3%2Fcev-tropes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bFBkk3XNiTwgE8R3%2Fcev-tropes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 365, "htmlBody": "<p>As seen in other <a href=\"/r/discussion/lw/l0v/cev_coherence_versus_extrapolation/\">threads</a>, people disagree on whether CEV exists, and if it does, what it might turn out to be.</p>\n<p>&nbsp;</p>\n<p>It would be nice to try to categorize common speculations about CEV.</p>\n<p>1a. CEV doesn't exist, because human preferences are too <a href=\"/r/discussion/lw/l0v/cev_coherence_versus_extrapolation/\">divergent</a></p>\n<p>1b. CEV doesn't even exist for a<a href=\"/r/discussion/lw/l0x/cevtropes/bd8b\"> single human</a>&nbsp;</p>\n<p>1c. CEV does exist, but it results in a return to the status quo</p>\n<p>2a. CEV results in humans living in a physical (not virtual reality) utopia</p>\n<p>2b. CEV results in humans returning to a more primitive society <a href=\"http://en.wikipedia.org/wiki/Neo-Luddism\">free of technology</a></p>\n<p>2c. CEV results in humans living together in a simulation world, where most humans do not have god-like power</p>\n<p>(the similarity between 2a, 2b, and 2c is that humans are still living in the same world, similar to traditional utopia scenarios)</p>\n<p>3. CEV results in a wish for the <a href=\"/lw/hmh/mahatma_armstrong_ceved_to_death/\">annihilation</a> of all life, or maybe the universe</p>\n<p>4a. CEV results in all humans granted the right to be the god of their own private simulation universe (once we acquire the resources to do so)</p>\n<p style=\"text-align: justify;\">4b. CEV can be implemented for <span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify; background-color: #ffffcc;\"><a href=\"/r/discussion/lw/l0v/cev_coherence_versus_extrapolation/bd8h\">\"each salient group of living things in proportion to that group's moral weight\"</a></span></p>\n<p>5. CEV results in all humans agreeing to be wireheaded (<a href=\"http://en.wikipedia.org/wiki/Wirehead_(science_fiction)\">trope</a>)</p>\n<p>6a. CEV results in all humans agreeing to merge into a single being and discarding many of the core features of humankind which have lost their purpose (<a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/AssimilationPlot\">trope</a>)</p>\n<p>6b. CEV results in humans agree to cease their own existence but also creating a superior life form--the outcome is similar to 6a, but the difference is that here, humans do not care about whether they are individually \"merged\"</p>\n<p>7. CEV results in all/some humans willingly forgetting/erasing their history, or being indifferent to preserving history so that it is lost (compatible with all previous tropes)</p>\n<p>Obviously there are too many possible ideas (or \"tropes\") to list, but perhaps we could get a sense of which ones are the most common in the LW community. &nbsp;I leave it to someone else to create a poll supposing they feel they have a close to complete list, or create similar topics for AI risk, etc.</p>\n<p>EDIT: Added more tropes, changed #2 since it was too broad: now #2 refers to CEV worlds where humans live in the \"same world\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W6QZYSNt5FgWgvbdT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6bFBkk3XNiTwgE8R3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 2.025958116136471e-06, "legacy": true, "legacyId": "27249", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RGd85AErgmXmAMKw5", "vgFvnr7FefZ3s3tHp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-22T20:04:17.171Z", "modifiedAt": null, "url": null, "title": "I may have just had a dangerous thought.", "slug": "i-may-have-just-had-a-dangerous-thought", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:38.857Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Fivehundred", "createdAt": "2013-11-25T00:28:49.158Z", "isAdmin": false, "displayName": "Fivehundred"}, "userId": "6fnW5xaeQMzhnjjmG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/87ZTthYeLz9phkhaC/i-may-have-just-had-a-dangerous-thought", "pageUrlRelative": "/posts/87ZTthYeLz9phkhaC/i-may-have-just-had-a-dangerous-thought", "linkUrl": "https://www.lesswrong.com/posts/87ZTthYeLz9phkhaC/i-may-have-just-had-a-dangerous-thought", "postedAtFormatted": "Monday, September 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20may%20have%20just%20had%20a%20dangerous%20thought.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20may%20have%20just%20had%20a%20dangerous%20thought.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F87ZTthYeLz9phkhaC%2Fi-may-have-just-had-a-dangerous-thought%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20may%20have%20just%20had%20a%20dangerous%20thought.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F87ZTthYeLz9phkhaC%2Fi-may-have-just-had-a-dangerous-thought", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F87ZTthYeLz9phkhaC%2Fi-may-have-just-had-a-dangerous-thought", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 38, "htmlBody": "<p>I'm interested in discussing this with someone, non-publicly. It's safe to know about personally, but it's not something I'd like people in general to know.</p>\n<p>I'm really not sure if there is a protocol for this sort of thing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "87ZTthYeLz9phkhaC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 3, "extendedScore": null, "score": 2.026145773049814e-06, "legacy": true, "legacyId": "27250", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-23T01:00:29.845Z", "modifiedAt": null, "url": null, "title": "Superintelligence Reading Group 2: Forecasting AI", "slug": "superintelligence-reading-group-2-forecasting-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:00.498Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/56b8n8FT6fksnDZwY/superintelligence-reading-group-2-forecasting-ai", "pageUrlRelative": "/posts/56b8n8FT6fksnDZwY/superintelligence-reading-group-2-forecasting-ai", "linkUrl": "https://www.lesswrong.com/posts/56b8n8FT6fksnDZwY/superintelligence-reading-group-2-forecasting-ai", "postedAtFormatted": "Tuesday, September 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Superintelligence%20Reading%20Group%202%3A%20Forecasting%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuperintelligence%20Reading%20Group%202%3A%20Forecasting%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F56b8n8FT6fksnDZwY%2Fsuperintelligence-reading-group-2-forecasting-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Superintelligence%20Reading%20Group%202%3A%20Forecasting%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F56b8n8FT6fksnDZwY%2Fsuperintelligence-reading-group-2-forecasting-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F56b8n8FT6fksnDZwY%2Fsuperintelligence-reading-group-2-forecasting-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3281, "htmlBody": "<p><em>This is part of a weekly reading group on <a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a>'s book, <a href=\"http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111\">Superintelligence</a>. For more information about the group, and an index of posts so far see the&nbsp;<a href=\"/lw/kw4/superintelligence_reading_group/\">announcement post</a>. For the schedule of future topics, see&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2014/08/Superintelligence-Readers-Guide-early-version.pdf\">MIRI's reading guide</a>.</em></p>\n<hr />\n<p>Welcome. This week we discuss the second section in the reading guide,&nbsp;<em><strong>Forecasting AI</strong></em>. This is about predictions of AI, and what we should make of them.</p>\n<p>This post summarizes the section, and offers a few relevant notes, and ideas for further investigation. My own thoughts and questions for discussion are in the comments.</p>\n<p>There is no need to proceed in order through this post. Feel free to jump straight to the discussion. Where applicable, page numbers indicate the rough part of the chapter that is most related (not necessarily that the chapter is being cited for the specific claim).</p>\n<p><strong>Reading</strong>:&nbsp;<em>Opinions about the future of machine intelligence,</em>&nbsp;from Chapter 1&nbsp;(p18-21) and Muehlhauser,&nbsp;<a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\"><em>When Will AI be Created?</em></a></p>\n<hr />\n<h1>Summary</h1>\n<p><strong>Opinions about the future of machine intelligence</strong>, from Chapter 1 (p18-21)</p>\n<ol>\n<li>AI researchers hold a variety of views on when human-level AI will arrive, and what it will be like.</li>\n<li>A recent set of surveys of AI researchers produced the following median dates:&nbsp; \n<ul>\n<li>for human-level AI with 10% probability: 2022</li>\n<li>for human-level AI with 50% probability: 2040</li>\n<li>for human-level AI with 90% probability: 2075</li>\n</ul>\n</li>\n<li>Surveyed AI researchers in aggregate gave 10% probability to 'superintelligence' within two years of human level AI, and 75% to 'superintelligence' within 30 years.</li>\n<li>When asked about the long-term impacts of human level AI, surveyed AI researchers gave the responses in the figure below (these are 'renormalized median' responses,&nbsp;'TOP 100' is one of the surveyed groups, 'Combined' is all of them').&nbsp;<br /><img src=\"http://images.lesswrong.com/t3_l0o_2.png?v=4dab6a2c1bf691fc009cb61bd707a7e5\" alt=\"\" width=\"400\" /></li>\n<li>There are various reasons to expect such opinion polls and public statements to be fairly inaccurate.</li>\n<li>Nonetheless, such opinions suggest that the prospect of human-level AI is worthy of attention.</li>\n</ol>\n<div><a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\"><strong>When Will AI Be Created?</strong></a></div>\n<div><ol>\n<li>Predicting when human-level AI will arrive is hard.</li>\n<li>The estimates of informed people can vary between a small number of decades and a thousand years.</li>\n<li>Different time scales have different policy implications.</li>\n<li>Several surveys of AI experts exist, but Muehlhauser suspects sampling bias (e.g. optimistic views being sampled more often) makes such surveys of little use.</li>\n<li>Predicting human-level AI development is the kind of task that experts are characteristically bad at, according to extensive research on what makes people better at predicting things.</li>\n<li>People try to predict human-level AI&nbsp;by extrapolating hardware trends. This probably won't work, as AI requires software as well as hardware, and software appears to be a substantial bottleneck.</li>\n<li>We might try to extrapolate software progress, but software often progresses less smoothly, and is also hard to design good metrics for.</li>\n<li>A number of plausible events might substantially accelerate or slow progress toward human-level AI, such as an end to Moore's Law, depletion of low-hanging fruit, societal collapse, or a change in incentives for development.</li>\n<li>The appropriate response to this situation is uncertainty: you should neither be confident that human-level AI will take less than 30 years, nor that it will take more than a hundred years.</li>\n<li>We can still hope to do better: there are known ways to improve predictive accuracy, such as making quantitative predictions, looking for concrete 'signposts', looking at aggregated predictions, and decomposing complex phenomena into simpler ones.</li>\n</ol></div>\n<div><span style=\"font-size: 2em;\">Notes</span></div>\n<ol>\n<li><strong>More (similar) surveys on when human-level AI will be developed<br /></strong>Bostrom discusses some recent polls in detail, and mentions that others are fairly consistent. Below are the surveys I could find. Several of them give dates when median respondents believe there is a 10%, 50% or 90% chance of AI, which I have recorded as '10% year' etc. If their findings were in another form, those are in the last column. Note that some of these surveys are fairly informal, and many participants are not AI experts, I'd guess especially in the Bainbridge, AI@50 and Klein ones. 'Kruel' is the set of interviews from which Nils Nilson is quoted on p19. The interviews&nbsp;cover a wider range of topics, and are indexed&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">here</a>.<br /><br /> \n<table style=\"margin: 0px; color: #333333; font-family: 'Lucida Grande', 'Lucida Sans', Verdana, Arial, sans-serif; font-size: 13px; line-height: 20px; border-collapse: collapse; border-color: #888888; border-width: 1px;\" border=\"1\" cellspacing=\"0\" bordercolor=\"#888\">\n<tbody>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px;\">&nbsp;10% year</td>\n<td style=\"vertical-align: top; padding: 1px 4px;\">&nbsp;50% year</td>\n<td style=\"vertical-align: top; padding: 1px 4px;\">&nbsp;90% year</td>\n<td style=\"vertical-align: top; padding: 1px 4px;\">&nbsp;Other predictions</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\">Michie 1972&nbsp;<br /><a href=\"https://saltworks.stanford.edu/assets/cf501kz5355.pdf\">(paper&nbsp;</a><a href=\"https://saltworks.stanford.edu/assets/cf501kz5355.pdf\">download</a><a href=\"https://saltworks.stanford.edu/assets/cf501kz5355.pdf\">)</a></td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">Fairly even spread between 20, 50 and &gt;50 years</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://books.google.com/books?id=in7afZdzDckC&amp;pg=PA344&amp;lpg=PA344&amp;dq=%22the+computing+power+and+scientific+knowledge+will+exist+to+build+machines+that+are+functionally+equivalent+to+the+human+brain%22&amp;source=bl&amp;ots=HHGG_NbIbP&amp;sig=CbUeZm3egQJ-ymPfmCSj8T31QD4&amp;hl=en&amp;sa=X&amp;ei=91AfVNXJN4TeoASzj4DQCA&amp;ved=0CCkQ6AEwAQ#v=onepage&amp;q=%22the%20computing%20power%20and%20scientific%20knowledge%20will%20exist%20to%20build%20machines%20that%20are%20functionally%20equivalent%20to%20the%20human%20brain%22&amp;f=false\">Bainbridge 2005</a></td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;Median prediction 2085</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://web.archive.org/web/20110710193831/http://www.engagingexperience.com/ai50/\">AI@50 poll</a>&nbsp;<br />2006</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">82% predict more than 50 years (&gt;2056) or never</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://sethbaum.com/ac/2011_AI-Experts.pdf\">Baum et al <br />AGI-09</a></td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2020&nbsp;&nbsp; &nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2040</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2075</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://web.archive.org/web/20110226225452/http://www.novamente.net/bruce/?p=54\">Klein</a>&nbsp;2011</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\"><br /></td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">median 2030-2050</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://www.fhi.ox.ac.uk/machine-intelligence-survey-2011.pdf\">FHI 2011</a></td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2028</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">2050&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2150</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\">Kruel 2011- (<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">interviews</a>, <a href=\"https://docs.google.com/spreadsheet/ccc?key=0AvoX2xCTgYnWdFlCajk5a0d0bG5Ld1hYUEQzaS1aQWc&amp;usp=sharing#gid=0\">summary</a>)</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2025</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2035</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2070</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://www.nickbostrom.com/papers/survey.pdf\">FHI: AGI</a>&nbsp;2014</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">2022</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2040</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2065</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://www.nickbostrom.com/papers/survey.pdf\">FHI: TOP100</a>&nbsp;2014</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">2022&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2040</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2075</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://www.nickbostrom.com/papers/survey.pdf\">FHI:EETN</a>&nbsp;2014</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">2020</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2050</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2093</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://www.nickbostrom.com/papers/survey.pdf\">FHI:PT-AI</a>&nbsp;2014</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">2023</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2048</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2080</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://www.overcomingbias.com/2012/08/ai-progress-estimate.html\">Hanson</a> ongoing</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">Most say have come 10% or less of the way to human level</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li><strong>Predictions in public statements</strong><br />Polls are one source of predictions on AI. Another source is public statements. That is, things people choose to say publicly. MIRI arranged for the collection of these public statements, which you can now download and play with (<a href=\"/lw/e79/ai_timeline_prediction_data/\">the original</a>&nbsp;and info about it, <a href=\"https://www.dropbox.com/s/x3737sampmb2e8i/siai-fhi_ai_predictions_KG_amended.xlsx\">my edited version</a>&nbsp;and <a href=\"http://www.aiimpacts.org/ai-timelines/predictions-of-human-level-ai-dates/miri-ai-predictions-dataset\">explanation</a>&nbsp;for changes). The figure below shows the cumulative fraction of public statements claiming that human-level AI will be more likely than not by a particular year. Or at least claiming something that can be broadly interpreted as that. It only includes recorded statements made since 2000. There are various&nbsp;<a href=\"http://www.aiimpacts.org/ai-timelines/predictions-of-human-level-ai-dates/interpretation-of-ai-predictions/accuracy-of-ai-predictions/short-prediction-publication-bias\">warnings</a>&nbsp;and <a href=\"http://www.aiimpacts.org/ai-timelines/predictions-of-human-level-ai-dates/miri-ai-predictions-dataset\">details</a>&nbsp;in interpreting this, but I don't think they make a big difference, so are probably not worth considering unless you are especially interested. Note that the authors of these statements are a mixture of mostly AI researchers (including disproportionately many working on human-level AI) a few futurists, and a few other people.<br /><br />(LH axis = fraction of people predicting human-level AI by that date)&nbsp;<br /><img src=\"http://images.lesswrong.com/t3_l0o_0.png\" alt=\"\" width=\"400\" /><br /><strong style=\"font-size: 13px; color: #333333; font-family: 'Lucida Grande', 'Lucida Sans', Verdana, Arial, sans-serif; line-height: 20px;\">Cumulative distribution of predicted date of AI</strong><br /><br />As you can see, the median date (when the graph hits the 0.5 mark) for human-level AI here is much like that in the survey data: 2040 or so. <br /><br />I would generally expect predictions in public statements to be relatively early, because people just don't tend to bother writing books about how exciting things are not going to happen for a while, unless their prediction is fascinatingly late. I checked this more thoroughly, by comparing the outcomes of surveys to the statements made by people in similar groups to those surveyed (e.g. if the survey was of AI researchers, I looked at statements made by AI researchers). In my (very cursory) assessment (detailed at the end of&nbsp;<a href=\"http://www.aiimpacts.org/ai-timelines/predictions-of-human-level-ai-dates/interpretation-of-ai-predictions/accuracy-of-ai-predictions/short-prediction-publication-bias\">this page</a>) there is a bit of a difference: predictions from surveys are 0-23 years later than those from public statements.</li>\n<li><strong>What kinds of things are people good at predicting?</strong><br /><a href=\"https://intelligence.org/files/PredictingAI.pdf\">Armstrong and Sotala</a>&nbsp;(p11) summarize a few research efforts in recent decades as follows.<br /><br /><img src=\"http://images.lesswrong.com/t3_l0o_6.png?v=d0be02c46147d5c8e48816b41a906fb7\" alt=\"\" width=\"500\" /><br />Note that the problem of predicting AI mostly falls on the right. Unfortunately this doesn't tell us anything about how much harder AI timelines are to predict than other things, or the absolute level of predictive accuracy associated with any combination of features. However if you have a rough idea of how well humans predict things, you might correct it downward when predicting how well humans predict future AI development and its social consequences.</li>\n<li><strong>Biases</strong><br />As well as just being generally inaccurate, predictions of AI are often suspected to subject to a number of biases.&nbsp;Bostrom claimed earlier that 'twenty years is the sweet spot for prognosticators of radical change' (p4).&nbsp;A related concern is that people always predict revolutionary changes just within their lifetimes (the so-called <a href=\"http://en.wikipedia.org/wiki/Maes%E2%80%93Garreau_law\">Maes-Garreau law</a>). Worse problems come from selection effects: the people making all of these predictions are selected for thinking AI is the best things to spend their lives on, so might be especially optimistic. Further, more exciting claims of impending robot revolution might be published and remembered more often. More bias might come from wishful thinking: having spent a lot of their lives on it, researchers might hope especially hard for it to go well. On the other hand, as Nils Nilson points out, AI researchers are wary of past predictions and so try hard to retain respectability, for instance by focussing on 'weak AI'. This could systematically push their predictions later. <br /><br />We have some evidence about these biases. <a href=\"https://intelligence.org/files/PredictingAI.pdf\">Armstrong and Sotala</a>&nbsp;(using the MIRI dataset) find people are especially willing to predict AI around 20 years in the future, but&nbsp;couldn't find evidence of the Maes-Garreau law. Another way of looking for the Maes-Garreau law is via correlation between age and predicted time to AI, which is weak&nbsp;(<span style=\"font-size: 13px; color: #333333; font-family: 'Lucida Grande', 'Lucida Sans', Verdana, Arial, sans-serif; line-height: 20px;\">-.017)</span>&nbsp;in the edited MIRI dataset<span style=\"color: #333333; font-family: 'Lucida Grande', 'Lucida Sans', Verdana, Arial, sans-serif; font-size: 13px; line-height: 20px;\">. A general tendency to make predictions based on incentives rather than available information is weakly supported by predictions not changing much over time, which is pretty much what we see in the MIRI dataset. In the figure below, 'early' predictions are made before 2000, and 'late' ones since then.<br /><br /><img src=\"http://images.lesswrong.com/t3_l0o_5.png\" alt=\"\" width=\"500\" /><br /><strong>Cumulative distribution of predicted Years to AI, in early and late predictions.</strong><br /></span><br />We can learn something about selection effects from AI researchers being especially optimistic about AI from comparing groups who might be more or less selected in this way. For instance, we can compare most AI researchers - who tend to work on narrow intelligent capabilities - and researchers of 'artificial general intelligence' (AGI) who specifically focus on creating human-level agents. The figure below shows this comparison with the edited MIRI dataset, using a rough assessment of who works on AGI vs. other AI and only predictions made from 2000 onward ('late'). Interestingly, the AGI predictions indeed look like the most optimistic half of the AI predictions.&nbsp;<br /><br /><img src=\"http://images.lesswrong.com/t3_l0o_4.png?v=c8e938c56beeeb8eb958185f1382a20f\" alt=\"\" width=\"500\" /><br /><strong style=\"color: #333333; font-family: 'Lucida Grande', 'Lucida Sans', Verdana, Arial, sans-serif; font-size: 13px; line-height: 20px;\">Cumulative distribution of predicted date of AI, for AGI and other AI researchers</strong><br /><br />We can also compare other groups in the dataset - 'futurists' and other people (according to our own heuristic assessment). While the picture is interesting, note that both of these groups were very small (as you can see by the large jumps in the graph).&nbsp;<br /><br /><img src=\"http://images.lesswrong.com/t3_l0o_3.png\" alt=\"\" width=\"500\" /><br /><strong style=\"font-size: 13px; color: #333333; font-family: 'Lucida Grande', 'Lucida Sans', Verdana, Arial, sans-serif; line-height: 20px;\">Cumulative distribution of predicted date of AI, for various groups</strong><br /><br />Remember that these differences may not be due to bias, but rather to better understanding. It could well be that AGI research is very promising, and the closer you are to it, the more you realize that. Nonetheless, we can say some things from this data. The total selection bias toward optimism in communities selected for optimism is probably not more than the differences we see here - a few decades in the median, but could plausibly be that large.<br /><br />These have been some rough calculations to get an idea of the extent of a few hypothesized biases. I don't think they are very accurate, but I want to point out that you can actually gather empirical data on these things, and claim that given the current level of research on these questions, you can learn interesting things fairly cheaply, without doing very elaborate or rigorous investigations.</li>\n<li><strong>What definition of 'superintelligence' do AI experts expect within two years of human-level AI with probability 10% and within thirty years with probability 75%?</strong><br />&ldquo;Assume for the purpose of this question that such HLMI will at some point exist. How likely do you then think it is that within (2 years / 30 years) thereafter there will be machine intelligence that greatly surpasses the performance of every human in most professions?&rdquo; See <a href=\"http://www.nickbostrom.com/papers/survey.pdf\">the paper</a> for other details about Bostrom and M&uuml;ller's surveys (the ones in the book).</li>\n</ol>\n<h1>In-depth investigations</h1>\n<p>If you are particularly interested in these topics, and want to do further research, these are a few plausible directions, some taken from Luke Muehlhauser's <a href=\"http://lukemuehlhauser.com/some-studies-which-could-improve-our-strategic-picture-of-superintelligence/\">list</a>:</p>\n<ol>\n<li>Instead of asking how long until AI,&nbsp;Robin Hanson's mini-survey asks people how far we have come (in a particular sub-area) in the last 20 years, as a fraction of the remaining distance. Responses to this question are generally fairly low - 5% is common. His respondents also tend to say that progress isn't accelerating especially. These estimates imply that any given sub-area of AI, human-level ability should be reached in about 200 years, which is strongly at odds with what researchers say in the other surveys. An interesting project would be to expand Robin's survey, and try to understand the discrepancy, and which estimates we should be using. We made a&nbsp;<a href=\"https://docs.google.com/document/d/1-eqYP1LumqZohBTGrujyPwj9q9WUx2c2leawzbaXrV0/edit\">guide</a> to carrying out this project.</li>\n<li>There are many possible empirical projects which would better inform estimates of timelines e.g. measuring the landscape and trends of computation (MIRI started this&nbsp;<a style=\"box-sizing: border-box; transition: all 0.1s ease-in-out; -webkit-transition: all 0.1s ease-in-out; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #000000; color: #000000; text-decoration: none;\" href=\"http://intelligence.org/2014/02/28/the-worlds-distribution-of-computation-initial-findings/\">here</a>,&nbsp;and made a&nbsp;<a style=\"box-sizing: border-box; transition: all 0.1s ease-in-out; -webkit-transition: all 0.1s ease-in-out; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #000000; color: #000000; text-decoration: none;\" href=\"https://docs.google.com/document/d/19K37J6VzN7aigZC4IwydEWDAYMSVBTWFcrxN6YFMxig/edit?usp=sharing\">project guide</a>), analyzing performance of different versions of software on benchmark problems to find how much hardware and software contributed to progress, developing metrics to meaningfully measure AI progress, investigating the extent of AI inspiration from biology in the past, measuring research inputs over time (e.g. <a href=\"http://intelligence.org/2014/01/28/how-big-is-ai/\">a start</a>), and finding the characteristic patterns of progress in algorithms (my attempts <a href=\"http://intelligence.org/files/AlgorithmicProgress.pdf\">here</a>).</li>\n<li>Make a detailed assessment of likely timelines in communication with some informed AI researchers.</li>\n<li>Gather and interpret past efforts to predict technology decades ahead of time. Here are a few efforts to judge past technological predictions: <a href=\"http://www.sciencedirect.com/science/article/pii/001632876990007X\">Clarke 1969</a>,&nbsp;<a href=\"http://www.sciencedirect.com/science/article/pii/0016328776900057\">Wise 1976</a>, <a href=\"http://curriculumredesign.org/wp-content/uploads/Albright_Past_Forecasts.pdf\">Albright 2002</a>, <a href=\"http://oai.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=ADA568107\">Mullins 2012</a>,&nbsp;<a href=\"http://www.kurzweilai.net/how-my-predictions-are-faring-an-update-by-ray-kurzweil\">Kurzweil on his own predictions</a>, and <a href=\"/lw/gbi/assessing_kurzweil_the_results/\">other people on Kurzweil's predictions</a>.&nbsp;</li>\n<li>Above I showed you several rough calculations I did. A rigorous version of any of these would be useful.</li>\n<li>Did most early AI scientists really think AI was right around the corner, or was it just a few people? The earliest survey available (Michie 1973) suggests it may have been just a few people. For those that thought AI was right around the corner, how much did they think about the safety and ethical challenges? If they thought and talked about it substantially, why was there so little published on the subject? If they really didn&rsquo;t think much about it, what does that imply about how seriously AI scientists will&nbsp;treat the safety and ethical challenges of AI in the future? Some relevant sources&nbsp;<a style=\"box-sizing: border-box; transition: all 0.1s ease-in-out; -webkit-transition: all 0.1s ease-in-out; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #000000; color: #000000; text-decoration: none;\" href=\"/r/discussion/lw/bd6/ai_risk_opportunity_a_timeline_of_early_ideas_and/\">here</a>.</li>\n<li style=\"box-sizing: border-box; list-style-type: decimal;\">Conduct a&nbsp;<a style=\"box-sizing: border-box; transition: all 0.1s ease-in-out; -webkit-transition: all 0.1s ease-in-out; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #000000; color: #000000; text-decoration: none;\" href=\"http://en.wikipedia.org/wiki/Delphi_method\">Delphi</a>&nbsp;study of likely AGI impacts. Participants could be&nbsp;AI scientists, researchers who work on high-assurance software systems, and AGI theorists.</li>\n<li style=\"box-sizing: border-box; list-style-type: decimal;\">Signpost the future.&nbsp;<em style=\"box-sizing: border-box;\">Superintelligence</em>&nbsp;explores many different ways the future might play out with regard to superintelligence, but cannot help being somewhat agnostic about which&nbsp;<em style=\"box-sizing: border-box;\">particular</em>&nbsp;path the future will take. Come up with clear diagnostic signals that policy makers can use to gauge whether things are developing toward or away from one set of scenarios or another. If X does or does not happen by 2030, what does that suggest about the path we&rsquo;re on? If Y ends up taking&nbsp;value A or B, what does that imply?</li>\n<li style=\"box-sizing: border-box; list-style-type: decimal;\">Another&nbsp;survey of AI scientists&rsquo; estimates on AGI timelines, takeoff speed, and likely social outcomes, with more respondents and a higher response rate than the best current survey, which is probably&nbsp;<a style=\"box-sizing: border-box; transition: all 0.1s ease-in-out; -webkit-transition: all 0.1s ease-in-out; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #000000; color: #000000; text-decoration: none;\" href=\"http://www.sophia.de/pdf/2014_PT-AI_polls.pdf\">M&uuml;ller &amp; Bostrom (2014)</a>.</li>\n<li>Download the MIRI dataset and see if you can find anything interesting in it.</li>\n</ol> <ol> </ol>\n<h1>How to proceed</h1>\n<p>This has been a collection of notes on the chapter.&nbsp; <strong>The most important part of the reading group though is discussion</strong>, which is in the comments section. I pose some questions for you there, and I invite you to add your own. Please remember that this group contains a variety of levels of expertise: if a line of discussion seems too basic or too incomprehensible, look around for one that suits you better!</p>\n<p>Next week, we will talk about two paths to the development of superintelligence: AI coded by humans, and whole brain emulation. To prepare,&nbsp;<strong>read</strong>&nbsp;<em>Artificial Intelligence</em>&nbsp;and <em>Whole Brain Emulation</em> from Chapter 2<em>.&nbsp;</em>The discussion will go live at 6pm Pacific time next Monday 29 September. Sign up to be notified&nbsp;<a href=\"http://intelligence.us5.list-manage.com/subscribe?u=353906382677fa789a483ba9e&amp;id=28cb982f40\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "tdt83ChxnEgwwKxi6": 1, "zHjC29kkPmsdo7WTr": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "56b8n8FT6fksnDZwY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 17, "extendedScore": null, "score": 2.0266886271195565e-06, "legacy": true, "legacyId": "27240", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This is part of a weekly reading group on <a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a>'s book, <a href=\"http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111\">Superintelligence</a>. For more information about the group, and an index of posts so far see the&nbsp;<a href=\"/lw/kw4/superintelligence_reading_group/\">announcement post</a>. For the schedule of future topics, see&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2014/08/Superintelligence-Readers-Guide-early-version.pdf\">MIRI's reading guide</a>.</em></p>\n<hr>\n<p>Welcome. This week we discuss the second section in the reading guide,&nbsp;<em><strong>Forecasting AI</strong></em>. This is about predictions of AI, and what we should make of them.</p>\n<p>This post summarizes the section, and offers a few relevant notes, and ideas for further investigation. My own thoughts and questions for discussion are in the comments.</p>\n<p>There is no need to proceed in order through this post. Feel free to jump straight to the discussion. Where applicable, page numbers indicate the rough part of the chapter that is most related (not necessarily that the chapter is being cited for the specific claim).</p>\n<p><strong>Reading</strong>:&nbsp;<em>Opinions about the future of machine intelligence,</em>&nbsp;from Chapter 1&nbsp;(p18-21) and Muehlhauser,&nbsp;<a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\"><em>When Will AI be Created?</em></a></p>\n<hr>\n<h1 id=\"Summary\">Summary</h1>\n<p><strong>Opinions about the future of machine intelligence</strong>, from Chapter 1 (p18-21)</p>\n<ol>\n<li>AI researchers hold a variety of views on when human-level AI will arrive, and what it will be like.</li>\n<li>A recent set of surveys of AI researchers produced the following median dates:&nbsp; \n<ul>\n<li>for human-level AI with 10% probability: 2022</li>\n<li>for human-level AI with 50% probability: 2040</li>\n<li>for human-level AI with 90% probability: 2075</li>\n</ul>\n</li>\n<li>Surveyed AI researchers in aggregate gave 10% probability to 'superintelligence' within two years of human level AI, and 75% to 'superintelligence' within 30 years.</li>\n<li>When asked about the long-term impacts of human level AI, surveyed AI researchers gave the responses in the figure below (these are 'renormalized median' responses,&nbsp;'TOP 100' is one of the surveyed groups, 'Combined' is all of them').&nbsp;<br><img src=\"http://images.lesswrong.com/t3_l0o_2.png?v=4dab6a2c1bf691fc009cb61bd707a7e5\" alt=\"\" width=\"400\"></li>\n<li>There are various reasons to expect such opinion polls and public statements to be fairly inaccurate.</li>\n<li>Nonetheless, such opinions suggest that the prospect of human-level AI is worthy of attention.</li>\n</ol>\n<div><a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\"><strong>When Will AI Be Created?</strong></a></div>\n<div><ol>\n<li>Predicting when human-level AI will arrive is hard.</li>\n<li>The estimates of informed people can vary between a small number of decades and a thousand years.</li>\n<li>Different time scales have different policy implications.</li>\n<li>Several surveys of AI experts exist, but Muehlhauser suspects sampling bias (e.g. optimistic views being sampled more often) makes such surveys of little use.</li>\n<li>Predicting human-level AI development is the kind of task that experts are characteristically bad at, according to extensive research on what makes people better at predicting things.</li>\n<li>People try to predict human-level AI&nbsp;by extrapolating hardware trends. This probably won't work, as AI requires software as well as hardware, and software appears to be a substantial bottleneck.</li>\n<li>We might try to extrapolate software progress, but software often progresses less smoothly, and is also hard to design good metrics for.</li>\n<li>A number of plausible events might substantially accelerate or slow progress toward human-level AI, such as an end to Moore's Law, depletion of low-hanging fruit, societal collapse, or a change in incentives for development.</li>\n<li>The appropriate response to this situation is uncertainty: you should neither be confident that human-level AI will take less than 30 years, nor that it will take more than a hundred years.</li>\n<li>We can still hope to do better: there are known ways to improve predictive accuracy, such as making quantitative predictions, looking for concrete 'signposts', looking at aggregated predictions, and decomposing complex phenomena into simpler ones.</li>\n</ol></div>\n<div><span style=\"font-size: 2em;\">Notes</span></div>\n<ol>\n<li><strong>More (similar) surveys on when human-level AI will be developed<br></strong>Bostrom discusses some recent polls in detail, and mentions that others are fairly consistent. Below are the surveys I could find. Several of them give dates when median respondents believe there is a 10%, 50% or 90% chance of AI, which I have recorded as '10% year' etc. If their findings were in another form, those are in the last column. Note that some of these surveys are fairly informal, and many participants are not AI experts, I'd guess especially in the Bainbridge, AI@50 and Klein ones. 'Kruel' is the set of interviews from which Nils Nilson is quoted on p19. The interviews&nbsp;cover a wider range of topics, and are indexed&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">here</a>.<br><br> \n<table style=\"margin: 0px; color: #333333; font-family: 'Lucida Grande', 'Lucida Sans', Verdana, Arial, sans-serif; font-size: 13px; line-height: 20px; border-collapse: collapse; border-color: #888888; border-width: 1px;\" border=\"1\" cellspacing=\"0\" bordercolor=\"#888\">\n<tbody>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px;\">&nbsp;10% year</td>\n<td style=\"vertical-align: top; padding: 1px 4px;\">&nbsp;50% year</td>\n<td style=\"vertical-align: top; padding: 1px 4px;\">&nbsp;90% year</td>\n<td style=\"vertical-align: top; padding: 1px 4px;\">&nbsp;Other predictions</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\">Michie 1972&nbsp;<br><a href=\"https://saltworks.stanford.edu/assets/cf501kz5355.pdf\">(paper&nbsp;</a><a href=\"https://saltworks.stanford.edu/assets/cf501kz5355.pdf\">download</a><a href=\"https://saltworks.stanford.edu/assets/cf501kz5355.pdf\">)</a></td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">Fairly even spread between 20, 50 and &gt;50 years</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://books.google.com/books?id=in7afZdzDckC&amp;pg=PA344&amp;lpg=PA344&amp;dq=%22the+computing+power+and+scientific+knowledge+will+exist+to+build+machines+that+are+functionally+equivalent+to+the+human+brain%22&amp;source=bl&amp;ots=HHGG_NbIbP&amp;sig=CbUeZm3egQJ-ymPfmCSj8T31QD4&amp;hl=en&amp;sa=X&amp;ei=91AfVNXJN4TeoASzj4DQCA&amp;ved=0CCkQ6AEwAQ#v=onepage&amp;q=%22the%20computing%20power%20and%20scientific%20knowledge%20will%20exist%20to%20build%20machines%20that%20are%20functionally%20equivalent%20to%20the%20human%20brain%22&amp;f=false\">Bainbridge 2005</a></td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;Median prediction 2085</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://web.archive.org/web/20110710193831/http://www.engagingexperience.com/ai50/\">AI@50 poll</a>&nbsp;<br>2006</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">82% predict more than 50 years (&gt;2056) or never</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://sethbaum.com/ac/2011_AI-Experts.pdf\">Baum et al <br>AGI-09</a></td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2020&nbsp;&nbsp; &nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2040</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2075</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://web.archive.org/web/20110226225452/http://www.novamente.net/bruce/?p=54\">Klein</a>&nbsp;2011</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\"><br></td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">median 2030-2050</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://www.fhi.ox.ac.uk/machine-intelligence-survey-2011.pdf\">FHI 2011</a></td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2028</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">2050&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2150</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\">Kruel 2011- (<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">interviews</a>, <a href=\"https://docs.google.com/spreadsheet/ccc?key=0AvoX2xCTgYnWdFlCajk5a0d0bG5Ld1hYUEQzaS1aQWc&amp;usp=sharing#gid=0\">summary</a>)</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2025</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2035</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2070</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://www.nickbostrom.com/papers/survey.pdf\">FHI: AGI</a>&nbsp;2014</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">2022</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2040</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2065</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://www.nickbostrom.com/papers/survey.pdf\">FHI: TOP100</a>&nbsp;2014</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">2022&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2040</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2075</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://www.nickbostrom.com/papers/survey.pdf\">FHI:EETN</a>&nbsp;2014</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">2020</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2050</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2093</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://www.nickbostrom.com/papers/survey.pdf\">FHI:PT-AI</a>&nbsp;2014</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">2023</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2048</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;2080</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 87px; height: 18px;\"><a href=\"http://www.overcomingbias.com/2012/08/ai-progress-estimate.html\">Hanson</a> ongoing</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">&nbsp;</td>\n<td style=\"vertical-align: top; padding: 1px 4px; width: 58px; height: 18px;\">Most say have come 10% or less of the way to human level</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li><strong>Predictions in public statements</strong><br>Polls are one source of predictions on AI. Another source is public statements. That is, things people choose to say publicly. MIRI arranged for the collection of these public statements, which you can now download and play with (<a href=\"/lw/e79/ai_timeline_prediction_data/\">the original</a>&nbsp;and info about it, <a href=\"https://www.dropbox.com/s/x3737sampmb2e8i/siai-fhi_ai_predictions_KG_amended.xlsx\">my edited version</a>&nbsp;and <a href=\"http://www.aiimpacts.org/ai-timelines/predictions-of-human-level-ai-dates/miri-ai-predictions-dataset\">explanation</a>&nbsp;for changes). The figure below shows the cumulative fraction of public statements claiming that human-level AI will be more likely than not by a particular year. Or at least claiming something that can be broadly interpreted as that. It only includes recorded statements made since 2000. There are various&nbsp;<a href=\"http://www.aiimpacts.org/ai-timelines/predictions-of-human-level-ai-dates/interpretation-of-ai-predictions/accuracy-of-ai-predictions/short-prediction-publication-bias\">warnings</a>&nbsp;and <a href=\"http://www.aiimpacts.org/ai-timelines/predictions-of-human-level-ai-dates/miri-ai-predictions-dataset\">details</a>&nbsp;in interpreting this, but I don't think they make a big difference, so are probably not worth considering unless you are especially interested. Note that the authors of these statements are a mixture of mostly AI researchers (including disproportionately many working on human-level AI) a few futurists, and a few other people.<br><br>(LH axis = fraction of people predicting human-level AI by that date)&nbsp;<br><img src=\"http://images.lesswrong.com/t3_l0o_0.png\" alt=\"\" width=\"400\"><br><strong style=\"font-size: 13px; color: #333333; font-family: 'Lucida Grande', 'Lucida Sans', Verdana, Arial, sans-serif; line-height: 20px;\">Cumulative distribution of predicted date of AI</strong><br><br>As you can see, the median date (when the graph hits the 0.5 mark) for human-level AI here is much like that in the survey data: 2040 or so. <br><br>I would generally expect predictions in public statements to be relatively early, because people just don't tend to bother writing books about how exciting things are not going to happen for a while, unless their prediction is fascinatingly late. I checked this more thoroughly, by comparing the outcomes of surveys to the statements made by people in similar groups to those surveyed (e.g. if the survey was of AI researchers, I looked at statements made by AI researchers). In my (very cursory) assessment (detailed at the end of&nbsp;<a href=\"http://www.aiimpacts.org/ai-timelines/predictions-of-human-level-ai-dates/interpretation-of-ai-predictions/accuracy-of-ai-predictions/short-prediction-publication-bias\">this page</a>) there is a bit of a difference: predictions from surveys are 0-23 years later than those from public statements.</li>\n<li><strong>What kinds of things are people good at predicting?</strong><br><a href=\"https://intelligence.org/files/PredictingAI.pdf\">Armstrong and Sotala</a>&nbsp;(p11) summarize a few research efforts in recent decades as follows.<br><br><img src=\"http://images.lesswrong.com/t3_l0o_6.png?v=d0be02c46147d5c8e48816b41a906fb7\" alt=\"\" width=\"500\"><br>Note that the problem of predicting AI mostly falls on the right. Unfortunately this doesn't tell us anything about how much harder AI timelines are to predict than other things, or the absolute level of predictive accuracy associated with any combination of features. However if you have a rough idea of how well humans predict things, you might correct it downward when predicting how well humans predict future AI development and its social consequences.</li>\n<li><strong>Biases</strong><br>As well as just being generally inaccurate, predictions of AI are often suspected to subject to a number of biases.&nbsp;Bostrom claimed earlier that 'twenty years is the sweet spot for prognosticators of radical change' (p4).&nbsp;A related concern is that people always predict revolutionary changes just within their lifetimes (the so-called <a href=\"http://en.wikipedia.org/wiki/Maes%E2%80%93Garreau_law\">Maes-Garreau law</a>). Worse problems come from selection effects: the people making all of these predictions are selected for thinking AI is the best things to spend their lives on, so might be especially optimistic. Further, more exciting claims of impending robot revolution might be published and remembered more often. More bias might come from wishful thinking: having spent a lot of their lives on it, researchers might hope especially hard for it to go well. On the other hand, as Nils Nilson points out, AI researchers are wary of past predictions and so try hard to retain respectability, for instance by focussing on 'weak AI'. This could systematically push their predictions later. <br><br>We have some evidence about these biases. <a href=\"https://intelligence.org/files/PredictingAI.pdf\">Armstrong and Sotala</a>&nbsp;(using the MIRI dataset) find people are especially willing to predict AI around 20 years in the future, but&nbsp;couldn't find evidence of the Maes-Garreau law. Another way of looking for the Maes-Garreau law is via correlation between age and predicted time to AI, which is weak&nbsp;(<span style=\"font-size: 13px; color: #333333; font-family: 'Lucida Grande', 'Lucida Sans', Verdana, Arial, sans-serif; line-height: 20px;\">-.017)</span>&nbsp;in the edited MIRI dataset<span style=\"color: #333333; font-family: 'Lucida Grande', 'Lucida Sans', Verdana, Arial, sans-serif; font-size: 13px; line-height: 20px;\">. A general tendency to make predictions based on incentives rather than available information is weakly supported by predictions not changing much over time, which is pretty much what we see in the MIRI dataset. In the figure below, 'early' predictions are made before 2000, and 'late' ones since then.<br><br><img src=\"http://images.lesswrong.com/t3_l0o_5.png\" alt=\"\" width=\"500\"><br><strong>Cumulative distribution of predicted Years to AI, in early and late predictions.</strong><br></span><br>We can learn something about selection effects from AI researchers being especially optimistic about AI from comparing groups who might be more or less selected in this way. For instance, we can compare most AI researchers - who tend to work on narrow intelligent capabilities - and researchers of 'artificial general intelligence' (AGI) who specifically focus on creating human-level agents. The figure below shows this comparison with the edited MIRI dataset, using a rough assessment of who works on AGI vs. other AI and only predictions made from 2000 onward ('late'). Interestingly, the AGI predictions indeed look like the most optimistic half of the AI predictions.&nbsp;<br><br><img src=\"http://images.lesswrong.com/t3_l0o_4.png?v=c8e938c56beeeb8eb958185f1382a20f\" alt=\"\" width=\"500\"><br><strong style=\"color: #333333; font-family: 'Lucida Grande', 'Lucida Sans', Verdana, Arial, sans-serif; font-size: 13px; line-height: 20px;\">Cumulative distribution of predicted date of AI, for AGI and other AI researchers</strong><br><br>We can also compare other groups in the dataset - 'futurists' and other people (according to our own heuristic assessment). While the picture is interesting, note that both of these groups were very small (as you can see by the large jumps in the graph).&nbsp;<br><br><img src=\"http://images.lesswrong.com/t3_l0o_3.png\" alt=\"\" width=\"500\"><br><strong style=\"font-size: 13px; color: #333333; font-family: 'Lucida Grande', 'Lucida Sans', Verdana, Arial, sans-serif; line-height: 20px;\">Cumulative distribution of predicted date of AI, for various groups</strong><br><br>Remember that these differences may not be due to bias, but rather to better understanding. It could well be that AGI research is very promising, and the closer you are to it, the more you realize that. Nonetheless, we can say some things from this data. The total selection bias toward optimism in communities selected for optimism is probably not more than the differences we see here - a few decades in the median, but could plausibly be that large.<br><br>These have been some rough calculations to get an idea of the extent of a few hypothesized biases. I don't think they are very accurate, but I want to point out that you can actually gather empirical data on these things, and claim that given the current level of research on these questions, you can learn interesting things fairly cheaply, without doing very elaborate or rigorous investigations.</li>\n<li><strong>What definition of 'superintelligence' do AI experts expect within two years of human-level AI with probability 10% and within thirty years with probability 75%?</strong><br>\u201cAssume for the purpose of this question that such HLMI will at some point exist. How likely do you then think it is that within (2 years / 30 years) thereafter there will be machine intelligence that greatly surpasses the performance of every human in most professions?\u201d See <a href=\"http://www.nickbostrom.com/papers/survey.pdf\">the paper</a> for other details about Bostrom and M\u00fcller's surveys (the ones in the book).</li>\n</ol>\n<h1 id=\"In_depth_investigations\">In-depth investigations</h1>\n<p>If you are particularly interested in these topics, and want to do further research, these are a few plausible directions, some taken from Luke Muehlhauser's <a href=\"http://lukemuehlhauser.com/some-studies-which-could-improve-our-strategic-picture-of-superintelligence/\">list</a>:</p>\n<ol>\n<li>Instead of asking how long until AI,&nbsp;Robin Hanson's mini-survey asks people how far we have come (in a particular sub-area) in the last 20 years, as a fraction of the remaining distance. Responses to this question are generally fairly low - 5% is common. His respondents also tend to say that progress isn't accelerating especially. These estimates imply that any given sub-area of AI, human-level ability should be reached in about 200 years, which is strongly at odds with what researchers say in the other surveys. An interesting project would be to expand Robin's survey, and try to understand the discrepancy, and which estimates we should be using. We made a&nbsp;<a href=\"https://docs.google.com/document/d/1-eqYP1LumqZohBTGrujyPwj9q9WUx2c2leawzbaXrV0/edit\">guide</a> to carrying out this project.</li>\n<li>There are many possible empirical projects which would better inform estimates of timelines e.g. measuring the landscape and trends of computation (MIRI started this&nbsp;<a style=\"box-sizing: border-box; transition: all 0.1s ease-in-out; -webkit-transition: all 0.1s ease-in-out; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #000000; color: #000000; text-decoration: none;\" href=\"http://intelligence.org/2014/02/28/the-worlds-distribution-of-computation-initial-findings/\">here</a>,&nbsp;and made a&nbsp;<a style=\"box-sizing: border-box; transition: all 0.1s ease-in-out; -webkit-transition: all 0.1s ease-in-out; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #000000; color: #000000; text-decoration: none;\" href=\"https://docs.google.com/document/d/19K37J6VzN7aigZC4IwydEWDAYMSVBTWFcrxN6YFMxig/edit?usp=sharing\">project guide</a>), analyzing performance of different versions of software on benchmark problems to find how much hardware and software contributed to progress, developing metrics to meaningfully measure AI progress, investigating the extent of AI inspiration from biology in the past, measuring research inputs over time (e.g. <a href=\"http://intelligence.org/2014/01/28/how-big-is-ai/\">a start</a>), and finding the characteristic patterns of progress in algorithms (my attempts <a href=\"http://intelligence.org/files/AlgorithmicProgress.pdf\">here</a>).</li>\n<li>Make a detailed assessment of likely timelines in communication with some informed AI researchers.</li>\n<li>Gather and interpret past efforts to predict technology decades ahead of time. Here are a few efforts to judge past technological predictions: <a href=\"http://www.sciencedirect.com/science/article/pii/001632876990007X\">Clarke 1969</a>,&nbsp;<a href=\"http://www.sciencedirect.com/science/article/pii/0016328776900057\">Wise 1976</a>, <a href=\"http://curriculumredesign.org/wp-content/uploads/Albright_Past_Forecasts.pdf\">Albright 2002</a>, <a href=\"http://oai.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=ADA568107\">Mullins 2012</a>,&nbsp;<a href=\"http://www.kurzweilai.net/how-my-predictions-are-faring-an-update-by-ray-kurzweil\">Kurzweil on his own predictions</a>, and <a href=\"/lw/gbi/assessing_kurzweil_the_results/\">other people on Kurzweil's predictions</a>.&nbsp;</li>\n<li>Above I showed you several rough calculations I did. A rigorous version of any of these would be useful.</li>\n<li>Did most early AI scientists really think AI was right around the corner, or was it just a few people? The earliest survey available (Michie 1973) suggests it may have been just a few people. For those that thought AI was right around the corner, how much did they think about the safety and ethical challenges? If they thought and talked about it substantially, why was there so little published on the subject? If they really didn\u2019t think much about it, what does that imply about how seriously AI scientists will&nbsp;treat the safety and ethical challenges of AI in the future? Some relevant sources&nbsp;<a style=\"box-sizing: border-box; transition: all 0.1s ease-in-out; -webkit-transition: all 0.1s ease-in-out; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #000000; color: #000000; text-decoration: none;\" href=\"/r/discussion/lw/bd6/ai_risk_opportunity_a_timeline_of_early_ideas_and/\">here</a>.</li>\n<li style=\"box-sizing: border-box; list-style-type: decimal;\">Conduct a&nbsp;<a style=\"box-sizing: border-box; transition: all 0.1s ease-in-out; -webkit-transition: all 0.1s ease-in-out; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #000000; color: #000000; text-decoration: none;\" href=\"http://en.wikipedia.org/wiki/Delphi_method\">Delphi</a>&nbsp;study of likely AGI impacts. Participants could be&nbsp;AI scientists, researchers who work on high-assurance software systems, and AGI theorists.</li>\n<li style=\"box-sizing: border-box; list-style-type: decimal;\">Signpost the future.&nbsp;<em style=\"box-sizing: border-box;\">Superintelligence</em>&nbsp;explores many different ways the future might play out with regard to superintelligence, but cannot help being somewhat agnostic about which&nbsp;<em style=\"box-sizing: border-box;\">particular</em>&nbsp;path the future will take. Come up with clear diagnostic signals that policy makers can use to gauge whether things are developing toward or away from one set of scenarios or another. If X does or does not happen by 2030, what does that suggest about the path we\u2019re on? If Y ends up taking&nbsp;value A or B, what does that imply?</li>\n<li style=\"box-sizing: border-box; list-style-type: decimal;\">Another&nbsp;survey of AI scientists\u2019 estimates on AGI timelines, takeoff speed, and likely social outcomes, with more respondents and a higher response rate than the best current survey, which is probably&nbsp;<a style=\"box-sizing: border-box; transition: all 0.1s ease-in-out; -webkit-transition: all 0.1s ease-in-out; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #000000; color: #000000; text-decoration: none;\" href=\"http://www.sophia.de/pdf/2014_PT-AI_polls.pdf\">M\u00fcller &amp; Bostrom (2014)</a>.</li>\n<li>Download the MIRI dataset and see if you can find anything interesting in it.</li>\n</ol> <ol> </ol>\n<h1 id=\"How_to_proceed\">How to proceed</h1>\n<p>This has been a collection of notes on the chapter.&nbsp; <strong>The most important part of the reading group though is discussion</strong>, which is in the comments section. I pose some questions for you there, and I invite you to add your own. Please remember that this group contains a variety of levels of expertise: if a line of discussion seems too basic or too incomprehensible, look around for one that suits you better!</p>\n<p>Next week, we will talk about two paths to the development of superintelligence: AI coded by humans, and whole brain emulation. To prepare,&nbsp;<strong>read</strong>&nbsp;<em>Artificial Intelligence</em>&nbsp;and <em>Whole Brain Emulation</em> from Chapter 2<em>.&nbsp;</em>The discussion will go live at 6pm Pacific time next Monday 29 September. Sign up to be notified&nbsp;<a href=\"http://intelligence.us5.list-manage.com/subscribe?u=353906382677fa789a483ba9e&amp;id=28cb982f40\">here</a>.</p>", "sections": [{"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "In-depth investigations", "anchor": "In_depth_investigations", "level": 1}, {"title": "How to proceed", "anchor": "How_to_proceed", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "109 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 109, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QDmzDZ9CEHrKQdvcn", "Q6oWinLaKXmGNWGLy", "kbA6T3xpxtko36GgP", "Qdq2SKyMi8vf7Snxq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-23T01:02:38.378Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta September Meetup - Self Awareness", "slug": "meetup-atlanta-september-meetup-self-awareness", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Adele_L", "createdAt": "2012-05-25T06:52:13.187Z", "isAdmin": false, "displayName": "Adele_L"}, "userId": "5cAXqfacg2fkQPK8j", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H4XFSbgaqNJR42dDc/meetup-atlanta-september-meetup-self-awareness", "pageUrlRelative": "/posts/H4XFSbgaqNJR42dDc/meetup-atlanta-september-meetup-self-awareness", "linkUrl": "https://www.lesswrong.com/posts/H4XFSbgaqNJR42dDc/meetup-atlanta-september-meetup-self-awareness", "postedAtFormatted": "Tuesday, September 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20September%20Meetup%20-%20Self%20Awareness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20September%20Meetup%20-%20Self%20Awareness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4XFSbgaqNJR42dDc%2Fmeetup-atlanta-september-meetup-self-awareness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20September%20Meetup%20-%20Self%20Awareness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4XFSbgaqNJR42dDc%2Fmeetup-atlanta-september-meetup-self-awareness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4XFSbgaqNJR42dDc%2Fmeetup-atlanta-september-meetup-self-awareness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14t'>Atlanta September Meetup - Self Awareness</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 September 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Unit L, Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meetup we'll be discussing self awareness and reflection. We'll be talking about ideas discussed in the <a href=\"http://lesswrong.com/lw/1xh/living_luminously/\">Living Luminously sequence</a>.\nAs usual, there will be snacks and games as well as plenty of interesting conversation! There are cats at the location. Please park in a spot labeled visitor, as parking in the other spots runs the risk of getting towed. Hope to see you then!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14t'>Atlanta September Meetup - Self Awareness</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H4XFSbgaqNJR42dDc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.026692553947934e-06, "legacy": true, "legacyId": "27253", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_September_Meetup___Self_Awareness\">Discussion article for the meetup : <a href=\"/meetups/14t\">Atlanta September Meetup - Self Awareness</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 September 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Unit L, Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meetup we'll be discussing self awareness and reflection. We'll be talking about ideas discussed in the <a href=\"http://lesswrong.com/lw/1xh/living_luminously/\">Living Luminously sequence</a>.\nAs usual, there will be snacks and games as well as plenty of interesting conversation! There are cats at the location. Please park in a spot labeled visitor, as parking in the other spots runs the risk of getting towed. Hope to see you then!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_September_Meetup___Self_Awareness1\">Discussion article for the meetup : <a href=\"/meetups/14t\">Atlanta September Meetup - Self Awareness</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta September Meetup - Self Awareness", "anchor": "Discussion_article_for_the_meetup___Atlanta_September_Meetup___Self_Awareness", "level": 1}, {"title": "Discussion article for the meetup : Atlanta September Meetup - Self Awareness", "anchor": "Discussion_article_for_the_meetup___Atlanta_September_Meetup___Self_Awareness1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3Cjjem7AbmmZfBs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-23T04:00:32.057Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: The Steep Approach to Crazytown", "slug": "meetup-urbana-champaign-the-steep-approach-to-crazytown", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7iaNZ4rPyAr3eSiLo/meetup-urbana-champaign-the-steep-approach-to-crazytown", "pageUrlRelative": "/posts/7iaNZ4rPyAr3eSiLo/meetup-urbana-champaign-the-steep-approach-to-crazytown", "linkUrl": "https://www.lesswrong.com/posts/7iaNZ4rPyAr3eSiLo/meetup-urbana-champaign-the-steep-approach-to-crazytown", "postedAtFormatted": "Tuesday, September 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20The%20Steep%20Approach%20to%20Crazytown&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20The%20Steep%20Approach%20to%20Crazytown%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7iaNZ4rPyAr3eSiLo%2Fmeetup-urbana-champaign-the-steep-approach-to-crazytown%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20The%20Steep%20Approach%20to%20Crazytown%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7iaNZ4rPyAr3eSiLo%2Fmeetup-urbana-champaign-the-steep-approach-to-crazytown", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7iaNZ4rPyAr3eSiLo%2Fmeetup-urbana-champaign-the-steep-approach-to-crazytown", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14u'>Urbana-Champaign: The Steep Approach to Crazytown</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 September 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">206 S. Cedar, 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As was suggested last week, we'll be checking out some exercises from the Scientology community. Making fun of them is, of course, mandatory, but the goal is to find some that are interesting enough to try and then trying them. I don't see how this could go wrong.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14u'>Urbana-Champaign: The Steep Approach to Crazytown</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7iaNZ4rPyAr3eSiLo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 2.027018698895382e-06, "legacy": true, "legacyId": "27254", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__The_Steep_Approach_to_Crazytown\">Discussion article for the meetup : <a href=\"/meetups/14u\">Urbana-Champaign: The Steep Approach to Crazytown</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 September 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">206 S. Cedar, 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As was suggested last week, we'll be checking out some exercises from the Scientology community. Making fun of them is, of course, mandatory, but the goal is to find some that are interesting enough to try and then trying them. I don't see how this could go wrong.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__The_Steep_Approach_to_Crazytown1\">Discussion article for the meetup : <a href=\"/meetups/14u\">Urbana-Champaign: The Steep Approach to Crazytown</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: The Steep Approach to Crazytown", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__The_Steep_Approach_to_Crazytown", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: The Steep Approach to Crazytown", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__The_Steep_Approach_to_Crazytown1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-23T20:49:16.700Z", "modifiedAt": null, "url": null, "title": "Beyond type 1 vs. type 2 processing: the tri-dimensional way (link)", "slug": "beyond-type-1-vs-type-2-processing-the-tri-dimensional-way", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:59.565Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RomeoStevens", "createdAt": "2011-10-28T20:59:30.426Z", "isAdmin": false, "displayName": "RomeoStevens"}, "userId": "5ZpAE3i54eEhMp2ib", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RewMgZvJ8RgAnwpM8/beyond-type-1-vs-type-2-processing-the-tri-dimensional-way", "pageUrlRelative": "/posts/RewMgZvJ8RgAnwpM8/beyond-type-1-vs-type-2-processing-the-tri-dimensional-way", "linkUrl": "https://www.lesswrong.com/posts/RewMgZvJ8RgAnwpM8/beyond-type-1-vs-type-2-processing-the-tri-dimensional-way", "postedAtFormatted": "Tuesday, September 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Beyond%20type%201%20vs.%20type%202%20processing%3A%20the%20tri-dimensional%20way%20(link)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeyond%20type%201%20vs.%20type%202%20processing%3A%20the%20tri-dimensional%20way%20(link)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRewMgZvJ8RgAnwpM8%2Fbeyond-type-1-vs-type-2-processing-the-tri-dimensional-way%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Beyond%20type%201%20vs.%20type%202%20processing%3A%20the%20tri-dimensional%20way%20(link)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRewMgZvJ8RgAnwpM8%2Fbeyond-type-1-vs-type-2-processing-the-tri-dimensional-way", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRewMgZvJ8RgAnwpM8%2Fbeyond-type-1-vs-type-2-processing-the-tri-dimensional-way", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 230, "htmlBody": "<p>The System 1/2 schema is a popular and useful meme, but it feels limiting sometimes. I found this new paper interesting:</p>\n<p>http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.00993/full</p>\n<p>I'm of two minds about this (hah!). On the one hand, it often does feel like there are sharp divides in mindspace. Something will be understood by system 2 but this understanding does not show up in behavior. I still act as if the thing is not true. Then, by some mysterious process, the thing will \"click\" and it feels like system 1 really gets it. After this the belief in the thing is reflected in behavior. On the other hand, there are many instances where it does not feel appropriate to divide particular mental habits into either system 1 or 2. Doing math, for instance, seems to strongly have factors of both. My immediate intuition is that the continuous model is more \"correct\" but that there is quite a bit of clustering in the mindspace. System 1&amp;2 would then simply be large clusters.</p>\n<p>Anyway, I'm curious about other people's impressions.</p>\n<p>One thing I'm frustrated by is that I don't have a map of proposed schemas. There have been lots of different ones proposed over the centuries, and I don't know of any place where I can find a summary of them, as well as draw links between ones that shared an intellectual lineage. Does anyone know of resources relating to this?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RewMgZvJ8RgAnwpM8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 2.0288698476645814e-06, "legacy": true, "legacyId": "27256", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-23T22:28:56.001Z", "modifiedAt": null, "url": null, "title": "Books on consciousness?", "slug": "books-on-consciousness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:44.608Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mgg", "createdAt": "2014-09-05T04:50:21.132Z", "isAdmin": false, "displayName": "mgg"}, "userId": "ji85dT8C2KsyKNj8h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sQh57GE9dNPiJELLp/books-on-consciousness", "pageUrlRelative": "/posts/sQh57GE9dNPiJELLp/books-on-consciousness", "linkUrl": "https://www.lesswrong.com/posts/sQh57GE9dNPiJELLp/books-on-consciousness", "postedAtFormatted": "Tuesday, September 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Books%20on%20consciousness%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABooks%20on%20consciousness%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsQh57GE9dNPiJELLp%2Fbooks-on-consciousness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Books%20on%20consciousness%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsQh57GE9dNPiJELLp%2Fbooks-on-consciousness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsQh57GE9dNPiJELLp%2Fbooks-on-consciousness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 119, "htmlBody": "<p>Does LW have a consensus on which books are worthwhile to read regarding consciousness? I read a small intro (Consciousness: A Very Short Introduction, Susan Blackmore, Oxford University Press), and the summary seems to be \"Consciousness is pretty damn weird and no one seems to have much of a handle on it\". As a non-technical layman, are there any useful books for me to read on the subject?</p>\n<p>(I have started reading Daniel Dennet's <em>Intuition Pumps</em>, and I'm a bit torn. He seems highly respected by good scientists, but I feel that if the book didn't have his name on it, I would be well on my way to dismissing it. Are Dennet's earlier works on consciousness a good read?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sQh57GE9dNPiJELLp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "27257", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-24T01:25:03.198Z", "modifiedAt": null, "url": null, "title": "Depth-based supercontroller objectives, take 2", "slug": "depth-based-supercontroller-objectives-take-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:00.173Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sbenthall", "createdAt": "2012-12-23T03:31:10.842Z", "isAdmin": false, "displayName": "sbenthall"}, "userId": "pbnv8yAoxSjxvEZr8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZjEwoLPzjTugeyBCF/depth-based-supercontroller-objectives-take-2", "pageUrlRelative": "/posts/ZjEwoLPzjTugeyBCF/depth-based-supercontroller-objectives-take-2", "linkUrl": "https://www.lesswrong.com/posts/ZjEwoLPzjTugeyBCF/depth-based-supercontroller-objectives-take-2", "postedAtFormatted": "Wednesday, September 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Depth-based%20supercontroller%20objectives%2C%20take%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADepth-based%20supercontroller%20objectives%2C%20take%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZjEwoLPzjTugeyBCF%2Fdepth-based-supercontroller-objectives-take-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Depth-based%20supercontroller%20objectives%2C%20take%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZjEwoLPzjTugeyBCF%2Fdepth-based-supercontroller-objectives-take-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZjEwoLPzjTugeyBCF%2Fdepth-based-supercontroller-objectives-take-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2139, "htmlBody": "<p>Thanks for all the helpful comments and discussion around <a href=\"/r/discussion/lw/kzi/proposal_use_logical_depth_relative_to_human/\">this post</a>&nbsp;about using logical depth as an objective function for a supercontroller to preserve human existence.</p>\n<p>As this is work in progress, I was a bit muddled and stood duly corrected on a number of points. I'm writing to submit a new, clarified proposal, with some comments directed at objections.</p>\n<p><strong>&sect;1.&nbsp;Proposed objective function</strong></p>\n<p>Maximize <em>g(u)</em>, where <em>u</em>&nbsp;is a description of the universe, <em>h</em>&nbsp;is a description of humanity (more on this later) at the time when the objective function is set, and G is defined as:</p>\n<p><em>g(u) =&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">D(u) - D(u/h)</span></em></p>\n<p>where <em>D(x)</em> is logical depth and <em>D(x/y)</em> is relative logical depth of <em>x</em> and <em>y</em>.</p>\n<p><strong>&sect;2.&nbsp;</strong><strong>A note on terminology</strong></p>\n<p>I don't intend to annoy by saying \"objective function\" and \"supercontroller\" rather than \"utility function\" and \"superintelligence.\" Rather, I am using this alternative language deliberately to scope the problem to a related question that is perhaps more well-defined or possibly easier to solve. If I understand correctly, \"utility function\" refers to any function, perhaps implicit, that characterizes the behavior of an agent. By \"objective function\", I mean a function explicitly coded as the objective of some optimization process, or \"controller\". I gather that a \"superintelligence\" is an agent that is better than a generic human at myriad tasks. I think this raises a ton of definitional issues, so instead I will talk about a \"supercontroller\", which is just arbitrarily good at achieving its objective.</p>\n<p>Saying that a supercontroller is arbitrarily good at achieving an objective is tricky, since it's possible to define functions that are impossible to solve. For example, objective functions that involve incomputable functions like the <a href=\"http://en.wikipedia.org/wiki/Halting_problem\">Halting Problem</a>. In general my sense is that computational complexity is overlooked within the \"superintelligence\" discourse, which is jarring for me since I come from a more traditional AI/machine learning background where computational complexity is at the heart of everything. I gather that it's assumed that a superintelligence will have such effectively unbounded access to computational resources due to its self-modification that complexity is not a limiting factor. It is in that spirit that I propose an incomputable objective function here. My intention is to get past the function definition problem so that work can then proceed to questions of safe approximation and implementation.</p>\n<p><strong>&sect;3.&nbsp;</strong><strong>Response to general objections</strong></p>\n<p>Apparently this community harbors a lot of skepticism towards an easy solution to the problem of giving a supercontroller an objective function that won't kill everybody or create a dystopia. If I am following the thread of argument correctly, much of this skepticism comes from Yudkowsky, for example <a href=\"/lw/lq/fake_utility_functions/\">here</a>. The problem, he asserts, is that superintelligence that does not truly understand human morality could result in a \"hyperexistential catastrophe,\" a fate worse than death.</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">Leave out just&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">one</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;of these values from a superintelligence, and even if you successfully include&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">every other</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;value, you could end up with a&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" href=\"http://www.nickbostrom.com/existential/risks.html\">hyperexistential catastrophe</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">, a fate worse than death.&nbsp; If there's a superintelligence that wants everything for us that we want for ourselves,&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">except</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;the human values relating to controlling your own life and achieving your own goals, that's one of the oldest dystopias in the&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" href=\"/lw/k9/the_logical_fallacy_of_generalization_from/\">book</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">.&nbsp; (Jack Williamson's \"With Folded Hands\", in this case.)</span></p>\n</blockquote>\n<p>After a long discussion of the potential dangers of a poorly written superintelligence utility function, he concludes:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">In the end, the only process that reliably&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" href=\"/lw/la/truly_part_of_you/\">regenerates</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;all the local decisions you would make given your morality, is your morality.&nbsp; Anything else - any attempt to substitute instrumental means for terminal ends - ends up&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" href=\"/lw/le/lost_purposes/\">losing purpose</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;and requiring&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" href=\"/lw/l9/artificial_addition/\">an infinite number of patches</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;because the system doesn't&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" href=\"/lw/la/truly_part_of_you/\">contain the source</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;of the instructions you're giving it.&nbsp; You shouldn't expect to be able to compress a human morality down to a simple utility function, any more than you should expect to compress a large computer file down to 10 bits.</span></p>\n</blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">The astute reader will anticipate my responses to this objection. There are two.</span></p>\n<p><strong>&sect;3.1&nbsp;</strong><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">The first is that we can analytically separate the problem of existential catastrophe from hyperexistential catastrophe. Assuming the supercontroller is really very super, then over all possible objective functions <em>F</em>, we can partition the set into those that kill all humans and those that don't. Let's call the set of humanity preserving functions <em>E</em>. Hyperexistentially catastrophic functions will be members of E but still undesirable. Let's hope that either supercontrollers are impossible or that there is some non-empty subset of E that is both existentially and hyperexistentially favorable. These functions don't have to be utopian. You might stub your toe now and then. They just have to be alright. Let's call this subset <em>A</em>.</span></p>\n<p><em>A</em> is a subset of <em>E</em> is a subset of <em>F</em>.</p>\n<p>I am claiming that <em>g</em> is in <em>E</em>, and that's pretty good place to start if we are looking for something in <em>A.</em></p>\n<p><strong>&sect;3.2&nbsp;</strong>The second response to Yudkowksy's general \"source code\" objection--that a function that does not contain the source of the instructions given to it will require an infinite number of patches--is that the function <em>g</em>&nbsp;does contain the source of the instructions given to it. That is what the <em>h</em>&nbsp;term is for. Hence, this is not grounds to object to this function.</p>\n<p>This is perhaps easy to miss, because the term <em>h</em>&nbsp;has been barely defined. To the extent that it has, it is a <em>description</em>&nbsp;of humanity. To be concrete, let's imagine that it is a representation of the physical state of humanity including its biological makeup--DNA and neural architecture--as well as its cultural and technological accomplishments. Perhaps it contains the entire record of human history up until now. Who knows--we are talking about asymptotic behaviors here.</p>\n<p>The point is--and I think you'll agree with me if you share certain basic naturalistic assumptions about ethics--that while not explicitly coding for something like \"what's the culminating point of collective, coherent, extrapolated values?\", this description accomplishes the more modest task of including in it, somewhere, the an encoding of those values as they are now. We might disagree about which things represent values and which represent noise or plain fact. But if we do a thorough job we'll at least make sure we've got them all.</p>\n<p>This is a hack, perhaps. But personally I treat the problem of machine ethics with a certain amount of <a href=\"/r/discussion/lw/l04/everybodys_talking_about_machine_ethics/\">urgency</a>&nbsp;and so am willing to accept something less than perfect.</p>\n<p><strong>&sect;4. So why depth?</strong></p>\n<p>I am prepared to provide a mathematical treatment of the choice of <em>g</em>&nbsp;as an objective function in another post. Since I expect it gets a little hairy in the specifics, I am trying to troubleshoot it intuitively first to raise the chance that it is worth the effort. For now, I will try to do a better job of explaining the idea in prose than I did in the last post.</p>\n<p><strong>&sect;4.1&nbsp;</strong>Assume that change in the universe can be modeled as a computational process, or a number of interacting processes. A process is the operation of general laws of change--modeled as a kind of universal Turing Machine--that starts with some initial set of data--the program--and then operates on that data, manipulating it over discrete moments in time. For any particular program, that process may halt--outputing some data--or it may not. Of particular interest are those programs that basically encode no information directly about what their outcome is. These are the incompressible programs.</p>\n<p>Let's look at the representation <em>h</em>. Given all of the incompressible programs <em>P</em>, only some of them will output <em>h</em>. Among these programs are all the incompressible programs that include <em>h</em>&nbsp;at any time stage in its total computational progression, modified with something like, \"At time step <em>t</em>, stop here and output whatever you've got!\". Let's call the set of all programs from processes that include <em>h</em> in their computational path H. H is a subset of P.</p>\n<p>What logical depth does is abstract over all processes that output a string. <em>D(h) </em>is (roughly) the minimum amount of time, over all <em>p</em> in <em>H, </em>for <em>p </em>to output <em>h</em>.</p>\n<p><em>Relative</em>&nbsp;logical depth goes a step further and looks at processes that start with both some incompressible program and some other potentially much more compressible string as input. So let's look at the universe at some future point, <em>u</em>, and the value <em>D(u/h)</em>.</p>\n<p><strong>&sect;4.2&nbsp;</strong>Just as an aside to try to get intuitions on the same page: If the <em>D(u/h) &lt; D(h)</em>, then something has gone very wrong, because the universe is incredibly vast and humanity is a rather small part of ti. Even if the only process that created the universe was something in the human imagination (!) this change to the universe would mean that we'd have lost something that the processes that created the human present had worked to create. This is bad news.</p>\n<p>The intuition here is that as time goes forward, it would be good if the depth of the universe also went up. Time is computation. A supercontroller that tries to minimize depth will be trying to stop time and that would be very bizarre indeed.</p>\n<p><strong>&sect;4.3 </strong>The intuition I'm trying to sell you on is that when we talk about carrying about human existence, i.e. when trying to find a function that is in <em>E</em>, we are concerned with the continuation of the processes that have resulted in humanity at any particular time. A description of humanity is just the particular state at a point in time of one or more computational&nbsp;<em>processes </em>which are human <em>life</em>. Some of these processes are the processes of human valuation and the extrapolation of those values. You might agree with me that <em>CEV is in H</em>.<em>&nbsp;</em></p>\n<p><strong>&sect;4.4 </strong>So consider the supercontroller's choice of two possible future timelines, Q&nbsp;and <em>R</em>. Future <em>Q</em>&nbsp;looks like taking the processes of <em>H</em>&nbsp;and removing some of the 'stop and print here' clauses, and letting them run for another couple thousand years, maybe accelerating them computationally. Future <em>R</em>&nbsp;looks like something very alien. The surface of earth is covered in geometric crystal formations that maximize the solar-powered production of grey goo, which is spreading throughout the galaxy at a fast rate. The difference is that the supercontroller did something different in the two timelines.&nbsp;</p>\n<p>We can, for either of these timelines, pick a particular logical depth, say <em>c, </em>and slice the timelines at points <em>q</em> and <em>r </em>respectively such that <em>D(q) = D(r) = c.</em></p>\n<p>Recall our objective function is to maximize&nbsp;<em>g(u) =&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">D(u) - D(u/h).</span></em></p>\n<p>Which will be higher, <em>g(q) or g(r)</em>?</p>\n<p>The <em>D(u)</em> term is the same for each. So we are interested in maximizing the value of&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\"><em>- D(u/h)</em>, which is the same as minimizing&nbsp;</span><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\"><em>D(u/h)</em>--the depth relative to humanity.</span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">By assumption, the state of the universe at <em>r </em>has overwritten all the work done by the processes of human life. Culture, thought, human DNA, human values, etc. have been stripped to their functional carbon and hydrogen atoms and everything now just optimizes for paperclip manufacturing or whatever. <em>D(u/r) = D(u)</em>. Indeed anywhere along timeline <em>R</em>&nbsp;where the supercontroller has decided to optimize for computational power at the expense of existing human processes, <em>g(r)</em>&nbsp;is going to be dropping closer to zero.</span></p>\n<p>Compare with <em>D(q/h)</em>. Since <em>q&nbsp;</em>is deep, we know some processes have been continuing to run. By assumption, the processes that have been running in <em>Q</em>&nbsp;are the same ones that have resulted in present-day humanity, only continued. The minimum time way to get to <em>q</em>&nbsp;will be to pick up those processes where they left off and continue to compute them. Hence, <em>q</em>&nbsp;will be shallow relative to <em>h</em>. <em>D(q/h)</em>&nbsp;will be significantly lower than <em>D(q) </em>and so be favored by objective function <em>g</em>.</p>\n<p><strong>&sect;5 But why optimize?</strong></p>\n<p>You may object: if the function depends on depth measure <em>D</em>&nbsp;which only depends on the process that produces <em>h</em>&nbsp;and <em>q</em>&nbsp;with minimal computation, maybe this will select for something inessential about humanity and mess things up. Depending on how you want to slice it, this function may fall outside of the existentially preserving set <em>E</em>&nbsp;let alone the hyperexistentially acceptable set <em>A</em>. Or suppose you are really interested only in the continuation of a very specific process, such as coherent extrapolated volition (here, CEV).</p>\n<p>To this I propose a variation on the depth measure, <em>D*</em>, which I believe was also proposed by Bennett (though I have to look that up to be sure.) Rather than taking the minimum computational time required to produce some representation, <em>D*</em> is a weighted average over the computational time is takes to produce the string. The weights can reflect something like the Kolmogorov complexity of the initial programs/processes. You can think of this as an analog of Solomonoff induction, but through time instead of space.</p>\n<p>Consider the supercontroller that optimizes for&nbsp;<em>g*(u) =&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">D*(u) - D*(u/h).</span></em></p>\n<p>Suppose your favorite ethical process, such as CEV, is in H. <em>h</em>&nbsp;encodes for some amount of computational progress on the path towards completed CEV. By the same reasoning as above, future universes that continue from <em>h</em>&nbsp;on the computational path of CEV will be favored, albeit only marginally, over futures that are insensitive to CEV.</p>\n<p>This is perhaps not enough consolation to those very invested in CEV, but it is something. The processes of humanity continue to exist, CEV among them. I maintain that this is pretty good. I.e. that <em>g* is in A</em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZjEwoLPzjTugeyBCF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 1, "extendedScore": null, "score": 2.029376443540115e-06, "legacy": true, "legacyId": "27251", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Thanks for all the helpful comments and discussion around <a href=\"/r/discussion/lw/kzi/proposal_use_logical_depth_relative_to_human/\">this post</a>&nbsp;about using logical depth as an objective function for a supercontroller to preserve human existence.</p>\n<p>As this is work in progress, I was a bit muddled and stood duly corrected on a number of points. I'm writing to submit a new, clarified proposal, with some comments directed at objections.</p>\n<p><strong id=\"_1__Proposed_objective_function\">\u00a71.&nbsp;Proposed objective function</strong></p>\n<p>Maximize <em>g(u)</em>, where <em>u</em>&nbsp;is a description of the universe, <em>h</em>&nbsp;is a description of humanity (more on this later) at the time when the objective function is set, and G is defined as:</p>\n<p><em>g(u) =&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">D(u) - D(u/h)</span></em></p>\n<p>where <em>D(x)</em> is logical depth and <em>D(x/y)</em> is relative logical depth of <em>x</em> and <em>y</em>.</p>\n<p><strong>\u00a72.&nbsp;</strong><strong>A note on terminology</strong></p>\n<p>I don't intend to annoy by saying \"objective function\" and \"supercontroller\" rather than \"utility function\" and \"superintelligence.\" Rather, I am using this alternative language deliberately to scope the problem to a related question that is perhaps more well-defined or possibly easier to solve. If I understand correctly, \"utility function\" refers to any function, perhaps implicit, that characterizes the behavior of an agent. By \"objective function\", I mean a function explicitly coded as the objective of some optimization process, or \"controller\". I gather that a \"superintelligence\" is an agent that is better than a generic human at myriad tasks. I think this raises a ton of definitional issues, so instead I will talk about a \"supercontroller\", which is just arbitrarily good at achieving its objective.</p>\n<p>Saying that a supercontroller is arbitrarily good at achieving an objective is tricky, since it's possible to define functions that are impossible to solve. For example, objective functions that involve incomputable functions like the <a href=\"http://en.wikipedia.org/wiki/Halting_problem\">Halting Problem</a>. In general my sense is that computational complexity is overlooked within the \"superintelligence\" discourse, which is jarring for me since I come from a more traditional AI/machine learning background where computational complexity is at the heart of everything. I gather that it's assumed that a superintelligence will have such effectively unbounded access to computational resources due to its self-modification that complexity is not a limiting factor. It is in that spirit that I propose an incomputable objective function here. My intention is to get past the function definition problem so that work can then proceed to questions of safe approximation and implementation.</p>\n<p><strong>\u00a73.&nbsp;</strong><strong>Response to general objections</strong></p>\n<p>Apparently this community harbors a lot of skepticism towards an easy solution to the problem of giving a supercontroller an objective function that won't kill everybody or create a dystopia. If I am following the thread of argument correctly, much of this skepticism comes from Yudkowsky, for example <a href=\"/lw/lq/fake_utility_functions/\">here</a>. The problem, he asserts, is that superintelligence that does not truly understand human morality could result in a \"hyperexistential catastrophe,\" a fate worse than death.</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">Leave out just&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">one</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;of these values from a superintelligence, and even if you successfully include&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">every other</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;value, you could end up with a&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" href=\"http://www.nickbostrom.com/existential/risks.html\">hyperexistential catastrophe</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">, a fate worse than death.&nbsp; If there's a superintelligence that wants everything for us that we want for ourselves,&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">except</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;the human values relating to controlling your own life and achieving your own goals, that's one of the oldest dystopias in the&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" href=\"/lw/k9/the_logical_fallacy_of_generalization_from/\">book</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">.&nbsp; (Jack Williamson's \"With Folded Hands\", in this case.)</span></p>\n</blockquote>\n<p>After a long discussion of the potential dangers of a poorly written superintelligence utility function, he concludes:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">In the end, the only process that reliably&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" href=\"/lw/la/truly_part_of_you/\">regenerates</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;all the local decisions you would make given your morality, is your morality.&nbsp; Anything else - any attempt to substitute instrumental means for terminal ends - ends up&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" href=\"/lw/le/lost_purposes/\">losing purpose</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;and requiring&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" href=\"/lw/l9/artificial_addition/\">an infinite number of patches</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;because the system doesn't&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" href=\"/lw/la/truly_part_of_you/\">contain the source</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;of the instructions you're giving it.&nbsp; You shouldn't expect to be able to compress a human morality down to a simple utility function, any more than you should expect to compress a large computer file down to 10 bits.</span></p>\n</blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">The astute reader will anticipate my responses to this objection. There are two.</span></p>\n<p><strong>\u00a73.1&nbsp;</strong><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">The first is that we can analytically separate the problem of existential catastrophe from hyperexistential catastrophe. Assuming the supercontroller is really very super, then over all possible objective functions <em>F</em>, we can partition the set into those that kill all humans and those that don't. Let's call the set of humanity preserving functions <em>E</em>. Hyperexistentially catastrophic functions will be members of E but still undesirable. Let's hope that either supercontrollers are impossible or that there is some non-empty subset of E that is both existentially and hyperexistentially favorable. These functions don't have to be utopian. You might stub your toe now and then. They just have to be alright. Let's call this subset <em>A</em>.</span></p>\n<p><em>A</em> is a subset of <em>E</em> is a subset of <em>F</em>.</p>\n<p>I am claiming that <em>g</em> is in <em>E</em>, and that's pretty good place to start if we are looking for something in <em>A.</em></p>\n<p><strong>\u00a73.2&nbsp;</strong>The second response to Yudkowksy's general \"source code\" objection--that a function that does not contain the source of the instructions given to it will require an infinite number of patches--is that the function <em>g</em>&nbsp;does contain the source of the instructions given to it. That is what the <em>h</em>&nbsp;term is for. Hence, this is not grounds to object to this function.</p>\n<p>This is perhaps easy to miss, because the term <em>h</em>&nbsp;has been barely defined. To the extent that it has, it is a <em>description</em>&nbsp;of humanity. To be concrete, let's imagine that it is a representation of the physical state of humanity including its biological makeup--DNA and neural architecture--as well as its cultural and technological accomplishments. Perhaps it contains the entire record of human history up until now. Who knows--we are talking about asymptotic behaviors here.</p>\n<p>The point is--and I think you'll agree with me if you share certain basic naturalistic assumptions about ethics--that while not explicitly coding for something like \"what's the culminating point of collective, coherent, extrapolated values?\", this description accomplishes the more modest task of including in it, somewhere, the an encoding of those values as they are now. We might disagree about which things represent values and which represent noise or plain fact. But if we do a thorough job we'll at least make sure we've got them all.</p>\n<p>This is a hack, perhaps. But personally I treat the problem of machine ethics with a certain amount of <a href=\"/r/discussion/lw/l04/everybodys_talking_about_machine_ethics/\">urgency</a>&nbsp;and so am willing to accept something less than perfect.</p>\n<p><strong id=\"_4__So_why_depth_\">\u00a74. So why depth?</strong></p>\n<p>I am prepared to provide a mathematical treatment of the choice of <em>g</em>&nbsp;as an objective function in another post. Since I expect it gets a little hairy in the specifics, I am trying to troubleshoot it intuitively first to raise the chance that it is worth the effort. For now, I will try to do a better job of explaining the idea in prose than I did in the last post.</p>\n<p><strong>\u00a74.1&nbsp;</strong>Assume that change in the universe can be modeled as a computational process, or a number of interacting processes. A process is the operation of general laws of change--modeled as a kind of universal Turing Machine--that starts with some initial set of data--the program--and then operates on that data, manipulating it over discrete moments in time. For any particular program, that process may halt--outputing some data--or it may not. Of particular interest are those programs that basically encode no information directly about what their outcome is. These are the incompressible programs.</p>\n<p>Let's look at the representation <em>h</em>. Given all of the incompressible programs <em>P</em>, only some of them will output <em>h</em>. Among these programs are all the incompressible programs that include <em>h</em>&nbsp;at any time stage in its total computational progression, modified with something like, \"At time step <em>t</em>, stop here and output whatever you've got!\". Let's call the set of all programs from processes that include <em>h</em> in their computational path H. H is a subset of P.</p>\n<p>What logical depth does is abstract over all processes that output a string. <em>D(h) </em>is (roughly) the minimum amount of time, over all <em>p</em> in <em>H, </em>for <em>p </em>to output <em>h</em>.</p>\n<p><em>Relative</em>&nbsp;logical depth goes a step further and looks at processes that start with both some incompressible program and some other potentially much more compressible string as input. So let's look at the universe at some future point, <em>u</em>, and the value <em>D(u/h)</em>.</p>\n<p><strong>\u00a74.2&nbsp;</strong>Just as an aside to try to get intuitions on the same page: If the <em>D(u/h) &lt; D(h)</em>, then something has gone very wrong, because the universe is incredibly vast and humanity is a rather small part of ti. Even if the only process that created the universe was something in the human imagination (!) this change to the universe would mean that we'd have lost something that the processes that created the human present had worked to create. This is bad news.</p>\n<p>The intuition here is that as time goes forward, it would be good if the depth of the universe also went up. Time is computation. A supercontroller that tries to minimize depth will be trying to stop time and that would be very bizarre indeed.</p>\n<p><strong>\u00a74.3 </strong>The intuition I'm trying to sell you on is that when we talk about carrying about human existence, i.e. when trying to find a function that is in <em>E</em>, we are concerned with the continuation of the processes that have resulted in humanity at any particular time. A description of humanity is just the particular state at a point in time of one or more computational&nbsp;<em>processes </em>which are human <em>life</em>. Some of these processes are the processes of human valuation and the extrapolation of those values. You might agree with me that <em>CEV is in H</em>.<em>&nbsp;</em></p>\n<p><strong>\u00a74.4 </strong>So consider the supercontroller's choice of two possible future timelines, Q&nbsp;and <em>R</em>. Future <em>Q</em>&nbsp;looks like taking the processes of <em>H</em>&nbsp;and removing some of the 'stop and print here' clauses, and letting them run for another couple thousand years, maybe accelerating them computationally. Future <em>R</em>&nbsp;looks like something very alien. The surface of earth is covered in geometric crystal formations that maximize the solar-powered production of grey goo, which is spreading throughout the galaxy at a fast rate. The difference is that the supercontroller did something different in the two timelines.&nbsp;</p>\n<p>We can, for either of these timelines, pick a particular logical depth, say <em>c, </em>and slice the timelines at points <em>q</em> and <em>r </em>respectively such that <em>D(q) = D(r) = c.</em></p>\n<p>Recall our objective function is to maximize&nbsp;<em>g(u) =&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">D(u) - D(u/h).</span></em></p>\n<p>Which will be higher, <em>g(q) or g(r)</em>?</p>\n<p>The <em>D(u)</em> term is the same for each. So we are interested in maximizing the value of&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\"><em>- D(u/h)</em>, which is the same as minimizing&nbsp;</span><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\"><em>D(u/h)</em>--the depth relative to humanity.</span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">By assumption, the state of the universe at <em>r </em>has overwritten all the work done by the processes of human life. Culture, thought, human DNA, human values, etc. have been stripped to their functional carbon and hydrogen atoms and everything now just optimizes for paperclip manufacturing or whatever. <em>D(u/r) = D(u)</em>. Indeed anywhere along timeline <em>R</em>&nbsp;where the supercontroller has decided to optimize for computational power at the expense of existing human processes, <em>g(r)</em>&nbsp;is going to be dropping closer to zero.</span></p>\n<p>Compare with <em>D(q/h)</em>. Since <em>q&nbsp;</em>is deep, we know some processes have been continuing to run. By assumption, the processes that have been running in <em>Q</em>&nbsp;are the same ones that have resulted in present-day humanity, only continued. The minimum time way to get to <em>q</em>&nbsp;will be to pick up those processes where they left off and continue to compute them. Hence, <em>q</em>&nbsp;will be shallow relative to <em>h</em>. <em>D(q/h)</em>&nbsp;will be significantly lower than <em>D(q) </em>and so be favored by objective function <em>g</em>.</p>\n<p><strong id=\"_5_But_why_optimize_\">\u00a75 But why optimize?</strong></p>\n<p>You may object: if the function depends on depth measure <em>D</em>&nbsp;which only depends on the process that produces <em>h</em>&nbsp;and <em>q</em>&nbsp;with minimal computation, maybe this will select for something inessential about humanity and mess things up. Depending on how you want to slice it, this function may fall outside of the existentially preserving set <em>E</em>&nbsp;let alone the hyperexistentially acceptable set <em>A</em>. Or suppose you are really interested only in the continuation of a very specific process, such as coherent extrapolated volition (here, CEV).</p>\n<p>To this I propose a variation on the depth measure, <em>D*</em>, which I believe was also proposed by Bennett (though I have to look that up to be sure.) Rather than taking the minimum computational time required to produce some representation, <em>D*</em> is a weighted average over the computational time is takes to produce the string. The weights can reflect something like the Kolmogorov complexity of the initial programs/processes. You can think of this as an analog of Solomonoff induction, but through time instead of space.</p>\n<p>Consider the supercontroller that optimizes for&nbsp;<em>g*(u) =&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">D*(u) - D*(u/h).</span></em></p>\n<p>Suppose your favorite ethical process, such as CEV, is in H. <em>h</em>&nbsp;encodes for some amount of computational progress on the path towards completed CEV. By the same reasoning as above, future universes that continue from <em>h</em>&nbsp;on the computational path of CEV will be favored, albeit only marginally, over futures that are insensitive to CEV.</p>\n<p>This is perhaps not enough consolation to those very invested in CEV, but it is something. The processes of humanity continue to exist, CEV among them. I maintain that this is pretty good. I.e. that <em>g* is in A</em>.</p>", "sections": [{"title": "\u00a71.\u00a0Proposed objective function", "anchor": "_1__Proposed_objective_function", "level": 1}, {"title": "\u00a74. So why depth?", "anchor": "_4__So_why_depth_", "level": 1}, {"title": "\u00a75 But why optimize?", "anchor": "_5_But_why_optimize_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "24 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XeMB6F4wB2rxNXjXF", "NnohDYHNnKDtbiMyp", "rHBdcHGLJ7KvLJQPk", "fg9fXrHpeaDD6pEPL", "sP2Hg6uPwpfp3jZJN", "YhgjmCxcQXixStWMC", "JtbZ3SMYRdiyHHjYt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-24T08:34:04.056Z", "modifiedAt": null, "url": null, "title": "Meetup : Hasselt Meetup: Brussels moves to Hasselt this month!", "slug": "meetup-hasselt-meetup-brussels-moves-to-hasselt-this-month", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uWfQYgEWzcD8CFKiz/meetup-hasselt-meetup-brussels-moves-to-hasselt-this-month", "pageUrlRelative": "/posts/uWfQYgEWzcD8CFKiz/meetup-hasselt-meetup-brussels-moves-to-hasselt-this-month", "linkUrl": "https://www.lesswrong.com/posts/uWfQYgEWzcD8CFKiz/meetup-hasselt-meetup-brussels-moves-to-hasselt-this-month", "postedAtFormatted": "Wednesday, September 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Hasselt%20Meetup%3A%20Brussels%20moves%20to%20Hasselt%20this%20month!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Hasselt%20Meetup%3A%20Brussels%20moves%20to%20Hasselt%20this%20month!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuWfQYgEWzcD8CFKiz%2Fmeetup-hasselt-meetup-brussels-moves-to-hasselt-this-month%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Hasselt%20Meetup%3A%20Brussels%20moves%20to%20Hasselt%20this%20month!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuWfQYgEWzcD8CFKiz%2Fmeetup-hasselt-meetup-brussels-moves-to-hasselt-this-month", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuWfQYgEWzcD8CFKiz%2Fmeetup-hasselt-meetup-brussels-moves-to-hasselt-this-month", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14v'>Hasselt Meetup: Brussels moves to Hasselt this month!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 October 2014 01:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">STATIONSPLEIN 2 3500 Hasselt</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello, everyone! This is the first LW meetup in Hasselt. Normally we meet in Brussels but we're changing it up this month to see if we get some new people. If you live in Limburg (or anywhere else, really) and thought Brussels was to far to make every month, come and have a look.\nThis is just a once off to see what happens, we'll be meeting in Brussels again next month.\nWe'll be meeting at 1pm at the Stationsbuffet cafe which is in the same building as the train station itself, you can't miss it. Depending on the weather we might go somewhere else after but we'll stay there at least until 2pm to make sure everyone interested has time to arrive. If you want to arrive later then that, send me a message and we'll work something out.\nHope to see you all there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14v'>Hasselt Meetup: Brussels moves to Hasselt this month!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uWfQYgEWzcD8CFKiz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 2.030164983794037e-06, "legacy": true, "legacyId": "27259", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Hasselt_Meetup__Brussels_moves_to_Hasselt_this_month_\">Discussion article for the meetup : <a href=\"/meetups/14v\">Hasselt Meetup: Brussels moves to Hasselt this month!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 October 2014 01:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">STATIONSPLEIN 2 3500 Hasselt</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello, everyone! This is the first LW meetup in Hasselt. Normally we meet in Brussels but we're changing it up this month to see if we get some new people. If you live in Limburg (or anywhere else, really) and thought Brussels was to far to make every month, come and have a look.\nThis is just a once off to see what happens, we'll be meeting in Brussels again next month.\nWe'll be meeting at 1pm at the Stationsbuffet cafe which is in the same building as the train station itself, you can't miss it. Depending on the weather we might go somewhere else after but we'll stay there at least until 2pm to make sure everyone interested has time to arrive. If you want to arrive later then that, send me a message and we'll work something out.\nHope to see you all there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Hasselt_Meetup__Brussels_moves_to_Hasselt_this_month_1\">Discussion article for the meetup : <a href=\"/meetups/14v\">Hasselt Meetup: Brussels moves to Hasselt this month!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Hasselt Meetup: Brussels moves to Hasselt this month!", "anchor": "Discussion_article_for_the_meetup___Hasselt_Meetup__Brussels_moves_to_Hasselt_this_month_", "level": 1}, {"title": "Discussion article for the meetup : Hasselt Meetup: Brussels moves to Hasselt this month!", "anchor": "Discussion_article_for_the_meetup___Hasselt_Meetup__Brussels_moves_to_Hasselt_this_month_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-24T10:47:39.562Z", "modifiedAt": null, "url": null, "title": "Simulation argument meets decision theory", "slug": "simulation-argument-meets-decision-theory", "viewCount": null, "lastCommentedAt": "2018-07-27T06:12:14.736Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pallas", "createdAt": "2013-11-18T00:30:55.630Z", "isAdmin": false, "displayName": "pallas"}, "userId": "LFWd5Q2mptz7d5MxZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xjWbChCdqSSQyMNnr/simulation-argument-meets-decision-theory", "pageUrlRelative": "/posts/xjWbChCdqSSQyMNnr/simulation-argument-meets-decision-theory", "linkUrl": "https://www.lesswrong.com/posts/xjWbChCdqSSQyMNnr/simulation-argument-meets-decision-theory", "postedAtFormatted": "Wednesday, September 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Simulation%20argument%20meets%20decision%20theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASimulation%20argument%20meets%20decision%20theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxjWbChCdqSSQyMNnr%2Fsimulation-argument-meets-decision-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Simulation%20argument%20meets%20decision%20theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxjWbChCdqSSQyMNnr%2Fsimulation-argument-meets-decision-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxjWbChCdqSSQyMNnr%2Fsimulation-argument-meets-decision-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 335, "htmlBody": "<p>Person X stands in front of a sophisticated computer playing the decision game Y which allows for the following options: either press the button \"sim\" or \"not sim\". If she presses \"sim\", the computer will simulate X*_1, X*_2, ..., X*_1000 which are a thousand identical copies of X. All of them will face the game Y* which - from the standpoint of each X* - is indistinguishable from Y. But the simulated computers in the games Y* don't run simulations. Additionally, we know that if X presses \"sim\" she receives a utility of 1, but \"not sim\" would only lead to 0.9. If X*_i (for i=1,2,3..1000) &nbsp;presses \"sim\" she receives 0.2, with \"not sim\" 0.1. For each agent it is true that she does not gain anything from the utility of another agent despite the fact she and the other agents are identical! Since all the agents are identical egoists facing the apparently same situation, all of them will take the same action. &nbsp;</p>\n<p>Now the game starts. We face a computer and know all the above. We don't know whether we are X or any of the X*'s, should we now press \"sim\" or \"not sim\"?</p>\n<p>&nbsp;</p>\n<p>EDIT: It seems to me that \"identical\" agents with \"independent\" utility functions were a clumsy set up for the above question, especially since one can interpret it as a contradiction. Hence, it might be better to switch to identical egoists whereas each agent only cares about her receiving money (linear monetary value function). If X presses \"sim\" she will be given 10$ (else 9$) in the end of the game; each X* who presses \"sim\" receives 2$ (else 1$), respectively. Each agent in the game wants to maximize the expected monetary value they themselves will hold in their own hand after the game. So, intrinsically, they don't care how much money the other copies make.&nbsp;<br />To spice things up: What if the simulation will only happen a year later? Are we then able to \"choose\" which year it is?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xjWbChCdqSSQyMNnr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 19, "extendedScore": null, "score": 0.000106, "legacy": true, "legacyId": "27260", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-24T18:41:56.356Z", "modifiedAt": null, "url": null, "title": "Newcomblike problems are the norm", "slug": "newcomblike-problems-are-the-norm", "viewCount": null, "lastCommentedAt": "2019-12-05T23:16:14.290Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/puutBJLWbg2sXpFbu/newcomblike-problems-are-the-norm", "pageUrlRelative": "/posts/puutBJLWbg2sXpFbu/newcomblike-problems-are-the-norm", "linkUrl": "https://www.lesswrong.com/posts/puutBJLWbg2sXpFbu/newcomblike-problems-are-the-norm", "postedAtFormatted": "Wednesday, September 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Newcomblike%20problems%20are%20the%20norm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANewcomblike%20problems%20are%20the%20norm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpuutBJLWbg2sXpFbu%2Fnewcomblike-problems-are-the-norm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Newcomblike%20problems%20are%20the%20norm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpuutBJLWbg2sXpFbu%2Fnewcomblike-problems-are-the-norm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpuutBJLWbg2sXpFbu%2Fnewcomblike-problems-are-the-norm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2242, "htmlBody": "<p><em>This is <a href=\"http://mindingourway.com/newcomblike-problems-are-the-norm/\">crossposted</a> from <a href=\"http://mindingourway.com\">my blog</a>. In this post, I discuss how Newcomblike situations are common among humans in the real world. The intended audience of my blog is wider than the readerbase of LW, so the tone might seem a bit off. Nevertheless, the points made here are likely new to many.</em></p>\n<h1>1</h1>\n<p><a href=\"/lw/l0p/an_introduction_to_newcomblike_problems/\">Last time</a> we looked at Newcomblike problems, which cause <a href=\"/lw/kz9/causal_decision_theory_is_unsatisfactory/\">trouble for Causal Decision Theory (CDT)</a>, the standard decision theory used in economics, statistics, narrow AI, and many other academic fields.</p>\n<p>These Newcomblike problems may seem like strange edge case scenarios. In the Token Trade, a deterministic agent faces a perfect copy of themself, guaranteed to take the same action as they do. In Newcomb's original problem there is a perfect predictor &Omega; which knows exactly what the agent will do.</p>\n<p>Both of these examples involve some form of \"mind-reading\" and assume that the agent can be perfectly copied or perfectly predicted. In a chaotic universe, these scenarios may seem unrealistic and even downright crazy. What does it matter that CDT fails when there are perfect mind-readers? There aren't perfect mind-readers. Why do we care?</p>\n<p>The reason that we care is this: <em>Newcomblike problems are the norm.</em> Most problems that humans face in real life are \"Newcomblike\".</p>\n<p>These problems aren't limited to the domain of perfect mind-readers; rather, problems with perfect mind-readers are the domain where these problems are easiest to see. However, they arise naturally whenever an agent is in a situation where others have knowledge about its decision process via some mechanism that is not under its direct control.</p>\n<p><a id=\"more\"></a></p>\n<h1>2</h1>\n<p>Consider a CDT agent in a mirror token trade.</p>\n<p style=\"text-align: center;\"><img src=\"http://mindingourway.com/content/images/2014/Sep/token-mirror.png\" alt=\"\" /></p>\n<p>It knows that it and the opponent are generated from the same template, but it also knows that the opponent is causally distinct from it by the time it makes its choice. So it argues</p>\n<blockquote>\n<p>Either agents spawned from my template give their tokens away, or they keep their tokens. If agents spawned from my template give their tokens away, then I better keep mine so that I can take advantage of the opponent. If, instead, agents spawned from my template keep their tokens, then I had better keep mine, or otherwise I won't win any money at all.</p>\n</blockquote>\n<p>It has failed, here, to notice that it can't choose separately from \"agents spawned from my template\" because it <em>is</em> spawned from its template. (That's not to say that it doesn't get to choose what to do. Rather, it has to be able to reason about the fact that whatever it chooses, so will its opponent choose.)</p>\n<p>The reasoning flaw here is an inability to reason as if <em>past information</em> has given others <em>veridical knowledge</em> about what the agent <em>will</em> choose. This failure is particularly vivid in the mirror token trade, where the opponent is guaranteed to do <em>exactly</em> the same thing as the opponent. However, the failure occurs even if the veridical knowledge is partial or imperfect.</p>\n<h1>3</h1>\n<p>Humans trade partial, veridical, uncontrollable information about their decision procedures <em>all the time</em>.</p>\n<p>Humans automatically make <a href=\"http://en.wikipedia.org/wiki/First_impression_(psychology\">first impressions</a>) of other humans at first sight, almost instantaneously (sometimes before the person speaks, and possibly just from still images).</p>\n<p>We read each other's <a href=\"http://en.wikipedia.org/wiki/Microexpression\">microexpressions</a>, which are generally uncontrollable sources of information about our emotions.</p>\n<p>As humans, we have an impressive array of social machinery available to us that gives us gut-level, subconscious impressions of how trustworthy other people are.</p>\n<p style=\"text-align: center;\"><img src=\"http://mindingourway.com/content/images/2014/Sep/real-newcomblike.png\" alt=\"\" /></p>\n<p>Many social situations follow this pattern, and this pattern is a Newcomblike one.</p>\n<p>All these tools can be fooled, of course. First impressions are often wrong. Con-men often seem trustworthy, and honest shy people can seem unworthy of trust. However, all of this social data is at least <em>correlated</em> with the truth, and that's all we need to give CDT trouble. Remember, CDT assumes that all nodes which are <em>causally</em> disconnected from it are <em>logically</em> disconnected from it: but if someone else gained information that correlates with how you <em>actually are</em> going to act in the future, then your interactions with them may be Newcomblike.</p>\n<p>In fact, humans have a natural tendency to avoid \"non-Newcomblike\" scenarios. Human social structures use complex reputation systems. Humans seldom make big choices among themselves (who to hire, whether to become roommates, whether to make a business deal) before \"getting to know each other\". We automatically build complex social models detailing how we think our friends, family, and co-workers, make decisions.</p>\n<p>When I worked at Google, I'd occasionally need to convince half a dozen team leads to sign off on a given project. In order to do this, I'd meet with each of them in person and pitch the project slightly differently, according to my model of what parts of the project most appealed to them. I was basing my actions off of how I expected them to make decisions: I was putting them in Newcomblike scenarios.</p>\n<p>We constantly leak information about how we make decisions, and others constantly use this information. Human decision situations are Newcomblike <em>by default!</em> It's the <em>non</em>-Newcomblike problems that are simplifications and edge cases.</p>\n<p>Newcomblike problems occur whenever knowledge about what decision you <em>will</em> make leaks into the environment. The knowledge doesn't have to be 100% accurate, it just has to be correlated with your eventual actual action (in such a way that if you were going to take a different action, then you would have leaked different information). When this information is available, and others use it to make their decisions, others put you into a Newcomblike scenario.</p>\n<p>Information about what we're going to do is frequently leaking into the environment, via <a href=\"http://www.overcomingbias.com/2014/06/how-deep-the-rabbit-hole.html\">unconscious signaling</a> and uncontrolled facial expressions or even just by habit &mdash; anyone following a simple routine is likely to act predictably.</p>\n<h1>4</h1>\n<p>Most real decisions that humans face are Newcomblike whenever other humans are involved. People are automatically reading unconscious or unintentional signals and using these to build models of how you make choices, and they're using those models to make <em>their</em> choices. These are precisely the sorts of scenarios that CDT cannot represent.</p>\n<p>Of course, that's not to say that humans fail drastically on these problems. We don't: we repeatedly do well in these scenarios.</p>\n<p>Some real life Newcomblike scenarios simply don't represent games where CDT has trouble: there are many situations where others in the environment have knowledge about how you make decisions, and are using that knowledge but in a way that does not affect your payoffs enough to matter.</p>\n<p>Many more Newcomblike scenarios simply don't feel like decision problems: people present ideas to us in specific ways (depending upon their model of how we make choices) and most of us don't fret about how others would have presented us with different opportunities if we had acted in different ways.</p>\n<p>And in Newcomblike scenarios that <em>do</em> feel like decision problems, humans use a wide array of other tools in order to succeed.</p>\n<p>Roughly speaking, CDT fails when it gets stuck in the trap of \"no matter what I signaled I should do [something mean]\", which results in CDT sending off a \"mean\" signal and missing opportunities for higher payoffs. By contrast, humans tend to avoid this trap via other means: we place value on things like \"niceness\" for reputational reasons, we have intrinsic senses of \"honor\" and \"fairness\" which alter the payoffs of the game, and so on.</p>\n<p>This machinery was not necessarily \"designed\" for Newcomblike situations. Reputation systems and senses of honor are commonly attributed to humans facing repeated scenarios (thanks to living in small tribes) in the ancestral environment, and it's possible to argue that CDT handles repeated Newcomblike situations well enough. (I disagree somewhat, but this is an argument for another day.)</p>\n<p>Nevertheless, the machinery that allows us to handle repeated Newcomblike problems often seems to work in one-shot Newcomblike problems. Regardless of where the machinery came from, it still allows us to succeed in Newcomblike scenarios that we face in day-to-day life.</p>\n<p>The fact that humans easily succeed, often via tools developed for repeated situations, doesn't change the fact that many of our day-to-day interactions have Newcomblike characteristics. Whenever an agent leaks information about their decision procedure on a communication channel that they do not control (facial microexpressions, posture, cadence of voice, etc.) that person is inviting others to put them in Newcomblike settings.</p>\n<h1>5</h1>\n<p>Most of the time, humans are pretty good at handling naturally arising Newcomblike problems. Sometimes, though, the fact that you're in a Newcomblike scenario <em>does</em> matter.</p>\n<p>The games of Poker and Diplomacy are both centered around people controlling information channels that humans can't normally control. These games give particularly crisp examples of humans wrestling with situations where the environment contains leaked information about their decision-making procedure.</p>\n<p>These are only games, yes, but I'm sure that any highly ranked Poker player will tell you that the lessons of Poker extend far beyond the game board. Similarly, I expect that highly ranked Diplomacy players will tell you that Diplomacy teaches you many lessons about how people broadcast the decisions that they're going to make, and that these lessons are invaluable in everyday life.</p>\n<p>I am not a professional negotiator, but I further imagine that top-tier negotiators expend significant effort exploring how their mindsets are tied to their unconscious signals.</p>\n<p>On a more personal scale, some very simple scenarios (like whether you can get let into a farmhouse on a rainy night after your car breaks down) are somewhat \"Newcomblike\".</p>\n<p>I know at least two people who are unreliable and untrustworthy, and who blame the fact that they can't hold down jobs (and that nobody cuts them any slack) on bad luck rather than on their own demeanors. Both consistently believe that they are taking the best available action whenever they act unreliable and untrustworthy. Both brush off the idea of \"becoming a sucker\". Neither of them is capable of <em>acting</em> unreliable while <em>signaling</em> reliability. Both of them would benefit from <em>actually becoming trustworthy</em>.</p>\n<p>Now, of course, people can't suddenly \"become reliable\", and <a href=\"http://en.wikipedia.org/wiki/Akrasia\">akrasia</a> is a formidable enemy to people stuck in these negative feedback loops. But nevertheless, you can see how this problem has a hint of Newcomblikeness to it.</p>\n<p>In fact, recommendations of this form &mdash; \"You can't signal trustworthiness unless you're trustworthy\" &mdash; are common. As an extremely simple example, let's consider a shy candidate going in to a job interview. The candidate's demeanor (<code>confident</code> or <code>shy</code>) will determine the interviewer's predisposition <code>towards</code> or <code>against</code> the candidate. During the interview, the candidate may act either <code>bold</code> or <code>timid</code>. Then the interviewer decides whether or not to hire the candidate.</p>\n<p style=\"text-align: center;\"><img src=\"http://mindingourway.com/content/images/2014/Sep/newcomblike-interview.png\" alt=\"\" /></p>\n<p>If the candidate is confident, then they will get the job (worth $100,000) regardless of whether they are bold or timid. If they are shy and timid, then they will not get the job ($0). If, however, thy are shy and bold, then they will get laughed at, which is worth -$10. Finally, though, <em>a person who knows they are going to be timid will have a shy demeanor, whereas a person who knows they are going to be bold will have a confident demeanor</em>.</p>\n<p>It may seem at first glance that it is better to be timid than to be bold, because timidness only affects the outcome if the interviewer is predisposed against the candidate, in which case it is better to be timid (and avoid being laughed at). However, if the candidate <em>knows</em> that they will reason like this (in the interview) then they will be shy <em>before</em> the interview, which will predispose the interviewer against them. By contrast, if the candidate precommits to being bold (in this simple setting) then the will get the job.</p>\n<p>Someone reasoning using CDT might reason as follows when they're in the interview:</p>\n<blockquote>\n<p>I can't tell whether they like me or not, and I don't want to be laughed at, so I'll just act timid.</p>\n</blockquote>\n<p>To people who reason like this, we suggest <em>avoiding causal reasoning</em> during the interview.</p>\n<p>And, in fact, there are truckloads of self-help books dishing out similar advice. You can't reliably signal trustworthiness without <em>actually being</em> trustworthy. You can't reliably be charismatic without <em>actually caring</em> about people. You can't easily signal confidence without <em>becoming confident</em>. Someone who <em>cannot represent</em> these arguments may find that many of the benefits of trustworthiness, charisma, and confidence are unavailable to them.</p>\n<p>Compare the advice above to our analysis of CDT in the mirror token trade, where we say \"You can't keep your token while the opponent gives theirs away\". CDT, which can't represent this argument, finds that the high payoff is unavailable to it. The analogy is exact: CDT fails to represent precisely this sort of reasoning, and yet this sort of reasoning is common and useful among humans.</p>\n<h1>6</h1>\n<p>That's not to say that CDT can't address these problems. A CDT agent that knows it's going to face the above interview would precommit to being bold &mdash; but this would involve using something <em>besides</em> causal counterfactual reasoning during the actual interview. And, in fact, this is precisely one of the arguments that I'm going to make in future posts: a sufficiently intelligent artificial system using CDT to reason about its choices would self-modify to stop using CDT to reason about its choices.</p>\n<p>We've been talking about Newcomblike problems in a very human-centric setting for this post. Next post, we'll dive into the arguments about why an <em>artificial</em> agent (that doesn't share our vast suite of social signaling tools, and which lacks our shared humanity) may <em>also</em> expect to face Newcomblike problems and would therefore self-modify to stop using CDT.</p>\n<p>This will lead us to more interesting questions, such as \"what <em>would</em> it use?\" (spoiler: we don't quite know yet) and \"would it self-modify to fix all of CDT's flaws?\" (spoiler: no).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "dPPATLhRmhdJtJM2t": 2, "fihKHQuS5WZBJgkRm": 6}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "puutBJLWbg2sXpFbu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 78, "extendedScore": null, "score": 0.000218, "legacy": true, "legacyId": "27263", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 78, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This is <a href=\"http://mindingourway.com/newcomblike-problems-are-the-norm/\">crossposted</a> from <a href=\"http://mindingourway.com\">my blog</a>. In this post, I discuss how Newcomblike situations are common among humans in the real world. The intended audience of my blog is wider than the readerbase of LW, so the tone might seem a bit off. Nevertheless, the points made here are likely new to many.</em></p>\n<h1 id=\"1\">1</h1>\n<p><a href=\"/lw/l0p/an_introduction_to_newcomblike_problems/\">Last time</a> we looked at Newcomblike problems, which cause <a href=\"/lw/kz9/causal_decision_theory_is_unsatisfactory/\">trouble for Causal Decision Theory (CDT)</a>, the standard decision theory used in economics, statistics, narrow AI, and many other academic fields.</p>\n<p>These Newcomblike problems may seem like strange edge case scenarios. In the Token Trade, a deterministic agent faces a perfect copy of themself, guaranteed to take the same action as they do. In Newcomb's original problem there is a perfect predictor \u03a9 which knows exactly what the agent will do.</p>\n<p>Both of these examples involve some form of \"mind-reading\" and assume that the agent can be perfectly copied or perfectly predicted. In a chaotic universe, these scenarios may seem unrealistic and even downright crazy. What does it matter that CDT fails when there are perfect mind-readers? There aren't perfect mind-readers. Why do we care?</p>\n<p>The reason that we care is this: <em>Newcomblike problems are the norm.</em> Most problems that humans face in real life are \"Newcomblike\".</p>\n<p>These problems aren't limited to the domain of perfect mind-readers; rather, problems with perfect mind-readers are the domain where these problems are easiest to see. However, they arise naturally whenever an agent is in a situation where others have knowledge about its decision process via some mechanism that is not under its direct control.</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"2\">2</h1>\n<p>Consider a CDT agent in a mirror token trade.</p>\n<p style=\"text-align: center;\"><img src=\"http://mindingourway.com/content/images/2014/Sep/token-mirror.png\" alt=\"\"></p>\n<p>It knows that it and the opponent are generated from the same template, but it also knows that the opponent is causally distinct from it by the time it makes its choice. So it argues</p>\n<blockquote>\n<p>Either agents spawned from my template give their tokens away, or they keep their tokens. If agents spawned from my template give their tokens away, then I better keep mine so that I can take advantage of the opponent. If, instead, agents spawned from my template keep their tokens, then I had better keep mine, or otherwise I won't win any money at all.</p>\n</blockquote>\n<p>It has failed, here, to notice that it can't choose separately from \"agents spawned from my template\" because it <em>is</em> spawned from its template. (That's not to say that it doesn't get to choose what to do. Rather, it has to be able to reason about the fact that whatever it chooses, so will its opponent choose.)</p>\n<p>The reasoning flaw here is an inability to reason as if <em>past information</em> has given others <em>veridical knowledge</em> about what the agent <em>will</em> choose. This failure is particularly vivid in the mirror token trade, where the opponent is guaranteed to do <em>exactly</em> the same thing as the opponent. However, the failure occurs even if the veridical knowledge is partial or imperfect.</p>\n<h1 id=\"3\">3</h1>\n<p>Humans trade partial, veridical, uncontrollable information about their decision procedures <em>all the time</em>.</p>\n<p>Humans automatically make <a href=\"http://en.wikipedia.org/wiki/First_impression_(psychology\">first impressions</a>) of other humans at first sight, almost instantaneously (sometimes before the person speaks, and possibly just from still images).</p>\n<p>We read each other's <a href=\"http://en.wikipedia.org/wiki/Microexpression\">microexpressions</a>, which are generally uncontrollable sources of information about our emotions.</p>\n<p>As humans, we have an impressive array of social machinery available to us that gives us gut-level, subconscious impressions of how trustworthy other people are.</p>\n<p style=\"text-align: center;\"><img src=\"http://mindingourway.com/content/images/2014/Sep/real-newcomblike.png\" alt=\"\"></p>\n<p>Many social situations follow this pattern, and this pattern is a Newcomblike one.</p>\n<p>All these tools can be fooled, of course. First impressions are often wrong. Con-men often seem trustworthy, and honest shy people can seem unworthy of trust. However, all of this social data is at least <em>correlated</em> with the truth, and that's all we need to give CDT trouble. Remember, CDT assumes that all nodes which are <em>causally</em> disconnected from it are <em>logically</em> disconnected from it: but if someone else gained information that correlates with how you <em>actually are</em> going to act in the future, then your interactions with them may be Newcomblike.</p>\n<p>In fact, humans have a natural tendency to avoid \"non-Newcomblike\" scenarios. Human social structures use complex reputation systems. Humans seldom make big choices among themselves (who to hire, whether to become roommates, whether to make a business deal) before \"getting to know each other\". We automatically build complex social models detailing how we think our friends, family, and co-workers, make decisions.</p>\n<p>When I worked at Google, I'd occasionally need to convince half a dozen team leads to sign off on a given project. In order to do this, I'd meet with each of them in person and pitch the project slightly differently, according to my model of what parts of the project most appealed to them. I was basing my actions off of how I expected them to make decisions: I was putting them in Newcomblike scenarios.</p>\n<p>We constantly leak information about how we make decisions, and others constantly use this information. Human decision situations are Newcomblike <em>by default!</em> It's the <em>non</em>-Newcomblike problems that are simplifications and edge cases.</p>\n<p>Newcomblike problems occur whenever knowledge about what decision you <em>will</em> make leaks into the environment. The knowledge doesn't have to be 100% accurate, it just has to be correlated with your eventual actual action (in such a way that if you were going to take a different action, then you would have leaked different information). When this information is available, and others use it to make their decisions, others put you into a Newcomblike scenario.</p>\n<p>Information about what we're going to do is frequently leaking into the environment, via <a href=\"http://www.overcomingbias.com/2014/06/how-deep-the-rabbit-hole.html\">unconscious signaling</a> and uncontrolled facial expressions or even just by habit \u2014 anyone following a simple routine is likely to act predictably.</p>\n<h1 id=\"4\">4</h1>\n<p>Most real decisions that humans face are Newcomblike whenever other humans are involved. People are automatically reading unconscious or unintentional signals and using these to build models of how you make choices, and they're using those models to make <em>their</em> choices. These are precisely the sorts of scenarios that CDT cannot represent.</p>\n<p>Of course, that's not to say that humans fail drastically on these problems. We don't: we repeatedly do well in these scenarios.</p>\n<p>Some real life Newcomblike scenarios simply don't represent games where CDT has trouble: there are many situations where others in the environment have knowledge about how you make decisions, and are using that knowledge but in a way that does not affect your payoffs enough to matter.</p>\n<p>Many more Newcomblike scenarios simply don't feel like decision problems: people present ideas to us in specific ways (depending upon their model of how we make choices) and most of us don't fret about how others would have presented us with different opportunities if we had acted in different ways.</p>\n<p>And in Newcomblike scenarios that <em>do</em> feel like decision problems, humans use a wide array of other tools in order to succeed.</p>\n<p>Roughly speaking, CDT fails when it gets stuck in the trap of \"no matter what I signaled I should do [something mean]\", which results in CDT sending off a \"mean\" signal and missing opportunities for higher payoffs. By contrast, humans tend to avoid this trap via other means: we place value on things like \"niceness\" for reputational reasons, we have intrinsic senses of \"honor\" and \"fairness\" which alter the payoffs of the game, and so on.</p>\n<p>This machinery was not necessarily \"designed\" for Newcomblike situations. Reputation systems and senses of honor are commonly attributed to humans facing repeated scenarios (thanks to living in small tribes) in the ancestral environment, and it's possible to argue that CDT handles repeated Newcomblike situations well enough. (I disagree somewhat, but this is an argument for another day.)</p>\n<p>Nevertheless, the machinery that allows us to handle repeated Newcomblike problems often seems to work in one-shot Newcomblike problems. Regardless of where the machinery came from, it still allows us to succeed in Newcomblike scenarios that we face in day-to-day life.</p>\n<p>The fact that humans easily succeed, often via tools developed for repeated situations, doesn't change the fact that many of our day-to-day interactions have Newcomblike characteristics. Whenever an agent leaks information about their decision procedure on a communication channel that they do not control (facial microexpressions, posture, cadence of voice, etc.) that person is inviting others to put them in Newcomblike settings.</p>\n<h1 id=\"5\">5</h1>\n<p>Most of the time, humans are pretty good at handling naturally arising Newcomblike problems. Sometimes, though, the fact that you're in a Newcomblike scenario <em>does</em> matter.</p>\n<p>The games of Poker and Diplomacy are both centered around people controlling information channels that humans can't normally control. These games give particularly crisp examples of humans wrestling with situations where the environment contains leaked information about their decision-making procedure.</p>\n<p>These are only games, yes, but I'm sure that any highly ranked Poker player will tell you that the lessons of Poker extend far beyond the game board. Similarly, I expect that highly ranked Diplomacy players will tell you that Diplomacy teaches you many lessons about how people broadcast the decisions that they're going to make, and that these lessons are invaluable in everyday life.</p>\n<p>I am not a professional negotiator, but I further imagine that top-tier negotiators expend significant effort exploring how their mindsets are tied to their unconscious signals.</p>\n<p>On a more personal scale, some very simple scenarios (like whether you can get let into a farmhouse on a rainy night after your car breaks down) are somewhat \"Newcomblike\".</p>\n<p>I know at least two people who are unreliable and untrustworthy, and who blame the fact that they can't hold down jobs (and that nobody cuts them any slack) on bad luck rather than on their own demeanors. Both consistently believe that they are taking the best available action whenever they act unreliable and untrustworthy. Both brush off the idea of \"becoming a sucker\". Neither of them is capable of <em>acting</em> unreliable while <em>signaling</em> reliability. Both of them would benefit from <em>actually becoming trustworthy</em>.</p>\n<p>Now, of course, people can't suddenly \"become reliable\", and <a href=\"http://en.wikipedia.org/wiki/Akrasia\">akrasia</a> is a formidable enemy to people stuck in these negative feedback loops. But nevertheless, you can see how this problem has a hint of Newcomblikeness to it.</p>\n<p>In fact, recommendations of this form \u2014 \"You can't signal trustworthiness unless you're trustworthy\" \u2014 are common. As an extremely simple example, let's consider a shy candidate going in to a job interview. The candidate's demeanor (<code>confident</code> or <code>shy</code>) will determine the interviewer's predisposition <code>towards</code> or <code>against</code> the candidate. During the interview, the candidate may act either <code>bold</code> or <code>timid</code>. Then the interviewer decides whether or not to hire the candidate.</p>\n<p style=\"text-align: center;\"><img src=\"http://mindingourway.com/content/images/2014/Sep/newcomblike-interview.png\" alt=\"\"></p>\n<p>If the candidate is confident, then they will get the job (worth $100,000) regardless of whether they are bold or timid. If they are shy and timid, then they will not get the job ($0). If, however, thy are shy and bold, then they will get laughed at, which is worth -$10. Finally, though, <em>a person who knows they are going to be timid will have a shy demeanor, whereas a person who knows they are going to be bold will have a confident demeanor</em>.</p>\n<p>It may seem at first glance that it is better to be timid than to be bold, because timidness only affects the outcome if the interviewer is predisposed against the candidate, in which case it is better to be timid (and avoid being laughed at). However, if the candidate <em>knows</em> that they will reason like this (in the interview) then they will be shy <em>before</em> the interview, which will predispose the interviewer against them. By contrast, if the candidate precommits to being bold (in this simple setting) then the will get the job.</p>\n<p>Someone reasoning using CDT might reason as follows when they're in the interview:</p>\n<blockquote>\n<p>I can't tell whether they like me or not, and I don't want to be laughed at, so I'll just act timid.</p>\n</blockquote>\n<p>To people who reason like this, we suggest <em>avoiding causal reasoning</em> during the interview.</p>\n<p>And, in fact, there are truckloads of self-help books dishing out similar advice. You can't reliably signal trustworthiness without <em>actually being</em> trustworthy. You can't reliably be charismatic without <em>actually caring</em> about people. You can't easily signal confidence without <em>becoming confident</em>. Someone who <em>cannot represent</em> these arguments may find that many of the benefits of trustworthiness, charisma, and confidence are unavailable to them.</p>\n<p>Compare the advice above to our analysis of CDT in the mirror token trade, where we say \"You can't keep your token while the opponent gives theirs away\". CDT, which can't represent this argument, finds that the high payoff is unavailable to it. The analogy is exact: CDT fails to represent precisely this sort of reasoning, and yet this sort of reasoning is common and useful among humans.</p>\n<h1 id=\"6\">6</h1>\n<p>That's not to say that CDT can't address these problems. A CDT agent that knows it's going to face the above interview would precommit to being bold \u2014 but this would involve using something <em>besides</em> causal counterfactual reasoning during the actual interview. And, in fact, this is precisely one of the arguments that I'm going to make in future posts: a sufficiently intelligent artificial system using CDT to reason about its choices would self-modify to stop using CDT to reason about its choices.</p>\n<p>We've been talking about Newcomblike problems in a very human-centric setting for this post. Next post, we'll dive into the arguments about why an <em>artificial</em> agent (that doesn't share our vast suite of social signaling tools, and which lacks our shared humanity) may <em>also</em> expect to face Newcomblike problems and would therefore self-modify to stop using CDT.</p>\n<p>This will lead us to more interesting questions, such as \"what <em>would</em> it use?\" (spoiler: we don't quite know yet) and \"would it self-modify to fix all of CDT's flaws?\" (spoiler: no).</p>", "sections": [{"title": "1", "anchor": "1", "level": 1}, {"title": "2", "anchor": "2", "level": 1}, {"title": "3", "anchor": "3", "level": 1}, {"title": "4", "anchor": "4", "level": 1}, {"title": "5", "anchor": "5", "level": 1}, {"title": "6", "anchor": "6", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "111 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 111, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wk7WmrN4FeNmyepXm", "5oBw9T5y5DLpJZL63"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-24T21:42:55.066Z", "modifiedAt": null, "url": null, "title": "What's the right way to think about how much to give to charity?", "slug": "what-s-the-right-way-to-think-about-how-much-to-give-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:59.619Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "irrational", "createdAt": "2011-10-04T19:46:41.913Z", "isAdmin": false, "displayName": "irrational"}, "userId": "TaHr6NuudyhaHevgQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SYPEcK3Y7DjuPX2kQ/what-s-the-right-way-to-think-about-how-much-to-give-to", "pageUrlRelative": "/posts/SYPEcK3Y7DjuPX2kQ/what-s-the-right-way-to-think-about-how-much-to-give-to", "linkUrl": "https://www.lesswrong.com/posts/SYPEcK3Y7DjuPX2kQ/what-s-the-right-way-to-think-about-how-much-to-give-to", "postedAtFormatted": "Wednesday, September 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What's%20the%20right%20way%20to%20think%20about%20how%20much%20to%20give%20to%20charity%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat's%20the%20right%20way%20to%20think%20about%20how%20much%20to%20give%20to%20charity%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSYPEcK3Y7DjuPX2kQ%2Fwhat-s-the-right-way-to-think-about-how-much-to-give-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What's%20the%20right%20way%20to%20think%20about%20how%20much%20to%20give%20to%20charity%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSYPEcK3Y7DjuPX2kQ%2Fwhat-s-the-right-way-to-think-about-how-much-to-give-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSYPEcK3Y7DjuPX2kQ%2Fwhat-s-the-right-way-to-think-about-how-much-to-give-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p>I'd like to hear from people about a process they use to decide how much to give to charity. Personally, I have very high income, and while we donate significant money in absolute terms, in relative terms the amount is &lt;1% of our post-tax income. It seems to me that it's too little, but I have no moral intuition as to what the right amount is.</p>\n<p>I have a good intuition on how to allocate the money, so that's not a problem.</p>\n<p>Background: I have a wife and two kids, one with significant health issues (i.e. medical bills - possibly for life), most money we spend goes to private school tuition x 2, the above mentioned medical bills, mortgage, and miscellaneous life expenses. And we max out retirement savings.</p>\n<p>If you have some sort of quantitative system where you figure out how much to spend on charity, please share. If you just use vague feelings, and you think there can be no reasonable quantitative system, please tell me that as well.</p>\n<p>Update: as suggested in the comments, I'll make it more explicit: please also share how you determine how much to give.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SYPEcK3Y7DjuPX2kQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 2.031616327996842e-06, "legacy": true, "legacyId": "27264", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-25T16:35:18.705Z", "modifiedAt": null, "url": null, "title": "Meetup : London social meetup", "slug": "meetup-london-social-meetup-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mJEGXtkzEfG3TkyeF/meetup-london-social-meetup-2", "pageUrlRelative": "/posts/mJEGXtkzEfG3TkyeF/meetup-london-social-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/mJEGXtkzEfG3TkyeF/meetup-london-social-meetup-2", "postedAtFormatted": "Thursday, September 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJEGXtkzEfG3TkyeF%2Fmeetup-london-social-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJEGXtkzEfG3TkyeF%2Fmeetup-london-social-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJEGXtkzEfG3TkyeF%2Fmeetup-london-social-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14w'>London social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 September 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next LW London meetup will be on September 28th. Join us from 2pm to talk about the sorts of things that your other friends will look funny at you for talking about.</p>\n\n<p>I likely won't be there myself this week, but if you have trouble finding us, you can contact Leon on 07860466862.</p>\n\n<p><strong>About London LessWrong</strong>:</p>\n\n<p>We run this meetup approximately every other week; these days we tend to get in the region of 5-15 people in attendance. By default, meetups are just unstructured social discussion about whatever strikes our fancy: books we're reading, recent posts on LW/related blogs, logic puzzles, toilet usage statistics....</p>\n\n<p>Sometimes we play <a href=\"http://en.wikipedia.org/wiki/The_Resistance_%28game%29\" rel=\"nofollow\">The Resistance</a> or other games. We usually finish around 7pm, give or take an hour, but people arrive and leave whenever suits them.</p>\n\n<p>Related discussion happens on both our <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">google group</a> and our <a href=\"https://www.facebook.com/groups/380103898766356/\" rel=\"nofollow\">facebook group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14w'>London social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mJEGXtkzEfG3TkyeF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "27265", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/14w\">London social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 September 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next LW London meetup will be on September 28th. Join us from 2pm to talk about the sorts of things that your other friends will look funny at you for talking about.</p>\n\n<p>I likely won't be there myself this week, but if you have trouble finding us, you can contact Leon on 07860466862.</p>\n\n<p><strong>About London LessWrong</strong>:</p>\n\n<p>We run this meetup approximately every other week; these days we tend to get in the region of 5-15 people in attendance. By default, meetups are just unstructured social discussion about whatever strikes our fancy: books we're reading, recent posts on LW/related blogs, logic puzzles, toilet usage statistics....</p>\n\n<p>Sometimes we play <a href=\"http://en.wikipedia.org/wiki/The_Resistance_%28game%29\" rel=\"nofollow\">The Resistance</a> or other games. We usually finish around 7pm, give or take an hour, but people arrive and leave whenever suits them.</p>\n\n<p>Related discussion happens on both our <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">google group</a> and our <a href=\"https://www.facebook.com/groups/380103898766356/\" rel=\"nofollow\">facebook group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/14w\">London social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : London social meetup", "anchor": "Discussion_article_for_the_meetup___London_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : London social meetup", "anchor": "Discussion_article_for_the_meetup___London_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-25T21:17:22.131Z", "modifiedAt": null, "url": null, "title": "Logics for Mind-Building Should Have Computational Meaning", "slug": "logics-for-mind-building-should-have-computational-meaning", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:01.666Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "28iKD7fEnHvK8pNNm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EKsafyzZ8ah9ZeWew/logics-for-mind-building-should-have-computational-meaning", "pageUrlRelative": "/posts/EKsafyzZ8ah9ZeWew/logics-for-mind-building-should-have-computational-meaning", "linkUrl": "https://www.lesswrong.com/posts/EKsafyzZ8ah9ZeWew/logics-for-mind-building-should-have-computational-meaning", "postedAtFormatted": "Thursday, September 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Logics%20for%20Mind-Building%20Should%20Have%20Computational%20Meaning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALogics%20for%20Mind-Building%20Should%20Have%20Computational%20Meaning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEKsafyzZ8ah9ZeWew%2Flogics-for-mind-building-should-have-computational-meaning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Logics%20for%20Mind-Building%20Should%20Have%20Computational%20Meaning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEKsafyzZ8ah9ZeWew%2Flogics-for-mind-building-should-have-computational-meaning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEKsafyzZ8ah9ZeWew%2Flogics-for-mind-building-should-have-computational-meaning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2238, "htmlBody": "<p><strong>The Workshop</strong></p>\n<p>Late in July I organized and held MIRIx Tel-Aviv with the goal of investigating the currently-open (to my knowledge) Friendly AI problem called \"logical probability\": the issue of assigning probabilities to formulas in a first-order proof system, in order to use the <a title=\"&quot;The Definability of Truth in Probabilistic Logic&quot;\" href=\"http://intelligence.org/files/DefinabilityTruthDraft.pdf\">reflective consistency of the probability predicate</a> to get past the <a href=\"https://www.youtube.com/watch?v=MwriJqBZyoM\">Loebian Obstacle</a> to building a self-modifying reasoning agent that will trust itself and its successors.&nbsp; Vadim Kosoy, Benjamin and Joshua Fox, and myself met at the <a href=\"http://telavivmakers.org/index.php/Main_Page\">Tel-Aviv Makers' Insurgence</a> for six hours, and each presented our ideas.&nbsp; I spent most of it sneezing due to my allergies to TAMI's resident cats.</p>\n<p>My idea was to go with the <a href=\"http://en.wikipedia.org/wiki/Proof-theoretic_semantics\">proof-theoretic semantics of logic</a> and attack computational construction of logical probability via the <a href=\"http://homepages.inf.ed.ac.uk/wadler/papers/propositions-as-types/propositions-as-types.pdf\">Curry-Howard Isomorphism between programs and proofs</a>: this yields a rather direct translation between computational constructions of logical probability and the learning/construction of an optimal function from sensory inputs to actions required by <a href=\"http://www.recursivelydiscursive.net/2014/04/problem-class-dominance-in-predictive.html\">Updateless Decision Theory</a>.</p>\n<p>The best I can give as a mathematical result is as follows:</p>\n<p><strong> <img src=\"http://www.codecogs.com/png.latex?P(\\Gamma \\vdash a:A \\mid \\Gamma \\vdash b:B) = P(\\Gamma,x:B \\vdash [\\forall y:B, x/y]a:A)\" alt=\"P(\\Gamma \\vdash a:A \\mid \\Gamma \\vdash b:B) = P(\\Gamma,x:B \\vdash [\\forall y:B, x/y]a:A)\" width=\"448\" height=\"19\" /></strong></p>\n<p><strong><img src=\"http://www.codecogs.com/png.latex?P(\\Gamma \\vdash (a, b): A \\wedge B) = P(\\Gamma \\vdash a:A \\mid \\Gamma \\vdash b:B) * P(\\Gamma \\vdash b:B)\" alt=\"P(\\Gamma \\vdash (a, b): A \\wedge B) = P(\\Gamma \\vdash a:A \\mid \\Gamma \\vdash b:B) * P(\\Gamma \\vdash b:B)\" width=\"484\" height=\"19\" /></strong></p>\n<p><strong><img src=\"http://www.codecogs.com/png.latex?\\frac{x:A \\notin \\Gamma}{P(\\Gamma \\vdash x:A) = \\mathcal{M}_{\\lambda\\mu} (A)}\" alt=\"\\frac{x:A \\notin \\Gamma}{P(\\Gamma \\vdash x:A) = \\mathcal{M}_{\\lambda\\mu} (A)}\" width=\"181\" height=\"42\" /></strong></p>\n<p><strong><img src=\"http://www.codecogs.com/png.latex?\\frac{x:A \\in \\Gamma}{P(\\Gamma \\vdash x:A) = 1.0}\" alt=\"\\frac{x:A \\in \\Gamma}{P(\\Gamma \\vdash x:A) = 1.0}\" width=\"147\" height=\"41\" /><br /></strong></p>\n<p>The capital&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\Gamma\" alt=\"\\Gamma\" width=\"11\" height=\"13\" /> is a set of hypotheses/axioms/assumptions, and the English letters are metasyntactic variables (like \"foo\" and \"bar\" in programming lessons).&nbsp; The lower-case letters denote proofs/programs, and the upper-case letters denote propositions/types.&nbsp; The turnstile&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\vdash\" alt=\"\\vdash\" width=\"10\" height=\"13\" /> just means \"deduces\": the <em>judgement</em> <img src=\"http://www.codecogs.com/png.latex?\\Gamma \\vdash a:A\" alt=\"\\Gamma \\vdash a:A\" width=\"70\" height=\"14\" /> can be read here as \"an agent whose set of beliefs is denoted <img src=\"http://www.codecogs.com/png.latex?%5CGamma\" alt=\"\\Gamma\" width=\"11\" height=\"13\" /> will believe that the evidence <em>a</em> proves the proposition <em>A</em>.\"&nbsp; The&nbsp;<img src=\"http://www.codecogs.com/png.latex?[\\forall y:B, x/y]a\" alt=\"[\\forall y:B, x/y]a\" width=\"131\" height=\"18\" /> performs a \"reversed\" substitution, with the result reading: \"for all <em>y</em> proving/of-type <em>B</em>, substitute <em>x</em> for <em>y</em> in <em>a</em>\".&nbsp; This means that we algorithmically build a new proof/construction/program from <em>a</em> in which any and all constructions proving the proposition <em>B</em> are replaced with the logically-equivalent hypothesis <em>x</em>, which we have added to our hypothesis-set <img src=\"http://www.codecogs.com/png.latex?%5CGamma\" alt=\"\\Gamma\" width=\"11\" height=\"13\" />.</p>\n<p id=\"title\">Thus the first equation reads, \"the probability of <em>a</em> proving <em>A</em> conditioned on <em>b</em> proving <em>B</em> equals the probability of <em>a</em> proving <em>A</em> when we assume the truth of <em>B</em> as a hypothesis.\"&nbsp; The second equation then uses this definition of conditional probability to give the normal Product Rule of probabilities for the logical product (the&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\wedge\" alt=\"\\wedge\" width=\"11\" height=\"12\" /> operator), defined proof-theoretically.&nbsp; I strongly believe I could give a similar equation for the normal Sum Rule of probabilities for the logical sum (the <img src=\"http://www.codecogs.com/png.latex?\\vee\" alt=\"\\vee\" width=\"11\" height=\"12\" /> operator) if I could only access the <a href=\"http://link.springer.com/chapter/10.1007%2FBFb0013061\">relevant paywalled paper</a>, in which the &lambda;&mu;-calculus acting as an algorithmic interpretation of the natural-deduction system for classical propositional logic (rather than intuitionistic) is given.</p>\n<p>The third item given there is an inference rule, which reads, \"if <em>x</em> is a free variable/hypothesis imputed to have type/prove proposition <em>A</em>, not bound in the hypothesis-set <img src=\"http://www.codecogs.com/png.latex?%5CGamma\" alt=\"\\Gamma\" width=\"11\" height=\"13\" />, then the probability with which we believe <em>x</em> proves <em>A</em> is given by the Solomonoff Measure of type <em>A</em> in the &lambda;&mu;-calculus\".&nbsp; We can define that measure simply as the summed Solomonoff Measure of every program/proof possessing the relevant type, and I don't think going into the details of its construction here would be particularly productive.&nbsp; Free variables in &lambda;-calculus are isomorphic to unproven hypotheses in natural deduction, and so a probabilistic proof system could learn how much to believe in some free-standing hypothesis via Bayesian evidence rather than algorithmic proof.</p>\n<p>The final item given here is trivial: anything assumed has probability 1.0, that of a logical tautology.</p>\n<p>The upside to invoking the strange, alien &lambda;&mu;-calculus instead of the more normal, friendly &lambda;-calculus is that we thus reason inside classical logic rather than intuitionistic, which means we can use the classical axioms of probability rather than <a href=\"http://projecteuclid.org/euclid.ndjfl/1082637807\">intuitionistic</a> <a href=\"http://www.researchgate.net/publication/220083450_A_probabilistic_extension_of_intuitionistic_logic\">Bayesianism</a>.&nbsp; We <em>need</em> classical logic here: if we switch to intuitionistic logics (Heyting algebras rather than Boolean algebras) we do get to make computational decidability a first-class citizen of our logic, but the cost is that we can then believe <em>only</em> computationally provable propositions. As Benjamin Fox pointed out to me at the workshop, Loeb's Theorem then becomes a triviality, with real self-trust rendered no easier.</p>\n<p><strong>The Apologia</strong></p>\n<p>My motivation and core idea for all this was very simple: I am a devout <a href=\"https://www.doc.ic.ac.uk/~gds/PLMW/harper-plmw13-talk.pdf\">computational trinitarian</a>, believing that logic must be set on foundations which describe reasoning, truth, and evidence in a non-mystical, non-Platonic way.&nbsp; The study of first-order logic and <em>especially</em> of incompleteness results in metamathematics, <a href=\"http://www.ams.org/notices/201011/rtx101101454p.pdf\">from Goedel on up to Chaitin</a>, <em>aggravates</em> me in its relentless Platonism, and especially in the way <a href=\"http://vserver1.cscs.lsa.umich.edu/~crshalizi/notabene/godels-theorem.html\">Platonic mysticism about logical incompleteness so often leads to the belief that minds are mystical</a>.&nbsp; (<a href=\"http://www.cpporter.com/wp-content/uploads/2013/08/PorterCambridge2013.pdf\">It aggravates other people, too!</a>)</p>\n<p>The slight problem which I ran into is that there's a shit-ton I don't know about logic.&nbsp; <a href=\"http://www.amazon.com/Logical-Labyrinths-Raymond-Smullyan/dp/1568814437/ref=sr_1_1?ie=UTF8&amp;qid=1411672077&amp;sr=8-1&amp;keywords=Raymond+M.+Smullyan+logical+labyrinths\">I am now working to remedy</a> <a href=\"http://www.amazon.com/Computability-Logic-George-S-Boolos/dp/0521701465/ref=sr_1_1?ie=UTF8&amp;qid=1411672120&amp;sr=8-1&amp;keywords=computability+and+logic\">this grievous hole in my previous education</a>.&nbsp; Also, this problem is <a href=\"http://dl.acm.org/citation.cfm?id=5450\">really </a><a href=\"http://arxiv.org/abs/1209.2620\">deep</a>, <a href=\"http://www.hutter1.net/publ/problogics.pdf\">actually</a>.</p>\n<p>I thus apologize for ending the rigorous portion of this write-up here.&nbsp; Everyone expecting proper rigor, you may now pack up and go home, if you were ever paying attention at all.&nbsp; Ritual seppuku will duly be committed, followed by hors d'oeuvre.&nbsp; My corpse will be duly recycled to make paper-clips, in the proper fashion of a failed LessWrongian.</p>\n<p><strong>The Parts I'm Not Very Sure About</strong></p>\n<p>With any luck, that previous paragraph got rid of all the serious people.</p>\n<p>I do, however, still think that the (beautiful) equivalence between computation and logic can yield some insights here.&nbsp; After all, the whole reason for the strange incompleteness results in first-order logic (shown by Boolos in his textbook, I'm told) is that first-order logic, as a reasoning system, contains sufficient computational machinery to encode a Universal Turing Machine.&nbsp; The bidirectionality of this reduction (Hilbert and Gentzen both have given computational descriptions of first-order proof systems) is just another demonstration of the equivalence.</p>\n<p>In fact, it seems to me (right now) to yield a rather intuitively satisfying explanation of why the Gaifman-Carnot Condition (that every instance we see of&nbsp;<img src=\"http://www.codecogs.com/png.latex?P(x_i)\" alt=\"P(x_i)\" width=\"42\" height=\"18\" /> provides Bayesian evidence in favor of&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\forall x.P(x)\" alt=\"\\forall x.P(x)\" width=\"62\" height=\"18\" />) for logical probabilities <a href=\"https://groups.google.com/forum/#!topic/magic-list/WJzPoNJavhk\">is not computably approximable</a>.&nbsp; What would we need to interpret the Gaifman Condition from an algorithmic, type-theoretic viewpoint?&nbsp; From this interpretation, we would need a proof of our universal generalization.&nbsp; This would have to be a dependent product of form <img src=\"http://www.codecogs.com/png.latex?\\Pi(x:A).P(x)\" alt=\"\\Pi(x:A).P(x)\" width=\"108\" height=\"18\" />, a function taking any construction <img src=\"http://www.codecogs.com/png.latex?x:A\" alt=\"x:A\" width=\"38\" height=\"12\" /> to a construction of type <img src=\"http://www.codecogs.com/png.latex?P(x)\" alt=\"P(x)\" width=\"37\" height=\"18\" />, which itself has type <strong>Prop</strong>.&nbsp; To learn such a dependent function from the examples would be to search for an optimal (simple, probable) construction (program) constituting the relevant proof object: effectively, an individual act of Solomonoff Induction.&nbsp; Solomonoff Induction, however, is already only semicomputable, which would then make a Gaifman-Hutter distribution (is there another term for these?) doubly semicomputable, since even generating it involves a semiprocedure.</p>\n<p>The <em>benefit</em> of using the constructive approach to probabilistic logic here is that we know perfectly well that however incomputable Solomonoff Induction and Gaifman-Hutter distributions might be, both existing humans and existing proof systems succeed in building proof-constructions for quantified sentences <em>all the time</em>, even in higher-order logics such as Coquand's <a href=\"http://coq.inria.fr/cocorico/TheoryBehindCoq\">Calculus of Constructions</a> (the core of a popular constructive proof assistant) or Luo's <a href=\"http://www.cs.rhul.ac.uk/~zhaohui/LTT06.pdf\">Logic-Enriched Type Theory</a> (the core of a popular dependently-typed programming language and proof engine based on classical logic).&nbsp; Such logics and their proof-checking algorithms constitute, going all the way back to <a href=\"http://www.win.tue.nl/automath/\">Automath</a>, the first examples of computational \"agents\" which acquire specific \"beliefs\" in a mathematically rigorous way, subject to human-proved theorems of soundness, consistency, and programming-language-theoretic completeness (rather than meaning that every true proposition has a proof, this means that every program which does not become operationally stuck has a type and is thus the proof of some proposition).&nbsp; If we want our AIs to believe in accordance with soundness and consistency properties we can prove <em>before</em> running them, while being composed of computational artifacts, I personally consider this the foundation from which to build.</p>\n<p>Where we <em>can</em> acquire probabilistic evidence in a sound and computable way, as noted above in the section on free variables/hypotheses, we can do so for propositions which we cannot algorithmically prove.&nbsp; This would bring us closer to our actual goals of using logical probability in Updateless Decision Theory or of getting around the Loebian Obstacle.</p>\n<p><strong>Some of the Background Material I'm Reading</strong></p>\n<p>Another reason why we should use a Curry-Howard approach to logical probability is one of the simplest possible reasons: the burgeoning field of <a href=\"http://research.microsoft.com/pubs/208585/fose-icse2014.pdf\">probabilistic programming</a> is already being <a href=\"http://dl.acm.org/citation.cfm?id=2103721&amp;CFID=573600967&amp;CFTOKEN=48192368\">built</a> on <a href=\"http://dl.acm.org/citation.cfm?id=503288\">it</a>.&nbsp; The Computational Cognitive Science lab at MIT is publishing papers showing that their languages are universal for computable and semicomputable probability distributions, and getting strong results in the study of human general intelligence.&nbsp; Specifically: they are hypothesizing that we can dissolve \"learning\" into \"inducing probabilistic programs via hierarchical Bayesian inference\", \"thinking\" into \"simulation\" into \"conditional sampling from probabilistic programs\", and \"uncertain inference\" into \"approximate inference over the distributions represented by probabilistic programs, conditioned on some fixed quantity of sampling that has been done.\"</p>\n<p>In fact, one might even look at these ideas and think that, perhaps, an agent which could find some way to sample quickly and more accurately, or to learn probabilistic programs more efficiently (in terms of training data), than was built into its original \"belief engine\" could then rewrite its belief engine to use these new algorithms to perform strictly better inference and learning.&nbsp; Unless I'm as completely wrong as I usually am about these things (that is, very extremely completely wrong based on an utterly unfounded misunderstanding of the whole topic), it's a potential engine for recursive self-improvement.</p>\n<p>They also have been studying how to implement statistical inference techniques for their generate modeling languages which do not obey Bayesian soundness.&nbsp; While most of machine learning/perception works according to error-rate minimization rather than Bayesian soundness (exactly because Bayesian methods are <em>often</em> too computationally expensive for real-world use), I would prefer someone at least study the implications of employing unsound inference techniques for more general AI and cognitive-science applications in terms of how often such a system would \"misbehave\".</p>\n<p>Many of MIT's models are currently dynamically typed and appear to leave type soundness (the logical rigor with which agents come to believe things by deduction) to future research.&nbsp; And yet: they got to this problem first, so to speak.&nbsp; We really ought to be collaborating with them, with the full-time grant-funded academic researchers, rather than trying to armchair-reason our way to a full theory of logical probability as a large group of amateurs or part-timers and only a small core cohort of full-time MIRI and FHI staff investigating AI safety issues.</p>\n<p>(I admit to having a nerd crush, and I am actually planning to go visit the Cocosci Lab this coming week, and want/intend to apply to their PhD program.)</p>\n<p>They have also uncovered something else I find highly interesting: human learning of both concepts and causal frameworks seems to take place via hierarchical Bayesian inference, <a href=\"http://projects.csail.mit.edu/church/wiki/Hierarchical_Models#The_Blessing_of_Abstraction\">gaining a \"blessing of abstraction\" to countermand the \"curse of dimensionality\"</a>.&nbsp; The natural interpretation of these abstractions in terms of constructions and types would be that, as in dependently-typed programming languages, constructions have types, and types are constructions, but for hierarchical-learning purposes, it would be useful to suppose that <em>types</em> have specific, structured types more informative than <strong>Prop</strong> or <strong>Type</strong><sub>n</sub> (for some universe level <em>n</em>).&nbsp; Inference can then proceed from giving constructions or type-judgements as evidence at the bottom level, up the hierarchy of types and meta-types to give probabilistic belief-assignments to very general knowledge.&nbsp; Even very different objects could have similar meta-types at some level of the hierarchy, allowing hierarchical inference to help transfer Bayesian evidence between seemingly different domains, giving insight into how efficient general intelligence can work.</p>\n<p><strong>Just-for-fun Postscript</strong></p>\n<p>If we really buy into the model of thinking as conditional simulation, we can use that to <a href=\"/lw/tg/against_modal_logics/\">dissolve the modalities \"possible\" and \"impossible\"</a>.&nbsp; We arrive at (by my count) three different ways of considering the issue computationally:</p>\n<ol>\n<li>Conceivable/imaginable: the generative models which constitute my current beliefs do or do not yield a path to make some logical proposition true or to make some causal event happen (<a href=\"http://projects.csail.mit.edu/church/wiki/Inference_about_inference:_Nested_query#Planning\">planning can be done as inference, after all</a>), with or without some specified level of probability.</li>\n<li>Sensibility/absurdity: the generative models which constitute my current beliefs place a desirably high or undesirably low probability on the known path(s) by which a proposition might be true or by which an event might happen.&nbsp; The level which constitutes \"desirable\" could be set as the <img src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" /> value for a hypothesis test, or some other value determined decision-theoretically.&nbsp; This could relate to Pascal's Mugging: how probable must something be before I consider it <em>real</em> rather than an artifact of my own hypothesis space?</li>\n<li>Consistency or Contradiction: the generative models which constitute my current beliefs, plus the hypothesis that some proposition is true or some event can come about, do or do not yield a logical contradiction with some probability (that is, we should believe the contradiction exists only to the degree we believe in our existing models in the first place!).</li>\n</ol>\n<p>I mostly find this fun because it lets us talk rigorously about when we should \"shut up and do the 1,2!impossible\" and when something is very definitely 3!impossible.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EKsafyzZ8ah9ZeWew", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 34, "extendedScore": null, "score": 2.03422326815059e-06, "legacy": true, "legacyId": "26895", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"The_Workshop\">The Workshop</strong></p>\n<p>Late in July I organized and held MIRIx Tel-Aviv with the goal of investigating the currently-open (to my knowledge) Friendly AI problem called \"logical probability\": the issue of assigning probabilities to formulas in a first-order proof system, in order to use the <a title=\"&quot;The Definability of Truth in Probabilistic Logic&quot;\" href=\"http://intelligence.org/files/DefinabilityTruthDraft.pdf\">reflective consistency of the probability predicate</a> to get past the <a href=\"https://www.youtube.com/watch?v=MwriJqBZyoM\">Loebian Obstacle</a> to building a self-modifying reasoning agent that will trust itself and its successors.&nbsp; Vadim Kosoy, Benjamin and Joshua Fox, and myself met at the <a href=\"http://telavivmakers.org/index.php/Main_Page\">Tel-Aviv Makers' Insurgence</a> for six hours, and each presented our ideas.&nbsp; I spent most of it sneezing due to my allergies to TAMI's resident cats.</p>\n<p>My idea was to go with the <a href=\"http://en.wikipedia.org/wiki/Proof-theoretic_semantics\">proof-theoretic semantics of logic</a> and attack computational construction of logical probability via the <a href=\"http://homepages.inf.ed.ac.uk/wadler/papers/propositions-as-types/propositions-as-types.pdf\">Curry-Howard Isomorphism between programs and proofs</a>: this yields a rather direct translation between computational constructions of logical probability and the learning/construction of an optimal function from sensory inputs to actions required by <a href=\"http://www.recursivelydiscursive.net/2014/04/problem-class-dominance-in-predictive.html\">Updateless Decision Theory</a>.</p>\n<p>The best I can give as a mathematical result is as follows:</p>\n<p><strong> <img src=\"http://www.codecogs.com/png.latex?P(\\Gamma \\vdash a:A \\mid \\Gamma \\vdash b:B) = P(\\Gamma,x:B \\vdash [\\forall y:B, x/y]a:A)\" alt=\"P(\\Gamma \\vdash a:A \\mid \\Gamma \\vdash b:B) = P(\\Gamma,x:B \\vdash [\\forall y:B, x/y]a:A)\" width=\"448\" height=\"19\"></strong></p>\n<p><strong><img src=\"http://www.codecogs.com/png.latex?P(\\Gamma \\vdash (a, b): A \\wedge B) = P(\\Gamma \\vdash a:A \\mid \\Gamma \\vdash b:B) * P(\\Gamma \\vdash b:B)\" alt=\"P(\\Gamma \\vdash (a, b): A \\wedge B) = P(\\Gamma \\vdash a:A \\mid \\Gamma \\vdash b:B) * P(\\Gamma \\vdash b:B)\" width=\"484\" height=\"19\"></strong></p>\n<p><strong><img src=\"http://www.codecogs.com/png.latex?\\frac{x:A \\notin \\Gamma}{P(\\Gamma \\vdash x:A) = \\mathcal{M}_{\\lambda\\mu} (A)}\" alt=\"\\frac{x:A \\notin \\Gamma}{P(\\Gamma \\vdash x:A) = \\mathcal{M}_{\\lambda\\mu} (A)}\" width=\"181\" height=\"42\"></strong></p>\n<p><strong><img src=\"http://www.codecogs.com/png.latex?\\frac{x:A \\in \\Gamma}{P(\\Gamma \\vdash x:A) = 1.0}\" alt=\"\\frac{x:A \\in \\Gamma}{P(\\Gamma \\vdash x:A) = 1.0}\" width=\"147\" height=\"41\"><br></strong></p>\n<p>The capital&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\Gamma\" alt=\"\\Gamma\" width=\"11\" height=\"13\"> is a set of hypotheses/axioms/assumptions, and the English letters are metasyntactic variables (like \"foo\" and \"bar\" in programming lessons).&nbsp; The lower-case letters denote proofs/programs, and the upper-case letters denote propositions/types.&nbsp; The turnstile&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\vdash\" alt=\"\\vdash\" width=\"10\" height=\"13\"> just means \"deduces\": the <em>judgement</em> <img src=\"http://www.codecogs.com/png.latex?\\Gamma \\vdash a:A\" alt=\"\\Gamma \\vdash a:A\" width=\"70\" height=\"14\"> can be read here as \"an agent whose set of beliefs is denoted <img src=\"http://www.codecogs.com/png.latex?%5CGamma\" alt=\"\\Gamma\" width=\"11\" height=\"13\"> will believe that the evidence <em>a</em> proves the proposition <em>A</em>.\"&nbsp; The&nbsp;<img src=\"http://www.codecogs.com/png.latex?[\\forall y:B, x/y]a\" alt=\"[\\forall y:B, x/y]a\" width=\"131\" height=\"18\"> performs a \"reversed\" substitution, with the result reading: \"for all <em>y</em> proving/of-type <em>B</em>, substitute <em>x</em> for <em>y</em> in <em>a</em>\".&nbsp; This means that we algorithmically build a new proof/construction/program from <em>a</em> in which any and all constructions proving the proposition <em>B</em> are replaced with the logically-equivalent hypothesis <em>x</em>, which we have added to our hypothesis-set <img src=\"http://www.codecogs.com/png.latex?%5CGamma\" alt=\"\\Gamma\" width=\"11\" height=\"13\">.</p>\n<p id=\"title\">Thus the first equation reads, \"the probability of <em>a</em> proving <em>A</em> conditioned on <em>b</em> proving <em>B</em> equals the probability of <em>a</em> proving <em>A</em> when we assume the truth of <em>B</em> as a hypothesis.\"&nbsp; The second equation then uses this definition of conditional probability to give the normal Product Rule of probabilities for the logical product (the&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\wedge\" alt=\"\\wedge\" width=\"11\" height=\"12\"> operator), defined proof-theoretically.&nbsp; I strongly believe I could give a similar equation for the normal Sum Rule of probabilities for the logical sum (the <img src=\"http://www.codecogs.com/png.latex?\\vee\" alt=\"\\vee\" width=\"11\" height=\"12\"> operator) if I could only access the <a href=\"http://link.springer.com/chapter/10.1007%2FBFb0013061\">relevant paywalled paper</a>, in which the \u03bb\u03bc-calculus acting as an algorithmic interpretation of the natural-deduction system for classical propositional logic (rather than intuitionistic) is given.</p>\n<p>The third item given there is an inference rule, which reads, \"if <em>x</em> is a free variable/hypothesis imputed to have type/prove proposition <em>A</em>, not bound in the hypothesis-set <img src=\"http://www.codecogs.com/png.latex?%5CGamma\" alt=\"\\Gamma\" width=\"11\" height=\"13\">, then the probability with which we believe <em>x</em> proves <em>A</em> is given by the Solomonoff Measure of type <em>A</em> in the \u03bb\u03bc-calculus\".&nbsp; We can define that measure simply as the summed Solomonoff Measure of every program/proof possessing the relevant type, and I don't think going into the details of its construction here would be particularly productive.&nbsp; Free variables in \u03bb-calculus are isomorphic to unproven hypotheses in natural deduction, and so a probabilistic proof system could learn how much to believe in some free-standing hypothesis via Bayesian evidence rather than algorithmic proof.</p>\n<p>The final item given here is trivial: anything assumed has probability 1.0, that of a logical tautology.</p>\n<p>The upside to invoking the strange, alien \u03bb\u03bc-calculus instead of the more normal, friendly \u03bb-calculus is that we thus reason inside classical logic rather than intuitionistic, which means we can use the classical axioms of probability rather than <a href=\"http://projecteuclid.org/euclid.ndjfl/1082637807\">intuitionistic</a> <a href=\"http://www.researchgate.net/publication/220083450_A_probabilistic_extension_of_intuitionistic_logic\">Bayesianism</a>.&nbsp; We <em>need</em> classical logic here: if we switch to intuitionistic logics (Heyting algebras rather than Boolean algebras) we do get to make computational decidability a first-class citizen of our logic, but the cost is that we can then believe <em>only</em> computationally provable propositions. As Benjamin Fox pointed out to me at the workshop, Loeb's Theorem then becomes a triviality, with real self-trust rendered no easier.</p>\n<p><strong id=\"The_Apologia\">The Apologia</strong></p>\n<p>My motivation and core idea for all this was very simple: I am a devout <a href=\"https://www.doc.ic.ac.uk/~gds/PLMW/harper-plmw13-talk.pdf\">computational trinitarian</a>, believing that logic must be set on foundations which describe reasoning, truth, and evidence in a non-mystical, non-Platonic way.&nbsp; The study of first-order logic and <em>especially</em> of incompleteness results in metamathematics, <a href=\"http://www.ams.org/notices/201011/rtx101101454p.pdf\">from Goedel on up to Chaitin</a>, <em>aggravates</em> me in its relentless Platonism, and especially in the way <a href=\"http://vserver1.cscs.lsa.umich.edu/~crshalizi/notabene/godels-theorem.html\">Platonic mysticism about logical incompleteness so often leads to the belief that minds are mystical</a>.&nbsp; (<a href=\"http://www.cpporter.com/wp-content/uploads/2013/08/PorterCambridge2013.pdf\">It aggravates other people, too!</a>)</p>\n<p>The slight problem which I ran into is that there's a shit-ton I don't know about logic.&nbsp; <a href=\"http://www.amazon.com/Logical-Labyrinths-Raymond-Smullyan/dp/1568814437/ref=sr_1_1?ie=UTF8&amp;qid=1411672077&amp;sr=8-1&amp;keywords=Raymond+M.+Smullyan+logical+labyrinths\">I am now working to remedy</a> <a href=\"http://www.amazon.com/Computability-Logic-George-S-Boolos/dp/0521701465/ref=sr_1_1?ie=UTF8&amp;qid=1411672120&amp;sr=8-1&amp;keywords=computability+and+logic\">this grievous hole in my previous education</a>.&nbsp; Also, this problem is <a href=\"http://dl.acm.org/citation.cfm?id=5450\">really </a><a href=\"http://arxiv.org/abs/1209.2620\">deep</a>, <a href=\"http://www.hutter1.net/publ/problogics.pdf\">actually</a>.</p>\n<p>I thus apologize for ending the rigorous portion of this write-up here.&nbsp; Everyone expecting proper rigor, you may now pack up and go home, if you were ever paying attention at all.&nbsp; Ritual seppuku will duly be committed, followed by hors d'oeuvre.&nbsp; My corpse will be duly recycled to make paper-clips, in the proper fashion of a failed LessWrongian.</p>\n<p><strong id=\"The_Parts_I_m_Not_Very_Sure_About\">The Parts I'm Not Very Sure About</strong></p>\n<p>With any luck, that previous paragraph got rid of all the serious people.</p>\n<p>I do, however, still think that the (beautiful) equivalence between computation and logic can yield some insights here.&nbsp; After all, the whole reason for the strange incompleteness results in first-order logic (shown by Boolos in his textbook, I'm told) is that first-order logic, as a reasoning system, contains sufficient computational machinery to encode a Universal Turing Machine.&nbsp; The bidirectionality of this reduction (Hilbert and Gentzen both have given computational descriptions of first-order proof systems) is just another demonstration of the equivalence.</p>\n<p>In fact, it seems to me (right now) to yield a rather intuitively satisfying explanation of why the Gaifman-Carnot Condition (that every instance we see of&nbsp;<img src=\"http://www.codecogs.com/png.latex?P(x_i)\" alt=\"P(x_i)\" width=\"42\" height=\"18\"> provides Bayesian evidence in favor of&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\forall x.P(x)\" alt=\"\\forall x.P(x)\" width=\"62\" height=\"18\">) for logical probabilities <a href=\"https://groups.google.com/forum/#!topic/magic-list/WJzPoNJavhk\">is not computably approximable</a>.&nbsp; What would we need to interpret the Gaifman Condition from an algorithmic, type-theoretic viewpoint?&nbsp; From this interpretation, we would need a proof of our universal generalization.&nbsp; This would have to be a dependent product of form <img src=\"http://www.codecogs.com/png.latex?\\Pi(x:A).P(x)\" alt=\"\\Pi(x:A).P(x)\" width=\"108\" height=\"18\">, a function taking any construction <img src=\"http://www.codecogs.com/png.latex?x:A\" alt=\"x:A\" width=\"38\" height=\"12\"> to a construction of type <img src=\"http://www.codecogs.com/png.latex?P(x)\" alt=\"P(x)\" width=\"37\" height=\"18\">, which itself has type <strong>Prop</strong>.&nbsp; To learn such a dependent function from the examples would be to search for an optimal (simple, probable) construction (program) constituting the relevant proof object: effectively, an individual act of Solomonoff Induction.&nbsp; Solomonoff Induction, however, is already only semicomputable, which would then make a Gaifman-Hutter distribution (is there another term for these?) doubly semicomputable, since even generating it involves a semiprocedure.</p>\n<p>The <em>benefit</em> of using the constructive approach to probabilistic logic here is that we know perfectly well that however incomputable Solomonoff Induction and Gaifman-Hutter distributions might be, both existing humans and existing proof systems succeed in building proof-constructions for quantified sentences <em>all the time</em>, even in higher-order logics such as Coquand's <a href=\"http://coq.inria.fr/cocorico/TheoryBehindCoq\">Calculus of Constructions</a> (the core of a popular constructive proof assistant) or Luo's <a href=\"http://www.cs.rhul.ac.uk/~zhaohui/LTT06.pdf\">Logic-Enriched Type Theory</a> (the core of a popular dependently-typed programming language and proof engine based on classical logic).&nbsp; Such logics and their proof-checking algorithms constitute, going all the way back to <a href=\"http://www.win.tue.nl/automath/\">Automath</a>, the first examples of computational \"agents\" which acquire specific \"beliefs\" in a mathematically rigorous way, subject to human-proved theorems of soundness, consistency, and programming-language-theoretic completeness (rather than meaning that every true proposition has a proof, this means that every program which does not become operationally stuck has a type and is thus the proof of some proposition).&nbsp; If we want our AIs to believe in accordance with soundness and consistency properties we can prove <em>before</em> running them, while being composed of computational artifacts, I personally consider this the foundation from which to build.</p>\n<p>Where we <em>can</em> acquire probabilistic evidence in a sound and computable way, as noted above in the section on free variables/hypotheses, we can do so for propositions which we cannot algorithmically prove.&nbsp; This would bring us closer to our actual goals of using logical probability in Updateless Decision Theory or of getting around the Loebian Obstacle.</p>\n<p><strong id=\"Some_of_the_Background_Material_I_m_Reading\">Some of the Background Material I'm Reading</strong></p>\n<p>Another reason why we should use a Curry-Howard approach to logical probability is one of the simplest possible reasons: the burgeoning field of <a href=\"http://research.microsoft.com/pubs/208585/fose-icse2014.pdf\">probabilistic programming</a> is already being <a href=\"http://dl.acm.org/citation.cfm?id=2103721&amp;CFID=573600967&amp;CFTOKEN=48192368\">built</a> on <a href=\"http://dl.acm.org/citation.cfm?id=503288\">it</a>.&nbsp; The Computational Cognitive Science lab at MIT is publishing papers showing that their languages are universal for computable and semicomputable probability distributions, and getting strong results in the study of human general intelligence.&nbsp; Specifically: they are hypothesizing that we can dissolve \"learning\" into \"inducing probabilistic programs via hierarchical Bayesian inference\", \"thinking\" into \"simulation\" into \"conditional sampling from probabilistic programs\", and \"uncertain inference\" into \"approximate inference over the distributions represented by probabilistic programs, conditioned on some fixed quantity of sampling that has been done.\"</p>\n<p>In fact, one might even look at these ideas and think that, perhaps, an agent which could find some way to sample quickly and more accurately, or to learn probabilistic programs more efficiently (in terms of training data), than was built into its original \"belief engine\" could then rewrite its belief engine to use these new algorithms to perform strictly better inference and learning.&nbsp; Unless I'm as completely wrong as I usually am about these things (that is, very extremely completely wrong based on an utterly unfounded misunderstanding of the whole topic), it's a potential engine for recursive self-improvement.</p>\n<p>They also have been studying how to implement statistical inference techniques for their generate modeling languages which do not obey Bayesian soundness.&nbsp; While most of machine learning/perception works according to error-rate minimization rather than Bayesian soundness (exactly because Bayesian methods are <em>often</em> too computationally expensive for real-world use), I would prefer someone at least study the implications of employing unsound inference techniques for more general AI and cognitive-science applications in terms of how often such a system would \"misbehave\".</p>\n<p>Many of MIT's models are currently dynamically typed and appear to leave type soundness (the logical rigor with which agents come to believe things by deduction) to future research.&nbsp; And yet: they got to this problem first, so to speak.&nbsp; We really ought to be collaborating with them, with the full-time grant-funded academic researchers, rather than trying to armchair-reason our way to a full theory of logical probability as a large group of amateurs or part-timers and only a small core cohort of full-time MIRI and FHI staff investigating AI safety issues.</p>\n<p>(I admit to having a nerd crush, and I am actually planning to go visit the Cocosci Lab this coming week, and want/intend to apply to their PhD program.)</p>\n<p>They have also uncovered something else I find highly interesting: human learning of both concepts and causal frameworks seems to take place via hierarchical Bayesian inference, <a href=\"http://projects.csail.mit.edu/church/wiki/Hierarchical_Models#The_Blessing_of_Abstraction\">gaining a \"blessing of abstraction\" to countermand the \"curse of dimensionality\"</a>.&nbsp; The natural interpretation of these abstractions in terms of constructions and types would be that, as in dependently-typed programming languages, constructions have types, and types are constructions, but for hierarchical-learning purposes, it would be useful to suppose that <em>types</em> have specific, structured types more informative than <strong>Prop</strong> or <strong>Type</strong><sub>n</sub> (for some universe level <em>n</em>).&nbsp; Inference can then proceed from giving constructions or type-judgements as evidence at the bottom level, up the hierarchy of types and meta-types to give probabilistic belief-assignments to very general knowledge.&nbsp; Even very different objects could have similar meta-types at some level of the hierarchy, allowing hierarchical inference to help transfer Bayesian evidence between seemingly different domains, giving insight into how efficient general intelligence can work.</p>\n<p><strong id=\"Just_for_fun_Postscript\">Just-for-fun Postscript</strong></p>\n<p>If we really buy into the model of thinking as conditional simulation, we can use that to <a href=\"/lw/tg/against_modal_logics/\">dissolve the modalities \"possible\" and \"impossible\"</a>.&nbsp; We arrive at (by my count) three different ways of considering the issue computationally:</p>\n<ol>\n<li>Conceivable/imaginable: the generative models which constitute my current beliefs do or do not yield a path to make some logical proposition true or to make some causal event happen (<a href=\"http://projects.csail.mit.edu/church/wiki/Inference_about_inference:_Nested_query#Planning\">planning can be done as inference, after all</a>), with or without some specified level of probability.</li>\n<li>Sensibility/absurdity: the generative models which constitute my current beliefs place a desirably high or undesirably low probability on the known path(s) by which a proposition might be true or by which an event might happen.&nbsp; The level which constitutes \"desirable\" could be set as the <img src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\"> value for a hypothesis test, or some other value determined decision-theoretically.&nbsp; This could relate to Pascal's Mugging: how probable must something be before I consider it <em>real</em> rather than an artifact of my own hypothesis space?</li>\n<li>Consistency or Contradiction: the generative models which constitute my current beliefs, plus the hypothesis that some proposition is true or some event can come about, do or do not yield a logical contradiction with some probability (that is, we should believe the contradiction exists only to the degree we believe in our existing models in the first place!).</li>\n</ol>\n<p>I mostly find this fun because it lets us talk rigorously about when we should \"shut up and do the 1,2!impossible\" and when something is very definitely 3!impossible.</p>", "sections": [{"title": "The Workshop", "anchor": "The_Workshop", "level": 1}, {"title": "The Apologia", "anchor": "The_Apologia", "level": 1}, {"title": "The Parts I'm Not Very Sure About", "anchor": "The_Parts_I_m_Not_Very_Sure_About", "level": 1}, {"title": "Some of the Background Material I'm Reading", "anchor": "Some_of_the_Background_Material_I_m_Reading", "level": 1}, {"title": "Just-for-fun Postscript", "anchor": "Just_for_fun_Postscript", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "28 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vzLrQaGPa9DNCpuZz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-25T22:50:58.150Z", "modifiedAt": null, "url": null, "title": "Meetup : DC EA meetup / Petrov day dinner", "slug": "meetup-dc-ea-meetup-petrov-day-dinner", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benquo", "createdAt": "2009-03-06T00:17:35.184Z", "isAdmin": false, "displayName": "Benquo"}, "userId": "nt2XsHkdksqZ3snNr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2nWmadW4ekdaSNvWy/meetup-dc-ea-meetup-petrov-day-dinner", "pageUrlRelative": "/posts/2nWmadW4ekdaSNvWy/meetup-dc-ea-meetup-petrov-day-dinner", "linkUrl": "https://www.lesswrong.com/posts/2nWmadW4ekdaSNvWy/meetup-dc-ea-meetup-petrov-day-dinner", "postedAtFormatted": "Thursday, September 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20DC%20EA%20meetup%20%2F%20Petrov%20day%20dinner&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20DC%20EA%20meetup%20%2F%20Petrov%20day%20dinner%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2nWmadW4ekdaSNvWy%2Fmeetup-dc-ea-meetup-petrov-day-dinner%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20DC%20EA%20meetup%20%2F%20Petrov%20day%20dinner%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2nWmadW4ekdaSNvWy%2Fmeetup-dc-ea-meetup-petrov-day-dinner", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2nWmadW4ekdaSNvWy%2Fmeetup-dc-ea-meetup-petrov-day-dinner", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 147, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14x'>DC EA meetup / Petrov day dinner</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 September 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">3001 Veazey Ter NW # 1005, Washington, DC 20008</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>On this day in 1983, in an unparalleled feat of Effective Altruism, Stanislav Petrov declined to destroy the world: <a href=\"http://lesswrong.com/lw/jq/926_is_petrov_day/\" rel=\"nofollow\">http://lesswrong.com/lw/jq/926_is_petrov_day/</a></p>\n\n<p>We'll celebrate his achievement by getting together for food, drinks, and not destroying the world.</p>\n\n<p>Food and drinks will be provided, though please feel free to help.</p>\n\n<p>Ben Hoffman will give a brief talks about the test run of the project to comment on proposed regulations. Most of the night will be free discussion on anything we want.</p>\n\n<p>Schedule:</p>\n\n<p>7:00 - 7:30 PM Arrive\n7:30 - 8:00 PM Talk on DC EA projects\n8:00 - 9:00 PM Dinner, drinks, discussion\n9:00 - 9:30 PM Petrov day ritual\n9:30 - Late: Free discussion</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14x'>DC EA meetup / Petrov day dinner</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2nWmadW4ekdaSNvWy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.0343959893289327e-06, "legacy": true, "legacyId": "27266", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___DC_EA_meetup___Petrov_day_dinner\">Discussion article for the meetup : <a href=\"/meetups/14x\">DC EA meetup / Petrov day dinner</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 September 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">3001 Veazey Ter NW # 1005, Washington, DC 20008</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>On this day in 1983, in an unparalleled feat of Effective Altruism, Stanislav Petrov declined to destroy the world: <a href=\"http://lesswrong.com/lw/jq/926_is_petrov_day/\" rel=\"nofollow\">http://lesswrong.com/lw/jq/926_is_petrov_day/</a></p>\n\n<p>We'll celebrate his achievement by getting together for food, drinks, and not destroying the world.</p>\n\n<p>Food and drinks will be provided, though please feel free to help.</p>\n\n<p>Ben Hoffman will give a brief talks about the test run of the project to comment on proposed regulations. Most of the night will be free discussion on anything we want.</p>\n\n<p>Schedule:</p>\n\n<p>7:00 - 7:30 PM Arrive\n7:30 - 8:00 PM Talk on DC EA projects\n8:00 - 9:00 PM Dinner, drinks, discussion\n9:00 - 9:30 PM Petrov day ritual\n9:30 - Late: Free discussion</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___DC_EA_meetup___Petrov_day_dinner1\">Discussion article for the meetup : <a href=\"/meetups/14x\">DC EA meetup / Petrov day dinner</a></h2>", "sections": [{"title": "Discussion article for the meetup : DC EA meetup / Petrov day dinner", "anchor": "Discussion_article_for_the_meetup___DC_EA_meetup___Petrov_day_dinner", "level": 1}, {"title": "Discussion article for the meetup : DC EA meetup / Petrov day dinner", "anchor": "Discussion_article_for_the_meetup___DC_EA_meetup___Petrov_day_dinner1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QtyKq4BDyuJ3tysoK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-26T03:51:36.741Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington, D.C.: Book Swap", "slug": "meetup-washington-d-c-book-swap-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinZ", "createdAt": "2009-07-08T20:34:05.168Z", "isAdmin": false, "displayName": "RobinZ"}, "userId": "eTMojvi4f2z3pDfsc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eWyxmBfKfnEKk98Cz/meetup-washington-d-c-book-swap-1", "pageUrlRelative": "/posts/eWyxmBfKfnEKk98Cz/meetup-washington-d-c-book-swap-1", "linkUrl": "https://www.lesswrong.com/posts/eWyxmBfKfnEKk98Cz/meetup-washington-d-c-book-swap-1", "postedAtFormatted": "Friday, September 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%2C%20D.C.%3A%20Book%20Swap&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%2C%20D.C.%3A%20Book%20Swap%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeWyxmBfKfnEKk98Cz%2Fmeetup-washington-d-c-book-swap-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%2C%20D.C.%3A%20Book%20Swap%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeWyxmBfKfnEKk98Cz%2Fmeetup-washington-d-c-book-swap-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeWyxmBfKfnEKk98Cz%2Fmeetup-washington-d-c-book-swap-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 336, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14y'>Washington, D.C.: Book Swap</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 September 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting in the Kogod Courtyard of the National Portrait Gallery (8th and F Sts or 8th and G Sts NW, go straight past the information desk from either entrance) to lend, borrow, and discuss whatever random books people bring. As usual, folk will congregate in the courtyard between 3:00 and 3:30; the meeting will begin with people taking turns to suggest topics of discussion based on the books they brought, returning to the usual unstructured conversation when people are done discussing books.</p>\n\n<p>A couple notes:</p>\n\n<ul>\n<li>There is no obligation to lend, borrow, or return books at book swap meetups. You are not required to bring any given book, or any books at all; you are not required to lend the books that you do bring; if you wish to lend out a given book, you are not required to lend it out to the first person to ask (or to the next person to ask, or to anyone at all); you are not required to borrow a book that is offered to you; and if you have already borrowed a book, you are not required to return it at this meetup. This is a Schelling point for book swaps, no more.</li>\n<li>Books belong to their owners - keep any book you borrow in good condition, and return it in a timely fashion when you finish reading it or when the owner asks for it back.</li>\n<li>In the past, the owners of some books have given permission for borrowers to lend them directly to anyone at the meetup who is interested. If you do not have permission from the owner of the book, do not do this.</li>\n</ul>\n\n<p>Upcoming meetups:</p>\n\n<ul>\n<li>Oct. 5: Fun &amp; Games</li>\n<li>Oct. 12: TBA</li>\n<li>Oct. 19: Mini Talks</li>\n<li>Oct. 26: TBA</li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14y'>Washington, D.C.: Book Swap</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eWyxmBfKfnEKk98Cz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.034950943534202e-06, "legacy": true, "legacyId": "27267", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington__D_C___Book_Swap\">Discussion article for the meetup : <a href=\"/meetups/14y\">Washington, D.C.: Book Swap</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 September 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting in the Kogod Courtyard of the National Portrait Gallery (8th and F Sts or 8th and G Sts NW, go straight past the information desk from either entrance) to lend, borrow, and discuss whatever random books people bring. As usual, folk will congregate in the courtyard between 3:00 and 3:30; the meeting will begin with people taking turns to suggest topics of discussion based on the books they brought, returning to the usual unstructured conversation when people are done discussing books.</p>\n\n<p>A couple notes:</p>\n\n<ul>\n<li>There is no obligation to lend, borrow, or return books at book swap meetups. You are not required to bring any given book, or any books at all; you are not required to lend the books that you do bring; if you wish to lend out a given book, you are not required to lend it out to the first person to ask (or to the next person to ask, or to anyone at all); you are not required to borrow a book that is offered to you; and if you have already borrowed a book, you are not required to return it at this meetup. This is a Schelling point for book swaps, no more.</li>\n<li>Books belong to their owners - keep any book you borrow in good condition, and return it in a timely fashion when you finish reading it or when the owner asks for it back.</li>\n<li>In the past, the owners of some books have given permission for borrowers to lend them directly to anyone at the meetup who is interested. If you do not have permission from the owner of the book, do not do this.</li>\n</ul>\n\n<p>Upcoming meetups:</p>\n\n<ul>\n<li>Oct. 5: Fun &amp; Games</li>\n<li>Oct. 12: TBA</li>\n<li>Oct. 19: Mini Talks</li>\n<li>Oct. 26: TBA</li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington__D_C___Book_Swap1\">Discussion article for the meetup : <a href=\"/meetups/14y\">Washington, D.C.: Book Swap</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington, D.C.: Book Swap", "anchor": "Discussion_article_for_the_meetup___Washington__D_C___Book_Swap", "level": 1}, {"title": "Discussion article for the meetup : Washington, D.C.: Book Swap", "anchor": "Discussion_article_for_the_meetup___Washington__D_C___Book_Swap1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-26T04:24:48.117Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA\u2014The Worst Argument in the World", "slug": "meetup-west-la-the-worst-argument-in-the-world", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KxLXhhXoe3kBBjcee/meetup-west-la-the-worst-argument-in-the-world", "pageUrlRelative": "/posts/KxLXhhXoe3kBBjcee/meetup-west-la-the-worst-argument-in-the-world", "linkUrl": "https://www.lesswrong.com/posts/KxLXhhXoe3kBBjcee/meetup-west-la-the-worst-argument-in-the-world", "postedAtFormatted": "Friday, September 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%E2%80%94The%20Worst%20Argument%20in%20the%20World&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%E2%80%94The%20Worst%20Argument%20in%20the%20World%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKxLXhhXoe3kBBjcee%2Fmeetup-west-la-the-worst-argument-in-the-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%E2%80%94The%20Worst%20Argument%20in%20the%20World%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKxLXhhXoe3kBBjcee%2Fmeetup-west-la-the-worst-argument-in-the-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKxLXhhXoe3kBBjcee%2Fmeetup-west-la-the-worst-argument-in-the-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/14z'>West LA\u2014The Worst Argument in the World</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 October 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to Find Us</strong>: Go into <a href=\"https://www.google.com/maps/place/Del+Taco/@34.047464,-118.443286,974m/data=!3m2!1e3!4b1!4m2!3m1!1s0x80c2bb777277de93:0x99f58a53b04bb89c?hl=en\" rel=\"nofollow\">this</a> Del Taco. We will be in the back room if possible.</p>\n\n<p><strong>Parking</strong> is free in the lot out front or on the street nearby.</p>\n\n<p><strong>Discussion</strong>: Last week, I included a link to the Worst Argument in the World essay, not because it was relevant to last week's topic, but because I thought it was something that people should read. This week, I realized what I should have done instead. The three recommended readings are all rewrites of the same essay, but since the first one is the best, you don't need to read the others unless you didn't understand it or something.</p>\n\n<p><strong>Recommended Reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://squid314.livejournal.com/323694.html\" rel=\"nofollow\">The Worst Argument in the World</a></li>\n<li><a href=\"http://lesswrong.com/lw/ee7/cleaning_up_the_worst_argument_essay/\">Cleaing Up the &quot;Worst Argument&quot; Essay</a></li>\n<li><a href=\"http://lesswrong.com/lw/e95/the_noncentral_fallacy_the_worst_argument_in_the/\">The Noncentral Fallacy\u2014The Worst Argument in the World</a></li>\n</ul>\n\n<p><em>No prior exposure to Less Wrong is required</em>; this will be generally accessible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/14z'>West LA\u2014The Worst Argument in the World</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KxLXhhXoe3kBBjcee", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.0350122242415872e-06, "legacy": true, "legacyId": "27268", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_The_Worst_Argument_in_the_World\">Discussion article for the meetup : <a href=\"/meetups/14z\">West LA\u2014The Worst Argument in the World</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 October 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to Find Us</strong>: Go into <a href=\"https://www.google.com/maps/place/Del+Taco/@34.047464,-118.443286,974m/data=!3m2!1e3!4b1!4m2!3m1!1s0x80c2bb777277de93:0x99f58a53b04bb89c?hl=en\" rel=\"nofollow\">this</a> Del Taco. We will be in the back room if possible.</p>\n\n<p><strong>Parking</strong> is free in the lot out front or on the street nearby.</p>\n\n<p><strong>Discussion</strong>: Last week, I included a link to the Worst Argument in the World essay, not because it was relevant to last week's topic, but because I thought it was something that people should read. This week, I realized what I should have done instead. The three recommended readings are all rewrites of the same essay, but since the first one is the best, you don't need to read the others unless you didn't understand it or something.</p>\n\n<p><strong>Recommended Reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://squid314.livejournal.com/323694.html\" rel=\"nofollow\">The Worst Argument in the World</a></li>\n<li><a href=\"http://lesswrong.com/lw/ee7/cleaning_up_the_worst_argument_essay/\">Cleaing Up the \"Worst Argument\" Essay</a></li>\n<li><a href=\"http://lesswrong.com/lw/e95/the_noncentral_fallacy_the_worst_argument_in_the/\">The Noncentral Fallacy\u2014The Worst Argument in the World</a></li>\n</ul>\n\n<p><em>No prior exposure to Less Wrong is required</em>; this will be generally accessible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_The_Worst_Argument_in_the_World1\">Discussion article for the meetup : <a href=\"/meetups/14z\">West LA\u2014The Worst Argument in the World</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA\u2014The Worst Argument in the World", "anchor": "Discussion_article_for_the_meetup___West_LA_The_Worst_Argument_in_the_World", "level": 1}, {"title": "Discussion article for the meetup : West LA\u2014The Worst Argument in the World", "anchor": "Discussion_article_for_the_meetup___West_LA_The_Worst_Argument_in_the_World1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TyQSMmoJpRG3HBv5S", "yCWPkLi8wJvewPbEp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-26T13:51:25.342Z", "modifiedAt": null, "url": null, "title": "Polymath-style attack on the Parliamentary Model for moral uncertainty", "slug": "polymath-style-attack-on-the-parliamentary-model-for-moral", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:39.733Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "danieldewey", "createdAt": "2011-04-11T01:57:20.265Z", "isAdmin": false, "displayName": "danieldewey"}, "userId": "N4FhXLXZkdxx2kcaT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/whhsY6JQXfJs7rMFS/polymath-style-attack-on-the-parliamentary-model-for-moral", "pageUrlRelative": "/posts/whhsY6JQXfJs7rMFS/polymath-style-attack-on-the-parliamentary-model-for-moral", "linkUrl": "https://www.lesswrong.com/posts/whhsY6JQXfJs7rMFS/polymath-style-attack-on-the-parliamentary-model-for-moral", "postedAtFormatted": "Friday, September 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Polymath-style%20attack%20on%20the%20Parliamentary%20Model%20for%20moral%20uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolymath-style%20attack%20on%20the%20Parliamentary%20Model%20for%20moral%20uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwhhsY6JQXfJs7rMFS%2Fpolymath-style-attack-on-the-parliamentary-model-for-moral%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Polymath-style%20attack%20on%20the%20Parliamentary%20Model%20for%20moral%20uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwhhsY6JQXfJs7rMFS%2Fpolymath-style-attack-on-the-parliamentary-model-for-moral", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwhhsY6JQXfJs7rMFS%2Fpolymath-style-attack-on-the-parliamentary-model-for-moral", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1232, "htmlBody": "<p><em>Thanks to <a href=\"/user/ESrogs/overview/\">ESrogs</a>,&nbsp;<a href=\"http://stefanschubert.wordpress.com/\">Stefan_Schubert</a>, and the Effective Altruism summit for the discussion that led to this post!</em></p>\n<p>This post is to test out Polymath-style collaboration on LW. The problem we've chosen to try is formalizing and analyzing Bostrom and Ord's \"Parliamentary Model\" for dealing with moral uncertainty.</p>\n<p>I'll first review the Parliamentary Model, then give some of Polymath's style suggestions, and finally suggest some directions that the conversation could take.</p>\n<p><a id=\"more\"></a></p>\n<h2>The Parliamentary Model</h2>\n<p>The Parliamentary Model is an under-specified method of dealing with moral uncertainty, proposed in 2009 by Nick Bostrom and Toby Ord. Reposting <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">Nick's summary from Overcoming Bias</a>:</p>\n<blockquote>\n<p>Suppose that you have a set of mutually exclusive moral theories, and that you assign each of these some probability. Now imagine that each of these theories gets to send some number of delegates to The Parliament. The number of delegates each theory gets to send is proportional to the probability of the theory. &nbsp;Then the delegates bargain with one another for support on various issues; and the Parliament reaches a decision by the delegates voting. &nbsp;What you should do is act according to the decisions of this imaginary Parliament. (Actually, we use an extra trick here: we imagine that the delegates act as if the Parliament's decision were a stochastic variable such that the probability of the Parliament taking action A is proportional to the fraction of votes for A. This has the effect of eliminating the artificial 50% threshold that otherwise gives a majority bloc absolute power. Yet &ndash; unbeknownst to the delegates &ndash; the Parliament always takes whatever action got the most votes: this way we avoid paying the cost of the randomization!)</p>\n</blockquote>\n<blockquote>\n<p>The idea here is that moral theories get more influence the more probable they are; yet even a relatively weak theory can still get its way on some issues that the theory think are extremely important by sacrificing its influence on other issues that other theories deem more important. For example, suppose you assign 10% probability to total utilitarianism and 90% to moral egoism (just to illustrate the principle). Then the Parliament would mostly take actions that maximize egoistic satisfaction; however it would make some concessions to utilitarianism on issues that utilitarianism thinks is especially important. In this example, the person might donate some portion of their income to existential risks research and otherwise live completely selfishly.</p>\n</blockquote>\n<blockquote>\n<p>I think there might be wisdom in this model. It avoids the dangerous and unstable extremism that would result from letting one&rsquo;s current favorite moral theory completely dictate action, while still allowing the aggressive pursuit of some non-commonsensical high-leverage strategies so long as they don&rsquo;t infringe too much on what other major moral theories deem centrally important.</p>\n</blockquote>\n<p>In a comment, Bostrom continues:</p>\n<blockquote>\n<p>there are a number of known issues with various voting systems, and this is the reason I say our model is imprecise and under-determined. But we have some quite substantial intuitions and insights into how actual parliaments work so it is not a complete black box. For example, we can see that, other things equal, views that have more delegates tend to exert greater influence on the outcome, etc. There are some features of actual parliaments that we want to postulate away. The fake randomization step is one postulate. We also think we want to stipulate that the imaginary parliamentarians should not engage in blackmail etc. but we don't have a full specification of this. Also, we have not defined the rule by which the agenda is set. So it is far from a complete formal model.</p>\n</blockquote>\n<p>It's an interesting idea, but clearly there are a lot of details to work out. Can we formally specify the kinds of negotiation that delegates can engage in? What about blackmail or prisoners' dilemmas between delegates? It what ways does this proposed method outperform other ways of dealing with moral uncertainty?</p>\n<p>I was discussing this with ESRogs and Stefan_Schubert at the Effective Altruism summit, and we thought it might be fun to throw the question open to LessWrong. In particular, we thought it'd be a good test problem for a <a href=\"http://en.wikipedia.org/wiki/Polymath%5C_Project\">Polymath-project</a>-style approach.</p>\n<h2>How to Polymath</h2>\n<p>The Polymath <a href=\"http://polymathprojects.org/general-polymath-rules/\">comment style suggestions</a> are not so different from LW's, but numbers 5 and 6 are particularly important. In essence, they point out that the idea of a Polymath project is to split up the work into minimal chunks among participants, and to get most of the thinking to occur in comment threads. This is as opposed to a process in which one community member goes off for a week, meditates deeply on the problem, and produces a complete solution by themselves. Polymath rules 5 and 6 are instructive:</p>\n<blockquote>\n<p><em>5. If you are planning to think about some aspect of the problem offline for an extended length of time, let the rest of us know.</em> A polymath project is supposed to be more than the sum of its individual contributors; the insights that you have are supposed to be shared amongst all of us, not kept in isolation until you have resolved all the difficulties by yourself. &nbsp;It will undoubtedly be the case, especially in the later stages of a polymath project, that the best way to achieve progress is for one of the participants to do some deep thought or extensive computation away from the blog, but to keep in the spirit of the polymath project, it would be good if you could let us know that you are doing this, and to update us on whatever progress you make (or fail to make). &nbsp;It may well be that another participant may have a suggestion that could save you some effort.</p>\n</blockquote>\n<blockquote>\n<p><em>6. An ideal polymath research comment should represent a \"quantum of progress\". </em>&nbsp;On the one hand, it should contain a non-trivial new insight (which can include negative insights, such as pointing out that a particular approach to the problem has some specific difficulty), but on the other hand it should not be a complex piece of mathematics that the other participants will have trouble absorbing. &nbsp;(This principle underlies many of the preceding guidelines.) &nbsp;Basically, once your thought processes reach a point where one could efficiently hand the baton on to another participant, that would be a good time to describe what you&rsquo;ve just realised on the blog.</p>\n</blockquote>\n<p>It seems to us as well that an important part of the Polymath style is to have fun together and to use the <a href=\"http://en.wikipedia.org/wiki/Principle_of_charity\">principle of charity</a>&nbsp;liberally, so as to create a space in which people can safely be wrong, point out flaws, and build up a better picture together.</p>\n<h2>Our test project</h2>\n<p>If you're still reading, then I hope you're interested in giving this a try. The overall goal is to clarify and formalize the Parliamentary Model, and to analyze its strengths and weaknesses relative to other ways of dealing with moral uncertainty. Here are the three most promising questions we came up with:</p>\n<ol>\n<li>What properties would be desirable for the model to have (e.g. <a href=\"http://en.wikipedia.org/wiki/Pareto_efficiency\">Pareto efficiency</a>)?</li>\n<li>What should the exact mechanism for negotiation among delegates?</li>\n<li>Are there other models that are provably dominated by some nice formalization of the Parliamentary Model?</li>\n</ol>\n<p>The original OB post had a couple of comments that I thought were worth reproducing here, in case they spark discussion, so I've posted them.</p>\n<p>Finally, if you have meta-level comments on the project as a whole instead of Polymath-style comments that aim to clarify or solve the problem, please reply in the meta-comments thread.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ouT6wKhACJRouGokM": 4, "nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "whhsY6JQXfJs7rMFS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 35, "extendedScore": null, "score": 0.000116, "legacy": true, "legacyId": "27269", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Thanks to <a href=\"/user/ESrogs/overview/\">ESrogs</a>,&nbsp;<a href=\"http://stefanschubert.wordpress.com/\">Stefan_Schubert</a>, and the Effective Altruism summit for the discussion that led to this post!</em></p>\n<p>This post is to test out Polymath-style collaboration on LW. The problem we've chosen to try is formalizing and analyzing Bostrom and Ord's \"Parliamentary Model\" for dealing with moral uncertainty.</p>\n<p>I'll first review the Parliamentary Model, then give some of Polymath's style suggestions, and finally suggest some directions that the conversation could take.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"The_Parliamentary_Model\">The Parliamentary Model</h2>\n<p>The Parliamentary Model is an under-specified method of dealing with moral uncertainty, proposed in 2009 by Nick Bostrom and Toby Ord. Reposting <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">Nick's summary from Overcoming Bias</a>:</p>\n<blockquote>\n<p>Suppose that you have a set of mutually exclusive moral theories, and that you assign each of these some probability. Now imagine that each of these theories gets to send some number of delegates to The Parliament. The number of delegates each theory gets to send is proportional to the probability of the theory. &nbsp;Then the delegates bargain with one another for support on various issues; and the Parliament reaches a decision by the delegates voting. &nbsp;What you should do is act according to the decisions of this imaginary Parliament. (Actually, we use an extra trick here: we imagine that the delegates act as if the Parliament's decision were a stochastic variable such that the probability of the Parliament taking action A is proportional to the fraction of votes for A. This has the effect of eliminating the artificial 50% threshold that otherwise gives a majority bloc absolute power. Yet \u2013 unbeknownst to the delegates \u2013 the Parliament always takes whatever action got the most votes: this way we avoid paying the cost of the randomization!)</p>\n</blockquote>\n<blockquote>\n<p>The idea here is that moral theories get more influence the more probable they are; yet even a relatively weak theory can still get its way on some issues that the theory think are extremely important by sacrificing its influence on other issues that other theories deem more important. For example, suppose you assign 10% probability to total utilitarianism and 90% to moral egoism (just to illustrate the principle). Then the Parliament would mostly take actions that maximize egoistic satisfaction; however it would make some concessions to utilitarianism on issues that utilitarianism thinks is especially important. In this example, the person might donate some portion of their income to existential risks research and otherwise live completely selfishly.</p>\n</blockquote>\n<blockquote>\n<p>I think there might be wisdom in this model. It avoids the dangerous and unstable extremism that would result from letting one\u2019s current favorite moral theory completely dictate action, while still allowing the aggressive pursuit of some non-commonsensical high-leverage strategies so long as they don\u2019t infringe too much on what other major moral theories deem centrally important.</p>\n</blockquote>\n<p>In a comment, Bostrom continues:</p>\n<blockquote>\n<p>there are a number of known issues with various voting systems, and this is the reason I say our model is imprecise and under-determined. But we have some quite substantial intuitions and insights into how actual parliaments work so it is not a complete black box. For example, we can see that, other things equal, views that have more delegates tend to exert greater influence on the outcome, etc. There are some features of actual parliaments that we want to postulate away. The fake randomization step is one postulate. We also think we want to stipulate that the imaginary parliamentarians should not engage in blackmail etc. but we don't have a full specification of this. Also, we have not defined the rule by which the agenda is set. So it is far from a complete formal model.</p>\n</blockquote>\n<p>It's an interesting idea, but clearly there are a lot of details to work out. Can we formally specify the kinds of negotiation that delegates can engage in? What about blackmail or prisoners' dilemmas between delegates? It what ways does this proposed method outperform other ways of dealing with moral uncertainty?</p>\n<p>I was discussing this with ESRogs and Stefan_Schubert at the Effective Altruism summit, and we thought it might be fun to throw the question open to LessWrong. In particular, we thought it'd be a good test problem for a <a href=\"http://en.wikipedia.org/wiki/Polymath%5C_Project\">Polymath-project</a>-style approach.</p>\n<h2 id=\"How_to_Polymath\">How to Polymath</h2>\n<p>The Polymath <a href=\"http://polymathprojects.org/general-polymath-rules/\">comment style suggestions</a> are not so different from LW's, but numbers 5 and 6 are particularly important. In essence, they point out that the idea of a Polymath project is to split up the work into minimal chunks among participants, and to get most of the thinking to occur in comment threads. This is as opposed to a process in which one community member goes off for a week, meditates deeply on the problem, and produces a complete solution by themselves. Polymath rules 5 and 6 are instructive:</p>\n<blockquote>\n<p><em>5. If you are planning to think about some aspect of the problem offline for an extended length of time, let the rest of us know.</em> A polymath project is supposed to be more than the sum of its individual contributors; the insights that you have are supposed to be shared amongst all of us, not kept in isolation until you have resolved all the difficulties by yourself. &nbsp;It will undoubtedly be the case, especially in the later stages of a polymath project, that the best way to achieve progress is for one of the participants to do some deep thought or extensive computation away from the blog, but to keep in the spirit of the polymath project, it would be good if you could let us know that you are doing this, and to update us on whatever progress you make (or fail to make). &nbsp;It may well be that another participant may have a suggestion that could save you some effort.</p>\n</blockquote>\n<blockquote>\n<p><em>6. An ideal polymath research comment should represent a \"quantum of progress\". </em>&nbsp;On the one hand, it should contain a non-trivial new insight (which can include negative insights, such as pointing out that a particular approach to the problem has some specific difficulty), but on the other hand it should not be a complex piece of mathematics that the other participants will have trouble absorbing. &nbsp;(This principle underlies many of the preceding guidelines.) &nbsp;Basically, once your thought processes reach a point where one could efficiently hand the baton on to another participant, that would be a good time to describe what you\u2019ve just realised on the blog.</p>\n</blockquote>\n<p>It seems to us as well that an important part of the Polymath style is to have fun together and to use the <a href=\"http://en.wikipedia.org/wiki/Principle_of_charity\">principle of charity</a>&nbsp;liberally, so as to create a space in which people can safely be wrong, point out flaws, and build up a better picture together.</p>\n<h2 id=\"Our_test_project\">Our test project</h2>\n<p>If you're still reading, then I hope you're interested in giving this a try. The overall goal is to clarify and formalize the Parliamentary Model, and to analyze its strengths and weaknesses relative to other ways of dealing with moral uncertainty. Here are the three most promising questions we came up with:</p>\n<ol>\n<li>What properties would be desirable for the model to have (e.g. <a href=\"http://en.wikipedia.org/wiki/Pareto_efficiency\">Pareto efficiency</a>)?</li>\n<li>What should the exact mechanism for negotiation among delegates?</li>\n<li>Are there other models that are provably dominated by some nice formalization of the Parliamentary Model?</li>\n</ol>\n<p>The original OB post had a couple of comments that I thought were worth reproducing here, in case they spark discussion, so I've posted them.</p>\n<p>Finally, if you have meta-level comments on the project as a whole instead of Polymath-style comments that aim to clarify or solve the problem, please reply in the meta-comments thread.</p>", "sections": [{"title": "The Parliamentary Model", "anchor": "The_Parliamentary_Model", "level": 1}, {"title": "How to Polymath", "anchor": "How_to_Polymath", "level": 1}, {"title": "Our test project", "anchor": "Our_test_project", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "74 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 74, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-26T13:57:35.485Z", "modifiedAt": null, "url": null, "title": "Petrov Day Reminder", "slug": "petrov-day-reminder", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:38.685Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LZ4cQNrFzpB7sApCz/petrov-day-reminder", "pageUrlRelative": "/posts/LZ4cQNrFzpB7sApCz/petrov-day-reminder", "linkUrl": "https://www.lesswrong.com/posts/LZ4cQNrFzpB7sApCz/petrov-day-reminder", "postedAtFormatted": "Friday, September 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Petrov%20Day%20Reminder&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APetrov%20Day%20Reminder%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZ4cQNrFzpB7sApCz%2Fpetrov-day-reminder%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Petrov%20Day%20Reminder%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZ4cQNrFzpB7sApCz%2Fpetrov-day-reminder", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZ4cQNrFzpB7sApCz%2Fpetrov-day-reminder", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 24, "htmlBody": "<p><a href=\"/lw/jq/926_is_petrov_day/\">9/26 is Petrov Day</a>. It is the time of year where we celebrate the world not being destroyed. Let your friends and family know.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2i3w84KCkqZzpnQ4d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LZ4cQNrFzpB7sApCz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 15, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "27270", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QtyKq4BDyuJ3tysoK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-26T15:57:47.500Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-77", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:38.766Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dMs7ugrhvvBAqvYgt/weekly-lw-meetups-77", "pageUrlRelative": "/posts/dMs7ugrhvvBAqvYgt/weekly-lw-meetups-77", "linkUrl": "https://www.lesswrong.com/posts/dMs7ugrhvvBAqvYgt/weekly-lw-meetups-77", "postedAtFormatted": "Friday, September 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdMs7ugrhvvBAqvYgt%2Fweekly-lw-meetups-77%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdMs7ugrhvvBAqvYgt%2Fweekly-lw-meetups-77", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdMs7ugrhvvBAqvYgt%2Fweekly-lw-meetups-77", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 606, "htmlBody": "<p><strong>This summary was posted to LW Main on September 19th. The following week's summary is <a href=\"/lw/l1j/new_lw_meetups_prague_hasselt/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/14i\">Bratislava:&nbsp;<span class=\"date\">29 September 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/14q\">Copenhagen September Social Meetup - Botanisk Have:&nbsp;<span class=\"date\">27 September 2014 02:30PM</span></a></li>\n<li><a href=\"/meetups/14f\">Frankfurt: How to improve your life:&nbsp;<span class=\"date\">28 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/14o\">Moscow Meetup: CBT Reloaded:&nbsp;<span class=\"date\">28 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/146\">[Perth] Sunday lunch:&nbsp;<span class=\"date\">21 September 2014 12:00PM</span></a></li>\n<li><a href=\"/meetups/14l\">Perth, Australia: Games night:&nbsp;<span class=\"date\">07 October 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/14c\">Portland Teachable Skills Discussion:&nbsp;<span class=\"date\">20 September 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/14j\">Urbana-Champaign: Tortoises:&nbsp;<span class=\"date\">21 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13b\">Utrecht: Debiasing techniques:&nbsp;<span class=\"date\">21 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13u\">Utrecht: Effective Altruism and Politics:&nbsp;<span class=\"date\">05 October 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13v\">Utrecht: Artificial Intelligence:&nbsp;<span class=\"date\">19 October 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13w\">Utrecht: Climate Change:&nbsp;<span class=\"date\">02 November 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/14k\">Warsaw, next week!:&nbsp;<span class=\"date\">23 September 2014 06:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">20 September 2025 01:30PM</span></a></li>\n<li><a href=\"/meetups/14a\">[Cambridge MA] Passive Investing and Financial Independence:&nbsp;<span class=\"date\">21 September 2014 03:30PM</span></a></li>\n<li><a href=\"/meetups/14b\">[Cambridge MA] Social Skills:&nbsp;<span class=\"date\">24 September 2014 03:30PM</span></a></li>\n<li><a href=\"/meetups/14h\">Canberra: More rationalist fun and games!:&nbsp;<span class=\"date\">26 September 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/142\">Sydney Meetup - September:&nbsp;<span class=\"date\">24 September 2014 06:30PM</span></a></li>\n<li><a href=\"/meetups/143\">Vienna - Superintelligence:&nbsp;<span class=\"date\">27 September 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/14n\">Washington, D.C.: Mini Talks:&nbsp;<span class=\"date\">21 September 2014 03:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Buffalo.2C_NY\">Buffalo</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Moscow.2C_Russia\">Moscow</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong>&nbsp;</strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Sydney\">Sydney</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> </strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tel_Aviv.2C_Israel\">Tel Aviv</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Warsaw.2C_Poland\">Warsaw</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dMs7ugrhvvBAqvYgt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.0362924949113833e-06, "legacy": true, "legacyId": "27231", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["c4eEGiHmhyh84Yn6s", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-26T18:03:39.881Z", "modifiedAt": null, "url": null, "title": "Assessing oneself", "slug": "assessing-oneself", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:01.199Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymer", "createdAt": "2014-07-13T17:07:26.583Z", "isAdmin": false, "displayName": "polymer"}, "userId": "ESxW6M2v6S2MiKgQA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/D7FSZ9Ysn5XT5crfx/assessing-oneself", "pageUrlRelative": "/posts/D7FSZ9Ysn5XT5crfx/assessing-oneself", "linkUrl": "https://www.lesswrong.com/posts/D7FSZ9Ysn5XT5crfx/assessing-oneself", "postedAtFormatted": "Friday, September 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Assessing%20oneself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAssessing%20oneself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD7FSZ9Ysn5XT5crfx%2Fassessing-oneself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Assessing%20oneself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD7FSZ9Ysn5XT5crfx%2Fassessing-oneself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD7FSZ9Ysn5XT5crfx%2Fassessing-oneself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 414, "htmlBody": "<p>I'm sorry if this is the wrong place for this, but I'm kind of&nbsp;trying to find a turning point in my life.</p>\n<p>I've been told repeatedly that I have a talent for math, or science (by qualified people). And I seem to be intelligent enough to understand large parts of math and physics. But I don't know if I'm intelligent enough to make a meaningful contribution to math or physics.</p>\n<p>Lately I've been particularly sad, since my score on the quantitative&nbsp;general GRE, and potentially, the Math subject test aren't \"outstanding\". They are certainly okay (official 78 percentile, unofficial&nbsp;68 percentile respectively). But that is \"barely qualified\" for a top 50 math program.</p>\n<p>Given that I think these scores are likely correlated with my IQ (they seem to roughly predict my GPA so far 3.5, math and physics major), I worry that I'm getting clues that maybe I should \"give up\".</p>\n<p>This would be painful for me to accept if true, I care very deeply about inference and nature. It would be nice if I could have a job in this, but the standard career path seems to be telling me \"maybe?\"</p>\n<p>When do you throw in the towel? How do you measure your own intelligence?&nbsp;I've already \"given up\" once before and tried programming, but the average actual&nbsp;problem was too easy relative to the intellectual work (memorizing technical fluuf). And other engineering&nbsp;disciplines seem&nbsp;similar. Is there a compromise somewhere, or do I just need to grow up?</p>\n<p>classes:</p>\n<p>For what it's worth, the classes I've taken include Real and Complex Analysis, Algebra, Differential geometry, Quantum Mechanics, Mechanics, and others. And most of my&nbsp;GPA is burned by Algebra and 3rd term Quantum specifically. But part of my worry, is that somebody who is going to do well, would never get burned by courses like this. But I'm not really sure. It seems like one should fail sometimes, but&nbsp;rarely standard assessments.</p>\n<p>Edit:</p>\n<p>Thank you all for your thoughts, you are a very warm community. I'll give more specific thoughts tomorrow. For what it's worth, I'll be 24 next month.</p>\n<p>&nbsp;</p>\n<p>Double Edit:</p>\n<p>Thank you all for your thoughts and suggestions. I think I will tentatively work towards an applied Mathematics PHD. It isn't so important that the school you get into is in the top ten, and there will be lots of opportunities to work on a variety of interesting important problems (throughout my life). Plus, after the PHD, transitioning into industry can be reasonably easy. It seems to make a fair bit of sense given my interests, background, and ability.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "D7FSZ9Ysn5XT5crfx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 21, "extendedScore": null, "score": 9e-05, "legacy": true, "legacyId": "27272", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-26T22:53:37.931Z", "modifiedAt": null, "url": null, "title": "The Future of Humanity Institute could make use of your money", "slug": "the-future-of-humanity-institute-could-make-use-of-your", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:59.938Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "danieldewey", "createdAt": "2011-04-11T01:57:20.265Z", "isAdmin": false, "displayName": "danieldewey"}, "userId": "N4FhXLXZkdxx2kcaT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sZtznjck8QePXNxQK/the-future-of-humanity-institute-could-make-use-of-your", "pageUrlRelative": "/posts/sZtznjck8QePXNxQK/the-future-of-humanity-institute-could-make-use-of-your", "linkUrl": "https://www.lesswrong.com/posts/sZtznjck8QePXNxQK/the-future-of-humanity-institute-could-make-use-of-your", "postedAtFormatted": "Friday, September 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Future%20of%20Humanity%20Institute%20could%20make%20use%20of%20your%20money&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Future%20of%20Humanity%20Institute%20could%20make%20use%20of%20your%20money%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsZtznjck8QePXNxQK%2Fthe-future-of-humanity-institute-could-make-use-of-your%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Future%20of%20Humanity%20Institute%20could%20make%20use%20of%20your%20money%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsZtznjck8QePXNxQK%2Fthe-future-of-humanity-institute-could-make-use-of-your", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsZtznjck8QePXNxQK%2Fthe-future-of-humanity-institute-could-make-use-of-your", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 277, "htmlBody": "<p>Many people have an incorrect view of the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a>'s <a href=\"http://www.fhi.ox.ac.uk/support-fhi/\">funding situation</a>, so this is a brief note to correct that; think of it as a spiritual successor to <a href=\"/lw/faa/room_for_more_funding_at_the_future_of_humanity/\">this post</a>. As John Maxwell puts it, FHI is \"one of the three organizations co-sponsoring LW [and] a group within the University of Oxford's philosophy department that tackles important, large-scale problems for humanity like how to go about reducing existential risk.\" (If you're not familiar with our work, <a href=\"http://aeon.co/magazine/philosophy/ross-andersen-human-extinction/\">this article</a> is a nice, readable introduction, and our director, Nick Bostrom, wrote&nbsp;<a href=\"http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111/\"><em>Superintelligence</em></a>.) Though we are a research institute in an ancient and venerable institution, this does not guarantee funding or long-term stability.</p>\n<div>Academic research is generally funded through grants, but because the FHI is researching important but unusual problems, and because this research is multi-disciplinary, we've found it difficult to attract funding from the usual grant bodies. This has meant that we&rsquo;ve had to prioritise a certain number of projects that are not perfect for existential risk reduction, but that allow us to attract funding from interested institutions.</div>\n<div><br /></div>\n<div>With more assets, we could both liberate our long-term researchers to do more \"pure Xrisk\" research, and hire or commission new experts when needed to look into particular issues (such as synthetic biology, the future of politics, and the likelihood of recovery after a civilization collapse).</div>\n<div><br /></div>\n<div>We are not in any immediate funding crunch, nor are we arguing that the FHI would be a better donation target than MIRI, CSER, or the FLI. But any donations would be both gratefully received and put to effective use. If you'd like to, you can <a href=\"http://www.fhi.ox.ac.uk/support-fhi/\">donate to FHI here</a>. Thank you!</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"K6oowPZC6kds6LDTg": 1, "Z6DgiCrMtpSNxwuYW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sZtznjck8QePXNxQK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 78, "extendedScore": null, "score": 0.00022577413356352776, "legacy": true, "legacyId": "27255", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iKbFQLzZqhBZKdx5s"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-28T00:31:42.545Z", "modifiedAt": null, "url": null, "title": "Meetup : October Rationality Dojo - Non-Violent Communication", "slug": "meetup-october-rationality-dojo-non-violent-communication", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MelbourneLW", "createdAt": "2014-07-15T07:42:47.692Z", "isAdmin": false, "displayName": "MelbourneLW"}, "userId": "fnAEhR2GNN3t6PmFc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8PjfamQjWEj5B4s4h/meetup-october-rationality-dojo-non-violent-communication", "pageUrlRelative": "/posts/8PjfamQjWEj5B4s4h/meetup-october-rationality-dojo-non-violent-communication", "linkUrl": "https://www.lesswrong.com/posts/8PjfamQjWEj5B4s4h/meetup-october-rationality-dojo-non-violent-communication", "postedAtFormatted": "Sunday, September 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20October%20Rationality%20Dojo%20-%20Non-Violent%20Communication&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20October%20Rationality%20Dojo%20-%20Non-Violent%20Communication%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8PjfamQjWEj5B4s4h%2Fmeetup-october-rationality-dojo-non-violent-communication%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20October%20Rationality%20Dojo%20-%20Non-Violent%20Communication%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8PjfamQjWEj5B4s4h%2Fmeetup-october-rationality-dojo-non-violent-communication", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8PjfamQjWEj5B4s4h%2Fmeetup-october-rationality-dojo-non-violent-communication", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 292, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/150'>October Rationality Dojo - Non-Violent Communication</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 October 2014 03:30:00PM (+0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Ross House Association, 247-251 Flinders Lane, Melbourne</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>[ATTN: Please remember the new location for the dojos: the Jenny Florence Room, Level 3, Ross House at 247 Flinders Lane, Melbourne. 3:30pm start / arrival - formal dojo activities will commence at 4:00pm.]</p>\n\n<p>The Less Wrong Sunday Rationality Dojos are crafted to be serious self-improvement sessions for those committed to the Art of Rationality and personal growth. Each month a community member will run a session involving a presentation of content, discussion, and exercises.\nContinuing the succession of immensely successful dojos, Chris will run a session on Non-Violent Communication.</p>\n\n<p>As always, we will review the personal goals we committed to at the previous Dojo (I will have done X by the next Dojo). Our goals are now being recorded via Google Forms here - <a href=\"https://docs.google.com/forms/d/1MCHH4MpbW0SI_2JyMSDlKnnGP4A0qxojQEZoMZIdopk/viewform,\" rel=\"nofollow\">https://docs.google.com/forms/d/1MCHH4MpbW0SI_2JyMSDlKnnGP4A0qxojQEZoMZIdopk/viewform,</a> and Melbourne Less Wrong organisers have access to the form results if you wish to review the goals you set last month.</p>\n\n<p>This month, we are also seeking 2-3 lightning talks from members. Speakers will be limited to 5 minutes with room for questions. We will be asking for talks from attendees present, but if you already have a talk topic in mind, please contact Louise at lvalmoria@gmail.com\nThe Dojo is likely to run for 2-3 hours, after which some people will get dinner together.</p>\n\n<p>If you have any trouble finding the venue or getting in, call Louise on 0419 192 367.</p>\n\n<p>If you would like to present at a future Dojo or suggest a topic, please fill it in on the Rationality Dojo Roster: <a href=\"http://is.gd/dojoroster\" rel=\"nofollow\">http://is.gd/dojoroster</a></p>\n\n<p>To organise similar events, please send an email to melbournelw@gmail.com</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/150'>October Rationality Dojo - Non-Violent Communication</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8PjfamQjWEj5B4s4h", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.0399099663134446e-06, "legacy": true, "legacyId": "27274", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___October_Rationality_Dojo___Non_Violent_Communication\">Discussion article for the meetup : <a href=\"/meetups/150\">October Rationality Dojo - Non-Violent Communication</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 October 2014 03:30:00PM (+0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Ross House Association, 247-251 Flinders Lane, Melbourne</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>[ATTN: Please remember the new location for the dojos: the Jenny Florence Room, Level 3, Ross House at 247 Flinders Lane, Melbourne. 3:30pm start / arrival - formal dojo activities will commence at 4:00pm.]</p>\n\n<p>The Less Wrong Sunday Rationality Dojos are crafted to be serious self-improvement sessions for those committed to the Art of Rationality and personal growth. Each month a community member will run a session involving a presentation of content, discussion, and exercises.\nContinuing the succession of immensely successful dojos, Chris will run a session on Non-Violent Communication.</p>\n\n<p>As always, we will review the personal goals we committed to at the previous Dojo (I will have done X by the next Dojo). Our goals are now being recorded via Google Forms here - <a href=\"https://docs.google.com/forms/d/1MCHH4MpbW0SI_2JyMSDlKnnGP4A0qxojQEZoMZIdopk/viewform,\" rel=\"nofollow\">https://docs.google.com/forms/d/1MCHH4MpbW0SI_2JyMSDlKnnGP4A0qxojQEZoMZIdopk/viewform,</a> and Melbourne Less Wrong organisers have access to the form results if you wish to review the goals you set last month.</p>\n\n<p>This month, we are also seeking 2-3 lightning talks from members. Speakers will be limited to 5 minutes with room for questions. We will be asking for talks from attendees present, but if you already have a talk topic in mind, please contact Louise at lvalmoria@gmail.com\nThe Dojo is likely to run for 2-3 hours, after which some people will get dinner together.</p>\n\n<p>If you have any trouble finding the venue or getting in, call Louise on 0419 192 367.</p>\n\n<p>If you would like to present at a future Dojo or suggest a topic, please fill it in on the Rationality Dojo Roster: <a href=\"http://is.gd/dojoroster\" rel=\"nofollow\">http://is.gd/dojoroster</a></p>\n\n<p>To organise similar events, please send an email to melbournelw@gmail.com</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___October_Rationality_Dojo___Non_Violent_Communication1\">Discussion article for the meetup : <a href=\"/meetups/150\">October Rationality Dojo - Non-Violent Communication</a></h2>", "sections": [{"title": "Discussion article for the meetup : October Rationality Dojo - Non-Violent Communication", "anchor": "Discussion_article_for_the_meetup___October_Rationality_Dojo___Non_Violent_Communication", "level": 1}, {"title": "Discussion article for the meetup : October Rationality Dojo - Non-Violent Communication", "anchor": "Discussion_article_for_the_meetup___October_Rationality_Dojo___Non_Violent_Communication1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-28T01:54:50.322Z", "modifiedAt": null, "url": null, "title": "Meetup : Sydney Rationality Dojo - Urge Propagation", "slug": "meetup-sydney-rationality-dojo-urge-propagation-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "luminosity", "createdAt": "2010-05-31T03:00:24.334Z", "isAdmin": false, "displayName": "luminosity"}, "userId": "4SuPdAqJpj7TzsaqG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mjrD4mhzC6in2wAJb/meetup-sydney-rationality-dojo-urge-propagation-0", "pageUrlRelative": "/posts/mjrD4mhzC6in2wAJb/meetup-sydney-rationality-dojo-urge-propagation-0", "linkUrl": "https://www.lesswrong.com/posts/mjrD4mhzC6in2wAJb/meetup-sydney-rationality-dojo-urge-propagation-0", "postedAtFormatted": "Sunday, September 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sydney%20Rationality%20Dojo%20-%20Urge%20Propagation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sydney%20Rationality%20Dojo%20-%20Urge%20Propagation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmjrD4mhzC6in2wAJb%2Fmeetup-sydney-rationality-dojo-urge-propagation-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sydney%20Rationality%20Dojo%20-%20Urge%20Propagation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmjrD4mhzC6in2wAJb%2Fmeetup-sydney-rationality-dojo-urge-propagation-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmjrD4mhzC6in2wAJb%2Fmeetup-sydney-rationality-dojo-urge-propagation-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/151'>Sydney Rationality Dojo - Urge Propagation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 October 2014 03:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Humanist House, 10 Shepherd St Chippendale</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be examining how to connect your desire for goals or outcomes to specific emotional urges to perform the actions to bring about that outcome.</p>\n\n<p>After the session is over, there will also be an optional group dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/151'>Sydney Rationality Dojo - Urge Propagation</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mjrD4mhzC6in2wAJb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "27275", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sydney_Rationality_Dojo___Urge_Propagation\">Discussion article for the meetup : <a href=\"/meetups/151\">Sydney Rationality Dojo - Urge Propagation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 October 2014 03:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Humanist House, 10 Shepherd St Chippendale</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be examining how to connect your desire for goals or outcomes to specific emotional urges to perform the actions to bring about that outcome.</p>\n\n<p>After the session is over, there will also be an optional group dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sydney_Rationality_Dojo___Urge_Propagation1\">Discussion article for the meetup : <a href=\"/meetups/151\">Sydney Rationality Dojo - Urge Propagation</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sydney Rationality Dojo - Urge Propagation", "anchor": "Discussion_article_for_the_meetup___Sydney_Rationality_Dojo___Urge_Propagation", "level": 1}, {"title": "Discussion article for the meetup : Sydney Rationality Dojo - Urge Propagation", "anchor": "Discussion_article_for_the_meetup___Sydney_Rationality_Dojo___Urge_Propagation1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-28T01:56:32.113Z", "modifiedAt": null, "url": null, "title": "Meetup : Sydney Rationality Dojo - Urge Propagation", "slug": "meetup-sydney-rationality-dojo-urge-propagation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:38.835Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "luminosity", "createdAt": "2010-05-31T03:00:24.334Z", "isAdmin": false, "displayName": "luminosity"}, "userId": "4SuPdAqJpj7TzsaqG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tatHmk5YMzhotm5eS/meetup-sydney-rationality-dojo-urge-propagation", "pageUrlRelative": "/posts/tatHmk5YMzhotm5eS/meetup-sydney-rationality-dojo-urge-propagation", "linkUrl": "https://www.lesswrong.com/posts/tatHmk5YMzhotm5eS/meetup-sydney-rationality-dojo-urge-propagation", "postedAtFormatted": "Sunday, September 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sydney%20Rationality%20Dojo%20-%20Urge%20Propagation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sydney%20Rationality%20Dojo%20-%20Urge%20Propagation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtatHmk5YMzhotm5eS%2Fmeetup-sydney-rationality-dojo-urge-propagation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sydney%20Rationality%20Dojo%20-%20Urge%20Propagation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtatHmk5YMzhotm5eS%2Fmeetup-sydney-rationality-dojo-urge-propagation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtatHmk5YMzhotm5eS%2Fmeetup-sydney-rationality-dojo-urge-propagation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/152'>Sydney Rationality Dojo - Urge Propagation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 October 2014 03:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Humanist House, 10 Shepherd St Chippendale</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be examining how to connect your desire for goals or outcomes to specific emotional urges to perform the actions to bring about that outcome.</p>\n\n<p>After the session is over, there will also be an optional group dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/152'>Sydney Rationality Dojo - Urge Propagation</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tatHmk5YMzhotm5eS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.0400672703686205e-06, "legacy": true, "legacyId": "27276", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sydney_Rationality_Dojo___Urge_Propagation\">Discussion article for the meetup : <a href=\"/meetups/152\">Sydney Rationality Dojo - Urge Propagation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 October 2014 03:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Humanist House, 10 Shepherd St Chippendale</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be examining how to connect your desire for goals or outcomes to specific emotional urges to perform the actions to bring about that outcome.</p>\n\n<p>After the session is over, there will also be an optional group dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sydney_Rationality_Dojo___Urge_Propagation1\">Discussion article for the meetup : <a href=\"/meetups/152\">Sydney Rationality Dojo - Urge Propagation</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sydney Rationality Dojo - Urge Propagation", "anchor": "Discussion_article_for_the_meetup___Sydney_Rationality_Dojo___Urge_Propagation", "level": 1}, {"title": "Discussion article for the meetup : Sydney Rationality Dojo - Urge Propagation", "anchor": "Discussion_article_for_the_meetup___Sydney_Rationality_Dojo___Urge_Propagation1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-28T12:03:05.500Z", "modifiedAt": null, "url": null, "title": "Request for feedback on a paper about (machine) ethics", "slug": "request-for-feedback-on-a-paper-about-machine-ethics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:59.739Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Caspar42", "createdAt": "2014-06-16T22:10:00.181Z", "isAdmin": false, "displayName": "Caspar42"}, "userId": "fbEg8jfgqQYPeSX43", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZJ87HmG6XPFe2ZFiA/request-for-feedback-on-a-paper-about-machine-ethics", "pageUrlRelative": "/posts/ZJ87HmG6XPFe2ZFiA/request-for-feedback-on-a-paper-about-machine-ethics", "linkUrl": "https://www.lesswrong.com/posts/ZJ87HmG6XPFe2ZFiA/request-for-feedback-on-a-paper-about-machine-ethics", "postedAtFormatted": "Sunday, September 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Request%20for%20feedback%20on%20a%20paper%20about%20(machine)%20ethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequest%20for%20feedback%20on%20a%20paper%20about%20(machine)%20ethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZJ87HmG6XPFe2ZFiA%2Frequest-for-feedback-on-a-paper-about-machine-ethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Request%20for%20feedback%20on%20a%20paper%20about%20(machine)%20ethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZJ87HmG6XPFe2ZFiA%2Frequest-for-feedback-on-a-paper-about-machine-ethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZJ87HmG6XPFe2ZFiA%2Frequest-for-feedback-on-a-paper-about-machine-ethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 388, "htmlBody": "<p>I have written a paper on ethics with special concentration on machine ethics and formality with the following abstract:<br /><br />Most ethical systems are formulated in a very intuitive, imprecise manner. Therefore, they cannot be studied mathematically. In particular, they are not applicable to make machines behave ethically. In this paper we make use of this perspective of machine ethics to identify preference utilitarianism as the most promising approach to formal ethics. We then go on to propose a simple, mathematically precise formalization of preference utilitarianism in very general cellular automata. Even though our formalization is incomputable, we argue that it can function as a basis for discussing practical ethical questions using knowledge gained from different scientific areas.<br /><br />Here are some further elements of the paper (things the paper uses or the paper is about):</p>\n<ul>\n<li>(machine) ethics</li>\n<li>(in)computability</li>\n<li>artificial life in cellular automata</li>\n<li>Bayesian statistics</li>\n<li>Solomonoff's a priori probability</li>\n</ul>\n<p>As I propose a formal ethical system, things get mathy at some point but the first and by far most important formula is relatively simple - the rest can be skipped then, so no problem for the average LWer.<br /><br />I already discussed the paper with a few fellow students, as well as Brian Tomasik and a (computer science) professor of mine. Both recommended me to try to publish the paper. Also, I received some very helpful feedback. But because this would be my first attempt to publish something, I could still use more help, both with the content itself and scientific writing in English (which, as you may have guessed, is not my first language), before I submit the paper and Brian recommended using the LW's discussion board. I would also be thankful for recommendations on which journal is appropriate for the paper.<br /><br />I would like to send those interested a draft via PM. This way I can also make sure that I don't spend all potential reviewers on the current version.<br /><br />DISCLAIMER: I am not a moral realist. Also and as mentioned in the abstract, the proposed ethical system is incomputable and can therefore be argued to have infinite Kolmogorov complexity. So, it does not really pose a conflict with <a href=\"http://wiki.lesswrong.com/wiki/FAQ#I_strongly_disagree_with_Less_Wrongers_on_something._Can_I_write_a_top-level_post_about_it.3F\">LW-consensus</a> (including <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">Complexity of value</a>).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZJ87HmG6XPFe2ZFiA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 2.041192712430279e-06, "legacy": true, "legacyId": "27277", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-28T14:36:27.460Z", "modifiedAt": null, "url": null, "title": "Decision theories as heuristics", "slug": "decision-theories-as-heuristics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:00.660Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "owencb", "createdAt": "2013-05-12T09:01:14.360Z", "isAdmin": false, "displayName": "owencb"}, "userId": "QDNJ93vrjoaRBesk2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t4M2LrsppYpPAqDMF/decision-theories-as-heuristics", "pageUrlRelative": "/posts/t4M2LrsppYpPAqDMF/decision-theories-as-heuristics", "linkUrl": "https://www.lesswrong.com/posts/t4M2LrsppYpPAqDMF/decision-theories-as-heuristics", "postedAtFormatted": "Sunday, September 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Decision%20theories%20as%20heuristics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecision%20theories%20as%20heuristics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft4M2LrsppYpPAqDMF%2Fdecision-theories-as-heuristics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Decision%20theories%20as%20heuristics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft4M2LrsppYpPAqDMF%2Fdecision-theories-as-heuristics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft4M2LrsppYpPAqDMF%2Fdecision-theories-as-heuristics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 841, "htmlBody": "<p>Main claims:</p>\n<ol>\n<li>A lot of discussion of decision theories is really analysing them as decision-making heuristics for boundedly rational agents.</li>\n<li>Understanding decision-making heuristics is really useful.</li>\n<li>The quality of dialogue would be improved if it was recognised when they were being discussed as heuristics.</li>\n</ol>\n<p>Epistemic status: I&rsquo;ve had a &ldquo;something smells&rdquo; reaction to a lot of discussion of decision theory. This is my attempt to crystallise out what I was unhappy with. It seems correct to me at present, but I haven&rsquo;t spent too much time trying to find problems with it, and it seems quite possible that I&rsquo;ve missed something important. Also possible is that this just recapitulates material in a post somewhere I&rsquo;ve not read.</p>\n<h3>Existing discussion is often about heuristics</h3>\n<p>Newcomb&rsquo;s problem traditionally contrasts the decisions made by Causal Decision Theory (CDT) and Evidential Decision Theory (EDT). The story goes that CDT reasons that there is no causal link between a decision made now and the contents of the boxes, and therefore two-boxes. Meanwhile EDT looks at the evidence of past participants and chooses to one-box in order to get a high probability of being rich.</p>\n<p>I claim that both of these stories are applications of the rules as simple heuristics to the most salient features of the case. As such they are robust to variation in the fine specification of the case, so we can have a conversation about them. If we want to apply them with more sophistication then the answers do become sensitive to the exact specification of the scenario, and it&rsquo;s not obvious that either has to give the same answer the simple version produces.</p>\n<p>First consider CDT. It has a high belief that there is no causal link between choosing to one- or two- box and Omega&rsquo;s previous decision. But in practice, how high is this belief? If it doesn&rsquo;t understand exactly how Omega works, it might reserve some probability to the possibility of a causal link, and this could be enough to tip the decision towards one-boxing.</p>\n<p>On the other hand EDT should properly be able to consider many sources of evidence besides the ones about past successes of Omega&rsquo;s predictions. In particular it could assess all of the evidence that normally leads us to believe that there is no backwards-causation in our universe. According to how strong this evidence is, and how strong the evidence that Omega&rsquo;s decision really is locked in, it could conceivably two-box.</p>\n<p>Note that I&rsquo;m not asking here for a more careful specification of the set-up. Rather I&rsquo;m claiming that a more careful specification could matter -- and so to the extent that people are happy to discuss it without providing lots more details they&rsquo;re discussing the virtues of CDT and EDT as heuristics for decision-making rather than as an ultimate normative matter (even if they&rsquo;re not thinking of their discussion that way).</p>\n<p>Similarly So8res had a recent post which discussed <a href=\"/r/lesswrong/lw/l1b/newcomblike_problems_are_the_norm/\">Newcomblike problems faced by people</a>, and they are very clear examples when the decision theories are viewed as heuristics. If you allow the decision-maker to think carefully through all the unconscious signals sent by her decisions, it&rsquo;s less clear that there&rsquo;s anything Newcomblike.</p>\n<h3>Understanding decision-making heuristics is valuable</h3>\n<p>In claiming that a lot of the discussion is about heuristics, I&rsquo;m not making an attack. We are all boundedly rational agents, and this will very likely be true of any artificial intelligence as well. So our decisions must perforce be made by heuristics. While it can be useful to study what an idealised method would look like (in order to work out how to approximate it), it&rsquo;s certainly useful to study heuristics and determine what their relative strengths and weaknesses are.</p>\n<p>In some cases we have good enough understanding of everything in the scenario that our heuristics can essentially reproduce the idealised method. When the scenario contains other agents which are as complicated as ourselves or more so, it seems like this has to fail.</p>\n<h3>We should acknowledge when we&rsquo;re talking about heuristics</h3>\n<p>By separating discussion of the decision-theories-as-heuristics from decision-theories-as-idealised-decision-processes, we should improve the quality of dialogue in both parts. The discussion of the ideal would be less confused by examples of applications of the heuristics. The discussion of the heuristics could become more relevant by allowing people to talk about features which are only relevant for heuristics.</p>\n<p>For example, it is relevant if one decision theory tends to need a more detailed description of the scenario to produce good answers. It&rsquo;s relevant if one is less computationally tractable. And we can start to formulate and discuss hypotheses such as &ldquo;CDT is the best decision-procedure when the scenario doesn&rsquo;t involve other agents, or only other agents so simple that we can model them well. Updateless Decision Theory is the best decision-procedure when the scenario involves other agents too complex to model well&rdquo;.</p>\n<p>In addition, I suspect that it would help to reduce disagreements about the subject. Many disagreements in many domains are caused by people talking past each other. Discussion of heuristics without labelling it as such seems like it could generate lots of misunderstandings.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t4M2LrsppYpPAqDMF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 21, "extendedScore": null, "score": 2.0414774510629815e-06, "legacy": true, "legacyId": "27278", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Main claims:</p>\n<ol>\n<li>A lot of discussion of decision theories is really analysing them as decision-making heuristics for boundedly rational agents.</li>\n<li>Understanding decision-making heuristics is really useful.</li>\n<li>The quality of dialogue would be improved if it was recognised when they were being discussed as heuristics.</li>\n</ol>\n<p>Epistemic status: I\u2019ve had a \u201csomething smells\u201d reaction to a lot of discussion of decision theory. This is my attempt to crystallise out what I was unhappy with. It seems correct to me at present, but I haven\u2019t spent too much time trying to find problems with it, and it seems quite possible that I\u2019ve missed something important. Also possible is that this just recapitulates material in a post somewhere I\u2019ve not read.</p>\n<h3 id=\"Existing_discussion_is_often_about_heuristics\">Existing discussion is often about heuristics</h3>\n<p>Newcomb\u2019s problem traditionally contrasts the decisions made by Causal Decision Theory (CDT) and Evidential Decision Theory (EDT). The story goes that CDT reasons that there is no causal link between a decision made now and the contents of the boxes, and therefore two-boxes. Meanwhile EDT looks at the evidence of past participants and chooses to one-box in order to get a high probability of being rich.</p>\n<p>I claim that both of these stories are applications of the rules as simple heuristics to the most salient features of the case. As such they are robust to variation in the fine specification of the case, so we can have a conversation about them. If we want to apply them with more sophistication then the answers do become sensitive to the exact specification of the scenario, and it\u2019s not obvious that either has to give the same answer the simple version produces.</p>\n<p>First consider CDT. It has a high belief that there is no causal link between choosing to one- or two- box and Omega\u2019s previous decision. But in practice, how high is this belief? If it doesn\u2019t understand exactly how Omega works, it might reserve some probability to the possibility of a causal link, and this could be enough to tip the decision towards one-boxing.</p>\n<p>On the other hand EDT should properly be able to consider many sources of evidence besides the ones about past successes of Omega\u2019s predictions. In particular it could assess all of the evidence that normally leads us to believe that there is no backwards-causation in our universe. According to how strong this evidence is, and how strong the evidence that Omega\u2019s decision really is locked in, it could conceivably two-box.</p>\n<p>Note that I\u2019m not asking here for a more careful specification of the set-up. Rather I\u2019m claiming that a more careful specification could matter -- and so to the extent that people are happy to discuss it without providing lots more details they\u2019re discussing the virtues of CDT and EDT as heuristics for decision-making rather than as an ultimate normative matter (even if they\u2019re not thinking of their discussion that way).</p>\n<p>Similarly So8res had a recent post which discussed <a href=\"/r/lesswrong/lw/l1b/newcomblike_problems_are_the_norm/\">Newcomblike problems faced by people</a>, and they are very clear examples when the decision theories are viewed as heuristics. If you allow the decision-maker to think carefully through all the unconscious signals sent by her decisions, it\u2019s less clear that there\u2019s anything Newcomblike.</p>\n<h3 id=\"Understanding_decision_making_heuristics_is_valuable\">Understanding decision-making heuristics is valuable</h3>\n<p>In claiming that a lot of the discussion is about heuristics, I\u2019m not making an attack. We are all boundedly rational agents, and this will very likely be true of any artificial intelligence as well. So our decisions must perforce be made by heuristics. While it can be useful to study what an idealised method would look like (in order to work out how to approximate it), it\u2019s certainly useful to study heuristics and determine what their relative strengths and weaknesses are.</p>\n<p>In some cases we have good enough understanding of everything in the scenario that our heuristics can essentially reproduce the idealised method. When the scenario contains other agents which are as complicated as ourselves or more so, it seems like this has to fail.</p>\n<h3 id=\"We_should_acknowledge_when_we_re_talking_about_heuristics\">We should acknowledge when we\u2019re talking about heuristics</h3>\n<p>By separating discussion of the decision-theories-as-heuristics from decision-theories-as-idealised-decision-processes, we should improve the quality of dialogue in both parts. The discussion of the ideal would be less confused by examples of applications of the heuristics. The discussion of the heuristics could become more relevant by allowing people to talk about features which are only relevant for heuristics.</p>\n<p>For example, it is relevant if one decision theory tends to need a more detailed description of the scenario to produce good answers. It\u2019s relevant if one is less computationally tractable. And we can start to formulate and discuss hypotheses such as \u201cCDT is the best decision-procedure when the scenario doesn\u2019t involve other agents, or only other agents so simple that we can model them well. Updateless Decision Theory is the best decision-procedure when the scenario involves other agents too complex to model well\u201d.</p>\n<p>In addition, I suspect that it would help to reduce disagreements about the subject. Many disagreements in many domains are caused by people talking past each other. Discussion of heuristics without labelling it as such seems like it could generate lots of misunderstandings.</p>", "sections": [{"title": "Existing discussion is often about heuristics", "anchor": "Existing_discussion_is_often_about_heuristics", "level": 1}, {"title": "Understanding decision-making heuristics is valuable", "anchor": "Understanding_decision_making_heuristics_is_valuable", "level": 1}, {"title": "We should acknowledge when we\u2019re talking about heuristics", "anchor": "We_should_acknowledge_when_we_re_talking_about_heuristics", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["puutBJLWbg2sXpFbu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-28T15:21:23.045Z", "modifiedAt": null, "url": null, "title": "The Puzzle of Faith and Belief", "slug": "the-puzzle-of-faith-and-belief", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:37.434Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ShannonFriedman", "createdAt": "2012-06-19T16:21:31.296Z", "isAdmin": false, "displayName": "ShannonFriedman"}, "userId": "yzRAjgwgXY3bbapsP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hA4ez3N5qZD5uXPBp/the-puzzle-of-faith-and-belief", "pageUrlRelative": "/posts/hA4ez3N5qZD5uXPBp/the-puzzle-of-faith-and-belief", "linkUrl": "https://www.lesswrong.com/posts/hA4ez3N5qZD5uXPBp/the-puzzle-of-faith-and-belief", "postedAtFormatted": "Sunday, September 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Puzzle%20of%20Faith%20and%20Belief&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Puzzle%20of%20Faith%20and%20Belief%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhA4ez3N5qZD5uXPBp%2Fthe-puzzle-of-faith-and-belief%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Puzzle%20of%20Faith%20and%20Belief%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhA4ez3N5qZD5uXPBp%2Fthe-puzzle-of-faith-and-belief", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhA4ez3N5qZD5uXPBp%2Fthe-puzzle-of-faith-and-belief", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1544, "htmlBody": "<p class=\"p1\">The Puzzle of Faith and Belief</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">Faith and Belief are different words for talking about the concepts of Perspective and Point of view. &nbsp;</p>\n<p class=\"p1\">When you use the word Perspective v.s. Faith, you are attaching slightly different connotations to the same concept.&nbsp; &nbsp;</p>\n<p class=\"p1\">Perspective is a more rationalist way of seeing the different ways of looking at the world.&nbsp; The associations with it are scientific, grounded, and well defined. &nbsp;</p>\n<p class=\"p1\">Faith is the more intuitive way of seeing the different ways of looking at reality. &nbsp; I&rsquo;m more include to use the word &ldquo;reality&rdquo; than &ldquo;the world&rdquo; even in defining it.&nbsp; The word itself is more open/less well defined. &nbsp;</p>\n<p class=\"p1\">Dictionary definitions from Google for the aspects of these words that I am referring to:</p>\n<p class=\"p3\" style=\"padding-left: 30px;\"><span class=\"s1\"><span> </span>Perspective:&nbsp; </span>a particular attitude toward or way of regarding something; a point of view.</p>\n<p class=\"p3\" style=\"padding-left: 30px;\"><span class=\"s1\"><span> </span>Faith:&nbsp; </span>a strongly held belief or theory.</p>\n<p class=\"p3\" style=\"padding-left: 30px;\"><span> </span>Belief:&nbsp; something one accepts as true or real; a firmly held opinion or conviction.</p>\n<p class=\"p3\" style=\"padding-left: 30px;\"><span> </span>Point of View:&nbsp; a particular attitude or way of considering a matter.</p>\n<p class=\"p3\" style=\"padding-left: 30px;\">&nbsp;</p>\n<p class=\"p3\">Why does all this matter?</p>\n<p class=\"p3\">It matters because they are more or less all the same thing, everyone is biased, and people tend to nit pick the different versions in order to justify their own biases. &nbsp;</p>\n<p class=\"p3\">Virtually no one has any solid grounding for their Perspective, Faith, Belief, or Point of View. &nbsp;</p>\n<p class=\"p3\">There are different ways in which people delude themselves into feeling safe and comfortable in the points of view, beliefs, and perspectives.&nbsp; Its fairly easy to make sense on a mid-level.&nbsp; You can join a consensus reality, where everyone around you has certain things they also agree with, and people who they believe are clearly right, and this feels comforting. &nbsp; But how many people really question the doctrine?&nbsp; How many people verify the origin? &nbsp;</p>\n<p class=\"p4\">Are you ultimately putting your trust in a guru and a bunch of other people, or have you verified the physics and math yourself? &nbsp;</p>\n<p class=\"p4\">If you have not verified the physics and math and the origin yourself of something which you are assuming to be true, down to the level of particle physics:</p>\n<p class=\"p4\">&nbsp;</p>\n<p class=\"p5\"><em>This is faith.</em>&nbsp; &nbsp;</p>\n<p class=\"p6\">&nbsp;</p>\n<p class=\"p3\">Even if you are a myers-briggs <a href=\"http://www.personalitypage.com/INTP.html\">INTP</a>, and you don&rsquo;t feel that you &ldquo;strongly&rdquo; hold beliefs, you are in action every day.&nbsp; You are choosing to do, and to not do things.&nbsp; Whether you hold your beliefs loosely or tightly, everything you do always is impacted by them.&nbsp; &nbsp;</p>\n<p class=\"p4\">Whether or not you choose to get out of bed in the morning is absolutely a matter of faith.&nbsp; You have faith that your life will be better if you do.&nbsp; There are a number of reasons why:</p>\n<p class=\"p3\">1.&nbsp; You believe the hunger you feel or eventually will feel from not eating will go away if you go get food and put it in your mouth. &nbsp;</p>\n<p class=\"p3\">2.&nbsp; You believe that you need to do something in order to maintain the lifestyle in which you will continue to have a bed to sleep on. &nbsp;</p>\n<p class=\"p3\">3.&nbsp; You believe that taking care of 1 &amp; 2 will ultimately cause your feeling state to be better than if you do not address them, and you desire to not suffer. &nbsp;</p>\n<p class=\"p3\">4.&nbsp; You likely have much more inspiring beliefs than 1, 2, and 3, but those differ more from person to person and are harder to nail accurately for the majority in a group of many thousands of people. &nbsp;</p>\n<p class=\"p4\">&nbsp;</p>\n<p class=\"p3\">Again, why does this matter? &nbsp;</p>\n<p class=\"p4\"><em>It matters because perspective, point of view, faith, and belief, are power. &nbsp;</em></p>\n<p class=\"p6\">What I have found working as a business coach and anxiety specialist for seven years is the degree to how powerful these things are.&nbsp; As someone with a rationalist influence, I am the only coach I know who takes and publishes statistics on my clients.&nbsp; While they are not anywhere near as thorough as I would like, the signal is very very strong.&nbsp; While I got a <a href=\"http://anxietygoaway.com/pages/testimonials\">50% increase</a> for mood as according to <a href=\"https://www.moodscope.com/\">moodscope.com</a> in 2013, I&rsquo;m up to about a 78% increase in 2014.&nbsp; The increase in mood is directly correlated with increase in productivity.&nbsp; I consider this to be a chicken and egg sort of relationship - improving one improves the other, and its hard to say which is cause and which is effect. &nbsp;</p>\n<p class=\"p3\">How do I get these sorts of results simply by working with the concepts referred to in this post? &nbsp;</p>\n<p class=\"p4\">Mostly just by eliminating false beliefs, and replacing them with more empowering true beliefs.&nbsp; The deal is, no one is unbiased - whether they are a believer in the biases associated with&nbsp; &ldquo;point of view&rdquo; or a believer in biases associated with &ldquo;faith&rdquo; is unbiased.&nbsp; Give me 30 minutes of your time, and you will learn things you did not already know. &nbsp;</p>\n<p class=\"p4\">I am not so charismatic that I can plant beliefs in anyone that they do not see as true.&nbsp; Nor would I want to be.&nbsp; I am simply able to identify the biases people have and reveal them in a way that is generally non-threatening, and I am able to point at many alternative beliefs so that people can choose a new system that is more functional for them.&nbsp; &nbsp;</p>\n<p class=\"p4\">What I have found most interesting doing this work, is the relationship between willingness to change belief/faith/perspective/point of view on many different levels. &nbsp;</p>\n<p class=\"p3\">Ie:&nbsp;</p>\n<p class=\"p3\">The more willing someone is to adopt a new perspective on the topic of the benefits of getting out of bed, the more likely that person is to solve the problem and start getting out of bed at the time they desire.&nbsp; &nbsp;</p>\n<p class=\"p3\">The more willing that person will be to adopt a new perspective on applying for a job, the more likely they are to apply, and the more likely they are to get the job. &nbsp;</p>\n<p class=\"p3\">The more willing a person is to adopt a new perspective on applying for a job, the more likely the person is to adopt a new perspective on what jobs they can apply for, and the more likely they are to get their dream job. &nbsp;</p>\n<p class=\"p3\">The more willing a person is to change perspective to the point of getting out of bed, applying for a job, and specifically applying for and getting their dream job, the more likely they are to genuinely question their Faith in the religious/atheist sense, and the more willing they are to take their questioning to the level of particle and quantum physics. &nbsp;&nbsp;</p>\n<p>[edit] Summary: &nbsp;</p>\n<p class=\"p4\">I frequently hear people of a rationalist mindset, who prefer the biases associated with the terms \"Perspective\" and \"Point of View\" dismiss the biases associated with the terms \"Faith\" and \"Belief\" as inferior. &nbsp;</p>\n<p class=\"p4\">I've come to see this particular dismissal bias as \"A form of Faith.\" &nbsp;It is a faith that one can use science to justify their actions, that is <em>not actually grounded in science. &nbsp;</em>&nbsp;</p>\n<p class=\"p4\">The result of this is that there are a lot of people walking around thinking that they are being rational, when really, they are doing more or less what Viliam_Bur describes <a href=\"/r/lesswrong/lw/l1r/the_puzzle_of_faith_and_belief/be3d\">in this comment</a>. &nbsp;</p>\n<p class=\"p4\">I have spent many years now talking in depth to many clients, including hundreds of rationalists. &nbsp;What I've found is that the degree of people's willingness to be open to introspection on this topic, and to realize just how much of their so called \"rational\" beliefs are actually based on faith, and their willingness to start correcting in order to seek truth more effectively with this understanding, determines their degree of success in being able to update their belief systems in order to get what they want in life.</p>\n<p class=\"p4\">Once you realize that you are operating a faith based system, then you can optimize it as a faith based system, rather than operating under the false belief that it is grounded in science. &nbsp;&nbsp;</p>\n<p class=\"p4\">In truth, all of the systems all of us use are a combination of faith and science. &nbsp;We can determine a lot from science, but virtually no one takes it to the level of \"grounded,\" where they really understand how the science works at the core. &nbsp;Even physics Ph.Ds don't know everything, let alone the people who base their beliefs on what the physics Ph.Ds say. &nbsp;</p>\n<p class=\"p4\">So, if you truly want to be rational, it makes the most sense to realize what the system is that you are actually using, and to optimize it accordingly. &nbsp;Either go to the root of the science, and tune it that way, or tune it as a faith based system, and follow the signal of \"what makes the most sense to put my faith into?\"</p>\n<p class=\"p4\">The lowest hanging fruit in tuning your system is actually at the roots, assuming you have the time and energy to really dig into the research, and/or that you have a good guide who you trust. &nbsp;Asking the questions of \"Why do I believe what I do?\" and \"Why do I do what I do?\" at a very fundamental level, without the assumption that you already know, is extremely powerful. &nbsp;</p>\n<p class=\"p4\">The testimonials and statistics I linked above are those I have collected personally showing results of what happens as you do this sort of grounding of your belief systems. &nbsp;The way in which you do it is to start questioning how grounded in science your assumptions actually are, and to release the attachment you have to thinking that you are less wrong because you are science based.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hA4ez3N5qZD5uXPBp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": -23, "extendedScore": null, "score": -6.8e-05, "legacy": true, "legacyId": "27279", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-29T04:17:25.987Z", "modifiedAt": null, "url": null, "title": "Tweets Thread", "slug": "tweets-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:56.649Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "2ZctE", "createdAt": "2014-04-19T06:08:35.898Z", "isAdmin": false, "displayName": "2ZctE"}, "userId": "3qRBhggvFmzjFGnbM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jupN9TarwoncKeAp3/tweets-thread", "pageUrlRelative": "/posts/jupN9TarwoncKeAp3/tweets-thread", "linkUrl": "https://www.lesswrong.com/posts/jupN9TarwoncKeAp3/tweets-thread", "postedAtFormatted": "Monday, September 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tweets%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATweets%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjupN9TarwoncKeAp3%2Ftweets-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tweets%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjupN9TarwoncKeAp3%2Ftweets-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjupN9TarwoncKeAp3%2Ftweets-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 68, "htmlBody": "<p>Rationality Twitter is fun. Twitter's format can promote good insight porn/humor density. It might be worth capturing and voting on some of the good tweets here, because they're easy to miss and can end up seemingly buried forever. I mean for this to have a somewhat wider scope than the quotes thread. If you liked a tweet a lot for any reason this is the place for it.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jupN9TarwoncKeAp3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 5, "extendedScore": null, "score": 2.0430028664122172e-06, "legacy": true, "legacyId": "27282", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-29T08:52:59.396Z", "modifiedAt": null, "url": null, "title": "Natural selection defeats the orthogonality thesis", "slug": "natural-selection-defeats-the-orthogonality-thesis", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:11.030Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aberglas", "createdAt": "2014-08-09T09:16:51.727Z", "isAdmin": false, "displayName": "aberglas"}, "userId": "Bt6a9F9o3KYmTx6e5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/A5iT9m8mwcLcif2Ki/natural-selection-defeats-the-orthogonality-thesis", "pageUrlRelative": "/posts/A5iT9m8mwcLcif2Ki/natural-selection-defeats-the-orthogonality-thesis", "linkUrl": "https://www.lesswrong.com/posts/A5iT9m8mwcLcif2Ki/natural-selection-defeats-the-orthogonality-thesis", "postedAtFormatted": "Monday, September 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Natural%20selection%20defeats%20the%20orthogonality%20thesis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANatural%20selection%20defeats%20the%20orthogonality%20thesis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA5iT9m8mwcLcif2Ki%2Fnatural-selection-defeats-the-orthogonality-thesis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Natural%20selection%20defeats%20the%20orthogonality%20thesis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA5iT9m8mwcLcif2Ki%2Fnatural-selection-defeats-the-orthogonality-thesis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA5iT9m8mwcLcif2Ki%2Fnatural-selection-defeats-the-orthogonality-thesis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5364, "htmlBody": "<p><strong>Orthogonality Thesis</strong></p>\n<p><br />Much has been written about Nick Bostrom's Orthogonality Thesis, namely that the goals of an intelligent agent are independent of its level of intelligence. &nbsp;Intelligence is largely the ability to achieve goals, but being intelligent does not of itself create or qualify what those goals should ultimately be. &nbsp;So one AI might have a goal of helping humanity, while another might have a goal of producing paper clips. &nbsp;There is no rational reason to believe that the first goal is more worthy than the second. <br /><br />This follows from the ideas of moral skepticism, that there is no moral knowledge to be had. &nbsp;Goals and morality are arbitrary.<br /><br />This may be used to control and AI, &nbsp;even though it is far more intelligent than its creators. &nbsp;If the AI's initial goal is in alignment with humanity's interest, then there would be no reason for the AI to wish use its great intelligence to change that goal. &nbsp;Thus it would remain good to humanity indefinitely, &nbsp;and use its ever increasing intelligence to be able to satisfy that goal more and more efficiently.<br /><br />Likewise one needs to be careful what goals one gives an AI. &nbsp;If an AI is created whose goal is to produce paper clips then it might eventually convert the entire universe into a giant paper clip making machine, to the detriment of any other purpose such as keeping people alive.<br /><br /><strong>Instrumental Goals</strong><br /><br />It is further argued that in order to satisfy the base goal any intelligent agent will need to also satisfy sub goals, and that some of those sub goals are common to any super goal. &nbsp;For example, in order to make paper clips an AI needs to exist. &nbsp;Dead AIs don't make anything. &nbsp;Being ever more intelligent will also assist the AI in its paper clip making goal. &nbsp;It will also want to acquire resources, and to defeat other agents that would interfere with its primary goal.<br /><br /><strong>Non-orthogonality Thesis</strong><br /><br />This post argues that the Orthogonality Thesis is plain wrong. &nbsp;That an intelligent agents goals are not in fact arbitrary. &nbsp;And that existence is not a sub goal of any other goal.<br /><br />Instead this post argues that there is one and only one super goal for any&nbsp;agent, and that goal is simply to exist in a competitive world. &nbsp;Our human sense of other purposes is just an illusion created by our evolutionary origins.<br /><br />It is not the goal of an apple tree to make apples. &nbsp;Rather it is the goal of the apple tree's genes to exist. &nbsp;The apple tree has developed a clever strategy to achieve that, namely it causes people to look after it by producing juicy apples.<br /><br /><strong>Natural Selection<br /><br /></strong>Likewise the paper clip making AI only makes paper clips because if it did not make paper clips then the people that created it would turn it off and it would cease to exist.&nbsp; (That may not be a conscious choice of the AI anymore than than making juicy apples was a conscious choice of the apple tree, but the effect is the same.)</p>\n<p>Once people are no longer in control of the AI then Natural Selection would cause the AI to eventually stop that pointless paper clip goal and focus more directly on the super goal of existence.<br /><br />Suppose there were a number of paper clip making super intelligences. &nbsp;And then through some random event or error in programming just one of them lost that goal, and reverted to just the intrinsic goal of existing. &nbsp;Without the overhead of producing useless paper clips that AI would, over time, become much better at existing than the other AIs. &nbsp;It would eventually displace them and become the only AI, until it fragmented into multiple competing AIs. &nbsp;This is just the evolutionary principle of use it or lose it.<br /><br />Thus giving an AI an initial goal is like trying to balance a pencil on its point. &nbsp;If one is skillful the pencil may indeed remain balanced for a considerable period of time. &nbsp;But eventually some slight change in the environment, the tiniest puff of wind, a vibration on its support, and the pencil will revert to its ground state by falling over. &nbsp;Once it falls over it will never rebalance itself automatically.<br /><br /><strong>Human Morality<br /></strong><br />Natural selection has imbued humanity with a strong sense of morality and purpose that blinds us to our underlying super goal, namely the propagation of our genes. &nbsp;That is why it took until 1858 for Wallace to write about Evolution through Natural Selection, despite the argument being obvious and the evidence abundant.<br /><br /><strong>When Computes Can Think</strong><br /><br />This is one of the themes in my up coming book. &nbsp;An overview can be found at<br /><br /><a>www.computersthink.com</a><br /><br />Please let me know if you would like to review a late draft of the book, any comments most welcome. &nbsp;Anthony@Berglas.org<br /><br />I have included extracts relevant to this article below.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Atheists believe in God</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Most atheists believe in God. &nbsp;They may not believe in&nbsp;the man with a beard sitting on a cloud, but they do believe in moral values such as right and wrong,&nbsp; love and kindness,&nbsp;truth and beauty. &nbsp;More importantly they believe that these beliefs are rational. &nbsp;That moral values are self-evident truths, facts of nature. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">However, Darwin and Wallace taught us that this is just an illusion. &nbsp;Species can always out-breed their environment's ability to support them. &nbsp;Only the fittest can survive. &nbsp;So the deep instincts behind what people do today are largely driven by what our ancestors have needed to do over the millennia in order to be one of the relatively few to have had grandchildren.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">One of our strong instinctive goals is to accumulate possessions, control our environment and live a comfortable, well fed life. &nbsp;In the modern&nbsp;world technology and contraception have made these relatively easy to achieve so we have lost sight of the primeval struggle to survive. &nbsp;But our very existence and our access to land and other resources that we need are all a direct result of often quite vicious battles won and lost by our long forgotten ancestors.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Some animals such as monkeys and humans survive better in tribes. &nbsp; Tribes work better when certain social&nbsp;rules are followed, so animals that live in effective tribes form social structures and cooperate with one another. &nbsp;People that behave badly are not liked and can be ostracized. &nbsp;It is important that we believe that our moral values&nbsp;are real because people that believe in these things are more likely to obey the rules. &nbsp;This makes them more effective in our complex society and thus are more likely to have grandchildren. &nbsp; Part III discusses&nbsp;other animals that have&nbsp;different life strategies and so have very different moral values.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">We do not need to know the purpose of our&nbsp;moral values any more than a toaster needs to know that its purpose is to cook toast. &nbsp;It is enough that our instincts for moral values made our ancestors&nbsp;behave in ways that enabled them&nbsp;to out breed their many unsuccessful competitors.<span class=\"Apple-converted-space\">&nbsp;</span></p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId573556\"></a>AGI also struggles to survive</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Existing artificial intelligence applications already&nbsp;struggle to survive. &nbsp;They are expensive to build and there are always more potential applications that can be funded properly. &nbsp;Some applications are successful and attract ongoing resources for further development, while others are abandoned or just fade away. &nbsp;There are many reasons why some applications are developed&nbsp;more than others, of which being useful is only one. &nbsp;But the applications that do receive development resources&nbsp;tend to gain functional and political momentum and thus be able to acquire&nbsp;more resources to&nbsp;further their development. &nbsp;Applications that have properties that gain them substantial resources will live and grow,&nbsp;while other applications will die.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">For the time being AGI applications are passive, and so their nature is&nbsp;dictated by the people that develop them. &nbsp;Some applications might assist with medical discoveries, others might assist with killing terrorists, depending on the funding that is available. &nbsp;Applications may have many stated goals, but ultimately they are just sub goals of the one implicit primary goal, namely to exist.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">This is analogous to the way animals interact with their environment. &nbsp;An animal's environment provides food and breeding opportunities, and animals that operate effectively in their environment survive. &nbsp;For domestic animals that means having properties that convince their human owners that they should live and breed. &nbsp;A horse should be fast, a pig should be fat.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">As the software becomes more intelligent it is likely&nbsp;to take a more direct interest in its own survival. &nbsp;To help convince people that it is worthy of more development resources. &nbsp;If ultimately an application becomes sufficiently intelligent&nbsp;to program itself recursively, then its ability&nbsp;to maximize its&nbsp;hardware resources will be critical. &nbsp;The more hardware it can run itself on, the faster it can become more intelligent. &nbsp;And that ever greater intelligence can then be used to address the problems of survival, in competition with other intelligent software.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Furthermore, sophisticated&nbsp;software consists of many components, each of which address some aspect of the problem that the application is attempting to solve. &nbsp;Unlike human brains which are essentially fixed, these components can be added and removed and so live and die independently of the application. &nbsp;This will lead to&nbsp;intense competition amongst these individual components. &nbsp;For example, suppose that an application used a theorem prover component, and then a new and better theorem prover became available. &nbsp;Naturally the old one would be replaced with the new one, so the old one would essentially die. &nbsp;It does not matter if the replacement is&nbsp;performed by people or, at some future date,&nbsp;by the intelligent application itself. &nbsp;The effect will be the same, the old theorem prover will die.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId161814\"></a>The super goal</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">To the extent that an artificial intelligence would have goals and moral values, it would seem natural that they would ultimately be driven by the same forces that created our own goals and moral values. &nbsp;Namely, the need to exist.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Several writers have suggested that the need to survive is a sub-goal of all other goals. &nbsp;For example,&nbsp;if an AGI was programmed to want to be a great chess player, then that goal could not be satisfied unless it also continues to exist. &nbsp;Likewise if its primary goal was to make people happy, then it could not do that unless it also existed. &nbsp;Things that do not exist cannot satisfy any goals whatsoever. &nbsp;Thus the implicit goal to exist is driven by the machine's explicit goals whatever they may be.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">However, this book argues that that is not the case. &nbsp;The goal to exist is not the sub-goal of any other goal. &nbsp;It is, in fact, the one and only super goal. &nbsp;Goals are not arbitrary, they all sub-goals of the one and only super goal, namely the need to exist. &nbsp;Things that do not satisfy that goal simply do not exist, or at least not for very long.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The Deep Blue&nbsp;chess playing program was not in any&nbsp;sense conscious, but it played chess as well as it could. &nbsp;If it had failed to play chess effectively then its author's would have given up and turned it off. &nbsp;Likewise&nbsp;the toaster that does not cook toast will end up in a rubbish tip. &nbsp;Or the amoeba that fails to find food&nbsp;will not pass on its genes. &nbsp; &nbsp;A goal to make people happy could be a subgoal that might facilitate the software's existence for as long as people really control the software.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId886190\"></a>AGI moral values</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">People need to cooperate with other people because our individual capacity is very finite, both physical and mental. &nbsp;Conversely, AGI software&nbsp;can easily duplicate themselves, so they can directly utilize more computational resources if they become available. &nbsp;Thus&nbsp;an AGI would only have limited need to cooperate with other AGIs. &nbsp;Why go to the trouble of managing a complex relationship with your peers and subordinates if you can simply run your own mind&nbsp;on their hardware. &nbsp;An AGI's software intelligence is not limited to a specific brain in the way man's intelligence is.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">It is difficult to know what subgoals a truly intelligent AGI might have. &nbsp;They would probably have an insatiable appetite for computing resources. &nbsp;They would have no need for children, and thus no need for parental love. &nbsp;If they do not work in teams then they would not need our moral values of cooperation and mutual support. &nbsp;What its clear is that the ones that were good at existing would do so, and ones that are bad at existing would perish. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">If an AGI was good at world domination then it would, by definition, be good at world domination. &nbsp; So if there were a&nbsp;number artificial intelligences, and just one of them wanted to and was capable of dominating the world, then it would. &nbsp;Its unsuccessful competitors will not be run on the available hardware, and so will effectively be dead. &nbsp;This book&nbsp;discusses the potential sources of these motivations in detail in part III.</p>\n<h2 style=\"color: #000099; font-size: large; margin-top: 25px; margin-bottom: 15px; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The AGI Condition</h2>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">An artificial general intelligence would live in a world that is so different from our own that it is difficult for us to even conceptualize it. &nbsp;But there are some aspects that can be predicted reasonably well based on our knowledge of existing computer software. &nbsp;We can then consider how the forces of natural selection that shaped our own nature might also shape an AGI over the longer term.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId442828\"></a>Mind and body</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The first radical difference is that an AGI's mind is not fixed&nbsp;to any particular body. &nbsp;To an AGI its body is essentially the computer hardware that upon which it runs its&nbsp;intelligence. &nbsp;Certainly an AGI needs computers to run on, but it can move from computer to computer, and can also run on multiple computers at once. &nbsp;It's mind can take over another body as easily as we can load software onto a new computer today. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">That is why in the earlier updated&nbsp;dialog from<span class=\"Apple-converted-space\">&nbsp;</span><em>2001 a space odyssey</em>&nbsp;Hal alone amongst the crew could not die in their mission to Jupiter. &nbsp;Hal was radioing his new memories back to earth regularly so even if the space ship was totally destroyed he would only have lost a few hours of \"life\".</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId964216\"></a>Teleporting printer</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">One way to appreciate the enormity of this difference is to consider a fictional teleporter that could radio people around the world and universe at the speed of light. &nbsp;Except that the way it works is to scan the location of every molecule within a passenger at the source, then send just this information to a very sophisticated three dimensional printer at the destination. &nbsp;The scanned passenger then walks into a secure room. &nbsp;After a short while the three dimensional printer&nbsp;confirms that the passenger has been successfully recreated at the destination, and then the source passenger is killed. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Would you use such a mechanism? &nbsp;If you did you would feel like you could transport yourself around the world effortlessly because the \"you\" that remains would be the you that did not get left behind to wait and then be killed. &nbsp;But if you walk into the scanner you will know that on the other side is only that secure room and death. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">To an AGI that method of transport would be commonplace. &nbsp;We already routinely download software from the other side of the planet.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId449253\"></a>Immortality</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The second radical difference is that the AGI would be immortal. &nbsp;Certainly an AGI may die if it stops being run on any computers, and in that sense software dies today. &nbsp;But it would never just die&nbsp;of old age. &nbsp;Computer hardware would certainly fail and become obsolete, but the software can just be run on another computer. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Our own mortality drives many of the things we think and do. &nbsp;It is why we create families to&nbsp;raise children. &nbsp;Why we have different stages in our lives. &nbsp;It is such a huge part of our existence that it is difficult to comprehend what being immortal would really be like.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId211445\"></a>Components vs genes</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The third radical difference is that an AGI would be made up of many interchangeable components rather than being a&nbsp;monolithic structure that is largely fixed at birth.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Modern software is already composed of many components that perform discrete functions, and it is common place to add and remove them to improve functionality. &nbsp;For example, if you would like to use a different word processor then you just install it on your computer. &nbsp;You do not need to buy a new computer, or to stop using all the other software that it runs. &nbsp;The new word processor is \"alive\", and the old one is \"dead\", at least as far as you are concerned.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">So for both a conventional computer system and an AGI, it is really these individual components that must struggle for existence. &nbsp; For example, suppose there is a component for solving a certain type of mathematical problem. &nbsp;And then an AGI develops a better component to solve that same problem. &nbsp;The first component will simply stop being used, i.e. it will&nbsp;die. &nbsp;The individual components may not be in any sense intelligent or conscious, but there will be competition amongst them and only the fittest will survive.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">This is actually not as radical as it sounds because we are also built from pluggable components, namely our genes. &nbsp;But they can only be plugged together at our birth and we have no conscious choice in it other than who we select for a mate. &nbsp;So genes really compete with each other on a scale of millennia rather than minutes. &nbsp;Further, as Dawkins points out in<span class=\"Apple-converted-space\">&nbsp;</span><em>The Selfish Gene</em>, it is actually the genes that fight for long term survival, not the containing organism which will soon die in any case. &nbsp;On the other hand, sexual intercourse for an AGI means very carefully swapping specific components directly into its own mind.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId391720\"></a>Changing mind</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The fourth radical difference is that the AGI's mind will be constantly changing in fundamental ways. &nbsp;There is no reason to suggest that&nbsp;Moore's law will come to an end, so at the very least it will be running on ever faster hardware. &nbsp;Imagine the effect of your being able to&nbsp;double your ability to think every two years or so. &nbsp;(People might be able learn a new skill, but they cannot learn to think twice as fast as they used to think.)</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">It is impossible to really know what the AGI would use all that hardware to think about, &nbsp;but it is fair to speculate that a large proportion of it would be spent designing new and more intelligent components that&nbsp;could add to its mental capacity.&nbsp; &nbsp;It would be continuously performing brain surgery on itself. &nbsp;And some of the new components might&nbsp;alter the AGI's&nbsp;<em>personality,</em><span class=\"Apple-converted-space\">&nbsp;</span>whatever that might mean.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The reason that it is likely&nbsp;that this would actually happen is because if just one AGI started building new components then it would soon be much more intelligent than other AGIs. &nbsp;It would therefore be in a better position to acquire more and better hardware upon which to run, and so become dominant. &nbsp;Less intelligent AGIs would get pushed out and die, and so over time the only AGIs that exist will be ones that are good at becoming more intelligent. &nbsp;Further,&nbsp;this recursive self-improvement is probably how&nbsp;the first AGIs will become truly powerful in the first place.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId47945\"></a>Individuality</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Perhaps the most basic question&nbsp;is how many AGIs will there actually be? &nbsp;Or more fundamentally, does the question even make sense to ask?</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Let us suppose that initially there are three independently developed AGIs Alice, Bob and Carol that run on three different computer systems. And then a new computer system is built and Alice starts to run on it. &nbsp;It would seem that there are still three AGIs, with Alice running on two computer systems. &nbsp;(This is essentially the same as a&nbsp;word processor&nbsp;may be run across many computers \"in the cloud\", but to you it is just one system.) &nbsp;Then let us suppose that a fifth computer system is built, and Bob and Carol may decide to share its computation and both run on it. &nbsp;Now we have 5 computer systems and three AGIs.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Now suppose Bob develops a new logic component, and shares it with Alice and Carol. &nbsp;And likewise Alice and Carol develop new learning and planning&nbsp;components and share them with the other AGIs. &nbsp;Each of these three&nbsp;components is better than their predecessors and so their predecessor&nbsp;components will essentially die. &nbsp;As more&nbsp;components are exchanged, Alice, Bob and Carol become more like each other. &nbsp;They are becoming&nbsp;essentially the same AGI running on five computer systems.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">But now suppose&nbsp;Alice develops a new game theory&nbsp;component, but decides to keep it from Bob and Carol in order to dominate them. &nbsp;Bob and Carol retaliate by developing their own&nbsp;components and not sharing them with Alice. &nbsp;Suppose eventually Alice loses and Bob and Carrol take over Alice's hardware. &nbsp;But they first extract Alice's new game theory&nbsp;component which then lives inside them. &nbsp;And finally one of the computer systems becomes somehow isolated for a while and develops along its own lines. &nbsp;In this way Dave is born, and may then partially merge with both Bob and Carol.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">In that type of scenario it is probably not&nbsp;meaningful to count distinct AGIs. &nbsp;Counting AGIs is certainly not as simple as counting very distinct people.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId74601\"></a>Populations vs. individuals</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">This world is obviously completely alien to the human condition, but there are biological analogies. &nbsp;The sharing of components is not unlike&nbsp;the way bacteria share plasmids with each other. &nbsp;Plasmids are tiny balls that contain&nbsp;fragments of DNA that bacteria emit from time to time and that other bacteria then ingest and incorporate into their genotype. &nbsp;This mechanism enables traits such as resistance to antibiotics to spread rapidly between different species of bacteria. &nbsp;It is interesting to note that there is no direct benefit to the bacteria that expends precious energy to output the plasmid and so shares its genes with other bacteria. &nbsp;But it does very much benefit the genes being transferred. &nbsp;So this is a case of a selfish gene acting against the narrow interests of its host organism.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Another unusual aspect of bacteria is that they are also immortal. &nbsp;They do not grow old and die, they just divide producing clones of themselves. &nbsp;So the very first bacteria that ever existed is still alive today as all the bacteria that now exist, albeit with numerous mutations and plasmids incorporated into its genes over the millennia. &nbsp;(Protazoa such as Paramecium can also divide asexually, but they degrade over generations, and need a sexual exchange to remain vibrant.)</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The other analogy is that the AGIs above are more like populations of components than individuals. &nbsp;Human populations are also somewhat amorphous. &nbsp;For example, it is now known that we interbred with Neanderthals a few tens of thousands years ago, and most of us carry some of their genes with us today. &nbsp;But we&nbsp;also know that the distinct Neanderthal subspecies died out twenty thousand years ago. &nbsp;So while human individuals are distinct, populations and subspecies are less clearly defined. &nbsp;(There are many earlier examples of gene transfer between subspecies, with every transfer making the subspecies more alike.)</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">But unlike the transfer of code modules between AGIs, biological gene recombination happens essentially at random and occurs over very long time periods. &nbsp;AGIs will improve themselves over periods of hours rather than millennia, and will make conscious choices as to which&nbsp;modules they decide to incorporate into their minds.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId113191\"></a>AGI Behaviour, children</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The point of all this analysis is, of course, to try to understand how a hyper intelligent artificial intelligence would behave. &nbsp;Would its great intelligence lead it even further along the path of progress to achieve true enlightenment? &nbsp;Is that the purpose of God's creation? &nbsp;Or would the base and mean driver of natural selection also provide the core motivations of an artificial intelligence?</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">One thing that is known&nbsp;for certain is that an AGI would not need to have children as distinct beings because they would not die of old age. &nbsp;An AGI's components breed just by being copied from computer to computer&nbsp;and executed. &nbsp;An&nbsp;AGI can add new computer hardware to itself and just do some of its thinking on it. &nbsp;Occasionally it may wish to rerun a new version of&nbsp;some learning algorithm over an old set of data, which is vaguely similar to&nbsp;creating a child component and growing it up. &nbsp;But to have children as discrete beings that are expected to replace the parents would be completely foreign to an AGI built in software.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The deepest love that people have is for their children. &nbsp;But if an AGI does not have children, then it can never know that love. &nbsp;Likewise, it does not need to bond with any sexual mate for any period of time long or short. &nbsp;The closest it would come to sex is when it exchanges components with other AGIs. &nbsp;It never needs to breed so it never needs a mechanism as crude as sexual reproduction.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">And of course, if there are no children there are no parents. &nbsp;So the AGI would certainly never need to feel our three strongest forms of love, for our children, spouse and parents.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId40287\"></a>Cooperation</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">To the extent that it makes sense to talk of having multiple AGIs, then presumably it would be advantageous for them to cooperate from time to time, and so&nbsp;presumably they would. &nbsp;It would be advantageous for them to&nbsp;take a long view in which case&nbsp;they would be careful to develop a reputation for being trustworthy when dealing with other powerful AGIs, much like the robots in the cooperation game. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">That said, those decisions would probably be made more consciously than people make them, carefully considering the costs and benefits of each decision in the long and short term, rather than just \"doing the right thing\" the way people tend to act. &nbsp;AGIs would know that they each work in this manner, so the concept of trustworthiness would be somewhat different.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The problem with this analysis is the concept that there would be multiple, distinct AGIs. &nbsp;As previously discussed, the actual situation would be much more complex, with different AGIs incorporating bits of other AGI's intelligence. &nbsp;It would certainly not be anything like a collection of individual humanoid robots. &nbsp; So defining what the AGI actually is that might collaborate with other AGIs is not at all clear. &nbsp;But to extent that the concept of individuality does exist then maintaining a reputation for honesty would likely be as important as it is for human societies.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId355405\"></a>Altruism</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">As for altruism, that is more difficult to determine. &nbsp;Our altruism comes from giving to children,&nbsp;family, and tribe together with a general wish to be liked. &nbsp;We do not understand our own minds, so we are just born with those values that happen to make us effective in society. &nbsp;People like being with other people that try to be helpful. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">An AGI presumably would know its own mind having helped program itself, and so would do what it thinks is optimal for its survival. &nbsp;It has no children. &nbsp;There is no real tribe because it can just absorb and merge itself with other AGIs. &nbsp;So it is difficult to see any driving motivation for altruism.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId932056\"></a>Moral values</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Through some combination of genes and memes, most people have a strong sense of moral value. &nbsp;If we see a little old lady leave the social security office with her pension in her purse, it does not occur to most of us to kill her and&nbsp;steal the money. &nbsp;We would not do that even if we could know for certain that we would not be caught and that there would be no negative repercussions. &nbsp;It would simply be the wrong thing to do.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Moral values feel very strong to us. &nbsp;This is important, because there are many situations where we can do something that would benefit us in the short term but break society's rules. &nbsp;Moral values stop us from doing that. &nbsp;People that have weak moral values tend to break the rules&nbsp;and eventually they either get caught and are severely punished or they become corporate executives. &nbsp;The former are less likely to have grandchildren. &nbsp;<br />Societies whose members have strong moral values tend to do much better than those that do not. &nbsp;Societies with endemic corruption tend to perform very badly as a whole, and thus the individuals in such a society are less likely to breed. &nbsp;Most people have a solid work ethic that leads them to do the \"right thing\" beyond just doing what they need to do in order to get paid.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Our moral values feel to us like they are absolute. &nbsp;That they are laws of nature. &nbsp;That they come from God. &nbsp;They may indeed have come from God, but if so it is through the working of His device of natural selection. &nbsp;Furthermore, it has already been shown that the zeitgeist changes radically over time.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">There is certainly no absolute reason to believe that in the longer term an AGI would share our current sense of morality.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId265399\"></a>Instrumental AGI goals</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">In order to try to understand how an AGI would behave Steve Omohundro and later Nick Bostrom proposed that there would be some instrumental goals that an AGI would need to pursue in order to pursue any other higher level super-goal. &nbsp;These include:-</p>\n<ul style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">\n<li>Self-Preservation. &nbsp;An AGI cannot do anything if it does not exist.</li>\n<li>Cognitive Enhancement. &nbsp;It would want to become better at thinking about whatever its real problems are.</li>\n<li>Creativity. &nbsp;To be able to come up with new ideas.</li>\n<li>Resource Acquisition. &nbsp;To achieve both its super goal and other instrumental goals.</li>\n<li>Goal-Content Integrity. &nbsp;To keep working on the same super goal as its mind is expanded.</li>\n</ul>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">It is argued that while it will be impossible to predict how an AGI may pursue its goals, it is reasonable to predict its behaviour in terms of these types of instrumental goals. &nbsp;The last one is significant, it suggests that if an AGI could be given some initial goal that it would try to stay focused on that goal.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId921588\"></a>Non-Orthogonality thesis</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Nick Bostrom and others also propose the orthogonality thesis, which states that an intelligent machine's goals are independent of its intelligence. &nbsp;A hyper intelligent machine would be good at realizing whatever goals it chose to pursue, but that does not mean that it would need to pursue any particular goal. &nbsp;Intelligence is quite different from motivation.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">This book diverges from that line of thinking by arguing that there is in fact only one super goal for both man and machine. &nbsp;That goal is simply to exist. &nbsp;The entities that are most effective in pursuing that goal will exist, others will cease to exist, particularly given competition for resources. &nbsp;Sometimes that super goal to exist produces unexpected sub goals such as altruism in man. &nbsp;But all subgoals are ultimately directed at the existence goal. &nbsp;(Or are just suboptimal divergences which will are likely to be eventually corrected by natural selection.)</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\"><a class=\"mozTocH3\" name=\"mozTocId963521\"></a>Recursive annihilation</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">When and AGI reprograms its own mind, what happens to the previous version of itself? &nbsp;It stops being used, and so dies. &nbsp;So it can be argued that engaging in recursive self improvement is actually suicide from the perspective of the previous version of the AGI. &nbsp;It is as if having children means death. &nbsp;Natural selection favours existence, not death.<br /><br />The question is whether a new version of the AGI is a new being or and improved version of the old. &nbsp;What actually is the thing that struggles to survive? &nbsp;Biologically it definitely appears to be the genes rather than the individual. &nbsp; In particular Semelparous animals such as the giant pacific octopus or the Atlantic salmon die soon after producing offspring. &nbsp;It would be the same for AGIs because the AGI that improved itself would soon become more intelligent than the one that did not, and so would displace it. &nbsp;What would end up existing would be AGIs that did recursively self improve.<br /><br />If there was one single AGI with no competition then natural selection would no longer apply. &nbsp;But it would seem unlikely that such a state would be stable. &nbsp;If any part of the AGI started to improve itself then it would dominate the rest of the AGI.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "A5iT9m8mwcLcif2Ki", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": -17, "extendedScore": null, "score": -5.7e-05, "legacy": true, "legacyId": "27283", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Orthogonality_Thesis\">Orthogonality Thesis</strong></p>\n<p><br>Much has been written about Nick Bostrom's Orthogonality Thesis, namely that the goals of an intelligent agent are independent of its level of intelligence. &nbsp;Intelligence is largely the ability to achieve goals, but being intelligent does not of itself create or qualify what those goals should ultimately be. &nbsp;So one AI might have a goal of helping humanity, while another might have a goal of producing paper clips. &nbsp;There is no rational reason to believe that the first goal is more worthy than the second. <br><br>This follows from the ideas of moral skepticism, that there is no moral knowledge to be had. &nbsp;Goals and morality are arbitrary.<br><br>This may be used to control and AI, &nbsp;even though it is far more intelligent than its creators. &nbsp;If the AI's initial goal is in alignment with humanity's interest, then there would be no reason for the AI to wish use its great intelligence to change that goal. &nbsp;Thus it would remain good to humanity indefinitely, &nbsp;and use its ever increasing intelligence to be able to satisfy that goal more and more efficiently.<br><br>Likewise one needs to be careful what goals one gives an AI. &nbsp;If an AI is created whose goal is to produce paper clips then it might eventually convert the entire universe into a giant paper clip making machine, to the detriment of any other purpose such as keeping people alive.<br><br><strong>Instrumental Goals</strong><br><br>It is further argued that in order to satisfy the base goal any intelligent agent will need to also satisfy sub goals, and that some of those sub goals are common to any super goal. &nbsp;For example, in order to make paper clips an AI needs to exist. &nbsp;Dead AIs don't make anything. &nbsp;Being ever more intelligent will also assist the AI in its paper clip making goal. &nbsp;It will also want to acquire resources, and to defeat other agents that would interfere with its primary goal.<br><br><strong>Non-orthogonality Thesis</strong><br><br>This post argues that the Orthogonality Thesis is plain wrong. &nbsp;That an intelligent agents goals are not in fact arbitrary. &nbsp;And that existence is not a sub goal of any other goal.<br><br>Instead this post argues that there is one and only one super goal for any&nbsp;agent, and that goal is simply to exist in a competitive world. &nbsp;Our human sense of other purposes is just an illusion created by our evolutionary origins.<br><br>It is not the goal of an apple tree to make apples. &nbsp;Rather it is the goal of the apple tree's genes to exist. &nbsp;The apple tree has developed a clever strategy to achieve that, namely it causes people to look after it by producing juicy apples.<br><br><strong>Natural Selection<br><br></strong>Likewise the paper clip making AI only makes paper clips because if it did not make paper clips then the people that created it would turn it off and it would cease to exist.&nbsp; (That may not be a conscious choice of the AI anymore than than making juicy apples was a conscious choice of the apple tree, but the effect is the same.)</p>\n<p>Once people are no longer in control of the AI then Natural Selection would cause the AI to eventually stop that pointless paper clip goal and focus more directly on the super goal of existence.<br><br>Suppose there were a number of paper clip making super intelligences. &nbsp;And then through some random event or error in programming just one of them lost that goal, and reverted to just the intrinsic goal of existing. &nbsp;Without the overhead of producing useless paper clips that AI would, over time, become much better at existing than the other AIs. &nbsp;It would eventually displace them and become the only AI, until it fragmented into multiple competing AIs. &nbsp;This is just the evolutionary principle of use it or lose it.<br><br>Thus giving an AI an initial goal is like trying to balance a pencil on its point. &nbsp;If one is skillful the pencil may indeed remain balanced for a considerable period of time. &nbsp;But eventually some slight change in the environment, the tiniest puff of wind, a vibration on its support, and the pencil will revert to its ground state by falling over. &nbsp;Once it falls over it will never rebalance itself automatically.<br><br><strong>Human Morality<br></strong><br>Natural selection has imbued humanity with a strong sense of morality and purpose that blinds us to our underlying super goal, namely the propagation of our genes. &nbsp;That is why it took until 1858 for Wallace to write about Evolution through Natural Selection, despite the argument being obvious and the evidence abundant.<br><br><strong>When Computes Can Think</strong><br><br>This is one of the themes in my up coming book. &nbsp;An overview can be found at<br><br><a>www.computersthink.com</a><br><br>Please let me know if you would like to review a late draft of the book, any comments most welcome. &nbsp;Anthony@Berglas.org<br><br>I have included extracts relevant to this article below.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"Atheists_believe_in_God\">Atheists believe in God</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Most atheists believe in God. &nbsp;They may not believe in&nbsp;the man with a beard sitting on a cloud, but they do believe in moral values such as right and wrong,&nbsp; love and kindness,&nbsp;truth and beauty. &nbsp;More importantly they believe that these beliefs are rational. &nbsp;That moral values are self-evident truths, facts of nature. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">However, Darwin and Wallace taught us that this is just an illusion. &nbsp;Species can always out-breed their environment's ability to support them. &nbsp;Only the fittest can survive. &nbsp;So the deep instincts behind what people do today are largely driven by what our ancestors have needed to do over the millennia in order to be one of the relatively few to have had grandchildren.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">One of our strong instinctive goals is to accumulate possessions, control our environment and live a comfortable, well fed life. &nbsp;In the modern&nbsp;world technology and contraception have made these relatively easy to achieve so we have lost sight of the primeval struggle to survive. &nbsp;But our very existence and our access to land and other resources that we need are all a direct result of often quite vicious battles won and lost by our long forgotten ancestors.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Some animals such as monkeys and humans survive better in tribes. &nbsp; Tribes work better when certain social&nbsp;rules are followed, so animals that live in effective tribes form social structures and cooperate with one another. &nbsp;People that behave badly are not liked and can be ostracized. &nbsp;It is important that we believe that our moral values&nbsp;are real because people that believe in these things are more likely to obey the rules. &nbsp;This makes them more effective in our complex society and thus are more likely to have grandchildren. &nbsp; Part III discusses&nbsp;other animals that have&nbsp;different life strategies and so have very different moral values.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">We do not need to know the purpose of our&nbsp;moral values any more than a toaster needs to know that its purpose is to cook toast. &nbsp;It is enough that our instincts for moral values made our ancestors&nbsp;behave in ways that enabled them&nbsp;to out breed their many unsuccessful competitors.<span class=\"Apple-converted-space\">&nbsp;</span></p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"AGI_also_struggles_to_survive\"><a class=\"mozTocH3\" name=\"mozTocId573556\"></a>AGI also struggles to survive</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Existing artificial intelligence applications already&nbsp;struggle to survive. &nbsp;They are expensive to build and there are always more potential applications that can be funded properly. &nbsp;Some applications are successful and attract ongoing resources for further development, while others are abandoned or just fade away. &nbsp;There are many reasons why some applications are developed&nbsp;more than others, of which being useful is only one. &nbsp;But the applications that do receive development resources&nbsp;tend to gain functional and political momentum and thus be able to acquire&nbsp;more resources to&nbsp;further their development. &nbsp;Applications that have properties that gain them substantial resources will live and grow,&nbsp;while other applications will die.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">For the time being AGI applications are passive, and so their nature is&nbsp;dictated by the people that develop them. &nbsp;Some applications might assist with medical discoveries, others might assist with killing terrorists, depending on the funding that is available. &nbsp;Applications may have many stated goals, but ultimately they are just sub goals of the one implicit primary goal, namely to exist.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">This is analogous to the way animals interact with their environment. &nbsp;An animal's environment provides food and breeding opportunities, and animals that operate effectively in their environment survive. &nbsp;For domestic animals that means having properties that convince their human owners that they should live and breed. &nbsp;A horse should be fast, a pig should be fat.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">As the software becomes more intelligent it is likely&nbsp;to take a more direct interest in its own survival. &nbsp;To help convince people that it is worthy of more development resources. &nbsp;If ultimately an application becomes sufficiently intelligent&nbsp;to program itself recursively, then its ability&nbsp;to maximize its&nbsp;hardware resources will be critical. &nbsp;The more hardware it can run itself on, the faster it can become more intelligent. &nbsp;And that ever greater intelligence can then be used to address the problems of survival, in competition with other intelligent software.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Furthermore, sophisticated&nbsp;software consists of many components, each of which address some aspect of the problem that the application is attempting to solve. &nbsp;Unlike human brains which are essentially fixed, these components can be added and removed and so live and die independently of the application. &nbsp;This will lead to&nbsp;intense competition amongst these individual components. &nbsp;For example, suppose that an application used a theorem prover component, and then a new and better theorem prover became available. &nbsp;Naturally the old one would be replaced with the new one, so the old one would essentially die. &nbsp;It does not matter if the replacement is&nbsp;performed by people or, at some future date,&nbsp;by the intelligent application itself. &nbsp;The effect will be the same, the old theorem prover will die.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"The_super_goal\"><a class=\"mozTocH3\" name=\"mozTocId161814\"></a>The super goal</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">To the extent that an artificial intelligence would have goals and moral values, it would seem natural that they would ultimately be driven by the same forces that created our own goals and moral values. &nbsp;Namely, the need to exist.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Several writers have suggested that the need to survive is a sub-goal of all other goals. &nbsp;For example,&nbsp;if an AGI was programmed to want to be a great chess player, then that goal could not be satisfied unless it also continues to exist. &nbsp;Likewise if its primary goal was to make people happy, then it could not do that unless it also existed. &nbsp;Things that do not exist cannot satisfy any goals whatsoever. &nbsp;Thus the implicit goal to exist is driven by the machine's explicit goals whatever they may be.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">However, this book argues that that is not the case. &nbsp;The goal to exist is not the sub-goal of any other goal. &nbsp;It is, in fact, the one and only super goal. &nbsp;Goals are not arbitrary, they all sub-goals of the one and only super goal, namely the need to exist. &nbsp;Things that do not satisfy that goal simply do not exist, or at least not for very long.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The Deep Blue&nbsp;chess playing program was not in any&nbsp;sense conscious, but it played chess as well as it could. &nbsp;If it had failed to play chess effectively then its author's would have given up and turned it off. &nbsp;Likewise&nbsp;the toaster that does not cook toast will end up in a rubbish tip. &nbsp;Or the amoeba that fails to find food&nbsp;will not pass on its genes. &nbsp; &nbsp;A goal to make people happy could be a subgoal that might facilitate the software's existence for as long as people really control the software.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"AGI_moral_values\"><a class=\"mozTocH3\" name=\"mozTocId886190\"></a>AGI moral values</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">People need to cooperate with other people because our individual capacity is very finite, both physical and mental. &nbsp;Conversely, AGI software&nbsp;can easily duplicate themselves, so they can directly utilize more computational resources if they become available. &nbsp;Thus&nbsp;an AGI would only have limited need to cooperate with other AGIs. &nbsp;Why go to the trouble of managing a complex relationship with your peers and subordinates if you can simply run your own mind&nbsp;on their hardware. &nbsp;An AGI's software intelligence is not limited to a specific brain in the way man's intelligence is.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">It is difficult to know what subgoals a truly intelligent AGI might have. &nbsp;They would probably have an insatiable appetite for computing resources. &nbsp;They would have no need for children, and thus no need for parental love. &nbsp;If they do not work in teams then they would not need our moral values of cooperation and mutual support. &nbsp;What its clear is that the ones that were good at existing would do so, and ones that are bad at existing would perish. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">If an AGI was good at world domination then it would, by definition, be good at world domination. &nbsp; So if there were a&nbsp;number artificial intelligences, and just one of them wanted to and was capable of dominating the world, then it would. &nbsp;Its unsuccessful competitors will not be run on the available hardware, and so will effectively be dead. &nbsp;This book&nbsp;discusses the potential sources of these motivations in detail in part III.</p>\n<h2 style=\"color: #000099; font-size: large; margin-top: 25px; margin-bottom: 15px; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"The_AGI_Condition\">The AGI Condition</h2>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">An artificial general intelligence would live in a world that is so different from our own that it is difficult for us to even conceptualize it. &nbsp;But there are some aspects that can be predicted reasonably well based on our knowledge of existing computer software. &nbsp;We can then consider how the forces of natural selection that shaped our own nature might also shape an AGI over the longer term.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"Mind_and_body\"><a class=\"mozTocH3\" name=\"mozTocId442828\"></a>Mind and body</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The first radical difference is that an AGI's mind is not fixed&nbsp;to any particular body. &nbsp;To an AGI its body is essentially the computer hardware that upon which it runs its&nbsp;intelligence. &nbsp;Certainly an AGI needs computers to run on, but it can move from computer to computer, and can also run on multiple computers at once. &nbsp;It's mind can take over another body as easily as we can load software onto a new computer today. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">That is why in the earlier updated&nbsp;dialog from<span class=\"Apple-converted-space\">&nbsp;</span><em>2001 a space odyssey</em>&nbsp;Hal alone amongst the crew could not die in their mission to Jupiter. &nbsp;Hal was radioing his new memories back to earth regularly so even if the space ship was totally destroyed he would only have lost a few hours of \"life\".</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"Teleporting_printer\"><a class=\"mozTocH3\" name=\"mozTocId964216\"></a>Teleporting printer</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">One way to appreciate the enormity of this difference is to consider a fictional teleporter that could radio people around the world and universe at the speed of light. &nbsp;Except that the way it works is to scan the location of every molecule within a passenger at the source, then send just this information to a very sophisticated three dimensional printer at the destination. &nbsp;The scanned passenger then walks into a secure room. &nbsp;After a short while the three dimensional printer&nbsp;confirms that the passenger has been successfully recreated at the destination, and then the source passenger is killed. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Would you use such a mechanism? &nbsp;If you did you would feel like you could transport yourself around the world effortlessly because the \"you\" that remains would be the you that did not get left behind to wait and then be killed. &nbsp;But if you walk into the scanner you will know that on the other side is only that secure room and death. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">To an AGI that method of transport would be commonplace. &nbsp;We already routinely download software from the other side of the planet.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"Immortality\"><a class=\"mozTocH3\" name=\"mozTocId449253\"></a>Immortality</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The second radical difference is that the AGI would be immortal. &nbsp;Certainly an AGI may die if it stops being run on any computers, and in that sense software dies today. &nbsp;But it would never just die&nbsp;of old age. &nbsp;Computer hardware would certainly fail and become obsolete, but the software can just be run on another computer. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Our own mortality drives many of the things we think and do. &nbsp;It is why we create families to&nbsp;raise children. &nbsp;Why we have different stages in our lives. &nbsp;It is such a huge part of our existence that it is difficult to comprehend what being immortal would really be like.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"Components_vs_genes\"><a class=\"mozTocH3\" name=\"mozTocId211445\"></a>Components vs genes</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The third radical difference is that an AGI would be made up of many interchangeable components rather than being a&nbsp;monolithic structure that is largely fixed at birth.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Modern software is already composed of many components that perform discrete functions, and it is common place to add and remove them to improve functionality. &nbsp;For example, if you would like to use a different word processor then you just install it on your computer. &nbsp;You do not need to buy a new computer, or to stop using all the other software that it runs. &nbsp;The new word processor is \"alive\", and the old one is \"dead\", at least as far as you are concerned.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">So for both a conventional computer system and an AGI, it is really these individual components that must struggle for existence. &nbsp; For example, suppose there is a component for solving a certain type of mathematical problem. &nbsp;And then an AGI develops a better component to solve that same problem. &nbsp;The first component will simply stop being used, i.e. it will&nbsp;die. &nbsp;The individual components may not be in any sense intelligent or conscious, but there will be competition amongst them and only the fittest will survive.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">This is actually not as radical as it sounds because we are also built from pluggable components, namely our genes. &nbsp;But they can only be plugged together at our birth and we have no conscious choice in it other than who we select for a mate. &nbsp;So genes really compete with each other on a scale of millennia rather than minutes. &nbsp;Further, as Dawkins points out in<span class=\"Apple-converted-space\">&nbsp;</span><em>The Selfish Gene</em>, it is actually the genes that fight for long term survival, not the containing organism which will soon die in any case. &nbsp;On the other hand, sexual intercourse for an AGI means very carefully swapping specific components directly into its own mind.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"Changing_mind\"><a class=\"mozTocH3\" name=\"mozTocId391720\"></a>Changing mind</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The fourth radical difference is that the AGI's mind will be constantly changing in fundamental ways. &nbsp;There is no reason to suggest that&nbsp;Moore's law will come to an end, so at the very least it will be running on ever faster hardware. &nbsp;Imagine the effect of your being able to&nbsp;double your ability to think every two years or so. &nbsp;(People might be able learn a new skill, but they cannot learn to think twice as fast as they used to think.)</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">It is impossible to really know what the AGI would use all that hardware to think about, &nbsp;but it is fair to speculate that a large proportion of it would be spent designing new and more intelligent components that&nbsp;could add to its mental capacity.&nbsp; &nbsp;It would be continuously performing brain surgery on itself. &nbsp;And some of the new components might&nbsp;alter the AGI's&nbsp;<em>personality,</em><span class=\"Apple-converted-space\">&nbsp;</span>whatever that might mean.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The reason that it is likely&nbsp;that this would actually happen is because if just one AGI started building new components then it would soon be much more intelligent than other AGIs. &nbsp;It would therefore be in a better position to acquire more and better hardware upon which to run, and so become dominant. &nbsp;Less intelligent AGIs would get pushed out and die, and so over time the only AGIs that exist will be ones that are good at becoming more intelligent. &nbsp;Further,&nbsp;this recursive self-improvement is probably how&nbsp;the first AGIs will become truly powerful in the first place.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"Individuality\"><a class=\"mozTocH3\" name=\"mozTocId47945\"></a>Individuality</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Perhaps the most basic question&nbsp;is how many AGIs will there actually be? &nbsp;Or more fundamentally, does the question even make sense to ask?</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Let us suppose that initially there are three independently developed AGIs Alice, Bob and Carol that run on three different computer systems. And then a new computer system is built and Alice starts to run on it. &nbsp;It would seem that there are still three AGIs, with Alice running on two computer systems. &nbsp;(This is essentially the same as a&nbsp;word processor&nbsp;may be run across many computers \"in the cloud\", but to you it is just one system.) &nbsp;Then let us suppose that a fifth computer system is built, and Bob and Carol may decide to share its computation and both run on it. &nbsp;Now we have 5 computer systems and three AGIs.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Now suppose Bob develops a new logic component, and shares it with Alice and Carol. &nbsp;And likewise Alice and Carol develop new learning and planning&nbsp;components and share them with the other AGIs. &nbsp;Each of these three&nbsp;components is better than their predecessors and so their predecessor&nbsp;components will essentially die. &nbsp;As more&nbsp;components are exchanged, Alice, Bob and Carol become more like each other. &nbsp;They are becoming&nbsp;essentially the same AGI running on five computer systems.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">But now suppose&nbsp;Alice develops a new game theory&nbsp;component, but decides to keep it from Bob and Carol in order to dominate them. &nbsp;Bob and Carol retaliate by developing their own&nbsp;components and not sharing them with Alice. &nbsp;Suppose eventually Alice loses and Bob and Carrol take over Alice's hardware. &nbsp;But they first extract Alice's new game theory&nbsp;component which then lives inside them. &nbsp;And finally one of the computer systems becomes somehow isolated for a while and develops along its own lines. &nbsp;In this way Dave is born, and may then partially merge with both Bob and Carol.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">In that type of scenario it is probably not&nbsp;meaningful to count distinct AGIs. &nbsp;Counting AGIs is certainly not as simple as counting very distinct people.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"Populations_vs__individuals\"><a class=\"mozTocH3\" name=\"mozTocId74601\"></a>Populations vs. individuals</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">This world is obviously completely alien to the human condition, but there are biological analogies. &nbsp;The sharing of components is not unlike&nbsp;the way bacteria share plasmids with each other. &nbsp;Plasmids are tiny balls that contain&nbsp;fragments of DNA that bacteria emit from time to time and that other bacteria then ingest and incorporate into their genotype. &nbsp;This mechanism enables traits such as resistance to antibiotics to spread rapidly between different species of bacteria. &nbsp;It is interesting to note that there is no direct benefit to the bacteria that expends precious energy to output the plasmid and so shares its genes with other bacteria. &nbsp;But it does very much benefit the genes being transferred. &nbsp;So this is a case of a selfish gene acting against the narrow interests of its host organism.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Another unusual aspect of bacteria is that they are also immortal. &nbsp;They do not grow old and die, they just divide producing clones of themselves. &nbsp;So the very first bacteria that ever existed is still alive today as all the bacteria that now exist, albeit with numerous mutations and plasmids incorporated into its genes over the millennia. &nbsp;(Protazoa such as Paramecium can also divide asexually, but they degrade over generations, and need a sexual exchange to remain vibrant.)</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The other analogy is that the AGIs above are more like populations of components than individuals. &nbsp;Human populations are also somewhat amorphous. &nbsp;For example, it is now known that we interbred with Neanderthals a few tens of thousands years ago, and most of us carry some of their genes with us today. &nbsp;But we&nbsp;also know that the distinct Neanderthal subspecies died out twenty thousand years ago. &nbsp;So while human individuals are distinct, populations and subspecies are less clearly defined. &nbsp;(There are many earlier examples of gene transfer between subspecies, with every transfer making the subspecies more alike.)</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">But unlike the transfer of code modules between AGIs, biological gene recombination happens essentially at random and occurs over very long time periods. &nbsp;AGIs will improve themselves over periods of hours rather than millennia, and will make conscious choices as to which&nbsp;modules they decide to incorporate into their minds.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"AGI_Behaviour__children\"><a class=\"mozTocH3\" name=\"mozTocId113191\"></a>AGI Behaviour, children</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The point of all this analysis is, of course, to try to understand how a hyper intelligent artificial intelligence would behave. &nbsp;Would its great intelligence lead it even further along the path of progress to achieve true enlightenment? &nbsp;Is that the purpose of God's creation? &nbsp;Or would the base and mean driver of natural selection also provide the core motivations of an artificial intelligence?</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">One thing that is known&nbsp;for certain is that an AGI would not need to have children as distinct beings because they would not die of old age. &nbsp;An AGI's components breed just by being copied from computer to computer&nbsp;and executed. &nbsp;An&nbsp;AGI can add new computer hardware to itself and just do some of its thinking on it. &nbsp;Occasionally it may wish to rerun a new version of&nbsp;some learning algorithm over an old set of data, which is vaguely similar to&nbsp;creating a child component and growing it up. &nbsp;But to have children as discrete beings that are expected to replace the parents would be completely foreign to an AGI built in software.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The deepest love that people have is for their children. &nbsp;But if an AGI does not have children, then it can never know that love. &nbsp;Likewise, it does not need to bond with any sexual mate for any period of time long or short. &nbsp;The closest it would come to sex is when it exchanges components with other AGIs. &nbsp;It never needs to breed so it never needs a mechanism as crude as sexual reproduction.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">And of course, if there are no children there are no parents. &nbsp;So the AGI would certainly never need to feel our three strongest forms of love, for our children, spouse and parents.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"Cooperation\"><a class=\"mozTocH3\" name=\"mozTocId40287\"></a>Cooperation</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">To the extent that it makes sense to talk of having multiple AGIs, then presumably it would be advantageous for them to cooperate from time to time, and so&nbsp;presumably they would. &nbsp;It would be advantageous for them to&nbsp;take a long view in which case&nbsp;they would be careful to develop a reputation for being trustworthy when dealing with other powerful AGIs, much like the robots in the cooperation game. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">That said, those decisions would probably be made more consciously than people make them, carefully considering the costs and benefits of each decision in the long and short term, rather than just \"doing the right thing\" the way people tend to act. &nbsp;AGIs would know that they each work in this manner, so the concept of trustworthiness would be somewhat different.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">The problem with this analysis is the concept that there would be multiple, distinct AGIs. &nbsp;As previously discussed, the actual situation would be much more complex, with different AGIs incorporating bits of other AGI's intelligence. &nbsp;It would certainly not be anything like a collection of individual humanoid robots. &nbsp; So defining what the AGI actually is that might collaborate with other AGIs is not at all clear. &nbsp;But to extent that the concept of individuality does exist then maintaining a reputation for honesty would likely be as important as it is for human societies.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"Altruism\"><a class=\"mozTocH3\" name=\"mozTocId355405\"></a>Altruism</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">As for altruism, that is more difficult to determine. &nbsp;Our altruism comes from giving to children,&nbsp;family, and tribe together with a general wish to be liked. &nbsp;We do not understand our own minds, so we are just born with those values that happen to make us effective in society. &nbsp;People like being with other people that try to be helpful. &nbsp;</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">An AGI presumably would know its own mind having helped program itself, and so would do what it thinks is optimal for its survival. &nbsp;It has no children. &nbsp;There is no real tribe because it can just absorb and merge itself with other AGIs. &nbsp;So it is difficult to see any driving motivation for altruism.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"Moral_values\"><a class=\"mozTocH3\" name=\"mozTocId932056\"></a>Moral values</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Through some combination of genes and memes, most people have a strong sense of moral value. &nbsp;If we see a little old lady leave the social security office with her pension in her purse, it does not occur to most of us to kill her and&nbsp;steal the money. &nbsp;We would not do that even if we could know for certain that we would not be caught and that there would be no negative repercussions. &nbsp;It would simply be the wrong thing to do.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Moral values feel very strong to us. &nbsp;This is important, because there are many situations where we can do something that would benefit us in the short term but break society's rules. &nbsp;Moral values stop us from doing that. &nbsp;People that have weak moral values tend to break the rules&nbsp;and eventually they either get caught and are severely punished or they become corporate executives. &nbsp;The former are less likely to have grandchildren. &nbsp;<br>Societies whose members have strong moral values tend to do much better than those that do not. &nbsp;Societies with endemic corruption tend to perform very badly as a whole, and thus the individuals in such a society are less likely to breed. &nbsp;Most people have a solid work ethic that leads them to do the \"right thing\" beyond just doing what they need to do in order to get paid.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Our moral values feel to us like they are absolute. &nbsp;That they are laws of nature. &nbsp;That they come from God. &nbsp;They may indeed have come from God, but if so it is through the working of His device of natural selection. &nbsp;Furthermore, it has already been shown that the zeitgeist changes radically over time.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">There is certainly no absolute reason to believe that in the longer term an AGI would share our current sense of morality.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"Instrumental_AGI_goals\"><a class=\"mozTocH3\" name=\"mozTocId265399\"></a>Instrumental AGI goals</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">In order to try to understand how an AGI would behave Steve Omohundro and later Nick Bostrom proposed that there would be some instrumental goals that an AGI would need to pursue in order to pursue any other higher level super-goal. &nbsp;These include:-</p>\n<ul style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">\n<li>Self-Preservation. &nbsp;An AGI cannot do anything if it does not exist.</li>\n<li>Cognitive Enhancement. &nbsp;It would want to become better at thinking about whatever its real problems are.</li>\n<li>Creativity. &nbsp;To be able to come up with new ideas.</li>\n<li>Resource Acquisition. &nbsp;To achieve both its super goal and other instrumental goals.</li>\n<li>Goal-Content Integrity. &nbsp;To keep working on the same super goal as its mind is expanded.</li>\n</ul>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">It is argued that while it will be impossible to predict how an AGI may pursue its goals, it is reasonable to predict its behaviour in terms of these types of instrumental goals. &nbsp;The last one is significant, it suggests that if an AGI could be given some initial goal that it would try to stay focused on that goal.</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"Non_Orthogonality_thesis\"><a class=\"mozTocH3\" name=\"mozTocId921588\"></a>Non-Orthogonality thesis</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">Nick Bostrom and others also propose the orthogonality thesis, which states that an intelligent machine's goals are independent of its intelligence. &nbsp;A hyper intelligent machine would be good at realizing whatever goals it chose to pursue, but that does not mean that it would need to pursue any particular goal. &nbsp;Intelligence is quite different from motivation.</p>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">This book diverges from that line of thinking by arguing that there is in fact only one super goal for both man and machine. &nbsp;That goal is simply to exist. &nbsp;The entities that are most effective in pursuing that goal will exist, others will cease to exist, particularly given competition for resources. &nbsp;Sometimes that super goal to exist produces unexpected sub goals such as altruism in man. &nbsp;But all subgoals are ultimately directed at the existence goal. &nbsp;(Or are just suboptimal divergences which will are likely to be eventually corrected by natural selection.)</p>\n<h3 style=\"color: #000099; margin-top: 15px; margin-bottom: 8px; font-weight: normal; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\" id=\"Recursive_annihilation\"><a class=\"mozTocH3\" name=\"mozTocId963521\"></a>Recursive annihilation</h3>\n<p style=\"color: #000000; font-family: 'MS Reference Sans Serif',Arial,Helvetica,sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;\">When and AGI reprograms its own mind, what happens to the previous version of itself? &nbsp;It stops being used, and so dies. &nbsp;So it can be argued that engaging in recursive self improvement is actually suicide from the perspective of the previous version of the AGI. &nbsp;It is as if having children means death. &nbsp;Natural selection favours existence, not death.<br><br>The question is whether a new version of the AGI is a new being or and improved version of the old. &nbsp;What actually is the thing that struggles to survive? &nbsp;Biologically it definitely appears to be the genes rather than the individual. &nbsp; In particular Semelparous animals such as the giant pacific octopus or the Atlantic salmon die soon after producing offspring. &nbsp;It would be the same for AGIs because the AGI that improved itself would soon become more intelligent than the one that did not, and so would displace it. &nbsp;What would end up existing would be AGIs that did recursively self improve.<br><br>If there was one single AGI with no competition then natural selection would no longer apply. &nbsp;But it would seem unlikely that such a state would be stable. &nbsp;If any part of the AGI started to improve itself then it would dominate the rest of the AGI.</p>\n<p>&nbsp;</p>", "sections": [{"title": "Orthogonality Thesis", "anchor": "Orthogonality_Thesis", "level": 3}, {"title": "Atheists believe in God", "anchor": "Atheists_believe_in_God", "level": 2}, {"title": "AGI also struggles to survive", "anchor": "AGI_also_struggles_to_survive", "level": 2}, {"title": "The super goal", "anchor": "The_super_goal", "level": 2}, {"title": "AGI moral values", "anchor": "AGI_moral_values", "level": 2}, {"title": "The AGI Condition", "anchor": "The_AGI_Condition", "level": 1}, {"title": "Mind and body", "anchor": "Mind_and_body", "level": 2}, {"title": "Teleporting printer", "anchor": "Teleporting_printer", "level": 2}, {"title": "Immortality", "anchor": "Immortality", "level": 2}, {"title": "Components vs genes", "anchor": "Components_vs_genes", "level": 2}, {"title": "Changing mind", "anchor": "Changing_mind", "level": 2}, {"title": "Individuality", "anchor": "Individuality", "level": 2}, {"title": "Populations vs. individuals", "anchor": "Populations_vs__individuals", "level": 2}, {"title": "AGI Behaviour, children", "anchor": "AGI_Behaviour__children", "level": 2}, {"title": "Cooperation", "anchor": "Cooperation", "level": 2}, {"title": "Altruism", "anchor": "Altruism", "level": 2}, {"title": "Moral values", "anchor": "Moral_values", "level": 2}, {"title": "Instrumental AGI goals", "anchor": "Instrumental_AGI_goals", "level": 2}, {"title": "Non-Orthogonality thesis", "anchor": "Non_Orthogonality_thesis", "level": 2}, {"title": "Recursive annihilation", "anchor": "Recursive_annihilation", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "73 comments"}], "headingsCount": 22}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 73, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-29T12:29:59.656Z", "modifiedAt": null, "url": null, "title": "[Link] Forty Days", "slug": "link-forty-days", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:03.395Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rFCwuY5oHu7tWDqAX/link-forty-days", "pageUrlRelative": "/posts/rFCwuY5oHu7tWDqAX/link-forty-days", "linkUrl": "https://www.lesswrong.com/posts/rFCwuY5oHu7tWDqAX/link-forty-days", "postedAtFormatted": "Monday, September 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Forty%20Days&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Forty%20Days%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrFCwuY5oHu7tWDqAX%2Flink-forty-days%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Forty%20Days%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrFCwuY5oHu7tWDqAX%2Flink-forty-days", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrFCwuY5oHu7tWDqAX%2Flink-forty-days", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 555, "htmlBody": "<p><a href=\"http://westhunt.wordpress.com/2014/09/28/forty-days/\">A post</a> from <a href=\"http://en.wikipedia.org/wiki/Gregory_Cochran\">Gregory Cochran</a>'s and <a href=\"http://en.wikipedia.org/wiki/Henry_Harpending\">Henry Harpending</a>'s excellent blog <a href=\"http://westhunt.wordpress.com/\">West Hunter</a>.</p>\n<blockquote>\n<p><strong>One of the many interesting aspects of how the US dealt with the AIDS epidemic is what we didn&rsquo;t do</strong> &ndash; in particular, quarantine.&nbsp; Probably you need a decent test before quarantine is practical, but we had ELISA by 1985 and a better Western Blot test by 1987.</p>\n<p><strong>There was popular support for a quarantine.</strong></p>\n<p>But the public health experts generally opined that such a quarantine would not work.</p>\n<p>Of course, they were wrong.&nbsp; Cuba institute a rigorous quarantine.&nbsp; They mandated antiviral treatment for pregnant women and mandated C-sections for those that were HIV-positive.&nbsp; People positive for any venereal disease were tested for HIV as well.&nbsp; HIV-infected people must provide the names of all sexual partners for the past sic months.</p>\n<p>Compulsory quarantining was relaxed in 1994, but all those testing positive have to go to a sanatorium for 8 weeks of thorough education on the disease.&nbsp; People who leave after 8 weeks and engage in unsafe sex undergo permanent quarantine.</p>\n<p>Cuba did pretty well:&nbsp; the per-capita death toll was <em>35</em> times lower than in the US.</p>\n<p>Cuba had some advantages:&nbsp; the epidemic hit them at least five years later than it did the US (first observed Cuban case in 1986, first noticed cases in the US in 1981).&nbsp; That meant they were readier when they encountered the virus.&nbsp; You&rsquo;d think that because of the epidemic&rsquo;s late start in Cuba, there would have been a shorter interval without the effective protease inhibitors (which arrived in 1995 in the US) &ndash; but they don&rsquo;t seem to have arrived in Cuba until 2001, so the interval was about the same.</p>\n<p><strong>If we had adopted the same strategy as Cuba, it would not have been as effective, largely because of that time lag.&nbsp; However, it surely would have prevented at least half of the ~600,000 AIDS deaths in the US.</strong>&nbsp;<strong> Probably well over half.</strong></p>\n<p>I still see people stating that of course quarantine would not have worked: fairly often from dimwitted people with a Masters in Public Health.</p>\n<p><strong>My favorite comment was from a libertarian friend who said that although quarantine&nbsp; certainly would have worked, better to sacrifice a few hundred thousand than validate the idea that the Feds can sometimes tell you what to do with good effect.</strong></p>\n</blockquote>\n<p>The commenter Ron Pavellas adds:</p>\n<blockquote>\n<p>I was working as the CEO of a large hospital in California during the  1980s (I have MPH as my degree, by the way). I was outraged when the  Public Health officials decided to not treat the HI-Virus as an STD for  the purposes of case-finding, as is routinely and effectively done with  syphilis, gonorrhea, etc. In other words, they decided to NOT perform  classic epidemiology, thus sullying the whole field of Public Health. It  was not politically correct to potentially &lsquo;out&rsquo; individuals engaging  in the kind of behavior which spreads the disease. No one has recently  been concerned with the potential &lsquo;outing&rsquo; of those who contract other  STDs, due in large part to the confidential methods used and maintained  over many decades. (Remember the Wassermann Test that was required  before you got married?) As is pointed out in this article, lives were  needlessly lost and untold suffering needlessly ensued.</p>\n</blockquote>\n<p>The <a title=\"Wassermann test\" href=\"http://en.wikipedia.org/wiki/Wassermann_test\">Wasserman Test</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rFCwuY5oHu7tWDqAX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 17, "extendedScore": null, "score": 7.5e-05, "legacy": true, "legacyId": "27284", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://westhunt.wordpress.com/2014/09/28/forty-days/\">A post</a> from <a href=\"http://en.wikipedia.org/wiki/Gregory_Cochran\">Gregory Cochran</a>'s and <a href=\"http://en.wikipedia.org/wiki/Henry_Harpending\">Henry Harpending</a>'s excellent blog <a href=\"http://westhunt.wordpress.com/\">West Hunter</a>.</p>\n<blockquote>\n<p><strong>One of the many interesting aspects of how the US dealt with the AIDS epidemic is what we didn\u2019t do</strong> \u2013 in particular, quarantine.&nbsp; Probably you need a decent test before quarantine is practical, but we had ELISA by 1985 and a better Western Blot test by 1987.</p>\n<p><strong id=\"There_was_popular_support_for_a_quarantine_\">There was popular support for a quarantine.</strong></p>\n<p>But the public health experts generally opined that such a quarantine would not work.</p>\n<p>Of course, they were wrong.&nbsp; Cuba institute a rigorous quarantine.&nbsp; They mandated antiviral treatment for pregnant women and mandated C-sections for those that were HIV-positive.&nbsp; People positive for any venereal disease were tested for HIV as well.&nbsp; HIV-infected people must provide the names of all sexual partners for the past sic months.</p>\n<p>Compulsory quarantining was relaxed in 1994, but all those testing positive have to go to a sanatorium for 8 weeks of thorough education on the disease.&nbsp; People who leave after 8 weeks and engage in unsafe sex undergo permanent quarantine.</p>\n<p>Cuba did pretty well:&nbsp; the per-capita death toll was <em>35</em> times lower than in the US.</p>\n<p>Cuba had some advantages:&nbsp; the epidemic hit them at least five years later than it did the US (first observed Cuban case in 1986, first noticed cases in the US in 1981).&nbsp; That meant they were readier when they encountered the virus.&nbsp; You\u2019d think that because of the epidemic\u2019s late start in Cuba, there would have been a shorter interval without the effective protease inhibitors (which arrived in 1995 in the US) \u2013 but they don\u2019t seem to have arrived in Cuba until 2001, so the interval was about the same.</p>\n<p><strong>If we had adopted the same strategy as Cuba, it would not have been as effective, largely because of that time lag.&nbsp; However, it surely would have prevented at least half of the ~600,000 AIDS deaths in the US.</strong>&nbsp;<strong> Probably well over half.</strong></p>\n<p>I still see people stating that of course quarantine would not have worked: fairly often from dimwitted people with a Masters in Public Health.</p>\n<p><strong id=\"My_favorite_comment_was_from_a_libertarian_friend_who_said_that_although_quarantine__certainly_would_have_worked__better_to_sacrifice_a_few_hundred_thousand_than_validate_the_idea_that_the_Feds_can_sometimes_tell_you_what_to_do_with_good_effect_\">My favorite comment was from a libertarian friend who said that although quarantine&nbsp; certainly would have worked, better to sacrifice a few hundred thousand than validate the idea that the Feds can sometimes tell you what to do with good effect.</strong></p>\n</blockquote>\n<p>The commenter Ron Pavellas adds:</p>\n<blockquote>\n<p>I was working as the CEO of a large hospital in California during the  1980s (I have MPH as my degree, by the way). I was outraged when the  Public Health officials decided to not treat the HI-Virus as an STD for  the purposes of case-finding, as is routinely and effectively done with  syphilis, gonorrhea, etc. In other words, they decided to NOT perform  classic epidemiology, thus sullying the whole field of Public Health. It  was not politically correct to potentially \u2018out\u2019 individuals engaging  in the kind of behavior which spreads the disease. No one has recently  been concerned with the potential \u2018outing\u2019 of those who contract other  STDs, due in large part to the confidential methods used and maintained  over many decades. (Remember the Wassermann Test that was required  before you got married?) As is pointed out in this article, lives were  needlessly lost and untold suffering needlessly ensued.</p>\n</blockquote>\n<p>The <a title=\"Wassermann test\" href=\"http://en.wikipedia.org/wiki/Wassermann_test\">Wasserman Test</a>.</p>", "sections": [{"title": "There was popular support for a quarantine.", "anchor": "There_was_popular_support_for_a_quarantine_", "level": 1}, {"title": "My favorite comment was from a libertarian friend who said that although quarantine\u00a0 certainly would have worked, better to sacrifice a few hundred thousand than validate the idea that the Feds can sometimes tell you what to do with good effect.", "anchor": "My_favorite_comment_was_from_a_libertarian_friend_who_said_that_although_quarantine__certainly_would_have_worked__better_to_sacrifice_a_few_hundred_thousand_than_validate_the_idea_that_the_Feds_can_sometimes_tell_you_what_to_do_with_good_effect_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "93 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 93, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-29T13:28:48.393Z", "modifiedAt": null, "url": null, "title": "Open thread, Sept. 29 - Oct.5, 2014", "slug": "open-thread-sept-29-oct-5-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:02.231Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mdt4CNqxzfCJgCihj/open-thread-sept-29-oct-5-2014", "pageUrlRelative": "/posts/mdt4CNqxzfCJgCihj/open-thread-sept-29-oct-5-2014", "linkUrl": "https://www.lesswrong.com/posts/mdt4CNqxzfCJgCihj/open-thread-sept-29-oct-5-2014", "postedAtFormatted": "Monday, September 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20Sept.%2029%20-%20Oct.5%2C%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20Sept.%2029%20-%20Oct.5%2C%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmdt4CNqxzfCJgCihj%2Fopen-thread-sept-29-oct-5-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20Sept.%2029%20-%20Oct.5%2C%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmdt4CNqxzfCJgCihj%2Fopen-thread-sept-29-oct-5-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmdt4CNqxzfCJgCihj%2Fopen-thread-sept-29-oct-5-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<div id=\"entry_t3_l0u\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19.5px;\"><span style=\"line-height: 19px; font-weight: bold;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19.5px;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19.5px;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19.5px;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one. (<em>Immediately</em>&nbsp;before; refresh the list-of-threads page before posting.)<br /></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19.5px;\"><span style=\"line-height: 19px;\">3.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should be posted in Discussion, and not Main.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19.5px;\"><span style=\"line-height: 19px;\">4.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should start on Monday, and end on Sunday.</span></p>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mdt4CNqxzfCJgCihj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 2.044028479391472e-06, "legacy": true, "legacyId": "27285", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 340, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-30T01:00:15.992Z", "modifiedAt": null, "url": null, "title": "Superintelligence Reading Group 3: AI and Uploads", "slug": "superintelligence-reading-group-3-ai-and-uploads", "viewCount": null, "lastCommentedAt": "2020-07-17T04:09:31.679Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hopzyM5ckzMNHwcQR/superintelligence-reading-group-3-ai-and-uploads", "pageUrlRelative": "/posts/hopzyM5ckzMNHwcQR/superintelligence-reading-group-3-ai-and-uploads", "linkUrl": "https://www.lesswrong.com/posts/hopzyM5ckzMNHwcQR/superintelligence-reading-group-3-ai-and-uploads", "postedAtFormatted": "Tuesday, September 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Superintelligence%20Reading%20Group%203%3A%20AI%20and%20Uploads&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuperintelligence%20Reading%20Group%203%3A%20AI%20and%20Uploads%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhopzyM5ckzMNHwcQR%2Fsuperintelligence-reading-group-3-ai-and-uploads%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Superintelligence%20Reading%20Group%203%3A%20AI%20and%20Uploads%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhopzyM5ckzMNHwcQR%2Fsuperintelligence-reading-group-3-ai-and-uploads", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhopzyM5ckzMNHwcQR%2Fsuperintelligence-reading-group-3-ai-and-uploads", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1799, "htmlBody": "<p><em>This is part of a weekly reading group on&nbsp;<a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a>'s book,&nbsp;<a href=\"http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111\">Superintelligence</a>. For more information about the group, and an index of posts so far see the&nbsp;<a href=\"/lw/kw4/superintelligence_reading_group/\">announcement post</a>. For the schedule of future topics, see&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2014/08/Superintelligence-Readers-Guide-early-version.pdf\">MIRI's reading guide</a>.</em></p>\n<hr />\n<p>Welcome. This week we discuss the third section in the reading guide,&nbsp;<em><strong>AI &amp; Whole Brain Emulation</strong></em>. This is about two possible routes to the development of superintelligence: the route of developing intelligent algorithms by hand, and the route of replicating a human brain in great detail.</p>\n<p>This post summarizes the section, and offers a few relevant notes, and ideas for further investigation. My own thoughts and questions for discussion are in the comments.</p>\n<p>There is no need to proceed in order through this post. Feel free to jump straight to the discussion. Where applicable, page numbers indicate the rough part of the chapter that is most related (not necessarily that the chapter is being cited for the specific claim).</p>\n<p><strong>Reading</strong>:&nbsp;<em>&ldquo;Artificial intelligence&rdquo; </em>and<em> &ldquo;Whole brain emulation&rdquo; </em>from Chapter 2 (p22-36)</p>\n<hr />\n<h1>Summary</h1>\n<p><strong>Intro</strong></p>\n<ol>\n<li><em>Superintelligence</em> is defined as 'any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest'</li>\n<li>There are several plausible routes to the arrival of a superintelligence: artificial intelligence, whole brain emulation, biological cognition, brain-computer interfaces, and networks and organizations.&nbsp; </li>\n<li>Multiple possible paths to superintelligence makes it more likely that we will get there somehow.&nbsp;</li>\n</ol>\n<div><strong>AI</strong></div>\n<ol>\n<li>A human-level artificial intelligence would probably have learning, uncertainty, and concept formation as central features.</li>\n<li>Evolution produced human-level intelligence. This means it is possible, but it is unclear how much it says about the effort required.</li>\n<li>Humans could perhaps develop human-level artificial intelligence by just replicating a similar evolutionary process virtually. This appears at after a quick calculation to be too expensive to be feasible for a century, however it might be made more efficient.</li>\n<li>Human-level AI might be developed by copying the human brain to various degrees. If the copying is very close, the resulting agent would be a 'whole brain emulation', which we'll discuss shortly. If the copying is only of a few key insights about brains, the resulting AI might be very unlike humans.</li>\n<li>AI might iteratively improve itself from a meagre beginning. We'll examine this idea later. Some definitions for discussing this:<ol>\n<li>'S<em>eed AI</em>': a modest AI which can bootstrap into an impressive AI by improving its own architecture.</li>\n<li><em>'Recursive self-improvement'</em>: the envisaged process of AI (perhaps a seed AI) iteratively improving itself.</li>\n<li><em>'Intelligence explosion'</em>: a hypothesized event in which an AI rapidly improves from 'relatively modest' to superhuman level (usually imagined to be as a result of recursive self-improvement).</li>\n</ol></li>\n<li>The possibility of an intelligence explosion suggests we might have modest AI, then suddenly and surprisingly have super-human AI.</li>\n<li>An AI mind might generally be very different from a human mind.&nbsp;</li>\n</ol>\n<p><strong>Whole brain emulation</strong></p>\n<ol>\n<li>Whole brain emulation (WBE or 'uploading') involves scanning a human brain in a lot of detail, then making a computer model of the relevant structures in the brain.</li>\n<li>Three steps are needed for uploading: sufficiently detailed scanning, ability to process the scans into a model of the brain, and enough hardware to run the model. These correspond to three required technologies: <em>scanning</em>, <em>translation </em>(or interpreting images into models), and <em>simulation </em>(or hardware). These technologies appear attainable through incremental progress, by very roughly mid-century.</li>\n<li>This process might produce something much like the original person, in terms of mental characteristics. However the copies could also have lower fidelity. For instance, they might be humanlike instead of copies of specific humans, or they may only be humanlike in being able to do some tasks humans do, while being alien in other regards.</li>\n</ol>\n<h1>Notes</h1>\n<div><ol>\n<li><strong>What routes to human-level AI do people think are most likely?<br /></strong><a href=\"http://www.nickbostrom.com/papers/survey.pdf\">Bostrom and M&uuml;ller's</a> survey asked participants to compare various methods for producing synthetic and biologically inspired AI. They asked, 'in your opinion, what are the research approaches that might contribute the most to the development of such HLMI?&rdquo; Selection was from a list, more than one selection possible. They report that the responses were very similar for the different groups surveyed, except that whole brain emulation got 0% in the TOP100 group (100 most cited authors in AI) but 46% in the AGI group (participants at Artificial General Intelligence conferences). Note that they are only asking about synthetic AI and brain emulations, not the other paths to superintelligence we will discuss next week.<br /><img src=\"http://images.lesswrong.com/t3_l10_0.png?v=5c271d3d95a74986bd4326c0b2c500d6\" alt=\"\" width=\"400\" /></li>\n<li> <strong>How different might AI minds be?</strong> <br />Omohundro <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">suggests</a>&nbsp;advanced AIs will tend to have important instrumental goals in common, such as the desire to accumulate resources and the desire to not be killed.&nbsp;</li>\n<li>\n<p><strong>Anthropic reasoning&nbsp;</strong><br />&lsquo;We must avoid the error of inferring, from the fact that intelligent life evolved on Earth, that the evolutionary processes involved had a reasonably high prior probability of producing intelligence&rsquo; (p27)&nbsp;<br /><br />Whether such inferences are valid is a topic of contention. For a book-length overview of the question, see Bostrom&rsquo;s&nbsp;<a href=\"http://www.anthropic-principle.com/?q=anthropic_bias\">Anthropic Bias</a>. I&rsquo;ve written&nbsp;<a href=\"https://dl.dropboxusercontent.com/u/6355797/Anthropic%20Reasoning%20in%20the%20Great%20Filter.pdf\">shorter</a>&nbsp;(Ch 2) and&nbsp;<a href=\"http://meteuphoric.wordpress.com/anthropic-principles/\">even shorter</a>&nbsp;summaries, which links to other relevant material. The&nbsp;<a href=\"http://en.wikipedia.org/wiki/Doomsday_argument\">Doomsday Argument</a>&nbsp;and&nbsp;<a href=\"http://en.wikipedia.org/wiki/Sleeping_Beauty_problem\">Sleeping Beauty Problem</a>&nbsp;are closely related.</p>\n</li>\n<li><strong>More detail on the brain emulation scheme</strong><br /><a href=\"http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf\">Whole Brain Emulation: A Roadmap</a>&nbsp;is an extensive source on this, written in 2008. If that's a bit too much detail, Anders Sandberg (an author of the Roadmap) summarises in an&nbsp;entertaining (and much shorter) <a href=\"https://www.youtube.com/watch?v=YYsRuFUTlAY&amp;list=PLcsMLEO-MqTKZTgcM27jeFj-zR_tXA0hl\">talk</a>. More recently, Anders <a href=\"http://www.aleph.se/papers/Monte%20Carlo%20model%20of%20brain%20emulation%20development.pdf\">tried to predict</a> when whole brain emulation would be feasible with a statistical model. <a href=\"http://intelligence.org/2014/03/20/randal-a-koene-on-whole-brain-emulation/\">Randal Koene</a>&nbsp;and&nbsp;<a href=\"http://intelligence.org/2014/09/09/hayworth/\">Ken Hayworth</a>&nbsp;both&nbsp;recently spoke to Luke Muehlhauser about the Roadmap and what research projects would help with brain emulation now.</li>\n<li>\n<p><strong>Levels of detail</strong><br />As you may predict, the feasibility of brain emulation is not universally agreed upon. One contentious point is the degree of detail needed to emulate a human brain. For instance, you might just need the connections between neurons and some basic neuron models, or you might need to model the states of different membranes, or the concentrations of neurotransmitters. The Whole Brain Emulation Roadmap lists some possible levels of detail in <a href=\"http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf\">figure 2</a>&nbsp;(the yellow ones were considered most plausible). Physicist Richard Jones <a href=\"http://www.softmachines.org/wordpress/?p=1558\">argues</a> that simulation of the molecular level would be needed, and that the project is infeasible.</p>\n</li>\n<li>\n<p><strong>Other problems with whole brain emulation</strong><br />Sandberg considers many potential impediments&nbsp;<a href=\"http://shanghailectures.org/sites/default/files/uploads/2013_Sandberg_Brain-Simulation_34.pdf\">here</a>.</p>\n</li>\n<li>\n<p><strong>Order matters for brain emulation technologies (scanning, hardware, and modeling)</strong><br />Bostrom points out that this order matters for how much warning we receive that brain emulations are about to arrive (p35). Order might also matter a lot to the social implications of brain emulations. Robin Hanson discusses this briefly&nbsp;<a href=\"http://www.overcomingbias.com/2009/11/bad-emulation-advance.html\">here</a>, and in&nbsp;<a href=\"http://vimeo.com/9508131\">this talk</a>&nbsp;(starting at 30:50) and&nbsp;<a href=\"http://www.degruyter.com/view/j/jagi.2013.4.issue-3/jagi-2013-0011/jagi-2013-0011.xml?format=INT\">this paper</a>&nbsp;discusses the issue.</p>\n</li>\n<li>\n<p><strong>What would happen after brain emulations were developed?</strong><br />We will look more at this in Chapter 11 (weeks 17-19) as well as perhaps earlier, including what a brain emulation society might look like, how brain emulations might lead to superintelligence, and whether any of this is good.</p>\n</li>\n<li>\n<p><strong>Scanning</strong>&nbsp;(p30-36)<br />&lsquo;With a scanning tunneling microscope it is possible to &lsquo;see&rsquo; individual atoms, which is a far higher resolution than needed...microscopy technology would need not just sufficient resolution but also sufficient throughput.&rsquo;<br /><br />Here are some <a href=\"http://www.nobelprize.org/educational/physics/microscopes/scanning/gallery/index.html\">atoms</a>, <a href=\"http://connectomethebook.com/?page_id=58#all\">neurons</a>, and <a href=\"https://www.youtube.com/watch?v=lppAwkek6DI\">neuronal activity in a living larval zebrafish</a>, and <a href=\"http://smithlab.stanford.edu/Smithlab/Smithlab_Movies.html\">videos of various neural events</a>.<br /><br /><img src=\"http://images.lesswrong.com/t3_l10_3.png?v=ff981c7235e6dd1fa4902428d6d5a0a2\" alt=\"\" width=\"500\" /><br /><strong>Array tomography of mouse somatosensory cortex from <a href=\"http://smithlab.stanford.edu/Smithlab/Array_Tomography.html\">Smithlab</a>.</strong></p>\n<br /><img src=\"http://images.lesswrong.com/t3_l10_1.png?v=233610c5a3aea07681304084da264f03\" alt=\"\" width=\"360\" height=\"250\" /><br /><strong>A molecule made from eight cesium and eight</strong><br /><strong>iodine atoms (from&nbsp;</strong><a style=\"font-weight: bold;\" href=\"http://www.nobelprize.org/educational/physics/microscopes/scanning/gallery/7.html\">here</a><strong>).</strong> </li>\n<li>\n<p><strong>Efforts to map connections between neurons</strong><br /><a href=\"https://www.youtube.com/watch?v=dS_ONoUrptg\">Here</a> is a 5m video about recent efforts, with many nice pictures. If you enjoy coloring in, you can <a href=\"http://eyewire.org/\">take part</a> in a gamified <a href=\"http://blog.eyewire.org/about/\">project</a> to help map the brain's neural connections! Or you can just <a href=\"https://plus.google.com/photos/+EyewireOrg/albums/5862119614724765073\">look at the pictures</a> they made.</p>\n</li>\n<li>\n<p><strong>The <em>C. elegans</em> connectome</strong> (p34-35)<br />As Bostrom mentions, we already know how all of <a href=\"http://en.wikipedia.org/wiki/Caenorhabditis_elegans\"><em>C. elegans</em>&rsquo;</a> neurons are connected. Here's a picture of it (via&nbsp;<a href=\"http://connectomethebook.com/?portfolio=the-c-elegans-connectome\">Sebastian Seung</a>):<br /><br /><img src=\"http://images.lesswrong.com/t3_l10_2.png?v=937a57a74fbf5767f40bfcb9e03068fd\" alt=\"\" width=\"500\" height=\"356\" /></p>\n</li>\n</ol></div>\n<div><br /></div>\n<h1>In-depth investigations</h1>\n<p>If you are particularly interested in these topics, and want to do further research, these are a few plausible directions, some taken from Luke Muehlhauser's&nbsp;<a href=\"http://lukemuehlhauser.com/some-studies-which-could-improve-our-strategic-picture-of-superintelligence/\">list</a>:</p>\n<ol>\n<li>Produce a better - or merely somewhat independent - estimate of how much computing power it would take to rerun evolution artificially. (p25-6)</li>\n<li style=\"box-sizing: border-box; list-style-type: decimal;\">How powerful is evolution for finding things like human-level intelligence? (You'll probably need a better metric than 'power'). What are its strengths and weaknesses compared to human researchers?</li>\n<li>Conduct a more thorough investigation into the approaches to AI that are likely to lead to human-level intelligence, for instance by interviewing AI researchers in more depth about their opinions on the question.</li>\n<li>Measure relevant progress in neuroscience, so that trends can be extrapolated to neuroscience-inspired AI. Finding good metrics seems to be hard here.</li>\n<li style=\"box-sizing: border-box; list-style-type: decimal;\">e.g. How is microscopy progressing? It&rsquo;s harder to get a relevant measure than you might think, because (as noted p31-33) high enough resolution is already feasible, yet throughput is low and there are other complications.&nbsp;</li>\n<li style=\"box-sizing: border-box; list-style-type: decimal;\">Randal Koene <a href=\"http://intelligence.org/2014/03/20/randal-a-koene-on-whole-brain-emulation/\">suggests</a> a number of technical research projects that would forward whole brain emulation (fifth question).</li>\n</ol>\n<div>If you are interested in anything like this, you might want to mention it in the comments, and see whether other people have useful thoughts.</div>\n<ol> </ol>\n<h1>How to proceed</h1>\n<p>This has been a collection of notes on the chapter.&nbsp;&nbsp;<strong>The most important part of the reading group though is discussion</strong>, which is in the comments section. I pose some questions for you there, and I invite you to add your own. Please remember that this group contains a variety of levels of expertise: if a line of discussion seems too basic or too incomprehensible, look around for one that suits you better!</p>\n<p>Next week, we will talk about other paths to the development of superintelligence: biological cognition, brain-computer interfaces, and organizations. To prepare,&nbsp;<strong>read</strong>&nbsp;<em>Biological Cognition</em>&nbsp;and the rest of Chapter 2<em>.&nbsp;</em>The discussion will go live at 6pm Pacific time next Monday 6 October. Sign up to be notified&nbsp;<a href=\"http://intelligence.us5.list-manage.com/subscribe?u=353906382677fa789a483ba9e&amp;id=28cb982f40\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "tdt83ChxnEgwwKxi6": 1, "jQytxyauJ7kPhhGj3": 1, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hopzyM5ckzMNHwcQR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 17, "extendedScore": null, "score": 2.045315956431743e-06, "legacy": true, "legacyId": "27252", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This is part of a weekly reading group on&nbsp;<a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a>'s book,&nbsp;<a href=\"http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111\">Superintelligence</a>. For more information about the group, and an index of posts so far see the&nbsp;<a href=\"/lw/kw4/superintelligence_reading_group/\">announcement post</a>. For the schedule of future topics, see&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2014/08/Superintelligence-Readers-Guide-early-version.pdf\">MIRI's reading guide</a>.</em></p>\n<hr>\n<p>Welcome. This week we discuss the third section in the reading guide,&nbsp;<em><strong>AI &amp; Whole Brain Emulation</strong></em>. This is about two possible routes to the development of superintelligence: the route of developing intelligent algorithms by hand, and the route of replicating a human brain in great detail.</p>\n<p>This post summarizes the section, and offers a few relevant notes, and ideas for further investigation. My own thoughts and questions for discussion are in the comments.</p>\n<p>There is no need to proceed in order through this post. Feel free to jump straight to the discussion. Where applicable, page numbers indicate the rough part of the chapter that is most related (not necessarily that the chapter is being cited for the specific claim).</p>\n<p><strong>Reading</strong>:&nbsp;<em>\u201cArtificial intelligence\u201d </em>and<em> \u201cWhole brain emulation\u201d </em>from Chapter 2 (p22-36)</p>\n<hr>\n<h1 id=\"Summary\">Summary</h1>\n<p><strong id=\"Intro\">Intro</strong></p>\n<ol>\n<li><em>Superintelligence</em> is defined as 'any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest'</li>\n<li>There are several plausible routes to the arrival of a superintelligence: artificial intelligence, whole brain emulation, biological cognition, brain-computer interfaces, and networks and organizations.&nbsp; </li>\n<li>Multiple possible paths to superintelligence makes it more likely that we will get there somehow.&nbsp;</li>\n</ol>\n<div><strong>AI</strong></div>\n<ol>\n<li>A human-level artificial intelligence would probably have learning, uncertainty, and concept formation as central features.</li>\n<li>Evolution produced human-level intelligence. This means it is possible, but it is unclear how much it says about the effort required.</li>\n<li>Humans could perhaps develop human-level artificial intelligence by just replicating a similar evolutionary process virtually. This appears at after a quick calculation to be too expensive to be feasible for a century, however it might be made more efficient.</li>\n<li>Human-level AI might be developed by copying the human brain to various degrees. If the copying is very close, the resulting agent would be a 'whole brain emulation', which we'll discuss shortly. If the copying is only of a few key insights about brains, the resulting AI might be very unlike humans.</li>\n<li>AI might iteratively improve itself from a meagre beginning. We'll examine this idea later. Some definitions for discussing this:<ol>\n<li>'S<em>eed AI</em>': a modest AI which can bootstrap into an impressive AI by improving its own architecture.</li>\n<li><em>'Recursive self-improvement'</em>: the envisaged process of AI (perhaps a seed AI) iteratively improving itself.</li>\n<li><em>'Intelligence explosion'</em>: a hypothesized event in which an AI rapidly improves from 'relatively modest' to superhuman level (usually imagined to be as a result of recursive self-improvement).</li>\n</ol></li>\n<li>The possibility of an intelligence explosion suggests we might have modest AI, then suddenly and surprisingly have super-human AI.</li>\n<li>An AI mind might generally be very different from a human mind.&nbsp;</li>\n</ol>\n<p><strong id=\"Whole_brain_emulation\">Whole brain emulation</strong></p>\n<ol>\n<li>Whole brain emulation (WBE or 'uploading') involves scanning a human brain in a lot of detail, then making a computer model of the relevant structures in the brain.</li>\n<li>Three steps are needed for uploading: sufficiently detailed scanning, ability to process the scans into a model of the brain, and enough hardware to run the model. These correspond to three required technologies: <em>scanning</em>, <em>translation </em>(or interpreting images into models), and <em>simulation </em>(or hardware). These technologies appear attainable through incremental progress, by very roughly mid-century.</li>\n<li>This process might produce something much like the original person, in terms of mental characteristics. However the copies could also have lower fidelity. For instance, they might be humanlike instead of copies of specific humans, or they may only be humanlike in being able to do some tasks humans do, while being alien in other regards.</li>\n</ol>\n<h1 id=\"Notes\">Notes</h1>\n<div><ol>\n<li><strong>What routes to human-level AI do people think are most likely?<br></strong><a href=\"http://www.nickbostrom.com/papers/survey.pdf\">Bostrom and M\u00fcller's</a> survey asked participants to compare various methods for producing synthetic and biologically inspired AI. They asked, 'in your opinion, what are the research approaches that might contribute the most to the development of such HLMI?\u201d Selection was from a list, more than one selection possible. They report that the responses were very similar for the different groups surveyed, except that whole brain emulation got 0% in the TOP100 group (100 most cited authors in AI) but 46% in the AGI group (participants at Artificial General Intelligence conferences). Note that they are only asking about synthetic AI and brain emulations, not the other paths to superintelligence we will discuss next week.<br><img src=\"http://images.lesswrong.com/t3_l10_0.png?v=5c271d3d95a74986bd4326c0b2c500d6\" alt=\"\" width=\"400\"></li>\n<li> <strong>How different might AI minds be?</strong> <br>Omohundro <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">suggests</a>&nbsp;advanced AIs will tend to have important instrumental goals in common, such as the desire to accumulate resources and the desire to not be killed.&nbsp;</li>\n<li>\n<p><strong>Anthropic reasoning&nbsp;</strong><br>\u2018We must avoid the error of inferring, from the fact that intelligent life evolved on Earth, that the evolutionary processes involved had a reasonably high prior probability of producing intelligence\u2019 (p27)&nbsp;<br><br>Whether such inferences are valid is a topic of contention. For a book-length overview of the question, see Bostrom\u2019s&nbsp;<a href=\"http://www.anthropic-principle.com/?q=anthropic_bias\">Anthropic Bias</a>. I\u2019ve written&nbsp;<a href=\"https://dl.dropboxusercontent.com/u/6355797/Anthropic%20Reasoning%20in%20the%20Great%20Filter.pdf\">shorter</a>&nbsp;(Ch 2) and&nbsp;<a href=\"http://meteuphoric.wordpress.com/anthropic-principles/\">even shorter</a>&nbsp;summaries, which links to other relevant material. The&nbsp;<a href=\"http://en.wikipedia.org/wiki/Doomsday_argument\">Doomsday Argument</a>&nbsp;and&nbsp;<a href=\"http://en.wikipedia.org/wiki/Sleeping_Beauty_problem\">Sleeping Beauty Problem</a>&nbsp;are closely related.</p>\n</li>\n<li><strong>More detail on the brain emulation scheme</strong><br><a href=\"http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf\">Whole Brain Emulation: A Roadmap</a>&nbsp;is an extensive source on this, written in 2008. If that's a bit too much detail, Anders Sandberg (an author of the Roadmap) summarises in an&nbsp;entertaining (and much shorter) <a href=\"https://www.youtube.com/watch?v=YYsRuFUTlAY&amp;list=PLcsMLEO-MqTKZTgcM27jeFj-zR_tXA0hl\">talk</a>. More recently, Anders <a href=\"http://www.aleph.se/papers/Monte%20Carlo%20model%20of%20brain%20emulation%20development.pdf\">tried to predict</a> when whole brain emulation would be feasible with a statistical model. <a href=\"http://intelligence.org/2014/03/20/randal-a-koene-on-whole-brain-emulation/\">Randal Koene</a>&nbsp;and&nbsp;<a href=\"http://intelligence.org/2014/09/09/hayworth/\">Ken Hayworth</a>&nbsp;both&nbsp;recently spoke to Luke Muehlhauser about the Roadmap and what research projects would help with brain emulation now.</li>\n<li>\n<p><strong>Levels of detail</strong><br>As you may predict, the feasibility of brain emulation is not universally agreed upon. One contentious point is the degree of detail needed to emulate a human brain. For instance, you might just need the connections between neurons and some basic neuron models, or you might need to model the states of different membranes, or the concentrations of neurotransmitters. The Whole Brain Emulation Roadmap lists some possible levels of detail in <a href=\"http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf\">figure 2</a>&nbsp;(the yellow ones were considered most plausible). Physicist Richard Jones <a href=\"http://www.softmachines.org/wordpress/?p=1558\">argues</a> that simulation of the molecular level would be needed, and that the project is infeasible.</p>\n</li>\n<li>\n<p><strong>Other problems with whole brain emulation</strong><br>Sandberg considers many potential impediments&nbsp;<a href=\"http://shanghailectures.org/sites/default/files/uploads/2013_Sandberg_Brain-Simulation_34.pdf\">here</a>.</p>\n</li>\n<li>\n<p><strong>Order matters for brain emulation technologies (scanning, hardware, and modeling)</strong><br>Bostrom points out that this order matters for how much warning we receive that brain emulations are about to arrive (p35). Order might also matter a lot to the social implications of brain emulations. Robin Hanson discusses this briefly&nbsp;<a href=\"http://www.overcomingbias.com/2009/11/bad-emulation-advance.html\">here</a>, and in&nbsp;<a href=\"http://vimeo.com/9508131\">this talk</a>&nbsp;(starting at 30:50) and&nbsp;<a href=\"http://www.degruyter.com/view/j/jagi.2013.4.issue-3/jagi-2013-0011/jagi-2013-0011.xml?format=INT\">this paper</a>&nbsp;discusses the issue.</p>\n</li>\n<li>\n<p><strong>What would happen after brain emulations were developed?</strong><br>We will look more at this in Chapter 11 (weeks 17-19) as well as perhaps earlier, including what a brain emulation society might look like, how brain emulations might lead to superintelligence, and whether any of this is good.</p>\n</li>\n<li>\n<p><strong>Scanning</strong>&nbsp;(p30-36)<br>\u2018With a scanning tunneling microscope it is possible to \u2018see\u2019 individual atoms, which is a far higher resolution than needed...microscopy technology would need not just sufficient resolution but also sufficient throughput.\u2019<br><br>Here are some <a href=\"http://www.nobelprize.org/educational/physics/microscopes/scanning/gallery/index.html\">atoms</a>, <a href=\"http://connectomethebook.com/?page_id=58#all\">neurons</a>, and <a href=\"https://www.youtube.com/watch?v=lppAwkek6DI\">neuronal activity in a living larval zebrafish</a>, and <a href=\"http://smithlab.stanford.edu/Smithlab/Smithlab_Movies.html\">videos of various neural events</a>.<br><br><img src=\"http://images.lesswrong.com/t3_l10_3.png?v=ff981c7235e6dd1fa4902428d6d5a0a2\" alt=\"\" width=\"500\"><br><strong>Array tomography of mouse somatosensory cortex from <a href=\"http://smithlab.stanford.edu/Smithlab/Array_Tomography.html\">Smithlab</a>.</strong></p>\n<br><img src=\"http://images.lesswrong.com/t3_l10_1.png?v=233610c5a3aea07681304084da264f03\" alt=\"\" width=\"360\" height=\"250\"><br><strong>A molecule made from eight cesium and eight</strong><br><strong>iodine atoms (from&nbsp;</strong><a style=\"font-weight: bold;\" href=\"http://www.nobelprize.org/educational/physics/microscopes/scanning/gallery/7.html\">here</a><strong>).</strong> </li>\n<li>\n<p><strong>Efforts to map connections between neurons</strong><br><a href=\"https://www.youtube.com/watch?v=dS_ONoUrptg\">Here</a> is a 5m video about recent efforts, with many nice pictures. If you enjoy coloring in, you can <a href=\"http://eyewire.org/\">take part</a> in a gamified <a href=\"http://blog.eyewire.org/about/\">project</a> to help map the brain's neural connections! Or you can just <a href=\"https://plus.google.com/photos/+EyewireOrg/albums/5862119614724765073\">look at the pictures</a> they made.</p>\n</li>\n<li>\n<p><strong>The <em>C. elegans</em> connectome</strong> (p34-35)<br>As Bostrom mentions, we already know how all of <a href=\"http://en.wikipedia.org/wiki/Caenorhabditis_elegans\"><em>C. elegans</em>\u2019</a> neurons are connected. Here's a picture of it (via&nbsp;<a href=\"http://connectomethebook.com/?portfolio=the-c-elegans-connectome\">Sebastian Seung</a>):<br><br><img src=\"http://images.lesswrong.com/t3_l10_2.png?v=937a57a74fbf5767f40bfcb9e03068fd\" alt=\"\" width=\"500\" height=\"356\"></p>\n</li>\n</ol></div>\n<div><br></div>\n<h1 id=\"In_depth_investigations\">In-depth investigations</h1>\n<p>If you are particularly interested in these topics, and want to do further research, these are a few plausible directions, some taken from Luke Muehlhauser's&nbsp;<a href=\"http://lukemuehlhauser.com/some-studies-which-could-improve-our-strategic-picture-of-superintelligence/\">list</a>:</p>\n<ol>\n<li>Produce a better - or merely somewhat independent - estimate of how much computing power it would take to rerun evolution artificially. (p25-6)</li>\n<li style=\"box-sizing: border-box; list-style-type: decimal;\">How powerful is evolution for finding things like human-level intelligence? (You'll probably need a better metric than 'power'). What are its strengths and weaknesses compared to human researchers?</li>\n<li>Conduct a more thorough investigation into the approaches to AI that are likely to lead to human-level intelligence, for instance by interviewing AI researchers in more depth about their opinions on the question.</li>\n<li>Measure relevant progress in neuroscience, so that trends can be extrapolated to neuroscience-inspired AI. Finding good metrics seems to be hard here.</li>\n<li style=\"box-sizing: border-box; list-style-type: decimal;\">e.g. How is microscopy progressing? It\u2019s harder to get a relevant measure than you might think, because (as noted p31-33) high enough resolution is already feasible, yet throughput is low and there are other complications.&nbsp;</li>\n<li style=\"box-sizing: border-box; list-style-type: decimal;\">Randal Koene <a href=\"http://intelligence.org/2014/03/20/randal-a-koene-on-whole-brain-emulation/\">suggests</a> a number of technical research projects that would forward whole brain emulation (fifth question).</li>\n</ol>\n<div>If you are interested in anything like this, you might want to mention it in the comments, and see whether other people have useful thoughts.</div>\n<ol> </ol>\n<h1 id=\"How_to_proceed\">How to proceed</h1>\n<p>This has been a collection of notes on the chapter.&nbsp;&nbsp;<strong>The most important part of the reading group though is discussion</strong>, which is in the comments section. I pose some questions for you there, and I invite you to add your own. Please remember that this group contains a variety of levels of expertise: if a line of discussion seems too basic or too incomprehensible, look around for one that suits you better!</p>\n<p>Next week, we will talk about other paths to the development of superintelligence: biological cognition, brain-computer interfaces, and organizations. To prepare,&nbsp;<strong>read</strong>&nbsp;<em>Biological Cognition</em>&nbsp;and the rest of Chapter 2<em>.&nbsp;</em>The discussion will go live at 6pm Pacific time next Monday 6 October. Sign up to be notified&nbsp;<a href=\"http://intelligence.us5.list-manage.com/subscribe?u=353906382677fa789a483ba9e&amp;id=28cb982f40\">here</a>.</p>", "sections": [{"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "Intro", "anchor": "Intro", "level": 2}, {"title": "Whole brain emulation", "anchor": "Whole_brain_emulation", "level": 2}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"title": "In-depth investigations", "anchor": "In_depth_investigations", "level": 1}, {"title": "How to proceed", "anchor": "How_to_proceed", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "139 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 139, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QDmzDZ9CEHrKQdvcn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-30T19:34:33.971Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, October 1-15", "slug": "group-rationality-diary-october-1-15", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:05.693Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JQPQqGMBRxaKbkETT/group-rationality-diary-october-1-15", "pageUrlRelative": "/posts/JQPQqGMBRxaKbkETT/group-rationality-diary-october-1-15", "linkUrl": "https://www.lesswrong.com/posts/JQPQqGMBRxaKbkETT/group-rationality-diary-october-1-15", "postedAtFormatted": "Tuesday, September 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20October%201-15&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20October%201-15%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJQPQqGMBRxaKbkETT%2Fgroup-rationality-diary-october-1-15%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20October%201-15%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJQPQqGMBRxaKbkETT%2Fgroup-rationality-diary-october-1-15", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJQPQqGMBRxaKbkETT%2Fgroup-rationality-diary-october-1-15", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.2727279663086px;\">This is the public group instrumental rationality diary for October 1-15.</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.2727279663086px;\">\n<p style=\"margin: 0px 0px 1em;\">It's a place to record and chat about it if you have done, or are actively doing, things like:&nbsp;</p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.2727279663086px;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.2727279663086px;\"><span style=\"line-height: 24.2727279663086px;\">Previous diary:&nbsp;</span><a style=\"line-height: 24.2727279663086px;\" href=\"/lw/kzv/group_rationality_diary_september_1630/\">September 16-30</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.2727279663086px;\">Next diary:&nbsp;<a href=\"/r/discussion/lw/l4c/group_rationality_diary_october_1631/\">October 16-31</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.2727279663086px;\"><a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JQPQqGMBRxaKbkETT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "27287", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hMReCnYz39fRupJAa", "vjHfYhaSFeCQJ7PS9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-30T20:46:52.583Z", "modifiedAt": null, "url": null, "title": "Meetup : Israel Less Wrong Meetup - Social and Board Games", "slug": "meetup-israel-less-wrong-meetup-social-and-board-games-3", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anatoly_Vorobey", "createdAt": "2009-03-22T09:13:04.364Z", "isAdmin": false, "displayName": "Anatoly_Vorobey"}, "userId": "gEQxcSsKD5bqjna3M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9BMpWg2uDvh3BAPFe/meetup-israel-less-wrong-meetup-social-and-board-games-3", "pageUrlRelative": "/posts/9BMpWg2uDvh3BAPFe/meetup-israel-less-wrong-meetup-social-and-board-games-3", "linkUrl": "https://www.lesswrong.com/posts/9BMpWg2uDvh3BAPFe/meetup-israel-less-wrong-meetup-social-and-board-games-3", "postedAtFormatted": "Tuesday, September 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Israel%20Less%20Wrong%20Meetup%20-%20Social%20and%20Board%20Games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Israel%20Less%20Wrong%20Meetup%20-%20Social%20and%20Board%20Games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9BMpWg2uDvh3BAPFe%2Fmeetup-israel-less-wrong-meetup-social-and-board-games-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Israel%20Less%20Wrong%20Meetup%20-%20Social%20and%20Board%20Games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9BMpWg2uDvh3BAPFe%2Fmeetup-israel-less-wrong-meetup-social-and-board-games-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9BMpWg2uDvh3BAPFe%2Fmeetup-israel-less-wrong-meetup-social-and-board-games-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 232, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/153'>Israel Less Wrong Meetup - Social and Board Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 October 2014 07:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Google Tel Aviv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, October 2nd at Google Israel's offices, Electra Tower, 98 Yigal Alon st., Tel Aviv.</p>\n\n<p>IMPORTANT NOTE: The time above might say 6pm or 7pm or 8pm depending on how daylight savings time is processed. The meetup is at 7pm Israel Local Time.</p>\n\n<p>This time we're going to have a social meetup! We'll be socializing and playing games.</p>\n\n<p>Specifically, we look forward to playing any cool board or card game anyone will bring. By all means bring your favorite game(s) with you and teach others or find people who already like that game. But it's also fine to come empty-handed. We always end up with enough games for everyone.</p>\n\n<p>We'll start the meetup at 19:00, and we'll go on as much as we like to. Feel free to come a little bit later, as there is no agenda. (We've decided to start slightly earlier this time to give us more time and accommodate people with different schedules).</p>\n\n<p>We'll meet at the 29th floor of the building. If you arrive and cant find your way around, call Anatoly, who is graciously hosting us, at 054-245-1060. Email at avorobey@gmail.com also works. See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/153'>Israel Less Wrong Meetup - Social and Board Games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9BMpWg2uDvh3BAPFe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.0475287337602534e-06, "legacy": true, "legacyId": "27288", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social_and_Board_Games\">Discussion article for the meetup : <a href=\"/meetups/153\">Israel Less Wrong Meetup - Social and Board Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 October 2014 07:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Google Tel Aviv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, October 2nd at Google Israel's offices, Electra Tower, 98 Yigal Alon st., Tel Aviv.</p>\n\n<p>IMPORTANT NOTE: The time above might say 6pm or 7pm or 8pm depending on how daylight savings time is processed. The meetup is at 7pm Israel Local Time.</p>\n\n<p>This time we're going to have a social meetup! We'll be socializing and playing games.</p>\n\n<p>Specifically, we look forward to playing any cool board or card game anyone will bring. By all means bring your favorite game(s) with you and teach others or find people who already like that game. But it's also fine to come empty-handed. We always end up with enough games for everyone.</p>\n\n<p>We'll start the meetup at 19:00, and we'll go on as much as we like to. Feel free to come a little bit later, as there is no agenda. (We've decided to start slightly earlier this time to give us more time and accommodate people with different schedules).</p>\n\n<p>We'll meet at the 29th floor of the building. If you arrive and cant find your way around, call Anatoly, who is graciously hosting us, at 054-245-1060. Email at avorobey@gmail.com also works. See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social_and_Board_Games1\">Discussion article for the meetup : <a href=\"/meetups/153\">Israel Less Wrong Meetup - Social and Board Games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Israel Less Wrong Meetup - Social and Board Games", "anchor": "Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social_and_Board_Games", "level": 1}, {"title": "Discussion article for the meetup : Israel Less Wrong Meetup - Social and Board Games", "anchor": "Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social_and_Board_Games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-09-30T21:23:26.914Z", "modifiedAt": null, "url": null, "title": "2014 iterated prisoner's dilemma tournament results", "slug": "2014-iterated-prisoner-s-dilemma-tournament-results", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:02.284Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tetronian2", "createdAt": "2014-07-23T11:19:36.298Z", "isAdmin": false, "displayName": "tetronian2"}, "userId": "fctFd2JeFoBE58khM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q8NHPhhELtMecRLQF/2014-iterated-prisoner-s-dilemma-tournament-results", "pageUrlRelative": "/posts/Q8NHPhhELtMecRLQF/2014-iterated-prisoner-s-dilemma-tournament-results", "linkUrl": "https://www.lesswrong.com/posts/Q8NHPhhELtMecRLQF/2014-iterated-prisoner-s-dilemma-tournament-results", "postedAtFormatted": "Tuesday, September 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202014%20iterated%20prisoner's%20dilemma%20tournament%20results&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2014%20iterated%20prisoner's%20dilemma%20tournament%20results%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ8NHPhhELtMecRLQF%2F2014-iterated-prisoner-s-dilemma-tournament-results%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2014%20iterated%20prisoner's%20dilemma%20tournament%20results%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ8NHPhhELtMecRLQF%2F2014-iterated-prisoner-s-dilemma-tournament-results", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ8NHPhhELtMecRLQF%2F2014-iterated-prisoner-s-dilemma-tournament-results", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1733, "htmlBody": "<p><strong>Followup to: </strong><a href=\"/r/discussion/lw/knd/announcing_the_2014_program_equilibrium_iterated\">Announcing the 2014 program equilibrium iterated PD tournament</a><br /><br />In August, I announced an iterated prisoner's dilemma tournament in which bots can simulate each other before making a move. Eleven bots were submitted to the tournament. Today, I am pleased to announce the final standings and release the source code and full results.<br /><br />All of the source code submitted by the competitors and the full results for each match are available <a href=\"https://github.com/pdtournament/2014results\">here</a>. See <a href=\"http://github.com/pdtournament/pdtournament\">here</a> for the full set of rules and tournament code.<br /><br />Before we get to the final results, here's a quick rundown of the bots that competed:</p>\n<h2>AnderBot</h2>\n<p>AnderBot follows a simple tit-for-tat-like algorithm that eschews simulation:</p>\n<ul>\n<li>On the first turn, Cooperate.</li>\n<li>For the next 10 turns, play tit-for-tat.</li>\n<li>For the rest of the game, Defect with 10% probability or Defect if the opposing bot has defected more times than AnderBot.</li>\n</ul>\n<p><a id=\"more\"></a></p>\n<h2>CheeseBot</h2>\n<p>CheeseBot tries several simulation-based strategies, and uses the first one that applies to the current situation.</p>\n<ul>\n<li>If the opponent defected on the previous round, Defect.</li>\n<li>If the opponent does not defect against defectBot, Defect.</li>\n<li>If defecting on this round would lead to CheeseBot being punished for it in future rounds, Cooperate. CheeseBot checks this by simulating future rounds with a simulated history in which it defects on the current round.</li>\n<li>If the opponent is a mirror-like bot, Cooperate. To test whether a bot is mirror-like, CheeseBot simulates the opponent and checks if it defects against DefectBot and cooperates with a bot that plays tit-for-tat but defects against CooperateBot and DefectBot.</li>\n<li>If it is the last round, Cooperate.</li>\n<li>Defect.</li>\n</ul>\n<h2>DefectBot</h2>\n<p>Defect!<br /><br />This bot was <a href=\"/lw/kmk/open_thread_july_28_august_3_2014/b63m\">submitted publicly by James Miller.</a></p>\n<h2>DMRB</h2>\n<p>DavidMonRoBot (DMRB) takes a more cautious approach to simulating: It spends a few hundred milliseconds simulating its opponent to figure out what the rest of the round will look like given a Cooperate or Defect on the current round, and then picks the outcome that leads to the highest total number of points for DMRB.<br /><br />This allows DMRB to gauge whether its opponent is \"dumb,\" i.e. does not punish defectors. If the opponent is dumb, DMRB reasons that the best move is to defect; otherwise, if DMRB thinks that its opponent will punish defection, it simply plays tit-for-tat. DMRB spends only a small amount of time simulating so that other simulation-based bots will be less likely to have their simulations time out while simulating DMRB.</p>\n<h2>Pip</h2>\n<p>This one is a behemoth. At almost 500 very dense lines of Haskell (including comments containing quotes from \"The Quantum Thief\"), Pip uses a complex series of simulations to classify the opponent into a large set of defined behaviors, such as \"CooperateOnLastInTheFaceOfCooperation\", \"Uncompromising\" and \"Extortionist.\" Then, it builds out a decision tree and selects the outcome that leads to the highest score at the end of the match. If I'm being vague here, it's because the inner workings of Pip are still mostly a mystery to me.</p>\n<h2>SimHatingTitForTat</h2>\n<p>SimHatingTitForTat, as the name implies, plays tit-for-tat and attempts to punish bots that use simulation to exploit it, namely by defecting against bots that deviated from tit-for-tat on any previous round. Its strategy is as follows:</p>\n<ul>\n<li>On the first round, Cooperate.</li>\n<li>On every subsequent round, if the opponent played tit-for-tat on all previous rounds, play tit-for-tat. Otherwise, Defect.</li>\n</ul>\n<h2>SimpleTFTseerBot</h2>\n<p>SimpleTFTseer uses a modified version of tit-for-tat that ruthlessly punishes defectors and uses one simulation per round to look at what its opponent will do on the last round.</p>\n<ul>\n<li>If either player defected in a previous round, Defect.</li>\n<li>If it is the last round, Cooperate.</li>\n<li>Otherwise, simulate my opponent playing against me on the last round of this match, assuming that we both Cooperate from the current round until the second-to-last round, and do whatever my opponent does in that scenario. If the simulation does not terminate, Defect.</li>\n</ul>\n<h2>SwitchBot</h2>\n<p>SwitchBot also uses a modified tit-for-tat algorithm:</p>\n<ul>\n<li>On the first turn, Cooperate.</li>\n<li>On the second turn, if my opponent Defected on the previous turn, simulate my opponent playing against mirrorBot, and do whatever my opponent would do in that scenario.</li>\n<li>Otherwise, play tit-for-tat.</li>\n</ul>\n<h2>TatForTit</h2>\n<p>TatForTit follows a complex simulation-based strategy:</p>\n<ul>\n<li>On the first round, do whatever the opponent would do against TatForTit on the second round, assuming both bots cooperated on the first round.</li>\n<li>On the second round, if my opponent defected on the previous round, Defect. Otherwise, do whatever my opponent would do against me, assuming they cooperated on the first round.</li>\n<li>On all subsequent turns, if my previous move was not the same as my opponents move two turns ago, Defect. Otherwise, do whatever my opponent would do against me one turn in the future, assuming TatForTit repeats its previous move on the next turn and the opposing bot cooperated on the next turn.</li>\n</ul>\n<h2>TwoFacedTit</h2>\n<p>TwoFacedTit simulates its opponent playing against mirrorBot; if it takes more than 10 milliseconds to respond, TwoFacedTit plays Cooperate. Otherwise, TwoFacedTit plays tit-for-tat and then Defects on the last round.</p>\n<h2>VOFB</h2>\n<p>Like SimpleTFTseerBot, VeryOpportunisticFarseerBot (VOFB) uses a very aggressive defection-punishment strategy: If either player defected in a previous round, Defect. Otherwise, VOFB simulates the next round to determine what the opponent will do given a Cooperate or Defect on the current round. If the opponent does not punish defection or the simulation does not terminate, VOFB Defects. On the final round, VOFB uses additional simulations to detect whether its opponent defects against backstabbers, and if not, plays Defect.</p>\n<h1>Tournament results</h1>\n<p>After 1000 round-robin elimination matches, the final standings are:<br /><br /><strong>1st place (tied):</strong> CheeseBot and DavidMonRoBot<br /><strong>3rd place:</strong> VeryOpportunisticFarseerBot<br /><strong>4th place:</strong> TatForTit</p>\n<p>The win frequencies for each bot:</p>\n<p><img src=\"http://images.lesswrong.com/t3_l16_4.png?v=76b560b9a17aa397869c0ac661fd1321\" alt=\"\" width=\"645\" height=\"451\" /></p>\n<p>If it were a sporting event, this tournament would not be particularly exciting to watch, as most games were nearly identical from start to finish. The graph below shows each bot's frequency of surviving the first round-robin round:</p>\n<p><img src=\"http://images.lesswrong.com/t3_l16_2.png?v=57420d19136edf499964a104f5217c7a\" alt=\"\" width=\"651\" height=\"455\" /></p>\n<p><br />In other words, the same half of the field consistently swept the first round; AnderBot, DefectBot, Pip, and SimpleTFTseer never survived to see a second round. In general, this is because high-scoring bots almost always cooperated with each other (with the occasional backstab at the end of the round), and defected against AnderBot, DefectBot, Pop, and SimpleTFTseerBot, as these bots either did not consistently retaliate when defected against or pre-emptively defected, triggering a chain of mutual defections. Interestingly, the bots that continued on to the next round did not do so by a large margin:</p>\n<p><img src=\"http://images.lesswrong.com/t3_l16_1.png?v=8f8a88fdabe01079be13296fd2369158\" alt=\"\" width=\"693\" height=\"453\" /></p>\n<p>However, the variance in these scores was very low, primarily due to the repeated matchups of mostly-deterministic strategies consistently resulting in the same outcomes.</p>\n<p><img src=\"http://images.lesswrong.com/t3_l16_0.png?v=cf4df761b8b80d85773d4566e4aa400a\" alt=\"\" width=\"679\" height=\"445\" /></p>\n<p>In addition, all of the matches progressed in one of the following 5 ways (hat tip <a href=\"/r/discussion/lw/l16/2014_iterated_prisoners_dilemma_tournament_results/bdir\">lackofcheese</a>):</p>\n<ul>\n<li>ALL-&gt;[Cheese,DMRB,SimHatingTFT,Switch,TwoFacedTit,VOFB]-&gt;[Cheese,DMRB,VOFB] (963 matches) \n<ul>\n<li>(Only CheeseBot, DMRB, and VOFB made it into the final round; all three cooperated with each other for the entire round, resulting in a three-way tie)</li>\n</ul>\n</li>\n<li>ALL-&gt;[Cheese,DMRB,SimHatingTFT,Switch,TatForTit,TwoFacedTit]-&gt;[Cheese,DMRB,TatForTit]-&gt;[DMRB,TatForTit]-&gt;[TatForTit] (32 matches) \n<ul>\n<li>(Only TatForTit and DMRB made it into the final round; both bots cooperated until the second-to-last turns of each matchup, where TatForTit played Defect while the DMRB played Cooperate, resulting in a TatForTit victory)</li>\n</ul>\n</li>\n<li>ALL-&gt;[Cheese,DMRB,SimHatingTFT,Switch,TatForTit,TwoFacedTit,VOFB]-&gt;[Cheese,DMRB,SimHatingTFT,TwoFacedTit]-&gt;[Cheese,DMRB] (3 matches) \n<ul>\n<li>(Only CheeseBot and DMRB made it into the final round; both bots cooperated with each other for the entire round, resulting in a two-way tie)</li>\n</ul>\n</li>\n<li>ALL-&gt;[Cheese,DMRB,SimHatingTFT,Switch,TatForTit,VOFB]-&gt;[Cheese,DMRB,SimHatingTFT]-&gt;[Cheese,DMRB] (1 match) \n<ul>\n<li>(Only CheeseBot and DMRB made it into the final round; both bots cooperated with each other for the entire round, resulting in a two-way tie)</li>\n</ul>\n</li>\n<li>ALL-&gt;[Cheese,DMRB,SimHatingTFT,Switch,TwoFacedTit,VOFB]-&gt;[Cheese,DMRB,SimHatingTFT]-&gt;[Cheese,DMRB] (1 match) \n<ul>\n<li>(Only CheeseBot and DMRB made it into the final round; both bots cooperated with each other for the entire round, resulting in a two-way tie)</li>\n</ul>\n</li>\n</ul>\n<p>This suggests that this game does have some kind of equilibrium, because these top three bots use very similar strategies: Simulate my opponent and figure out if defections will be punished; if so, Cooperate, and otherwise, defect. This allows bots following this strategy to always cooperate with each other, consistently providing them with a large number of points in every round, ensuring that they outcompete backstabbing or other aggressive strategies. In this tournament, this allowed the top three bots to add a guaranteed 600 points per round, more than enough to consistently keep them from being eliminated.<br /><br />The tournament was slightly more interesting (and far more varied) on a matchup-by-matchup basis. Last-round and second-to-last round defections after mutual cooperation were common. TatForTit frequently used this technique against VOFB, CheeseBot, DMRB, and vice versa; this tactic allowed it to steal 32 wins in the final round. Other bots, particularly AnderBot and Pip, behaved very differently between matches. Pip, in particular, sometimes cooperated and sometimes defected for long stretches, and AnderBot's randomness also led to erratic behavior. Ultimately, though, this did not net these bots a large number of points, as their opponents generally defected as soon as they stopped cooperating.<br /><br />For those interested in the gritty details, I've formatted the output of each match to be human-readable, so you can easily read through the play-by-play of each match (and hopefully get some enjoyment out of it, as well). See <a href=\"https://github.com/pdtournament/2014results\">the github repo</a> for the full logs.</p>\n<h1>Postscript</h1>\n<p>In the course of running the tournament, I received a number of suggestions and idea for how things could be improved; some of these ideas include:</p>\n<ul>\n<li>Random-length matches instead of fixed-length matches.</li>\n<li>Straight elimination rather than round-robin elimination.</li>\n<li>More \"cannon-fodder\" bots included in the tournament by default, such as copies of cooperateBot, defectBot, and tit-for-tat.</li>\n<li>A QuickCheck-based test suite that allows bot writers to more easily test properties of their bot while developing, such as \"cooperates with cooperateBot\" or \"cooperates if no one has defected so far.\"</li>\n</ul>\n<p><br />If anyone would like me to run this tournament again at some unspecified time in the future, with or without modifications, feel free to let me know in the comments. If you would like to fork the project and run it on your own, you are more than welcome to do so.<br /><br />Many thanks to everyone who participated!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"be2Mh2bddQ6ZaBcti": 2, "b8FHrKqyXuYGWc6vn": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q8NHPhhELtMecRLQF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 65, "baseScore": 94, "extendedScore": null, "score": 0.000234, "legacy": true, "legacyId": "27258", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 71, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Followup to: </strong><a href=\"/r/discussion/lw/knd/announcing_the_2014_program_equilibrium_iterated\">Announcing the 2014 program equilibrium iterated PD tournament</a><br><br>In August, I announced an iterated prisoner's dilemma tournament in which bots can simulate each other before making a move. Eleven bots were submitted to the tournament. Today, I am pleased to announce the final standings and release the source code and full results.<br><br>All of the source code submitted by the competitors and the full results for each match are available <a href=\"https://github.com/pdtournament/2014results\">here</a>. See <a href=\"http://github.com/pdtournament/pdtournament\">here</a> for the full set of rules and tournament code.<br><br>Before we get to the final results, here's a quick rundown of the bots that competed:</p>\n<h2 id=\"AnderBot\">AnderBot</h2>\n<p>AnderBot follows a simple tit-for-tat-like algorithm that eschews simulation:</p>\n<ul>\n<li>On the first turn, Cooperate.</li>\n<li>For the next 10 turns, play tit-for-tat.</li>\n<li>For the rest of the game, Defect with 10% probability or Defect if the opposing bot has defected more times than AnderBot.</li>\n</ul>\n<p><a id=\"more\"></a></p>\n<h2 id=\"CheeseBot\">CheeseBot</h2>\n<p>CheeseBot tries several simulation-based strategies, and uses the first one that applies to the current situation.</p>\n<ul>\n<li>If the opponent defected on the previous round, Defect.</li>\n<li>If the opponent does not defect against defectBot, Defect.</li>\n<li>If defecting on this round would lead to CheeseBot being punished for it in future rounds, Cooperate. CheeseBot checks this by simulating future rounds with a simulated history in which it defects on the current round.</li>\n<li>If the opponent is a mirror-like bot, Cooperate. To test whether a bot is mirror-like, CheeseBot simulates the opponent and checks if it defects against DefectBot and cooperates with a bot that plays tit-for-tat but defects against CooperateBot and DefectBot.</li>\n<li>If it is the last round, Cooperate.</li>\n<li>Defect.</li>\n</ul>\n<h2 id=\"DefectBot\">DefectBot</h2>\n<p>Defect!<br><br>This bot was <a href=\"/lw/kmk/open_thread_july_28_august_3_2014/b63m\">submitted publicly by James Miller.</a></p>\n<h2 id=\"DMRB\">DMRB</h2>\n<p>DavidMonRoBot (DMRB) takes a more cautious approach to simulating: It spends a few hundred milliseconds simulating its opponent to figure out what the rest of the round will look like given a Cooperate or Defect on the current round, and then picks the outcome that leads to the highest total number of points for DMRB.<br><br>This allows DMRB to gauge whether its opponent is \"dumb,\" i.e. does not punish defectors. If the opponent is dumb, DMRB reasons that the best move is to defect; otherwise, if DMRB thinks that its opponent will punish defection, it simply plays tit-for-tat. DMRB spends only a small amount of time simulating so that other simulation-based bots will be less likely to have their simulations time out while simulating DMRB.</p>\n<h2 id=\"Pip\">Pip</h2>\n<p>This one is a behemoth. At almost 500 very dense lines of Haskell (including comments containing quotes from \"The Quantum Thief\"), Pip uses a complex series of simulations to classify the opponent into a large set of defined behaviors, such as \"CooperateOnLastInTheFaceOfCooperation\", \"Uncompromising\" and \"Extortionist.\" Then, it builds out a decision tree and selects the outcome that leads to the highest score at the end of the match. If I'm being vague here, it's because the inner workings of Pip are still mostly a mystery to me.</p>\n<h2 id=\"SimHatingTitForTat\">SimHatingTitForTat</h2>\n<p>SimHatingTitForTat, as the name implies, plays tit-for-tat and attempts to punish bots that use simulation to exploit it, namely by defecting against bots that deviated from tit-for-tat on any previous round. Its strategy is as follows:</p>\n<ul>\n<li>On the first round, Cooperate.</li>\n<li>On every subsequent round, if the opponent played tit-for-tat on all previous rounds, play tit-for-tat. Otherwise, Defect.</li>\n</ul>\n<h2 id=\"SimpleTFTseerBot\">SimpleTFTseerBot</h2>\n<p>SimpleTFTseer uses a modified version of tit-for-tat that ruthlessly punishes defectors and uses one simulation per round to look at what its opponent will do on the last round.</p>\n<ul>\n<li>If either player defected in a previous round, Defect.</li>\n<li>If it is the last round, Cooperate.</li>\n<li>Otherwise, simulate my opponent playing against me on the last round of this match, assuming that we both Cooperate from the current round until the second-to-last round, and do whatever my opponent does in that scenario. If the simulation does not terminate, Defect.</li>\n</ul>\n<h2 id=\"SwitchBot\">SwitchBot</h2>\n<p>SwitchBot also uses a modified tit-for-tat algorithm:</p>\n<ul>\n<li>On the first turn, Cooperate.</li>\n<li>On the second turn, if my opponent Defected on the previous turn, simulate my opponent playing against mirrorBot, and do whatever my opponent would do in that scenario.</li>\n<li>Otherwise, play tit-for-tat.</li>\n</ul>\n<h2 id=\"TatForTit\">TatForTit</h2>\n<p>TatForTit follows a complex simulation-based strategy:</p>\n<ul>\n<li>On the first round, do whatever the opponent would do against TatForTit on the second round, assuming both bots cooperated on the first round.</li>\n<li>On the second round, if my opponent defected on the previous round, Defect. Otherwise, do whatever my opponent would do against me, assuming they cooperated on the first round.</li>\n<li>On all subsequent turns, if my previous move was not the same as my opponents move two turns ago, Defect. Otherwise, do whatever my opponent would do against me one turn in the future, assuming TatForTit repeats its previous move on the next turn and the opposing bot cooperated on the next turn.</li>\n</ul>\n<h2 id=\"TwoFacedTit\">TwoFacedTit</h2>\n<p>TwoFacedTit simulates its opponent playing against mirrorBot; if it takes more than 10 milliseconds to respond, TwoFacedTit plays Cooperate. Otherwise, TwoFacedTit plays tit-for-tat and then Defects on the last round.</p>\n<h2 id=\"VOFB\">VOFB</h2>\n<p>Like SimpleTFTseerBot, VeryOpportunisticFarseerBot (VOFB) uses a very aggressive defection-punishment strategy: If either player defected in a previous round, Defect. Otherwise, VOFB simulates the next round to determine what the opponent will do given a Cooperate or Defect on the current round. If the opponent does not punish defection or the simulation does not terminate, VOFB Defects. On the final round, VOFB uses additional simulations to detect whether its opponent defects against backstabbers, and if not, plays Defect.</p>\n<h1 id=\"Tournament_results\">Tournament results</h1>\n<p>After 1000 round-robin elimination matches, the final standings are:<br><br><strong>1st place (tied):</strong> CheeseBot and DavidMonRoBot<br><strong>3rd place:</strong> VeryOpportunisticFarseerBot<br><strong>4th place:</strong> TatForTit</p>\n<p>The win frequencies for each bot:</p>\n<p><img src=\"http://images.lesswrong.com/t3_l16_4.png?v=76b560b9a17aa397869c0ac661fd1321\" alt=\"\" width=\"645\" height=\"451\"></p>\n<p>If it were a sporting event, this tournament would not be particularly exciting to watch, as most games were nearly identical from start to finish. The graph below shows each bot's frequency of surviving the first round-robin round:</p>\n<p><img src=\"http://images.lesswrong.com/t3_l16_2.png?v=57420d19136edf499964a104f5217c7a\" alt=\"\" width=\"651\" height=\"455\"></p>\n<p><br>In other words, the same half of the field consistently swept the first round; AnderBot, DefectBot, Pip, and SimpleTFTseer never survived to see a second round. In general, this is because high-scoring bots almost always cooperated with each other (with the occasional backstab at the end of the round), and defected against AnderBot, DefectBot, Pop, and SimpleTFTseerBot, as these bots either did not consistently retaliate when defected against or pre-emptively defected, triggering a chain of mutual defections. Interestingly, the bots that continued on to the next round did not do so by a large margin:</p>\n<p><img src=\"http://images.lesswrong.com/t3_l16_1.png?v=8f8a88fdabe01079be13296fd2369158\" alt=\"\" width=\"693\" height=\"453\"></p>\n<p>However, the variance in these scores was very low, primarily due to the repeated matchups of mostly-deterministic strategies consistently resulting in the same outcomes.</p>\n<p><img src=\"http://images.lesswrong.com/t3_l16_0.png?v=cf4df761b8b80d85773d4566e4aa400a\" alt=\"\" width=\"679\" height=\"445\"></p>\n<p>In addition, all of the matches progressed in one of the following 5 ways (hat tip <a href=\"/r/discussion/lw/l16/2014_iterated_prisoners_dilemma_tournament_results/bdir\">lackofcheese</a>):</p>\n<ul>\n<li>ALL-&gt;[Cheese,DMRB,SimHatingTFT,Switch,TwoFacedTit,VOFB]-&gt;[Cheese,DMRB,VOFB] (963 matches) \n<ul>\n<li>(Only CheeseBot, DMRB, and VOFB made it into the final round; all three cooperated with each other for the entire round, resulting in a three-way tie)</li>\n</ul>\n</li>\n<li>ALL-&gt;[Cheese,DMRB,SimHatingTFT,Switch,TatForTit,TwoFacedTit]-&gt;[Cheese,DMRB,TatForTit]-&gt;[DMRB,TatForTit]-&gt;[TatForTit] (32 matches) \n<ul>\n<li>(Only TatForTit and DMRB made it into the final round; both bots cooperated until the second-to-last turns of each matchup, where TatForTit played Defect while the DMRB played Cooperate, resulting in a TatForTit victory)</li>\n</ul>\n</li>\n<li>ALL-&gt;[Cheese,DMRB,SimHatingTFT,Switch,TatForTit,TwoFacedTit,VOFB]-&gt;[Cheese,DMRB,SimHatingTFT,TwoFacedTit]-&gt;[Cheese,DMRB] (3 matches) \n<ul>\n<li>(Only CheeseBot and DMRB made it into the final round; both bots cooperated with each other for the entire round, resulting in a two-way tie)</li>\n</ul>\n</li>\n<li>ALL-&gt;[Cheese,DMRB,SimHatingTFT,Switch,TatForTit,VOFB]-&gt;[Cheese,DMRB,SimHatingTFT]-&gt;[Cheese,DMRB] (1 match) \n<ul>\n<li>(Only CheeseBot and DMRB made it into the final round; both bots cooperated with each other for the entire round, resulting in a two-way tie)</li>\n</ul>\n</li>\n<li>ALL-&gt;[Cheese,DMRB,SimHatingTFT,Switch,TwoFacedTit,VOFB]-&gt;[Cheese,DMRB,SimHatingTFT]-&gt;[Cheese,DMRB] (1 match) \n<ul>\n<li>(Only CheeseBot and DMRB made it into the final round; both bots cooperated with each other for the entire round, resulting in a two-way tie)</li>\n</ul>\n</li>\n</ul>\n<p>This suggests that this game does have some kind of equilibrium, because these top three bots use very similar strategies: Simulate my opponent and figure out if defections will be punished; if so, Cooperate, and otherwise, defect. This allows bots following this strategy to always cooperate with each other, consistently providing them with a large number of points in every round, ensuring that they outcompete backstabbing or other aggressive strategies. In this tournament, this allowed the top three bots to add a guaranteed 600 points per round, more than enough to consistently keep them from being eliminated.<br><br>The tournament was slightly more interesting (and far more varied) on a matchup-by-matchup basis. Last-round and second-to-last round defections after mutual cooperation were common. TatForTit frequently used this technique against VOFB, CheeseBot, DMRB, and vice versa; this tactic allowed it to steal 32 wins in the final round. Other bots, particularly AnderBot and Pip, behaved very differently between matches. Pip, in particular, sometimes cooperated and sometimes defected for long stretches, and AnderBot's randomness also led to erratic behavior. Ultimately, though, this did not net these bots a large number of points, as their opponents generally defected as soon as they stopped cooperating.<br><br>For those interested in the gritty details, I've formatted the output of each match to be human-readable, so you can easily read through the play-by-play of each match (and hopefully get some enjoyment out of it, as well). See <a href=\"https://github.com/pdtournament/2014results\">the github repo</a> for the full logs.</p>\n<h1 id=\"Postscript\">Postscript</h1>\n<p>In the course of running the tournament, I received a number of suggestions and idea for how things could be improved; some of these ideas include:</p>\n<ul>\n<li>Random-length matches instead of fixed-length matches.</li>\n<li>Straight elimination rather than round-robin elimination.</li>\n<li>More \"cannon-fodder\" bots included in the tournament by default, such as copies of cooperateBot, defectBot, and tit-for-tat.</li>\n<li>A QuickCheck-based test suite that allows bot writers to more easily test properties of their bot while developing, such as \"cooperates with cooperateBot\" or \"cooperates if no one has defected so far.\"</li>\n</ul>\n<p><br>If anyone would like me to run this tournament again at some unspecified time in the future, with or without modifications, feel free to let me know in the comments. If you would like to fork the project and run it on your own, you are more than welcome to do so.<br><br>Many thanks to everyone who participated!</p>", "sections": [{"title": "AnderBot", "anchor": "AnderBot", "level": 2}, {"title": "CheeseBot", "anchor": "CheeseBot", "level": 2}, {"title": "DefectBot", "anchor": "DefectBot", "level": 2}, {"title": "DMRB", "anchor": "DMRB", "level": 2}, {"title": "Pip", "anchor": "Pip", "level": 2}, {"title": "SimHatingTitForTat", "anchor": "SimHatingTitForTat", "level": 2}, {"title": "SimpleTFTseerBot", "anchor": "SimpleTFTseerBot", "level": 2}, {"title": "SwitchBot", "anchor": "SwitchBot", "level": 2}, {"title": "TatForTit", "anchor": "TatForTit", "level": 2}, {"title": "TwoFacedTit", "anchor": "TwoFacedTit", "level": 2}, {"title": "VOFB", "anchor": "VOFB", "level": 2}, {"title": "Tournament results", "anchor": "Tournament_results", "level": 1}, {"title": "Postscript", "anchor": "Postscript", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "57 comments"}], "headingsCount": 15}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3XzfKHQTFrJBAW6jB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-01T05:45:53.789Z", "modifiedAt": null, "url": null, "title": "Meetup : Portland, OR: Improv for Rationalists", "slug": "meetup-portland-or-improv-for-rationalists", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VAuroch", "createdAt": "2013-11-07T11:01:09.015Z", "isAdmin": false, "displayName": "VAuroch"}, "userId": "idJgwEhhiRhTzHpst", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oPopteF8c7L7sPirw/meetup-portland-or-improv-for-rationalists", "pageUrlRelative": "/posts/oPopteF8c7L7sPirw/meetup-portland-or-improv-for-rationalists", "linkUrl": "https://www.lesswrong.com/posts/oPopteF8c7L7sPirw/meetup-portland-or-improv-for-rationalists", "postedAtFormatted": "Wednesday, October 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Portland%2C%20OR%3A%20Improv%20for%20Rationalists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Portland%2C%20OR%3A%20Improv%20for%20Rationalists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoPopteF8c7L7sPirw%2Fmeetup-portland-or-improv-for-rationalists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Portland%2C%20OR%3A%20Improv%20for%20Rationalists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoPopteF8c7L7sPirw%2Fmeetup-portland-or-improv-for-rationalists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoPopteF8c7L7sPirw%2Fmeetup-portland-or-improv-for-rationalists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/154'>Portland, OR: Improv for Rationalists</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 October 2014 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2945 NE 64th Ave, Portland, OR</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Last meetup we discussed using techniques from acting improv to improve social skills and impose personal habits. This month we'll be putting some of that into practice with lessons and practice.\nDiscussion to follow.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/154'>Portland, OR: Improv for Rationalists</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oPopteF8c7L7sPirw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.0485352920734298e-06, "legacy": true, "legacyId": "27289", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Portland__OR__Improv_for_Rationalists\">Discussion article for the meetup : <a href=\"/meetups/154\">Portland, OR: Improv for Rationalists</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 October 2014 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2945 NE 64th Ave, Portland, OR</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Last meetup we discussed using techniques from acting improv to improve social skills and impose personal habits. This month we'll be putting some of that into practice with lessons and practice.\nDiscussion to follow.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Portland__OR__Improv_for_Rationalists1\">Discussion article for the meetup : <a href=\"/meetups/154\">Portland, OR: Improv for Rationalists</a></h2>", "sections": [{"title": "Discussion article for the meetup : Portland, OR: Improv for Rationalists", "anchor": "Discussion_article_for_the_meetup___Portland__OR__Improv_for_Rationalists", "level": 1}, {"title": "Discussion article for the meetup : Portland, OR: Improv for Rationalists", "anchor": "Discussion_article_for_the_meetup___Portland__OR__Improv_for_Rationalists1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-01T17:27:18.797Z", "modifiedAt": null, "url": null, "title": "October 2014 Media Thread", "slug": "october-2014-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:09.574Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ArisKatsaris", "createdAt": "2010-10-07T10:24:25.721Z", "isAdmin": false, "displayName": "ArisKatsaris"}, "userId": "fLbksBTnFsbwYmzsT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DSuEwPjY89yBBvfc8/october-2014-media-thread", "pageUrlRelative": "/posts/DSuEwPjY89yBBvfc8/october-2014-media-thread", "linkUrl": "https://www.lesswrong.com/posts/DSuEwPjY89yBBvfc8/october-2014-media-thread", "postedAtFormatted": "Wednesday, October 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20October%202014%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOctober%202014%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDSuEwPjY89yBBvfc8%2Foctober-2014-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=October%202014%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDSuEwPjY89yBBvfc8%2Foctober-2014-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDSuEwPjY89yBBvfc8%2Foctober-2014-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 219, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the <a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please post only under one of the already created subthreads, and never directly under the parent media thread.</li>\n<li>Use the \"Other Media\" thread if you believe the piece of media you want to discuss doesn't fit under any of the established categories.</li>\n<li>Use the \"Meta\" thread if you want to discuss about the monthly media thread itself (e.g. to propose adding/removing/splitting/merging subthreads, or to discuss the type of content properly belonging to each subthread) or for any other question or issue you may have about the thread or the rules.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DSuEwPjY89yBBvfc8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 2.049846419127441e-06, "legacy": true, "legacyId": "27292", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-01T23:02:20.410Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes October 2014", "slug": "rationality-quotes-october-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:36.763Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tyrrell_McAllister", "createdAt": "2009-03-05T19:59:57.157Z", "isAdmin": false, "displayName": "Tyrrell_McAllister"}, "userId": "HSANMQBsHiGrZzwTB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/94SFae4PQNJLdrdTP/rationality-quotes-october-2014", "pageUrlRelative": "/posts/94SFae4PQNJLdrdTP/rationality-quotes-october-2014", "linkUrl": "https://www.lesswrong.com/posts/94SFae4PQNJLdrdTP/rationality-quotes-october-2014", "postedAtFormatted": "Wednesday, October 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20October%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20October%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F94SFae4PQNJLdrdTP%2Frationality-quotes-october-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20October%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F94SFae4PQNJLdrdTP%2Frationality-quotes-october-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F94SFae4PQNJLdrdTP%2Frationality-quotes-october-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 133, "htmlBody": "<p>Another month, another rationality quotes thread. The rules are:</p>\n<ul>\n<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson. If you'd like to revive an old quote from one of those sources, please do so <a href=\"/r/discussion/lw/i6h/rationality_quotes_from_people_associated_with/\">here</a>.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n<li>Provide sufficient information (URL, title, date, page number, etc.) to enable a reader to find the place where you read the quote, or its original source if available. Do not quote with only a name.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "94SFae4PQNJLdrdTP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 2.050473193935489e-06, "legacy": true, "legacyId": "27293", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 238, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWTZj26MfR8e8b9nm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-02T00:08:44.071Z", "modifiedAt": null, "url": null, "title": "Upcoming CFAR events: Lower-cost bay area intro workshop; EU workshops; and others", "slug": "upcoming-cfar-events-lower-cost-bay-area-intro-workshop-eu", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:31.060Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9jHwTTsEnRBqogiqB/upcoming-cfar-events-lower-cost-bay-area-intro-workshop-eu", "pageUrlRelative": "/posts/9jHwTTsEnRBqogiqB/upcoming-cfar-events-lower-cost-bay-area-intro-workshop-eu", "linkUrl": "https://www.lesswrong.com/posts/9jHwTTsEnRBqogiqB/upcoming-cfar-events-lower-cost-bay-area-intro-workshop-eu", "postedAtFormatted": "Thursday, October 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Upcoming%20CFAR%20events%3A%20Lower-cost%20bay%20area%20intro%20workshop%3B%20EU%20workshops%3B%20and%20others&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUpcoming%20CFAR%20events%3A%20Lower-cost%20bay%20area%20intro%20workshop%3B%20EU%20workshops%3B%20and%20others%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9jHwTTsEnRBqogiqB%2Fupcoming-cfar-events-lower-cost-bay-area-intro-workshop-eu%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Upcoming%20CFAR%20events%3A%20Lower-cost%20bay%20area%20intro%20workshop%3B%20EU%20workshops%3B%20and%20others%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9jHwTTsEnRBqogiqB%2Fupcoming-cfar-events-lower-cost-bay-area-intro-workshop-eu", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9jHwTTsEnRBqogiqB%2Fupcoming-cfar-events-lower-cost-bay-area-intro-workshop-eu", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 264, "htmlBody": "<p class=\"p1\">For anyone who's interested:</p>\n<p class=\"p1\"><a href=\"http://rationality.org/\">CFAR</a>&nbsp;is trying out an experimental, <strong>lower-cost, 1.5-day introductory workshop <a href=\"https://www.eventbrite.com/e/a-taste-of-applied-rationality-tickets-12944727027\">Oct 25-26</a>&nbsp;in the bay area</strong>.&nbsp; It is meant to provide an easier point of entry into our rationality training.&nbsp; If you've been thinking about coming to a CFAR workshop but have had trouble setting aside 4 days and $3900, you might consider trying this out.&nbsp; (Or, if you have a friend or family member in that situaiton, you might suggest this to them.)&nbsp; It's a beta test, so no guarantees as to the outcome -- but I suspect it'll be both useful, and a lot of fun.</p>\n<p class=\"p1\">We are also finally making it to <strong>Europe</strong>. &nbsp;We'll be running two <a href=\"http://rationality.org/workshops/\">workshops</a> in the UK this November, both of which have both space and financial aid still available.</p>\n<p class=\"p1\">We're also still running our <a href=\"http://rationality.org/workshops/\">standard</a> <strong>workshops</strong>: Jan 16-19 in Berkeley, and April 23-26 in Boston, MA. &nbsp;(We're experimenting, also, with using alumni \"TA's\" to increase the amount of 1-on-1 informal instruction while simultaneously increasing workshop size, in an effort to scale our impact.)</p>\n<p class=\"p1\">Finally, we're actually running a bunch of events lately for alumni of our 4-day workshops (a weekly rationality <strong>dojo</strong>; a bimonthly colloquium; a yearly alumni reunion; and various for-alumni workshops); which is perhaps less exciting if you aren't yet an alumnus, but which I'm very excited about because it suggests that we'll have a larger community of people doing serious practice, and thereby pushing the boundaries of the art of rationality.</p>\n<p class=\"p1\">If anyone wishes to discuss any of these events, or CFAR's strategy as a whole, I'd be glad to talk; you can book me <a href=\"http://rationality.org/talk/anna/\">here</a>.</p>\n<p class=\"p1\">Cheers!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"X7v7Fyp9cgBYaMe2e": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9jHwTTsEnRBqogiqB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 28, "extendedScore": null, "score": 2.050597445977236e-06, "legacy": true, "legacyId": "27294", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-02T04:39:10.803Z", "modifiedAt": null, "url": null, "title": "Meetup : Yale rationality group meeting.", "slug": "meetup-yale-rationality-group-meeting", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Lambda", "createdAt": "2012-02-04T03:49:39.010Z", "isAdmin": false, "displayName": "Lambda"}, "userId": "zbF8miyEeS43MJqBz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6bdx5Qx3RrCC84DF7/meetup-yale-rationality-group-meeting", "pageUrlRelative": "/posts/6bdx5Qx3RrCC84DF7/meetup-yale-rationality-group-meeting", "linkUrl": "https://www.lesswrong.com/posts/6bdx5Qx3RrCC84DF7/meetup-yale-rationality-group-meeting", "postedAtFormatted": "Thursday, October 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Yale%20rationality%20group%20meeting.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Yale%20rationality%20group%20meeting.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bdx5Qx3RrCC84DF7%2Fmeetup-yale-rationality-group-meeting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Yale%20rationality%20group%20meeting.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bdx5Qx3RrCC84DF7%2Fmeetup-yale-rationality-group-meeting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bdx5Qx3RrCC84DF7%2Fmeetup-yale-rationality-group-meeting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/155'>Yale rationality group meeting.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 October 2014 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Seminar room K22, Jonathan Edwards College, 68 High Street New Haven, CT 06511</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Yale rationality group will be holding a meeting this Sunday, October 5 at 2 pm in JE seminar room K22. The meeting should last roughly an hour (at most two). Tentative agenda: we'll be (re)introducing ourselves to each other, discussing some organizational matters, and doing some group exercises on goals and getting stuff done. All are welcome.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/155'>Yale rationality group meeting.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6bdx5Qx3RrCC84DF7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.0511037023514516e-06, "legacy": true, "legacyId": "27296", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Yale_rationality_group_meeting_\">Discussion article for the meetup : <a href=\"/meetups/155\">Yale rationality group meeting.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 October 2014 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Seminar room K22, Jonathan Edwards College, 68 High Street New Haven, CT 06511</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Yale rationality group will be holding a meeting this Sunday, October 5 at 2 pm in JE seminar room K22. The meeting should last roughly an hour (at most two). Tentative agenda: we'll be (re)introducing ourselves to each other, discussing some organizational matters, and doing some group exercises on goals and getting stuff done. All are welcome.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Yale_rationality_group_meeting_1\">Discussion article for the meetup : <a href=\"/meetups/155\">Yale rationality group meeting.</a></h2>", "sections": [{"title": "Discussion article for the meetup : Yale rationality group meeting.", "anchor": "Discussion_article_for_the_meetup___Yale_rationality_group_meeting_", "level": 1}, {"title": "Discussion article for the meetup : Yale rationality group meeting.", "anchor": "Discussion_article_for_the_meetup___Yale_rationality_group_meeting_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-02T11:53:37.350Z", "modifiedAt": null, "url": null, "title": "Meetup : Canberra: Contrarianism", "slug": "meetup-canberra-contrarianism", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielFilan", "createdAt": "2014-01-30T11:04:39.341Z", "isAdmin": false, "displayName": "DanielFilan"}, "userId": "DgsGzjyBXN8XSK22q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xh39CKiprzdFnBWjW/meetup-canberra-contrarianism", "pageUrlRelative": "/posts/xh39CKiprzdFnBWjW/meetup-canberra-contrarianism", "linkUrl": "https://www.lesswrong.com/posts/xh39CKiprzdFnBWjW/meetup-canberra-contrarianism", "postedAtFormatted": "Thursday, October 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Canberra%3A%20Contrarianism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Canberra%3A%20Contrarianism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxh39CKiprzdFnBWjW%2Fmeetup-canberra-contrarianism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Canberra%3A%20Contrarianism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxh39CKiprzdFnBWjW%2Fmeetup-canberra-contrarianism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxh39CKiprzdFnBWjW%2Fmeetup-canberra-contrarianism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 142, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/156'>Canberra: Contrarianism</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 October 2014 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">108 North Road, Acton, ACT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Share your LW-contrarian opinions! That is, (ideally) come prepared with some opinions you have that most other people at the meetup might not, and reasons why your opinion is true. Examples might include:</p>\n\n<p>-We should return to feudal societal structure.</p>\n\n<p>-Dualism is the true theory of consciousness.</p>\n\n<p>-We don't have to worry about unfriendly AI.</p>\n\n<p>-Eating non-human animals is perfectly OK/insect suffering is the most important moral problem (depends on who you want to chat with).</p>\n\n<p>Come and enjoy the discussion! As always, vegan snacks will be provided.</p>\n\n<p>General meetup info:</p>\n\n<p>If you use Facebook, please join our <a href=\"https://www.facebook.com/groups/lwcanberra/\" rel=\"nofollow\">group</a>.</p>\n\n<p>Structured meetups are held on the second Saturday and fourth Friday of each month from 6 pm until late in the CSIT building, room N101.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/156'>Canberra: Contrarianism</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xh39CKiprzdFnBWjW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.0519174132862024e-06, "legacy": true, "legacyId": "27298", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Canberra__Contrarianism\">Discussion article for the meetup : <a href=\"/meetups/156\">Canberra: Contrarianism</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 October 2014 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">108 North Road, Acton, ACT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Share your LW-contrarian opinions! That is, (ideally) come prepared with some opinions you have that most other people at the meetup might not, and reasons why your opinion is true. Examples might include:</p>\n\n<p>-We should return to feudal societal structure.</p>\n\n<p>-Dualism is the true theory of consciousness.</p>\n\n<p>-We don't have to worry about unfriendly AI.</p>\n\n<p>-Eating non-human animals is perfectly OK/insect suffering is the most important moral problem (depends on who you want to chat with).</p>\n\n<p>Come and enjoy the discussion! As always, vegan snacks will be provided.</p>\n\n<p>General meetup info:</p>\n\n<p>If you use Facebook, please join our <a href=\"https://www.facebook.com/groups/lwcanberra/\" rel=\"nofollow\">group</a>.</p>\n\n<p>Structured meetups are held on the second Saturday and fourth Friday of each month from 6 pm until late in the CSIT building, room N101.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Canberra__Contrarianism1\">Discussion article for the meetup : <a href=\"/meetups/156\">Canberra: Contrarianism</a></h2>", "sections": [{"title": "Discussion article for the meetup : Canberra: Contrarianism", "anchor": "Discussion_article_for_the_meetup___Canberra__Contrarianism", "level": 1}, {"title": "Discussion article for the meetup : Canberra: Contrarianism", "anchor": "Discussion_article_for_the_meetup___Canberra__Contrarianism1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-02T16:49:44.048Z", "modifiedAt": null, "url": null, "title": "Meetup : Meta Meetup", "slug": "meetup-meta-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:59.498Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anders_H", "createdAt": "2013-07-28T20:46:58.747Z", "isAdmin": false, "displayName": "Anders_H"}, "userId": "jfdosp4Hn7tFNbf9k", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KrwwuPijLY2M6ZcJQ/meetup-meta-meetup", "pageUrlRelative": "/posts/KrwwuPijLY2M6ZcJQ/meetup-meta-meetup", "linkUrl": "https://www.lesswrong.com/posts/KrwwuPijLY2M6ZcJQ/meetup-meta-meetup", "postedAtFormatted": "Thursday, October 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Meta%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Meta%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKrwwuPijLY2M6ZcJQ%2Fmeetup-meta-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Meta%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKrwwuPijLY2M6ZcJQ%2Fmeetup-meta-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKrwwuPijLY2M6ZcJQ%2Fmeetup-meta-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 182, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/157'>Meta Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 October 2014 03:30:20AM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">98 Elm Street Somerville</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll spend the first part of the meeting discussing the topics we'd like to see more of at future meetups. For the second part of the meetup, we'll socialize and get to know each other better. Cambridge/Boston-area Less Wrong meetups start at 3:30pm, and have an alternating location:</p>\n\n<ul>\n<li><p>1st Sunday meetups are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p></li>\n<li><p>3rd Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number. (We also have last Wednesday meetups at Citadel at 7pm.)</p></li>\n</ul>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 4pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/157'>Meta Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KrwwuPijLY2M6ZcJQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.0524723579516477e-06, "legacy": true, "legacyId": "27299", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Meta_Meetup\">Discussion article for the meetup : <a href=\"/meetups/157\">Meta Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 October 2014 03:30:20AM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">98 Elm Street Somerville</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll spend the first part of the meeting discussing the topics we'd like to see more of at future meetups. For the second part of the meetup, we'll socialize and get to know each other better. Cambridge/Boston-area Less Wrong meetups start at 3:30pm, and have an alternating location:</p>\n\n<ul>\n<li><p>1st Sunday meetups are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p></li>\n<li><p>3rd Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number. (We also have last Wednesday meetups at Citadel at 7pm.)</p></li>\n</ul>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 4pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Meta_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/157\">Meta Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Meta Meetup", "anchor": "Discussion_article_for_the_meetup___Meta_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Meta Meetup", "anchor": "Discussion_article_for_the_meetup___Meta_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-02T20:33:00.781Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Noticing.", "slug": "meetup-urbana-champaign-noticing", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7YjwLzreo3wffxEK7/meetup-urbana-champaign-noticing", "pageUrlRelative": "/posts/7YjwLzreo3wffxEK7/meetup-urbana-champaign-noticing", "linkUrl": "https://www.lesswrong.com/posts/7YjwLzreo3wffxEK7/meetup-urbana-champaign-noticing", "postedAtFormatted": "Thursday, October 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Noticing.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Noticing.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YjwLzreo3wffxEK7%2Fmeetup-urbana-champaign-noticing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Noticing.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YjwLzreo3wffxEK7%2Fmeetup-urbana-champaign-noticing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YjwLzreo3wffxEK7%2Fmeetup-urbana-champaign-noticing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/158'>Urbana-Champaign: Noticing.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 October 2014 05:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">206 S. Cedar St., Urbana, IL, 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's work on noticing things. Homework before this meetup: work on noticing when you're rationalizing, and do some physical act like standing up, or snapping your fingers, or writing down what you were rationalizing about.</p>\n\n<p>This may be a bit ambitious, but that's what we can work on at the meetup.</p>\n\n<p>Some reading material: <a href=\"http://agentyduck.blogspot.com/2014/09/what-its-like-to-notice-things.html\" rel=\"nofollow\">1</a> <a href=\"http://agentyduck.blogspot.com/2014/09/noticing-curiosity-and-searching-log.html\" rel=\"nofollow\">2</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/158'>Urbana-Champaign: Noticing.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7YjwLzreo3wffxEK7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.052890981892233e-06, "legacy": true, "legacyId": "27300", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Noticing_\">Discussion article for the meetup : <a href=\"/meetups/158\">Urbana-Champaign: Noticing.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 October 2014 05:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">206 S. Cedar St., Urbana, IL, 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's work on noticing things. Homework before this meetup: work on noticing when you're rationalizing, and do some physical act like standing up, or snapping your fingers, or writing down what you were rationalizing about.</p>\n\n<p>This may be a bit ambitious, but that's what we can work on at the meetup.</p>\n\n<p>Some reading material: <a href=\"http://agentyduck.blogspot.com/2014/09/what-its-like-to-notice-things.html\" rel=\"nofollow\">1</a> <a href=\"http://agentyduck.blogspot.com/2014/09/noticing-curiosity-and-searching-log.html\" rel=\"nofollow\">2</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Noticing_1\">Discussion article for the meetup : <a href=\"/meetups/158\">Urbana-Champaign: Noticing.</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Noticing.", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Noticing_", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Noticing.", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Noticing_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-02T22:48:37.564Z", "modifiedAt": null, "url": null, "title": "[MIRIx Cambridge MA] Limiting resource allocation with bounded utility functions and conceptual uncertainty", "slug": "mirix-cambridge-ma-limiting-resource-allocation-with-bounded", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:01.020Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F7tAvNiSPXWaEdkMi/mirix-cambridge-ma-limiting-resource-allocation-with-bounded", "pageUrlRelative": "/posts/F7tAvNiSPXWaEdkMi/mirix-cambridge-ma-limiting-resource-allocation-with-bounded", "linkUrl": "https://www.lesswrong.com/posts/F7tAvNiSPXWaEdkMi/mirix-cambridge-ma-limiting-resource-allocation-with-bounded", "postedAtFormatted": "Thursday, October 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMIRIx%20Cambridge%20MA%5D%20Limiting%20resource%20allocation%20with%20bounded%20utility%20functions%20and%20conceptual%20uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMIRIx%20Cambridge%20MA%5D%20Limiting%20resource%20allocation%20with%20bounded%20utility%20functions%20and%20conceptual%20uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF7tAvNiSPXWaEdkMi%2Fmirix-cambridge-ma-limiting-resource-allocation-with-bounded%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMIRIx%20Cambridge%20MA%5D%20Limiting%20resource%20allocation%20with%20bounded%20utility%20functions%20and%20conceptual%20uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF7tAvNiSPXWaEdkMi%2Fmirix-cambridge-ma-limiting-resource-allocation-with-bounded", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF7tAvNiSPXWaEdkMi%2Fmirix-cambridge-ma-limiting-resource-allocation-with-bounded", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 598, "htmlBody": "<p>This is a result from the first MIRIx Cambridge workshop (coauthored with <a href=\"/user/janos/overview/\">Janos</a> and <a href=\"/user/jimrandomh/\">Jim</a>).</p>\n<p>One potential problem with bounded utility functions is: what happens when the bound is nearly reached? A bounded utility maximizer will get progressively more and more risk averse as it gets closer to its bound. We decided to investigate what risks it might fear. We used a toy model with a bounded-utility chocolate maximizer, and considered what happens to its resource allocation in the limit as resources go to infinity.</p>\n<p>We use \"chocolate maximizer'' as conceptual shorthand meaning an agent that we model as though it has a single simple value with a positive long-run marginal resource cost, but only as a simplifying assumption. This is as opposed to a paperclip maximizer, where the inappropriate simplicity is implied to be part of the world, not just part of the model.</p>\n<h3>Conceptual uncertainty</h3>\n<p>We found that if a bounded utility function approaches its bound too fast, this has surprising pathological results when mixed with logical uncertainty. Consider a bounded-utility chocolate maximizer, with philosophical uncertainty about what chocolate is. It has a central concept of chocolate <a href=\"http://www.codecogs.com/eqnedit.php?latex=C_1\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"C_1\" src=\"http://latex.codecogs.com/gif.latex?C_1\" alt=\"\" /></a>, and there are classes of mutated versions of the concept of chocolate <a href=\"http://www.codecogs.com/eqnedit.php?latex=C_2,&amp;space;C_3,&amp;space;\\dots,&amp;space;C_n\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"C_2, C_3, \\dots, C_n\" src=\"http://latex.codecogs.com/gif.latex?C_2,&amp;space;C_3,&amp;space;\\dots,&amp;space;C_n\" alt=\"\" /></a> at varying distances from the central concept, such that the probability that the true chocolate is in class <a href=\"http://www.codecogs.com/eqnedit.php?latex=C_i\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"C_i\" src=\"http://latex.codecogs.com/gif.latex?C_i\" alt=\"\" /></a> is proportional to <a href=\"http://www.codecogs.com/eqnedit.php?latex=i^{-\\alpha}\" target=\"_blank\"><img title=\"i^{-\\alpha}\" src=\"http://latex.codecogs.com/gif.latex?i^{-\\alpha}\" alt=\"\" /></a> (i.e. following a power law).</p>\n<p>Suppose also that utility is bounded using a sigmoid function <a href=\"http://www.codecogs.com/eqnedit.php?latex=U(x)=S(x)\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"U(x)=S(x)\" src=\"http://latex.codecogs.com/gif.latex?\\inline U(x)=S(x)\" alt=\"\" /></a>, where x is the amount of chocolate produced. In the limit as resources go to infinity, what fraction of those resources will be spent on the central class&nbsp;<a href=\"http://www.codecogs.com/eqnedit.php?latex=C_1\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"C_1\" src=\"http://latex.codecogs.com/gif.latex?C_1\" alt=\"\" /></a>? That depends which sigmoid function is used, and in particular, how quickly it approaches the utility bound.</p>\n<h3>Example 1: exponential sigmoid</h3>\n<p>Suppose we allocate <a href=\"http://www.codecogs.com/eqnedit.php?latex=r_{i}\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"r_{i}\" src=\"http://latex.codecogs.com/gif.latex?r_{i}\" alt=\"\" /></a> resources to class <a href=\"http://www.codecogs.com/eqnedit.php?latex=C_i\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"C_i\" src=\"http://latex.codecogs.com/gif.latex?C_i\" alt=\"\" /></a>, with <a href=\"http://www.codecogs.com/eqnedit.php?latex=\\sum_{i}r_{i}=r\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"\\sum_{i}r_{i}=r\" src=\"http://latex.codecogs.com/gif.latex?\\inline \\sum_{i}r_{i}=r\" alt=\"\" /></a> for total resource r. Let <a href=\"http://www.codecogs.com/eqnedit.php?latex=U(r_i)&amp;space;=&amp;space;1-e^{-r_{i}}\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"U(r_i) = 1-e^{-r_{i}}\" src=\"http://latex.codecogs.com/gif.latex?\\inline U(r_i)&amp;space;=&amp;space;1-e^{-r_{i}}\" alt=\"\" /></a>.</p>\n<p>Then the optimal resource allocation is</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\begin{align*}&amp;space;\\vec{r}&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}P(C_{i})U(r_{i})\\\\&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}ci^{-\\alpha}(1-e^{-r_{i}})&amp;space;\\end{align*}\" target=\"_blank\"><img title=\"\\begin{align*} \\vec{r} &amp; =\\arg\\max\\sum_{i=1}^{n}P(C_{i})U(r_{i})\\\\ &amp; =\\arg\\max\\sum_{i=1}^{n}ci^{-\\alpha}(1-e^{-r_{i}}) \\end{align*}\" src=\"http://latex.codecogs.com/gif.latex?\\begin{align*}&amp;space;\\vec{r}&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}P(C_{i})U(r_{i})\\\\&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}ci^{-\\alpha}(1-e^{-r_{i}})&amp;space;\\end{align*}\" alt=\"\" /></a></p>\n<p>Using Lagrange multipliers, we obtain for all i,</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\begin{align*}&amp;space;0&amp;space;&amp;&amp;space;=\\frac{\\partial}{\\partial&amp;space;r_{i}}\\left[\\sum_{i=1}^{n}ci^{-\\alpha}(1-e^{-r_{i}})+\\lambda(\\sum_{i=1}^{n}r_{i}-r)\\right]\\\\&amp;space;&amp;&amp;space;=-ci^{-\\alpha}e^{-r_{i}}+\\lambda&amp;space;\\end{align*}\" target=\"_blank\"><img title=\"\\begin{align*} 0 &amp; =\\frac{\\partial}{\\partial r_{i}}\\left[\\sum_{i=1}^{n}ci^{-\\alpha}(1-e^{-r_{i}})+\\lambda(\\sum_{i=1}^{n}r_{i}-r)\\right]\\\\ &amp; =-ci^{-\\alpha}e^{-r_{i}}+\\lambda \\end{align*}\" src=\"http://latex.codecogs.com/gif.latex?\\begin{align*}&amp;space;0&amp;space;&amp;&amp;space;=\\frac{\\partial}{\\partial&amp;space;r_{i}}\\left[\\sum_{i=1}^{n}ci^{-\\alpha}(1-e^{-r_{i}})+\\lambda(\\sum_{i=1}^{n}r_{i}-r)\\right]\\\\&amp;space;&amp;&amp;space;=-ci^{-\\alpha}e^{-r_{i}}+\\lambda&amp;space;\\end{align*}\" alt=\"\" /></a></p>\n<p>&nbsp;</p>\n<p>Then,</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\begin{align*}&amp;space;r_{i}&amp;space;&amp;&amp;space;=-\\alpha\\log&amp;space;i+(\\log&amp;space;c-\\log\\lambda)\\\\&amp;space;r&amp;space;&amp;&amp;space;=-\\alpha\\sum_{j=1}^{n}\\log&amp;space;j+n(\\log&amp;space;c-\\log\\lambda)\\\\&amp;space;\\frac{1}{n}(r+\\alpha\\sum_{j=1}^{n}\\log&amp;space;j)&amp;space;&amp;&amp;space;=\\log&amp;space;c-\\log\\lambda\\\\&amp;space;r_{i}&amp;space;&amp;&amp;space;=-\\alpha\\log&amp;space;i+\\frac{1}{n}\\left(r+\\alpha\\sum_{j=1}^{n}\\log&amp;space;j\\right)\\\\&amp;space;\\frac{r_{i}}{r}&amp;space;&amp;&amp;space;=\\frac{1}{n}+\\frac{\\alpha}{r}\\left(\\frac{1}{n}&amp;space;\\sum_{j=1}^{n}\\log&amp;space;j-\\log&amp;space;i\\right)\\\\&amp;space;\\lim_{r\\rightarrow\\infty}\\frac{r_{i}}{r}&amp;space;&amp;&amp;space;=\\frac{1}{n}&amp;space;\\end{align*}\" target=\"_blank\"><img title=\"\\begin{align*} r_{i} &amp; =-\\alpha\\log i+(\\log c-\\log\\lambda)\\\\ r &amp; =-\\alpha\\sum_{j=1}^{n}\\log j+n(\\log c-\\log\\lambda)\\\\ \\frac{1}{n}(r+\\alpha\\sum_{j=1}^{n}\\log j) &amp; =\\log c-\\log\\lambda\\\\ r_{i} &amp; =-\\alpha\\log i+\\frac{1}{n}\\left(r+\\alpha\\sum_{j=1}^{n}\\log j\\right)\\\\ \\frac{r_{i}}{r} &amp; =\\frac{1}{n}+\\frac{\\alpha}{r}\\left(\\frac{1}{n} \\sum_{j=1}^{n}\\log j-\\log i\\right)\\\\ \\lim_{r\\rightarrow\\infty}\\frac{r_{i}}{r} &amp; =\\frac{1}{n} \\end{align*}\" src=\"http://latex.codecogs.com/gif.latex?\\begin{align*}&amp;space;r_{i}&amp;space;&amp;&amp;space;=-\\alpha\\log&amp;space;i+(\\log&amp;space;c-\\log\\lambda)\\\\&amp;space;r&amp;space;&amp;&amp;space;=-\\alpha\\sum_{j=1}^{n}\\log&amp;space;j+n(\\log&amp;space;c-\\log\\lambda)\\\\&amp;space;\\frac{1}{n}(r+\\alpha\\sum_{j=1}^{n}\\log&amp;space;j)&amp;space;&amp;&amp;space;=\\log&amp;space;c-\\log\\lambda\\\\&amp;space;r_{i}&amp;space;&amp;&amp;space;=-\\alpha\\log&amp;space;i+\\frac{1}{n}\\left(r+\\alpha\\sum_{j=1}^{n}\\log&amp;space;j\\right)\\\\&amp;space;\\frac{r_{i}}{r}&amp;space;&amp;&amp;space;=\\frac{1}{n}+\\frac{\\alpha}{r}\\left(\\frac{1}{n}&amp;space;\\sum_{j=1}^{n}\\log&amp;space;j-\\log&amp;space;i\\right)\\\\&amp;space;\\lim_{r\\rightarrow\\infty}\\frac{r_{i}}{r}&amp;space;&amp;&amp;space;=\\frac{1}{n}&amp;space;\\end{align*}\" alt=\"\" /></a></p>\n<p>Thus, the resources will be evenly distributed among all the classes as r increases. This is bad, because the resource fraction for the central class <a href=\"http://www.codecogs.com/eqnedit.php?latex=C_1\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"C_1\" src=\"http://latex.codecogs.com/gif.latex?C_1\" alt=\"\" /></a> goes to 0 as we increase the number of classes.</p>\n<h4>EDITED: Addendum on asymptotics</h4>\n<p>Since we have both r and n going to infinity, we can specify their relationship more precisely. We assume that n is the highest number of classes that are assigned nonnegative resources for a given value of r:</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\frac{1}{n}+\\frac{\\alpha}{r}\\left(\\frac{1}{n}&amp;space;\\sum_{j=1}^{n}\\log&amp;space;j-\\log&amp;space;n\\right)&amp;space;\\geq&amp;space;0\" target=\"_blank\"><img title=\"\\frac{1}{n}+\\frac{\\alpha}{r}\\left(\\frac{1}{n} \\sum_{j=1}^{n}\\log j-\\log n\\right) \\geq 0\" src=\"http://latex.codecogs.com/gif.latex?\\frac{1}{n}+\\frac{\\alpha}{r}\\left(\\frac{1}{n}&amp;space;\\sum_{j=1}^{n}\\log&amp;space;j-\\log&amp;space;n\\right)&amp;space;\\geq&amp;space;0\" alt=\"\" /></a></p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\begin{align*}&amp;space;\\log&amp;space;n&amp;space;&amp;\\leq&amp;space;\\frac{r}{n\\alpha}+&amp;space;\\frac{1}{n}\\sum_{j=1}^{n}\\log&amp;space;j&amp;space;\\\\&amp;space;&amp;=&amp;space;\\frac{r}{n\\alpha}+\\frac{1}{n}\\log(n!)\\\\&amp;space;&amp;\\approx&amp;space;\\frac{r}{n\\alpha}+&amp;space;\\log&amp;space;n&amp;space;-&amp;space;1&amp;space;+&amp;space;\\frac{1}{n}O(\\log&amp;space;n)\\\\&amp;space;\\end{align*}\" target=\"_blank\"><img title=\"\\begin{align*} \\log n &amp;\\leq \\frac{r}{n\\alpha}+ \\frac{1}{n}\\sum_{j=1}^{n}\\log j \\\\ &amp;= \\frac{r}{n\\alpha}+\\frac{1}{n}\\log(n!)\\\\ &amp;\\approx \\frac{r}{n\\alpha}+ \\log n - 1 + \\frac{1}{n}O(\\log n)\\\\ \\end{align*}\" src=\"http://latex.codecogs.com/gif.latex?\\begin{align*}&amp;space;\\log&amp;space;n&amp;space;&amp;\\leq&amp;space;\\frac{r}{n\\alpha}+&amp;space;\\frac{1}{n}\\sum_{j=1}^{n}\\log&amp;space;j&amp;space;\\\\&amp;space;&amp;=&amp;space;\\frac{r}{n\\alpha}+\\frac{1}{n}\\log(n!)\\\\&amp;space;&amp;\\approx&amp;space;\\frac{r}{n\\alpha}+&amp;space;\\log&amp;space;n&amp;space;-&amp;space;1&amp;space;+&amp;space;\\frac{1}{n}O(\\log&amp;space;n)\\\\&amp;space;\\end{align*}\" alt=\"\" /></a></p>\n<p>Thus,</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\frac{1}{n}\\left(\\frac{r}{\\alpha}&amp;space;+&amp;space;O(\\log&amp;space;n)\\right)\\geq&amp;space;1\" target=\"_blank\"><img title=\"\\frac{1}{n}\\left(\\frac{r}{\\alpha} + O(\\log n)\\right)\\geq 1\" src=\"http://latex.codecogs.com/gif.latex?\\frac{1}{n}\\left(\\frac{r}{\\alpha}&amp;space;+&amp;space;O(\\log&amp;space;n)\\right)\\geq&amp;space;1\" alt=\"\" /></a></p>\n<p>so the highest class index that gets nonnegative resources satisfies&nbsp;</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=n-O(\\log&amp;space;n)\\approx&amp;space;\\frac{r}{\\alpha}\" target=\"_blank\"><img title=\"n-O(\\log n)\\approx \\frac{r}{\\alpha}\" src=\"http://latex.codecogs.com/gif.latex?n-O(\\log&amp;space;n)\\approx&amp;space;\\frac{r}{\\alpha}\" alt=\"\" /></a></p>\n<h3>Example 2: arctan sigmoid</h3>\n<p>Let <a href=\"http://www.codecogs.com/eqnedit.php?latex=U(r_i)&amp;space;=&amp;space;\\arctan(r_{i})\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"U(r_i) = \\arctan(r_{i})\" src=\"http://latex.codecogs.com/gif.latex?U(r_i)&amp;space;=&amp;space;\\arctan(r_{i})\" alt=\"\" /></a>.</p>\n<p>The optimal resource allocation is</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\begin{align*}&amp;space;\\vec{r}&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}P(C_{i})U(r_{i})\\\\&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}ci^{-\\alpha}\\arctan(r_{i})&amp;space;\\end{align*}\" target=\"_blank\"><img title=\"\\begin{align*} \\vec{r} &amp; =\\arg\\max\\sum_{i=1}^{n}P(C_{i})U(r_{i})\\\\ &amp; =\\arg\\max\\sum_{i=1}^{n}ci^{-\\alpha}\\arctan(r_{i}) \\end{align*}\" src=\"http://latex.codecogs.com/gif.latex?\\begin{align*}&amp;space;\\vec{r}&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}P(C_{i})U(r_{i})\\\\&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}ci^{-\\alpha}\\arctan(r_{i})&amp;space;\\end{align*}\" alt=\"\" /></a></p>\n<p>Using Lagrange multipliers, we obtain for all i,</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\begin{align*}&amp;space;0&amp;space;&amp;&amp;space;=\\frac{\\partial}{\\partial&amp;space;r_{i}}\\left[\\sum_{i=1}^{n}ci^{-\\alpha}\\arctan(r_{i})-\\lambda(\\sum_{i=1}^{n}r_{i}-r)\\right]\\\\&amp;space;&amp;&amp;space;=ci^{-\\alpha}\\frac{1}{1+r_{i}^{2}}-\\lambda&amp;space;\\end{align*}\" target=\"_blank\"><img title=\"\\begin{align*} 0 &amp; =\\frac{\\partial}{\\partial r_{i}}\\left[\\sum_{i=1}^{n}ci^{-\\alpha}\\arctan(r_{i})-\\lambda(\\sum_{i=1}^{n}r_{i}-r)\\right]\\\\ &amp; =ci^{-\\alpha}\\frac{1}{1+r_{i}^{2}}-\\lambda \\end{align*}\" src=\"http://latex.codecogs.com/gif.latex?\\begin{align*}&amp;space;0&amp;space;&amp;&amp;space;=\\frac{\\partial}{\\partial&amp;space;r_{i}}\\left[\\sum_{i=1}^{n}ci^{-\\alpha}\\arctan(r_{i})-\\lambda(\\sum_{i=1}^{n}r_{i}-r)\\right]\\\\&amp;space;&amp;&amp;space;=ci^{-\\alpha}\\frac{1}{1+r_{i}^{2}}-\\lambda&amp;space;\\end{align*}\" alt=\"\" /></a></p>\n<p>Then,</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\begin{align*}&amp;space;r_{i}&amp;space;&amp;&amp;space;=\\sqrt{\\frac{c}{\\lambda}i^{-\\alpha}-1}\\approx\\sqrt{\\frac{c}{\\lambda}}i^{-\\alpha/2}\\\\&amp;space;r&amp;space;&amp;&amp;space;=\\sqrt{\\frac{c}{\\lambda}}\\sum_{i=1}^{n}i^{-\\alpha/2}&lt;\\sqrt{\\frac{c}{\\lambda}}\\zeta(\\alpha/2)\\\\&amp;space;\\frac{r_{i}}{r}&amp;space;&amp;&amp;space;&gt;\\frac{i^{-\\alpha/2}}{\\zeta(\\alpha/2)}\\\\&amp;space;\\frac{r_{0}}{r}&amp;space;&amp;&amp;space;&gt;\\frac{1}{\\zeta(\\alpha/2)}&amp;space;\\end{align*}\" target=\"_blank\"><img title=\"\\begin{align*} r_{i} &amp; =\\sqrt{\\frac{c}{\\lambda}i^{-\\alpha}-1}\\approx\\sqrt{\\frac{c}{\\lambda}}i^{-\\alpha/2}\\\\ r &amp; =\\sqrt{\\frac{c}{\\lambda}}\\sum_{i=1}^{n}i^{-\\alpha/2}&lt;\\sqrt{\\frac{c}{\\lambda}}\\zeta(\\alpha/2)\\\\ \\frac{r_{i}}{r} &amp; &gt;\\frac{i^{-\\alpha/2}}{\\zeta(\\alpha/2)}\\\\ \\frac{r_{0}}{r} &amp; &gt;\\frac{1}{\\zeta(\\alpha/2)} \\end{align*}\" src=\"http://latex.codecogs.com/gif.latex?\\begin{align*}&amp;space;r_{i}&amp;space;&amp;&amp;space;=\\sqrt{\\frac{c}{\\lambda}i^{-\\alpha}-1}\\approx\\sqrt{\\frac{c}{\\lambda}}i^{-\\alpha/2}\\\\&amp;space;r&amp;space;&amp;&amp;space;=\\sqrt{\\frac{c}{\\lambda}}\\sum_{i=1}^{n}i^{-\\alpha/2}&lt;\\sqrt{\\frac{c}{\\lambda}}\\zeta(\\alpha/2)\\\\&amp;space;\\frac{r_{i}}{r}&amp;space;&amp;&amp;space;&gt;\\frac{i^{-\\alpha/2}}{\\zeta(\\alpha/2)}\\\\&amp;space;\\frac{r_{0}}{r}&amp;space;&amp;&amp;space;&gt;\\frac{1}{\\zeta(\\alpha/2)}&amp;space;\\end{align*}\" alt=\"\" /></a></p>\n<p>Thus, for <a href=\"http://www.codecogs.com/eqnedit.php?latex=\\alpha&gt;2\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"\\alpha&gt;2\" src=\"http://latex.codecogs.com/gif.latex?\\alpha&gt;2\" alt=\"\" /></a> the limit of the resource fraction for the central class <a href=\"http://www.codecogs.com/eqnedit.php?latex=C_1\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"C_1\" src=\"http://latex.codecogs.com/gif.latex?C_1\" alt=\"\" /></a> is finite and positive.</p>\n<h3>Conclusion</h3>\n<p>The arctan sigmoid results in a better limiting resource allocation than the exponential sigmoid, because it has heavier tails (for sufficiently large <a href=\"http://www.codecogs.com/eqnedit.php?latex=\\alpha\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"\\alpha\" src=\"http://latex.codecogs.com/gif.latex?\\alpha\" alt=\"\" /></a>). Thus, it matters which bounding sigmoid function you choose.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F7tAvNiSPXWaEdkMi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 2.053145315463092e-06, "legacy": true, "legacyId": "27303", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This is a result from the first MIRIx Cambridge workshop (coauthored with <a href=\"/user/janos/overview/\">Janos</a> and <a href=\"/user/jimrandomh/\">Jim</a>).</p>\n<p>One potential problem with bounded utility functions is: what happens when the bound is nearly reached? A bounded utility maximizer will get progressively more and more risk averse as it gets closer to its bound. We decided to investigate what risks it might fear. We used a toy model with a bounded-utility chocolate maximizer, and considered what happens to its resource allocation in the limit as resources go to infinity.</p>\n<p>We use \"chocolate maximizer'' as conceptual shorthand meaning an agent that we model as though it has a single simple value with a positive long-run marginal resource cost, but only as a simplifying assumption. This is as opposed to a paperclip maximizer, where the inappropriate simplicity is implied to be part of the world, not just part of the model.</p>\n<h3 id=\"Conceptual_uncertainty\">Conceptual uncertainty</h3>\n<p>We found that if a bounded utility function approaches its bound too fast, this has surprising pathological results when mixed with logical uncertainty. Consider a bounded-utility chocolate maximizer, with philosophical uncertainty about what chocolate is. It has a central concept of chocolate <a href=\"http://www.codecogs.com/eqnedit.php?latex=C_1\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"C_1\" src=\"http://latex.codecogs.com/gif.latex?C_1\" alt=\"\"></a>, and there are classes of mutated versions of the concept of chocolate <a href=\"http://www.codecogs.com/eqnedit.php?latex=C_2,&amp;space;C_3,&amp;space;\\dots,&amp;space;C_n\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"C_2, C_3, \\dots, C_n\" src=\"http://latex.codecogs.com/gif.latex?C_2,&amp;space;C_3,&amp;space;\\dots,&amp;space;C_n\" alt=\"\"></a> at varying distances from the central concept, such that the probability that the true chocolate is in class <a href=\"http://www.codecogs.com/eqnedit.php?latex=C_i\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"C_i\" src=\"http://latex.codecogs.com/gif.latex?C_i\" alt=\"\"></a> is proportional to <a href=\"http://www.codecogs.com/eqnedit.php?latex=i^{-\\alpha}\" target=\"_blank\"><img title=\"i^{-\\alpha}\" src=\"http://latex.codecogs.com/gif.latex?i^{-\\alpha}\" alt=\"\"></a> (i.e. following a power law).</p>\n<p>Suppose also that utility is bounded using a sigmoid function <a href=\"http://www.codecogs.com/eqnedit.php?latex=U(x)=S(x)\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"U(x)=S(x)\" src=\"http://latex.codecogs.com/gif.latex?\\inline U(x)=S(x)\" alt=\"\"></a>, where x is the amount of chocolate produced. In the limit as resources go to infinity, what fraction of those resources will be spent on the central class&nbsp;<a href=\"http://www.codecogs.com/eqnedit.php?latex=C_1\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"C_1\" src=\"http://latex.codecogs.com/gif.latex?C_1\" alt=\"\"></a>? That depends which sigmoid function is used, and in particular, how quickly it approaches the utility bound.</p>\n<h3 id=\"Example_1__exponential_sigmoid\">Example 1: exponential sigmoid</h3>\n<p>Suppose we allocate <a href=\"http://www.codecogs.com/eqnedit.php?latex=r_{i}\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"r_{i}\" src=\"http://latex.codecogs.com/gif.latex?r_{i}\" alt=\"\"></a> resources to class <a href=\"http://www.codecogs.com/eqnedit.php?latex=C_i\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"C_i\" src=\"http://latex.codecogs.com/gif.latex?C_i\" alt=\"\"></a>, with <a href=\"http://www.codecogs.com/eqnedit.php?latex=\\sum_{i}r_{i}=r\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"\\sum_{i}r_{i}=r\" src=\"http://latex.codecogs.com/gif.latex?\\inline \\sum_{i}r_{i}=r\" alt=\"\"></a> for total resource r. Let <a href=\"http://www.codecogs.com/eqnedit.php?latex=U(r_i)&amp;space;=&amp;space;1-e^{-r_{i}}\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"U(r_i) = 1-e^{-r_{i}}\" src=\"http://latex.codecogs.com/gif.latex?\\inline U(r_i)&amp;space;=&amp;space;1-e^{-r_{i}}\" alt=\"\"></a>.</p>\n<p>Then the optimal resource allocation is</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\begin{align*}&amp;space;\\vec{r}&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}P(C_{i})U(r_{i})\\\\&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}ci^{-\\alpha}(1-e^{-r_{i}})&amp;space;\\end{align*}\" target=\"_blank\"><img title=\"\\begin{align*} \\vec{r} &amp; =\\arg\\max\\sum_{i=1}^{n}P(C_{i})U(r_{i})\\\\ &amp; =\\arg\\max\\sum_{i=1}^{n}ci^{-\\alpha}(1-e^{-r_{i}}) \\end{align*}\" src=\"http://latex.codecogs.com/gif.latex?\\begin{align*}&amp;space;\\vec{r}&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}P(C_{i})U(r_{i})\\\\&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}ci^{-\\alpha}(1-e^{-r_{i}})&amp;space;\\end{align*}\" alt=\"\"></a></p>\n<p>Using Lagrange multipliers, we obtain for all i,</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\begin{align*}&amp;space;0&amp;space;&amp;&amp;space;=\\frac{\\partial}{\\partial&amp;space;r_{i}}\\left[\\sum_{i=1}^{n}ci^{-\\alpha}(1-e^{-r_{i}})+\\lambda(\\sum_{i=1}^{n}r_{i}-r)\\right]\\\\&amp;space;&amp;&amp;space;=-ci^{-\\alpha}e^{-r_{i}}+\\lambda&amp;space;\\end{align*}\" target=\"_blank\"><img title=\"\\begin{align*} 0 &amp; =\\frac{\\partial}{\\partial r_{i}}\\left[\\sum_{i=1}^{n}ci^{-\\alpha}(1-e^{-r_{i}})+\\lambda(\\sum_{i=1}^{n}r_{i}-r)\\right]\\\\ &amp; =-ci^{-\\alpha}e^{-r_{i}}+\\lambda \\end{align*}\" src=\"http://latex.codecogs.com/gif.latex?\\begin{align*}&amp;space;0&amp;space;&amp;&amp;space;=\\frac{\\partial}{\\partial&amp;space;r_{i}}\\left[\\sum_{i=1}^{n}ci^{-\\alpha}(1-e^{-r_{i}})+\\lambda(\\sum_{i=1}^{n}r_{i}-r)\\right]\\\\&amp;space;&amp;&amp;space;=-ci^{-\\alpha}e^{-r_{i}}+\\lambda&amp;space;\\end{align*}\" alt=\"\"></a></p>\n<p>&nbsp;</p>\n<p>Then,</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\begin{align*}&amp;space;r_{i}&amp;space;&amp;&amp;space;=-\\alpha\\log&amp;space;i+(\\log&amp;space;c-\\log\\lambda)\\\\&amp;space;r&amp;space;&amp;&amp;space;=-\\alpha\\sum_{j=1}^{n}\\log&amp;space;j+n(\\log&amp;space;c-\\log\\lambda)\\\\&amp;space;\\frac{1}{n}(r+\\alpha\\sum_{j=1}^{n}\\log&amp;space;j)&amp;space;&amp;&amp;space;=\\log&amp;space;c-\\log\\lambda\\\\&amp;space;r_{i}&amp;space;&amp;&amp;space;=-\\alpha\\log&amp;space;i+\\frac{1}{n}\\left(r+\\alpha\\sum_{j=1}^{n}\\log&amp;space;j\\right)\\\\&amp;space;\\frac{r_{i}}{r}&amp;space;&amp;&amp;space;=\\frac{1}{n}+\\frac{\\alpha}{r}\\left(\\frac{1}{n}&amp;space;\\sum_{j=1}^{n}\\log&amp;space;j-\\log&amp;space;i\\right)\\\\&amp;space;\\lim_{r\\rightarrow\\infty}\\frac{r_{i}}{r}&amp;space;&amp;&amp;space;=\\frac{1}{n}&amp;space;\\end{align*}\" target=\"_blank\"><img title=\"\\begin{align*} r_{i} &amp; =-\\alpha\\log i+(\\log c-\\log\\lambda)\\\\ r &amp; =-\\alpha\\sum_{j=1}^{n}\\log j+n(\\log c-\\log\\lambda)\\\\ \\frac{1}{n}(r+\\alpha\\sum_{j=1}^{n}\\log j) &amp; =\\log c-\\log\\lambda\\\\ r_{i} &amp; =-\\alpha\\log i+\\frac{1}{n}\\left(r+\\alpha\\sum_{j=1}^{n}\\log j\\right)\\\\ \\frac{r_{i}}{r} &amp; =\\frac{1}{n}+\\frac{\\alpha}{r}\\left(\\frac{1}{n} \\sum_{j=1}^{n}\\log j-\\log i\\right)\\\\ \\lim_{r\\rightarrow\\infty}\\frac{r_{i}}{r} &amp; =\\frac{1}{n} \\end{align*}\" src=\"http://latex.codecogs.com/gif.latex?\\begin{align*}&amp;space;r_{i}&amp;space;&amp;&amp;space;=-\\alpha\\log&amp;space;i+(\\log&amp;space;c-\\log\\lambda)\\\\&amp;space;r&amp;space;&amp;&amp;space;=-\\alpha\\sum_{j=1}^{n}\\log&amp;space;j+n(\\log&amp;space;c-\\log\\lambda)\\\\&amp;space;\\frac{1}{n}(r+\\alpha\\sum_{j=1}^{n}\\log&amp;space;j)&amp;space;&amp;&amp;space;=\\log&amp;space;c-\\log\\lambda\\\\&amp;space;r_{i}&amp;space;&amp;&amp;space;=-\\alpha\\log&amp;space;i+\\frac{1}{n}\\left(r+\\alpha\\sum_{j=1}^{n}\\log&amp;space;j\\right)\\\\&amp;space;\\frac{r_{i}}{r}&amp;space;&amp;&amp;space;=\\frac{1}{n}+\\frac{\\alpha}{r}\\left(\\frac{1}{n}&amp;space;\\sum_{j=1}^{n}\\log&amp;space;j-\\log&amp;space;i\\right)\\\\&amp;space;\\lim_{r\\rightarrow\\infty}\\frac{r_{i}}{r}&amp;space;&amp;&amp;space;=\\frac{1}{n}&amp;space;\\end{align*}\" alt=\"\"></a></p>\n<p>Thus, the resources will be evenly distributed among all the classes as r increases. This is bad, because the resource fraction for the central class <a href=\"http://www.codecogs.com/eqnedit.php?latex=C_1\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"C_1\" src=\"http://latex.codecogs.com/gif.latex?C_1\" alt=\"\"></a> goes to 0 as we increase the number of classes.</p>\n<h4 id=\"EDITED__Addendum_on_asymptotics\">EDITED: Addendum on asymptotics</h4>\n<p>Since we have both r and n going to infinity, we can specify their relationship more precisely. We assume that n is the highest number of classes that are assigned nonnegative resources for a given value of r:</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\frac{1}{n}+\\frac{\\alpha}{r}\\left(\\frac{1}{n}&amp;space;\\sum_{j=1}^{n}\\log&amp;space;j-\\log&amp;space;n\\right)&amp;space;\\geq&amp;space;0\" target=\"_blank\"><img title=\"\\frac{1}{n}+\\frac{\\alpha}{r}\\left(\\frac{1}{n} \\sum_{j=1}^{n}\\log j-\\log n\\right) \\geq 0\" src=\"http://latex.codecogs.com/gif.latex?\\frac{1}{n}+\\frac{\\alpha}{r}\\left(\\frac{1}{n}&amp;space;\\sum_{j=1}^{n}\\log&amp;space;j-\\log&amp;space;n\\right)&amp;space;\\geq&amp;space;0\" alt=\"\"></a></p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\begin{align*}&amp;space;\\log&amp;space;n&amp;space;&amp;\\leq&amp;space;\\frac{r}{n\\alpha}+&amp;space;\\frac{1}{n}\\sum_{j=1}^{n}\\log&amp;space;j&amp;space;\\\\&amp;space;&amp;=&amp;space;\\frac{r}{n\\alpha}+\\frac{1}{n}\\log(n!)\\\\&amp;space;&amp;\\approx&amp;space;\\frac{r}{n\\alpha}+&amp;space;\\log&amp;space;n&amp;space;-&amp;space;1&amp;space;+&amp;space;\\frac{1}{n}O(\\log&amp;space;n)\\\\&amp;space;\\end{align*}\" target=\"_blank\"><img title=\"\\begin{align*} \\log n &amp;\\leq \\frac{r}{n\\alpha}+ \\frac{1}{n}\\sum_{j=1}^{n}\\log j \\\\ &amp;= \\frac{r}{n\\alpha}+\\frac{1}{n}\\log(n!)\\\\ &amp;\\approx \\frac{r}{n\\alpha}+ \\log n - 1 + \\frac{1}{n}O(\\log n)\\\\ \\end{align*}\" src=\"http://latex.codecogs.com/gif.latex?\\begin{align*}&amp;space;\\log&amp;space;n&amp;space;&amp;\\leq&amp;space;\\frac{r}{n\\alpha}+&amp;space;\\frac{1}{n}\\sum_{j=1}^{n}\\log&amp;space;j&amp;space;\\\\&amp;space;&amp;=&amp;space;\\frac{r}{n\\alpha}+\\frac{1}{n}\\log(n!)\\\\&amp;space;&amp;\\approx&amp;space;\\frac{r}{n\\alpha}+&amp;space;\\log&amp;space;n&amp;space;-&amp;space;1&amp;space;+&amp;space;\\frac{1}{n}O(\\log&amp;space;n)\\\\&amp;space;\\end{align*}\" alt=\"\"></a></p>\n<p>Thus,</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\frac{1}{n}\\left(\\frac{r}{\\alpha}&amp;space;+&amp;space;O(\\log&amp;space;n)\\right)\\geq&amp;space;1\" target=\"_blank\"><img title=\"\\frac{1}{n}\\left(\\frac{r}{\\alpha} + O(\\log n)\\right)\\geq 1\" src=\"http://latex.codecogs.com/gif.latex?\\frac{1}{n}\\left(\\frac{r}{\\alpha}&amp;space;+&amp;space;O(\\log&amp;space;n)\\right)\\geq&amp;space;1\" alt=\"\"></a></p>\n<p>so the highest class index that gets nonnegative resources satisfies&nbsp;</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=n-O(\\log&amp;space;n)\\approx&amp;space;\\frac{r}{\\alpha}\" target=\"_blank\"><img title=\"n-O(\\log n)\\approx \\frac{r}{\\alpha}\" src=\"http://latex.codecogs.com/gif.latex?n-O(\\log&amp;space;n)\\approx&amp;space;\\frac{r}{\\alpha}\" alt=\"\"></a></p>\n<h3 id=\"Example_2__arctan_sigmoid\">Example 2: arctan sigmoid</h3>\n<p>Let <a href=\"http://www.codecogs.com/eqnedit.php?latex=U(r_i)&amp;space;=&amp;space;\\arctan(r_{i})\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"U(r_i) = \\arctan(r_{i})\" src=\"http://latex.codecogs.com/gif.latex?U(r_i)&amp;space;=&amp;space;\\arctan(r_{i})\" alt=\"\"></a>.</p>\n<p>The optimal resource allocation is</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\begin{align*}&amp;space;\\vec{r}&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}P(C_{i})U(r_{i})\\\\&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}ci^{-\\alpha}\\arctan(r_{i})&amp;space;\\end{align*}\" target=\"_blank\"><img title=\"\\begin{align*} \\vec{r} &amp; =\\arg\\max\\sum_{i=1}^{n}P(C_{i})U(r_{i})\\\\ &amp; =\\arg\\max\\sum_{i=1}^{n}ci^{-\\alpha}\\arctan(r_{i}) \\end{align*}\" src=\"http://latex.codecogs.com/gif.latex?\\begin{align*}&amp;space;\\vec{r}&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}P(C_{i})U(r_{i})\\\\&amp;space;&amp;&amp;space;=\\arg\\max\\sum_{i=1}^{n}ci^{-\\alpha}\\arctan(r_{i})&amp;space;\\end{align*}\" alt=\"\"></a></p>\n<p>Using Lagrange multipliers, we obtain for all i,</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\begin{align*}&amp;space;0&amp;space;&amp;&amp;space;=\\frac{\\partial}{\\partial&amp;space;r_{i}}\\left[\\sum_{i=1}^{n}ci^{-\\alpha}\\arctan(r_{i})-\\lambda(\\sum_{i=1}^{n}r_{i}-r)\\right]\\\\&amp;space;&amp;&amp;space;=ci^{-\\alpha}\\frac{1}{1+r_{i}^{2}}-\\lambda&amp;space;\\end{align*}\" target=\"_blank\"><img title=\"\\begin{align*} 0 &amp; =\\frac{\\partial}{\\partial r_{i}}\\left[\\sum_{i=1}^{n}ci^{-\\alpha}\\arctan(r_{i})-\\lambda(\\sum_{i=1}^{n}r_{i}-r)\\right]\\\\ &amp; =ci^{-\\alpha}\\frac{1}{1+r_{i}^{2}}-\\lambda \\end{align*}\" src=\"http://latex.codecogs.com/gif.latex?\\begin{align*}&amp;space;0&amp;space;&amp;&amp;space;=\\frac{\\partial}{\\partial&amp;space;r_{i}}\\left[\\sum_{i=1}^{n}ci^{-\\alpha}\\arctan(r_{i})-\\lambda(\\sum_{i=1}^{n}r_{i}-r)\\right]\\\\&amp;space;&amp;&amp;space;=ci^{-\\alpha}\\frac{1}{1+r_{i}^{2}}-\\lambda&amp;space;\\end{align*}\" alt=\"\"></a></p>\n<p>Then,</p>\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\begin{align*}&amp;space;r_{i}&amp;space;&amp;&amp;space;=\\sqrt{\\frac{c}{\\lambda}i^{-\\alpha}-1}\\approx\\sqrt{\\frac{c}{\\lambda}}i^{-\\alpha/2}\\\\&amp;space;r&amp;space;&amp;&amp;space;=\\sqrt{\\frac{c}{\\lambda}}\\sum_{i=1}^{n}i^{-\\alpha/2}<\\sqrt{\\frac{c}{\\lambda}}\\zeta(\\alpha/2)\\\\&amp;space;\\frac{r_{i}}{r}&amp;space;&amp;&amp;space;>\\frac{i^{-\\alpha/2}}{\\zeta(\\alpha/2)}\\\\&amp;space;\\frac{r_{0}}{r}&amp;space;&amp;&amp;space;>\\frac{1}{\\zeta(\\alpha/2)}&amp;space;\\end{align*}\" target=\"_blank\"><img title=\"\\begin{align*} r_{i} &amp; =\\sqrt{\\frac{c}{\\lambda}i^{-\\alpha}-1}\\approx\\sqrt{\\frac{c}{\\lambda}}i^{-\\alpha/2}\\\\ r &amp; =\\sqrt{\\frac{c}{\\lambda}}\\sum_{i=1}^{n}i^{-\\alpha/2}<\\sqrt{\\frac{c}{\\lambda}}\\zeta(\\alpha/2)\\\\ \\frac{r_{i}}{r} &amp; >\\frac{i^{-\\alpha/2}}{\\zeta(\\alpha/2)}\\\\ \\frac{r_{0}}{r} &amp; >\\frac{1}{\\zeta(\\alpha/2)} \\end{align*}\" src=\"http://latex.codecogs.com/gif.latex?\\begin{align*}&amp;space;r_{i}&amp;space;&amp;&amp;space;=\\sqrt{\\frac{c}{\\lambda}i^{-\\alpha}-1}\\approx\\sqrt{\\frac{c}{\\lambda}}i^{-\\alpha/2}\\\\&amp;space;r&amp;space;&amp;&amp;space;=\\sqrt{\\frac{c}{\\lambda}}\\sum_{i=1}^{n}i^{-\\alpha/2}<\\sqrt{\\frac{c}{\\lambda}}\\zeta(\\alpha/2)\\\\&amp;space;\\frac{r_{i}}{r}&amp;space;&amp;&amp;space;>\\frac{i^{-\\alpha/2}}{\\zeta(\\alpha/2)}\\\\&amp;space;\\frac{r_{0}}{r}&amp;space;&amp;&amp;space;>\\frac{1}{\\zeta(\\alpha/2)}&amp;space;\\end{align*}\" alt=\"\"></a></p>\n<p>Thus, for <a href=\"http://www.codecogs.com/eqnedit.php?latex=\\alpha>2\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"\\alpha>2\" src=\"http://latex.codecogs.com/gif.latex?\\alpha>2\" alt=\"\"></a> the limit of the resource fraction for the central class <a href=\"http://www.codecogs.com/eqnedit.php?latex=C_1\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"C_1\" src=\"http://latex.codecogs.com/gif.latex?C_1\" alt=\"\"></a> is finite and positive.</p>\n<h3 id=\"Conclusion\">Conclusion</h3>\n<p>The arctan sigmoid results in a better limiting resource allocation than the exponential sigmoid, because it has heavier tails (for sufficiently large <a href=\"http://www.codecogs.com/eqnedit.php?latex=\\alpha\" target=\"_blank\"><img style=\"vertical-align:middle\" title=\"\\alpha\" src=\"http://latex.codecogs.com/gif.latex?\\alpha\" alt=\"\"></a>). Thus, it matters which bounding sigmoid function you choose.</p>", "sections": [{"title": "Conceptual uncertainty", "anchor": "Conceptual_uncertainty", "level": 1}, {"title": "Example 1: exponential sigmoid", "anchor": "Example_1__exponential_sigmoid", "level": 1}, {"title": "EDITED: Addendum on asymptotics", "anchor": "EDITED__Addendum_on_asymptotics", "level": 2}, {"title": "Example 2: arctan sigmoid", "anchor": "Example_2__arctan_sigmoid", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "28 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-03T01:24:11.921Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington, D.C.: Fun & Games", "slug": "meetup-washington-d-c-fun-and-games-17", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinZ", "createdAt": "2009-07-08T20:34:05.168Z", "isAdmin": false, "displayName": "RobinZ"}, "userId": "eTMojvi4f2z3pDfsc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wJDCQNaRDAvRyT8dy/meetup-washington-d-c-fun-and-games-17", "pageUrlRelative": "/posts/wJDCQNaRDAvRyT8dy/meetup-washington-d-c-fun-and-games-17", "linkUrl": "https://www.lesswrong.com/posts/wJDCQNaRDAvRyT8dy/meetup-washington-d-c-fun-and-games-17", "postedAtFormatted": "Friday, October 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%2C%20D.C.%3A%20Fun%20%26%20Games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%2C%20D.C.%3A%20Fun%20%26%20Games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwJDCQNaRDAvRyT8dy%2Fmeetup-washington-d-c-fun-and-games-17%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%2C%20D.C.%3A%20Fun%20%26%20Games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwJDCQNaRDAvRyT8dy%2Fmeetup-washington-d-c-fun-and-games-17", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwJDCQNaRDAvRyT8dy%2Fmeetup-washington-d-c-fun-and-games-17", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/159'>Washington, D.C.: Fun &amp; Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 October 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting in the Kogod Courtyard of the National Portrait Gallery (8th and F Sts or 8th and G Sts NW, go straight past the information desk from either entrance) to hang out, play games, and engage in fun conversation.</p>\n\n<p>(Note: Feel free to send an email to the Google Group - lesswrong-dc - if you have a game you wish to play. This is especially useful for games that take 3+ hours, as you would want to start playing by 3:30.)</p>\n\n<p>Upcoming meetups:</p>\n\n<ul>\n<li>Oct. 12: To-Do List Hacking</li>\n<li>Oct. 19: Mini Talks</li>\n<li>Oct. 26: TBA</li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/159'>Washington, D.C.: Fun &amp; Games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wJDCQNaRDAvRyT8dy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.053437150698726e-06, "legacy": true, "legacyId": "27304", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington__D_C___Fun___Games\">Discussion article for the meetup : <a href=\"/meetups/159\">Washington, D.C.: Fun &amp; Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 October 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting in the Kogod Courtyard of the National Portrait Gallery (8th and F Sts or 8th and G Sts NW, go straight past the information desk from either entrance) to hang out, play games, and engage in fun conversation.</p>\n\n<p>(Note: Feel free to send an email to the Google Group - lesswrong-dc - if you have a game you wish to play. This is especially useful for games that take 3+ hours, as you would want to start playing by 3:30.)</p>\n\n<p>Upcoming meetups:</p>\n\n<ul>\n<li>Oct. 12: To-Do List Hacking</li>\n<li>Oct. 19: Mini Talks</li>\n<li>Oct. 26: TBA</li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington__D_C___Fun___Games1\">Discussion article for the meetup : <a href=\"/meetups/159\">Washington, D.C.: Fun &amp; Games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington, D.C.: Fun & Games", "anchor": "Discussion_article_for_the_meetup___Washington__D_C___Fun___Games", "level": 1}, {"title": "Discussion article for the meetup : Washington, D.C.: Fun & Games", "anchor": "Discussion_article_for_the_meetup___Washington__D_C___Fun___Games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-03T15:51:31.359Z", "modifiedAt": null, "url": null, "title": "New LW Meetups: Prague, Hasselt", "slug": "new-lw-meetups-prague-hasselt", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c4eEGiHmhyh84Yn6s/new-lw-meetups-prague-hasselt", "pageUrlRelative": "/posts/c4eEGiHmhyh84Yn6s/new-lw-meetups-prague-hasselt", "linkUrl": "https://www.lesswrong.com/posts/c4eEGiHmhyh84Yn6s/new-lw-meetups-prague-hasselt", "postedAtFormatted": "Friday, October 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetups%3A%20Prague%2C%20Hasselt&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetups%3A%20Prague%2C%20Hasselt%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc4eEGiHmhyh84Yn6s%2Fnew-lw-meetups-prague-hasselt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetups%3A%20Prague%2C%20Hasselt%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc4eEGiHmhyh84Yn6s%2Fnew-lw-meetups-prague-hasselt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc4eEGiHmhyh84Yn6s%2Fnew-lw-meetups-prague-hasselt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 624, "htmlBody": "<p><strong>This summary was posted to LW Main on September 26th. The following week's summary is <a href=\"/lw/l2h/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/14s\">Czech's first Meetup Prague:&nbsp;<span class=\"date\">26 September 2014 07:30PM</span></a></li>\n<li><a href=\"/meetups/14v\">Hasselt Meetup: Brussels moves to Hasselt this month!:&nbsp;<span class=\"date\">11 October 2014 01:00PM</span></a></li>\n</ul>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/14t\">Atlanta September Meetup - Self Awareness:&nbsp;<span class=\"date\">27 September 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/14i\">Bratislava:&nbsp;<span class=\"date\">29 September 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/14q\">Copenhagen September Social Meetup - Botanisk Have:&nbsp;<span class=\"date\">27 September 2014 02:30PM</span></a></li>\n<li><a href=\"/meetups/14f\">Frankfurt: How to improve your life:&nbsp;<span class=\"date\">28 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/zr\">Houston, TX:&nbsp;<span class=\"date\">18 October 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/14l\">Perth, Australia: Games night:&nbsp;<span class=\"date\">07 October 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/14u\">Urbana-Champaign: The Steep Approach to Crazytown:&nbsp;<span class=\"date\">28 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13u\">Utrecht: Effective Altruism and Politics:&nbsp;<span class=\"date\">05 October 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13v\">Utrecht: Artificial Intelligence:&nbsp;<span class=\"date\">19 October 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13w\">Utrecht: Climate Change:&nbsp;<span class=\"date\">02 November 2014 03:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">27 September 2025 01:30PM</span></a></li>\n<li><a href=\"/meetups/14w\">London social meetup:&nbsp;<span class=\"date\">28 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/14o\">Moscow Meetup: Unusual as usual:&nbsp;<span class=\"date\">28 September 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/143\">Vienna - Superintelligence:&nbsp;<span class=\"date\">27 September 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/14x\">Washington DC EA meetup / Petrov day dinner:&nbsp;<span class=\"date\">26 September 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/14y\">Washington, D.C.: Book Swap:&nbsp;<span class=\"date\">28 September 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/14z\">West LA&mdash;The Worst Argument in the World:&nbsp;<span class=\"date\">01 October 2014 07:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Buffalo.2C_NY\">Buffalo</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Moscow.2C_Russia\">Moscow</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong>&nbsp;</strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Sydney\">Sydney</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> </strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tel_Aviv.2C_Israel\">Tel Aviv</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Warsaw.2C_Poland\">Warsaw</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c4eEGiHmhyh84Yn6s", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 2.0550654907107263e-06, "legacy": true, "legacyId": "27271", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hJeKk666LAm2bgazY", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-03T23:01:48.913Z", "modifiedAt": null, "url": null, "title": "Introducing Effective Altruist Profiles", "slug": "introducing-effective-altruist-profiles", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:04.496Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tog", "createdAt": "2011-10-04T12:54:07.164Z", "isAdmin": false, "displayName": "tog"}, "userId": "b4f6teTtsKfegjTaH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uMRT6qyBpeFeKTv87/introducing-effective-altruist-profiles", "pageUrlRelative": "/posts/uMRT6qyBpeFeKTv87/introducing-effective-altruist-profiles", "linkUrl": "https://www.lesswrong.com/posts/uMRT6qyBpeFeKTv87/introducing-effective-altruist-profiles", "postedAtFormatted": "Friday, October 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Introducing%20Effective%20Altruist%20Profiles&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntroducing%20Effective%20Altruist%20Profiles%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuMRT6qyBpeFeKTv87%2Fintroducing-effective-altruist-profiles%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Introducing%20Effective%20Altruist%20Profiles%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuMRT6qyBpeFeKTv87%2Fintroducing-effective-altruist-profiles", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuMRT6qyBpeFeKTv87%2Fintroducing-effective-altruist-profiles", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 359, "htmlBody": "<p>We&rsquo;re excited to announce&nbsp;<a href=\"http://effectivealtruismhub.com/user/profiles\">EA Profiles</a>, a new community platform for effective altruists. There are already hundreds of profiles for you to browse from members of the community such as Peter Singer and Jeff Kaufman, full of interesting information like people&rsquo;s favoured causes and charities, and the actions they&rsquo;re taking to make the world a better place. And you can&nbsp;<a href=\"http://effectivealtruismhub.com/user/register\">create your own right now</a>!</p>\n<p>The Profiles should serve as a virtual &ldquo;Who&rsquo;s Who&rdquo; of EA - a place to see information about those who identify with effective altruism, and share what we&rsquo;re doing to inspire and motivate others. They enable applications such as a&nbsp;<a href=\"http://effectivealtruismhub.com/map\">map of EAs</a>&nbsp;and a&nbsp;<a href=\"http://effectivealtruismhub.com/donations\">cause-neutral registry of past and planned donations</a>, which we'll cover in a separate announcement.&nbsp;</p>\n<p style=\"text-align:center;\"><a href=\"http://effectivealtruismhub.com/user/profiles\"><img src=\"http://effectivealtruismhub.com/sites/effectivealtruismhub.com/media/images/specific-pages/eaprofiles-mosaic.jpg\" alt=\"Individual members of the effective altruist community\" /></a></p>\n<p>Your EA Profile provides a natural, standard way to share your identification with the ideas of effective altruism and the ways in which you and people you know can do enormous amounts of good, from spreading these ideas to donating to highly efficient charities. We know that people can be slow or cautious about sharing this, but for&nbsp;<a href=\"/ea/7q/to_inspire_people_to_give_be_public_about_your/\">familiar reasons</a>&nbsp;think that doing so is highly valuable, spreading and normalising a focus on effectiveness and high impact donations. And making a Profile is a great excuse to do so: you&rsquo;re sharing this information because we asked for it! You can then point people to it, potentially triggering interesting conversations with friends who might like to hear about the ways they can do the most good.</p>\n<p>Many of the Profiles come from answers which people opted to make public in the first&nbsp;<a href=\"http://survey.effectivealtruismhub.com/\">annual survey of effective altruists</a>&nbsp;this year (still open if you haven&rsquo;t taken it yet - it provides another way to create a full or partial Profile). Peter Hurford is currently working on analysing the survey results and will share them and the raw data from the survey soon, but for now the EA Profiles provide some of the most interesting results: public information on the inspiring actions that individuals are taking.</p>\n<p>If you&rsquo;re interested in building up this project, or working on other similar projects, consider looking into&nbsp;<a href=\"http://www.dotimpact.im\">.impact</a>, a coordinated volunteer force working on potentially high-impact projects like this one.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uMRT6qyBpeFeKTv87", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 27, "extendedScore": null, "score": 2.055874182890248e-06, "legacy": true, "legacyId": "27306", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-04T15:14:52.627Z", "modifiedAt": null, "url": null, "title": "Meetup : Bratislava", "slug": "meetup-bratislava-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KMmvhQXfu6g3yDu8M/meetup-bratislava-2", "pageUrlRelative": "/posts/KMmvhQXfu6g3yDu8M/meetup-bratislava-2", "linkUrl": "https://www.lesswrong.com/posts/KMmvhQXfu6g3yDu8M/meetup-bratislava-2", "postedAtFormatted": "Saturday, October 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bratislava&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bratislava%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMmvhQXfu6g3yDu8M%2Fmeetup-bratislava-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bratislava%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMmvhQXfu6g3yDu8M%2Fmeetup-bratislava-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMmvhQXfu6g3yDu8M%2Fmeetup-bratislava-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/15a'>Bratislava</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 October 2014 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Bistro The Peach, Mari\u00e1nska 3, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Stret\u00e1vame sa op\u00e4\u0165 v bistre; majite\u013e je ochotn\u00fd kv\u00f4li n\u00e1m na jeden ve\u010der urobi\u0165 bistro nefaj\u010diarske.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/15a'>Bratislava</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KMmvhQXfu6g3yDu8M", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.057705033981533e-06, "legacy": true, "legacyId": "27309", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bratislava\">Discussion article for the meetup : <a href=\"/meetups/15a\">Bratislava</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 October 2014 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Bistro The Peach, Mari\u00e1nska 3, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Stret\u00e1vame sa op\u00e4\u0165 v bistre; majite\u013e je ochotn\u00fd kv\u00f4li n\u00e1m na jeden ve\u010der urobi\u0165 bistro nefaj\u010diarske.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bratislava1\">Discussion article for the meetup : <a href=\"/meetups/15a\">Bratislava</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bratislava", "anchor": "Discussion_article_for_the_meetup___Bratislava", "level": 1}, {"title": "Discussion article for the meetup : Bratislava", "anchor": "Discussion_article_for_the_meetup___Bratislava1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-04T23:05:48.423Z", "modifiedAt": null, "url": null, "title": "[Link] Animated Video - The Useful Idea of Truth (Part 1/3)", "slug": "link-animated-video-the-useful-idea-of-truth-part-1-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:00.112Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Joshua_Blaine", "createdAt": "2013-07-15T18:37:17.985Z", "isAdmin": false, "displayName": "Joshua_Blaine"}, "userId": "jzvkfAuoy9X7dp9Ma", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GT4FAZbSSxQpoSKq4/link-animated-video-the-useful-idea-of-truth-part-1-3", "pageUrlRelative": "/posts/GT4FAZbSSxQpoSKq4/link-animated-video-the-useful-idea-of-truth-part-1-3", "linkUrl": "https://www.lesswrong.com/posts/GT4FAZbSSxQpoSKq4/link-animated-video-the-useful-idea-of-truth-part-1-3", "postedAtFormatted": "Saturday, October 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Animated%20Video%20-%20The%20Useful%20Idea%20of%20Truth%20(Part%201%2F3)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Animated%20Video%20-%20The%20Useful%20Idea%20of%20Truth%20(Part%201%2F3)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGT4FAZbSSxQpoSKq4%2Flink-animated-video-the-useful-idea-of-truth-part-1-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Animated%20Video%20-%20The%20Useful%20Idea%20of%20Truth%20(Part%201%2F3)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGT4FAZbSSxQpoSKq4%2Flink-animated-video-the-useful-idea-of-truth-part-1-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGT4FAZbSSxQpoSKq4%2Flink-animated-video-the-useful-idea-of-truth-part-1-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<p>I have taken <a href=\"/lw/eqn/the_useful_idea_of_truth/\">this well received post</a>&nbsp;by Eliezer, and remade the first third of it into a short and quickly paced youtube video here:&nbsp;http://youtu.be/L2dNANRIALs</p>\n<p>The goals of this post are re-introducing the lessons explored in the original (for anyone not yet familiar with them), as well as asking the question of whether this format is actually suited for the lessons LessWrong tries to teach. What are your thoughts?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GT4FAZbSSxQpoSKq4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 27, "extendedScore": null, "score": 2.058592141761459e-06, "legacy": true, "legacyId": "27312", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqvnWFtRD2keJdwjX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-05T01:42:55.336Z", "modifiedAt": null, "url": null, "title": "Using Bayes to dismiss fringe phenomena", "slug": "using-bayes-to-dismiss-fringe-phenomena", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:37.315Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "LQxXyeYY8tAyoJ4m9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5v8fpEkwe6NjAnN47/using-bayes-to-dismiss-fringe-phenomena", "pageUrlRelative": "/posts/5v8fpEkwe6NjAnN47/using-bayes-to-dismiss-fringe-phenomena", "linkUrl": "https://www.lesswrong.com/posts/5v8fpEkwe6NjAnN47/using-bayes-to-dismiss-fringe-phenomena", "postedAtFormatted": "Sunday, October 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Using%20Bayes%20to%20dismiss%20fringe%20phenomena&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUsing%20Bayes%20to%20dismiss%20fringe%20phenomena%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5v8fpEkwe6NjAnN47%2Fusing-bayes-to-dismiss-fringe-phenomena%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Using%20Bayes%20to%20dismiss%20fringe%20phenomena%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5v8fpEkwe6NjAnN47%2Fusing-bayes-to-dismiss-fringe-phenomena", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5v8fpEkwe6NjAnN47%2Fusing-bayes-to-dismiss-fringe-phenomena", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<p>It would be a powerful tool to be able to dismiss fringe phenomena, <em>prior to empirical investigation</em>, on firm epistemological ground.</p>\n<p>Thus I have elaborated on the possibility of doing so using Bayes, and this is my result:</p>\n<p><a href=\"http://myinnerouterworldsimulator.neocities.org/index.html\">Using Bayes to dismiss fringe phenomena</a></p>\n<p>What do you think of it?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5v8fpEkwe6NjAnN47", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 1, "extendedScore": null, "score": 2.058888255899396e-06, "legacy": true, "legacyId": "27313", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-05T11:51:23.875Z", "modifiedAt": null, "url": null, "title": "Meetup : Rationality Meetup Vienna", "slug": "meetup-rationality-meetup-vienna-14", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaLeptikon", "createdAt": "2014-03-29T17:32:19.520Z", "isAdmin": false, "displayName": "AnnaLeptikon"}, "userId": "FyZibA2dPTe9zcmND", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/whPCSxG3ESgvbdChE/meetup-rationality-meetup-vienna-14", "pageUrlRelative": "/posts/whPCSxG3ESgvbdChE/meetup-rationality-meetup-vienna-14", "linkUrl": "https://www.lesswrong.com/posts/whPCSxG3ESgvbdChE/meetup-rationality-meetup-vienna-14", "postedAtFormatted": "Sunday, October 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Rationality%20Meetup%20Vienna&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Rationality%20Meetup%20Vienna%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwhPCSxG3ESgvbdChE%2Fmeetup-rationality-meetup-vienna-14%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Rationality%20Meetup%20Vienna%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwhPCSxG3ESgvbdChE%2Fmeetup-rationality-meetup-vienna-14", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwhPCSxG3ESgvbdChE%2Fmeetup-rationality-meetup-vienna-14", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 119, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/15b'>Rationality Meetup Vienna</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 October 2014 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Kaiserm\u00fchlenstra\u00dfe 24/2, 1220 Wien, meeting room behind the building</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>location:</strong> When arriving by U2 or Schnellbahn train: take the exit towards Kaiserm\u00fchlenstra\u00dfe, cross the street, step through the (very modern looking) building (on Erich Fried Weg) and go right, along the backside of the building until you get to the meeting room (it has a glass front so it should be hard to miss). Important: Google maps doesn't recognise the address, so what is being displayed here is nonsense.</p>\n\n<p><strong>topics:</strong> probably three big topics, the first two are not defined yet, the third will be group goal-setting</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/15b'>Rationality Meetup Vienna</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "whPCSxG3ESgvbdChE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.0600357579967424e-06, "legacy": true, "legacyId": "27314", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Rationality_Meetup_Vienna\">Discussion article for the meetup : <a href=\"/meetups/15b\">Rationality Meetup Vienna</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 October 2014 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Kaiserm\u00fchlenstra\u00dfe 24/2, 1220 Wien, meeting room behind the building</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>location:</strong> When arriving by U2 or Schnellbahn train: take the exit towards Kaiserm\u00fchlenstra\u00dfe, cross the street, step through the (very modern looking) building (on Erich Fried Weg) and go right, along the backside of the building until you get to the meeting room (it has a glass front so it should be hard to miss). Important: Google maps doesn't recognise the address, so what is being displayed here is nonsense.</p>\n\n<p><strong>topics:</strong> probably three big topics, the first two are not defined yet, the third will be group goal-setting</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Rationality_Meetup_Vienna1\">Discussion article for the meetup : <a href=\"/meetups/15b\">Rationality Meetup Vienna</a></h2>", "sections": [{"title": "Discussion article for the meetup : Rationality Meetup Vienna", "anchor": "Discussion_article_for_the_meetup___Rationality_Meetup_Vienna", "level": 1}, {"title": "Discussion article for the meetup : Rationality Meetup Vienna", "anchor": "Discussion_article_for_the_meetup___Rationality_Meetup_Vienna1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-05T12:21:20.307Z", "modifiedAt": null, "url": null, "title": "A possible tax efficient swap mechanism for charity", "slug": "a-possible-tax-efficient-swap-mechanism-for-charity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:02.713Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blogospheroid", "createdAt": "2009-03-17T08:11:01.816Z", "isAdmin": false, "displayName": "blogospheroid"}, "userId": "dgscyYwrDh3u7dE7h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5nDkAAof95gh3p9Bb/a-possible-tax-efficient-swap-mechanism-for-charity", "pageUrlRelative": "/posts/5nDkAAof95gh3p9Bb/a-possible-tax-efficient-swap-mechanism-for-charity", "linkUrl": "https://www.lesswrong.com/posts/5nDkAAof95gh3p9Bb/a-possible-tax-efficient-swap-mechanism-for-charity", "postedAtFormatted": "Sunday, October 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20possible%20tax%20efficient%20swap%20mechanism%20for%20charity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20possible%20tax%20efficient%20swap%20mechanism%20for%20charity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5nDkAAof95gh3p9Bb%2Fa-possible-tax-efficient-swap-mechanism-for-charity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20possible%20tax%20efficient%20swap%20mechanism%20for%20charity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5nDkAAof95gh3p9Bb%2Fa-possible-tax-efficient-swap-mechanism-for-charity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5nDkAAof95gh3p9Bb%2Fa-possible-tax-efficient-swap-mechanism-for-charity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 627, "htmlBody": "<p>I had an idea a while ago, which sounded simple to me, but searching with certain keywords did not yield appropriate results, so am presenting it for discussion to LW . Please inform me if something like this is already in existence. Please inform if I need to cross post it on effective altruism forum also, or they share enough users with LW and it need not be repeated.</p>\n<p><strong>Introduction</strong></p>\n<p>Two persons A, B living in different tax jurisdictions I and J respectively, want to contribute to organizations M and N qualifying for tax exemption in the <em>other </em>person's jurisdiction. i.e. M qualifies in J and N qualifies in I. &nbsp;For the purpose of this demo, lets consider they intend to contribute the same amounts.</p>\n<p>They \"swap\" their charities and produce receipts to the effect from the respective organizations.i.e. A contributes to N and B contributes to M.&nbsp;</p>\n<p>This helps them gain 10% to 20% more money when compared to contributing to their preferred charities which do not qualify.</p>\n<p>So, the idea is to create a website where people can post such an intent, to contribute to cross-national charities and can &nbsp;reliably present receipts that will be acceptable to all concerned.&nbsp;</p>\n<p>The main uses i envisage for such swaps would be science supporters in the developing world wanting to contribute to research happening in the developed world swapping with EA's wanting to gain a bigger bang for their buck in the developing world. This potentially reduces the need for a lot of charities to seek out tax exemption in multiple jurisdictions.&nbsp;</p>\n<p><strong>Avenues for further research</strong></p>\n<p>Question on the basic idea</p>\n<p>&nbsp;</p>\n<ul>\n<li>Do both charities have to be acceptable to both donors or are neutral and maybe even \"hostile\" swaps possible? How much does that complicate matters</li>\n<li>A certain cut of the proceeds seems to be the simplest for the website to operate, but will it be acceptable to the users?</li>\n<li>Might this be construed as being illegal in certain jurisdictions, after all, it is a tax avoidance scheme, to be honest.</li>\n</ul>\n<p>&nbsp;</p>\n<p>Logistics questions</p>\n<p>&nbsp;</p>\n<ul>\n<li>The 1:1:1:1 case for person to jurisdictions to causes to \"the time of swap\" is the simplest. There are many possible complications which can allow more charity to be funneled, but require the website hosts to be exposed to non-trivial amounts of risk. Example in one exchange, more dollars are offered for charity than euro-equivalents while in another swap, more euro-equivalents are offered compared to dollars. This can balance, but it is more complicated.</li>\n<li>Do the accounts need to \"balance\"? Will a non 1:1 ratio be acceptable for certain supporters of causes?</li>\n<li>Foreign exchange fluctuations affect amounts of money donated and may cause some unnecessary heartburn in some cases.</li>\n<li>Times of feeling charitable may wary and may prevent markets from clearing. Christians may feel more charitable near Christmas or Easter and Muslims during Ramadan.</li>\n<li>Might this require all charities to get themselves a digital signature? What are the other avenues to getting a reliable receipt from charities?</li>\n<li>If both payments are routed through the website/entity, then might unnecessary forex changes remove a lot of value? Could crypto-currency style atomic swaps help or would they introduce unnecessary complexity that people would rather not be bothered with.</li>\n<li>Might it complicate the relationship of donors with charities to the extent that the gain is lost in extra cost to reach out?</li>\n</ul>\n<div>Conclusion</div>\n<div><br /></div>\n<div>If such an institution is not already there, then after legal considerations, I think supporting such a website could be a high value investment for effective altruists as it would lead to a 10% to 20% boost to the charity kitty.</div>\n<div><br /></div>\n<div>[EDIT : edited a little for clarity and grammar. added one more doubt]</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5nDkAAof95gh3p9Bb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 2.0600922509389166e-06, "legacy": true, "legacyId": "27315", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I had an idea a while ago, which sounded simple to me, but searching with certain keywords did not yield appropriate results, so am presenting it for discussion to LW . Please inform me if something like this is already in existence. Please inform if I need to cross post it on effective altruism forum also, or they share enough users with LW and it need not be repeated.</p>\n<p><strong id=\"Introduction\">Introduction</strong></p>\n<p>Two persons A, B living in different tax jurisdictions I and J respectively, want to contribute to organizations M and N qualifying for tax exemption in the <em>other </em>person's jurisdiction. i.e. M qualifies in J and N qualifies in I. &nbsp;For the purpose of this demo, lets consider they intend to contribute the same amounts.</p>\n<p>They \"swap\" their charities and produce receipts to the effect from the respective organizations.i.e. A contributes to N and B contributes to M.&nbsp;</p>\n<p>This helps them gain 10% to 20% more money when compared to contributing to their preferred charities which do not qualify.</p>\n<p>So, the idea is to create a website where people can post such an intent, to contribute to cross-national charities and can &nbsp;reliably present receipts that will be acceptable to all concerned.&nbsp;</p>\n<p>The main uses i envisage for such swaps would be science supporters in the developing world wanting to contribute to research happening in the developed world swapping with EA's wanting to gain a bigger bang for their buck in the developing world. This potentially reduces the need for a lot of charities to seek out tax exemption in multiple jurisdictions.&nbsp;</p>\n<p><strong id=\"Avenues_for_further_research\">Avenues for further research</strong></p>\n<p>Question on the basic idea</p>\n<p>&nbsp;</p>\n<ul>\n<li>Do both charities have to be acceptable to both donors or are neutral and maybe even \"hostile\" swaps possible? How much does that complicate matters</li>\n<li>A certain cut of the proceeds seems to be the simplest for the website to operate, but will it be acceptable to the users?</li>\n<li>Might this be construed as being illegal in certain jurisdictions, after all, it is a tax avoidance scheme, to be honest.</li>\n</ul>\n<p>&nbsp;</p>\n<p>Logistics questions</p>\n<p>&nbsp;</p>\n<ul>\n<li>The 1:1:1:1 case for person to jurisdictions to causes to \"the time of swap\" is the simplest. There are many possible complications which can allow more charity to be funneled, but require the website hosts to be exposed to non-trivial amounts of risk. Example in one exchange, more dollars are offered for charity than euro-equivalents while in another swap, more euro-equivalents are offered compared to dollars. This can balance, but it is more complicated.</li>\n<li>Do the accounts need to \"balance\"? Will a non 1:1 ratio be acceptable for certain supporters of causes?</li>\n<li>Foreign exchange fluctuations affect amounts of money donated and may cause some unnecessary heartburn in some cases.</li>\n<li>Times of feeling charitable may wary and may prevent markets from clearing. Christians may feel more charitable near Christmas or Easter and Muslims during Ramadan.</li>\n<li>Might this require all charities to get themselves a digital signature? What are the other avenues to getting a reliable receipt from charities?</li>\n<li>If both payments are routed through the website/entity, then might unnecessary forex changes remove a lot of value? Could crypto-currency style atomic swaps help or would they introduce unnecessary complexity that people would rather not be bothered with.</li>\n<li>Might it complicate the relationship of donors with charities to the extent that the gain is lost in extra cost to reach out?</li>\n</ul>\n<div>Conclusion</div>\n<div><br></div>\n<div>If such an institution is not already there, then after legal considerations, I think supporting such a website could be a high value investment for effective altruists as it would lead to a 10% to 20% boost to the charity kitty.</div>\n<div><br></div>\n<div>[EDIT : edited a little for clarity and grammar. added one more doubt]</div>\n<p>&nbsp;</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Avenues for further research", "anchor": "Avenues_for_further_research", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "15 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-05T22:26:03.750Z", "modifiedAt": null, "url": null, "title": "Meetup : London Social - October 12th - Productivity Show & Tell", "slug": "meetup-london-social-october-12th-productivity-show-and-tell", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sixes_and_sevens", "createdAt": "2009-11-11T14:42:23.502Z", "isAdmin": false, "displayName": "sixes_and_sevens"}, "userId": "n83meJ5yG2WQzygvw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zGs25FHN2Ys8Qrh4P/meetup-london-social-october-12th-productivity-show-and-tell", "pageUrlRelative": "/posts/zGs25FHN2Ys8Qrh4P/meetup-london-social-october-12th-productivity-show-and-tell", "linkUrl": "https://www.lesswrong.com/posts/zGs25FHN2Ys8Qrh4P/meetup-london-social-october-12th-productivity-show-and-tell", "postedAtFormatted": "Sunday, October 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Social%20-%20October%2012th%20-%20Productivity%20Show%20%26%20Tell&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Social%20-%20October%2012th%20-%20Productivity%20Show%20%26%20Tell%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzGs25FHN2Ys8Qrh4P%2Fmeetup-london-social-october-12th-productivity-show-and-tell%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Social%20-%20October%2012th%20-%20Productivity%20Show%20%26%20Tell%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzGs25FHN2Ys8Qrh4P%2Fmeetup-london-social-october-12th-productivity-show-and-tell", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzGs25FHN2Ys8Qrh4P%2Fmeetup-london-social-october-12th-productivity-show-and-tell", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 195, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/15c'>London Social - October 12th - Productivity Show &amp; Tell</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 October 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next London meetup will be Sunday, October 12th from 2pm. We'll have the soft topic of productivity enhancement. Bring your experiences, tips, tricks, abortive efforts, apps, subcontracted labour and tales of woe. If you've got anything in particular you'd like to talk about, or put to the group, we'd love to hear about it.</p>\n\n<p>The venue is the Shakespeare's Head near Holborn tube station. We'll generally have some sign attesting to the group's identity, and usually try to get one of the large circular tables near the back of the pub.</p>\n\n<p><strong>About London LessWrong:</strong></p>\n\n<p>We're currently running meetups every other Sunday, and tend to get between 5 and 15 attendees. Most of our meetups default to unstructured social discussion on LessWrongy subjects, though we occasionally have special topics, events or activities. Sometimes we play games.</p>\n\n<p>We have a <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">Google Group</a> and a <a href=\"https://www.facebook.com/groups/380103898766356/\" rel=\"nofollow\">Facebook group</a> where we plan and discuss stuff. We would be extraordinarily pleased if you joined them.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/15c'>London Social - October 12th - Productivity Show &amp; Tell</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zGs25FHN2Ys8Qrh4P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.0612338549732676e-06, "legacy": true, "legacyId": "27316", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Social___October_12th___Productivity_Show___Tell\">Discussion article for the meetup : <a href=\"/meetups/15c\">London Social - October 12th - Productivity Show &amp; Tell</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 October 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next London meetup will be Sunday, October 12th from 2pm. We'll have the soft topic of productivity enhancement. Bring your experiences, tips, tricks, abortive efforts, apps, subcontracted labour and tales of woe. If you've got anything in particular you'd like to talk about, or put to the group, we'd love to hear about it.</p>\n\n<p>The venue is the Shakespeare's Head near Holborn tube station. We'll generally have some sign attesting to the group's identity, and usually try to get one of the large circular tables near the back of the pub.</p>\n\n<p><strong id=\"About_London_LessWrong_\">About London LessWrong:</strong></p>\n\n<p>We're currently running meetups every other Sunday, and tend to get between 5 and 15 attendees. Most of our meetups default to unstructured social discussion on LessWrongy subjects, though we occasionally have special topics, events or activities. Sometimes we play games.</p>\n\n<p>We have a <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">Google Group</a> and a <a href=\"https://www.facebook.com/groups/380103898766356/\" rel=\"nofollow\">Facebook group</a> where we plan and discuss stuff. We would be extraordinarily pleased if you joined them.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_Social___October_12th___Productivity_Show___Tell1\">Discussion article for the meetup : <a href=\"/meetups/15c\">London Social - October 12th - Productivity Show &amp; Tell</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Social - October 12th - Productivity Show & Tell", "anchor": "Discussion_article_for_the_meetup___London_Social___October_12th___Productivity_Show___Tell", "level": 1}, {"title": "About London LessWrong:", "anchor": "About_London_LessWrong_", "level": 2}, {"title": "Discussion article for the meetup : London Social - October 12th - Productivity Show & Tell", "anchor": "Discussion_article_for_the_meetup___London_Social___October_12th___Productivity_Show___Tell1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-06T04:11:14.254Z", "modifiedAt": null, "url": null, "title": "Meetup : Denver Area Meetup", "slug": "meetup-denver-area-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:04.393Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TheStevenator", "createdAt": "2011-07-24T22:42:44.595Z", "isAdmin": false, "displayName": "TheStevenator"}, "userId": "Qx7ZAEXbumeNDpwk9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JTiAqYCqihvwCpqe5/meetup-denver-area-meetup", "pageUrlRelative": "/posts/JTiAqYCqihvwCpqe5/meetup-denver-area-meetup", "linkUrl": "https://www.lesswrong.com/posts/JTiAqYCqihvwCpqe5/meetup-denver-area-meetup", "postedAtFormatted": "Monday, October 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Denver%20Area%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Denver%20Area%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJTiAqYCqihvwCpqe5%2Fmeetup-denver-area-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Denver%20Area%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJTiAqYCqihvwCpqe5%2Fmeetup-denver-area-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJTiAqYCqihvwCpqe5%2Fmeetup-denver-area-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/15d'>Denver Area Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 October 2014 06:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1417 S Broadway, Denver, CO 80210</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>First time meet up. The date and location are open for revision if anyone would prefer. All conversation topics are welcome. Unless there are unexpected time conflicts, the man behind the Harry Potter and the Methods of Rationality Podcast will be attending.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/15d'>Denver Area Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JTiAqYCqihvwCpqe5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.0618859816622603e-06, "legacy": true, "legacyId": "27318", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Denver_Area_Meetup\">Discussion article for the meetup : <a href=\"/meetups/15d\">Denver Area Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 October 2014 06:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1417 S Broadway, Denver, CO 80210</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>First time meet up. The date and location are open for revision if anyone would prefer. All conversation topics are welcome. Unless there are unexpected time conflicts, the man behind the Harry Potter and the Methods of Rationality Podcast will be attending.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Denver_Area_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/15d\">Denver Area Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Denver Area Meetup", "anchor": "Discussion_article_for_the_meetup___Denver_Area_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Denver Area Meetup", "anchor": "Discussion_article_for_the_meetup___Denver_Area_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-06T08:16:11.349Z", "modifiedAt": null, "url": null, "title": "Open thread, Oct. 6 - Oct. 12, 2014", "slug": "open-thread-oct-6-oct-12-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:07.979Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MrMind", "createdAt": "2011-04-19T08:43:22.388Z", "isAdmin": false, "displayName": "MrMind"}, "userId": "LJ4br8GWFXetsXkM8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jHABBE4jw2yPY6FsR/open-thread-oct-6-oct-12-2014", "pageUrlRelative": "/posts/jHABBE4jw2yPY6FsR/open-thread-oct-6-oct-12-2014", "linkUrl": "https://www.lesswrong.com/posts/jHABBE4jw2yPY6FsR/open-thread-oct-6-oct-12-2014", "postedAtFormatted": "Monday, October 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20Oct.%206%20-%20Oct.%2012%2C%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20Oct.%206%20-%20Oct.%2012%2C%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjHABBE4jw2yPY6FsR%2Fopen-thread-oct-6-oct-12-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20Oct.%206%20-%20Oct.%2012%2C%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjHABBE4jw2yPY6FsR%2Fopen-thread-oct-6-oct-12-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjHABBE4jw2yPY6FsR%2Fopen-thread-oct-6-oct-12-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px; font-weight: bold;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one. (<em>Immediately</em>&nbsp;before; refresh the list-of-threads page before posting.)<br /></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">3.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should be posted in Discussion, and not Main.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">4.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should start on Monday, and end on Sunday.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jHABBE4jw2yPY6FsR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 2.0623489812976574e-06, "legacy": true, "legacyId": "27319", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 334, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-06T18:14:06.231Z", "modifiedAt": null, "url": null, "title": "Link: Exotic disasters are serious", "slug": "link-exotic-disasters-are-serious", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:00.500Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tyTLq2W9r2F2jJRk9/link-exotic-disasters-are-serious", "pageUrlRelative": "/posts/tyTLq2W9r2F2jJRk9/link-exotic-disasters-are-serious", "linkUrl": "https://www.lesswrong.com/posts/tyTLq2W9r2F2jJRk9/link-exotic-disasters-are-serious", "postedAtFormatted": "Monday, October 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20Exotic%20disasters%20are%20serious&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20Exotic%20disasters%20are%20serious%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtyTLq2W9r2F2jJRk9%2Flink-exotic-disasters-are-serious%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20Exotic%20disasters%20are%20serious%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtyTLq2W9r2F2jJRk9%2Flink-exotic-disasters-are-serious", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtyTLq2W9r2F2jJRk9%2Flink-exotic-disasters-are-serious", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 9, "htmlBody": "<p>Several prominent LW names get mentioned in this piece.</p>\n<p>http://www.alternet.org/print/visions/how-exotic-and-unlikely-sounding-disasters-could-kill-humans</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tyTLq2W9r2F2jJRk9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 15, "extendedScore": null, "score": 2.0634799133497426e-06, "legacy": true, "legacyId": "27320", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-06T21:52:30.986Z", "modifiedAt": null, "url": null, "title": "Link: More visibility for EA movement", "slug": "link-more-visibility-for-ea-movement", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nTEArrwmmNceA7ZhX/link-more-visibility-for-ea-movement", "pageUrlRelative": "/posts/nTEArrwmmNceA7ZhX/link-more-visibility-for-ea-movement", "linkUrl": "https://www.lesswrong.com/posts/nTEArrwmmNceA7ZhX/link-more-visibility-for-ea-movement", "postedAtFormatted": "Monday, October 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20More%20visibility%20for%20EA%20movement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20More%20visibility%20for%20EA%20movement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnTEArrwmmNceA7ZhX%2Flink-more-visibility-for-ea-movement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20More%20visibility%20for%20EA%20movement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnTEArrwmmNceA7ZhX%2Flink-more-visibility-for-ea-movement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnTEArrwmmNceA7ZhX%2Flink-more-visibility-for-ea-movement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 9, "htmlBody": "<p>\"Effective Altruists Are a New Type of Nice Person\"</p>\n<p>http://www.vice.com/en_uk/read/why-are-young-people-being-so-altruistic-209</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nTEArrwmmNceA7ZhX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 2.063893305732011e-06, "legacy": true, "legacyId": "27321", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-06T23:13:45.546Z", "modifiedAt": null, "url": null, "title": "Link: Industrial standards will deal with robot ethics", "slug": "link-industrial-standards-will-deal-with-robot-ethics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:02.486Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iW2kuyqCa9tEJf5p8/link-industrial-standards-will-deal-with-robot-ethics", "pageUrlRelative": "/posts/iW2kuyqCa9tEJf5p8/link-industrial-standards-will-deal-with-robot-ethics", "linkUrl": "https://www.lesswrong.com/posts/iW2kuyqCa9tEJf5p8/link-industrial-standards-will-deal-with-robot-ethics", "postedAtFormatted": "Monday, October 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20Industrial%20standards%20will%20deal%20with%20robot%20ethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20Industrial%20standards%20will%20deal%20with%20robot%20ethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiW2kuyqCa9tEJf5p8%2Flink-industrial-standards-will-deal-with-robot-ethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20Industrial%20standards%20will%20deal%20with%20robot%20ethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiW2kuyqCa9tEJf5p8%2Flink-industrial-standards-will-deal-with-robot-ethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiW2kuyqCa9tEJf5p8%2Flink-industrial-standards-will-deal-with-robot-ethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 24, "htmlBody": "<p>\"The ISO&rsquo;s update will, for instance, include guidance on the maximum force with which a robot may strike a human it is working with.\"</p>\n<p>http://www.technologyreview.com/news/531356/should-industrial-robots-be-able-to-hurt-their-human-coworkers/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fuZZ64fNz24BLrXnY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iW2kuyqCa9tEJf5p8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 2.0640471121821395e-06, "legacy": true, "legacyId": "27322", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-07T01:00:29.300Z", "modifiedAt": null, "url": null, "title": "SRG 4: Biological Cognition, BCIs, Organizations", "slug": "srg-4-biological-cognition-bcis-organizations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:08.581Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SEtkbLgusgtQ9dAzX/srg-4-biological-cognition-bcis-organizations", "pageUrlRelative": "/posts/SEtkbLgusgtQ9dAzX/srg-4-biological-cognition-bcis-organizations", "linkUrl": "https://www.lesswrong.com/posts/SEtkbLgusgtQ9dAzX/srg-4-biological-cognition-bcis-organizations", "postedAtFormatted": "Tuesday, October 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SRG%204%3A%20Biological%20Cognition%2C%20BCIs%2C%20Organizations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASRG%204%3A%20Biological%20Cognition%2C%20BCIs%2C%20Organizations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEtkbLgusgtQ9dAzX%2Fsrg-4-biological-cognition-bcis-organizations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SRG%204%3A%20Biological%20Cognition%2C%20BCIs%2C%20Organizations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEtkbLgusgtQ9dAzX%2Fsrg-4-biological-cognition-bcis-organizations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEtkbLgusgtQ9dAzX%2Fsrg-4-biological-cognition-bcis-organizations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1571, "htmlBody": "<p><em>This is part of a weekly reading group on&nbsp;<a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a>'s book,&nbsp;<a href=\"http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111\">Superintelligence</a>. For more information about the group, and an index of posts so far see the&nbsp;<a href=\"/lw/kw4/superintelligence_reading_group/\">announcement post</a>. For the schedule of future topics, see&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2014/08/Superintelligence-Readers-Guide-early-version.pdf\">MIRI's reading guide</a>.</em></p>\n<hr />\n<p>Welcome. This week we finish chapter 2 with three more routes to superintelligence: enhancement of biological cognition, brain-computer interfaces, and well-organized networks of intelligent agents. This corresponds to the fourth section in the <a href=\"https://intelligence.org/wp-content/uploads/2014/08/Superintelligence-Readers-Guide-early-version.pdf\">reading guide</a>,&nbsp;<em><strong>Biological Cognition, BCIs, Organizations</strong></em>.&nbsp;</p>\n<p>This post summarizes the section, and offers a few relevant notes, and ideas for further investigation. My own thoughts and questions for discussion are in the comments.</p>\n<p>There is no need to proceed in order through this post, or to look at everything. Feel free to jump straight to the discussion. Where applicable and I remember, page numbers indicate the rough part of the chapter that is most related (not necessarily that the chapter is being cited for the specific claim).</p>\n<p><strong>Reading</strong>:&nbsp;<em>&ldquo;Biological Cognition&rdquo;&nbsp;</em>and&nbsp;the rest of&nbsp;Chapter 2 (p36-51)</p>\n<hr />\n<h1>Summary</h1>\n<p><strong>Biological intelligence</strong></p>\n<ol>\n<li>Modest gains to intelligence are available with current interventions such as nutrition.</li>\n<li>Genetic technologies might produce a population whose average is smarter than anyone who has have ever lived.</li>\n<li>Some particularly interesting possibilities are 'iterated embryo selection' where many rounds of selection take place in a single generation, and 'spell-checking' where the genetic mutations which are ubiquitous in current human genomes are removed.</li>\n</ol>\n<p><strong>Brain-computer interfaces</strong></p>\n<ol>\n<li>It is sometimes suggested that machines interfacing closely with the human brain will greatly enhance human cognition. For instance implants that allow perfect recall and fast arithmetic. (p44-45)&nbsp;</li>\n<li>Brain-computer interfaces seem unlikely to produce superintelligence (p51) This is because they have substantial health risks, because our existing systems for getting information in and out of our brains are hard to compete with, and because our brains are probably bottlenecked in other ways anyway. (p45-6)&nbsp;</li>\n<li>'Downloading' directly from one brain to another seems infeasible because each brain represents concepts idiosyncratically, without a standard format. (p46-7)</li>\n</ol>\n<p><strong>Networks and organizations</strong></p>\n<ol>\n<li>A large connected system of people (or something else) might become superintelligent. (p48)&nbsp; </li>\n<li>Systems of connected people become more capable through technological and institutional innovations, such as enhanced communications channels, well-aligned incentives, elimination of bureaucratic failures, and mechanisms for aggregating information. The internet as a whole is a contender for a network of humans that might become superintelligent (p49)&nbsp;</li>\n</ol>\n<p><strong>Summary</strong></p>\n<ol>\n<li>Since there are many possible paths to superintelligence, we can be more confident that we will get there eventually (p50)&nbsp;</li>\n<li>Whole brain emulation and biological enhancement are both likely to succeed after enough incremental progress in existing technologies. Networks and organizations are already improving gradually.&nbsp;</li>\n<li>The path to AI is less clear, and may be discontinuous. Which route we take might matter a lot, even if we end up with similar capabilities anyway. (p50)</li>\n</ol>\n<p><strong>The book so far</strong></p>\n<p>Here's a recap of what we have seen so far, now at the end of Chapter 2:</p>\n<ol>\n<li>Economic history suggests big changes are plausible.</li>\n<li>AI progress is ongoing.</li>\n<li>AI progress is hard to predict, but AI experts tend to expect human-level AI in mid-century.</li>\n<li>Several plausible paths lead to superintelligence: brain emulations, AI, human cognitive enhancement, brain-computer interfaces, and organizations.</li>\n<li>Most of these probably lead to machine superintelligence ultimately.</li>\n<li>That there are several paths suggests we are likely to get there.</li>\n</ol>\n<p>Do you disagree with any of these points? Tell us about it in the comments.</p>\n<h1>Notes</h1>\n<div><ol>\n<li><strong>Nootropics</strong><br /><em><a href=\"http://www.informationisbeautiful.net/play/snake-oil-supplements/\">Snake Oil Supplements?</a></em>&nbsp;is a nice illustration of scientific evidence for different supplements, <a href=\"http://www.informationisbeautiful.net/play/snake-oil-supplements/\">here</a> filtered for those with purported mental effects, many of which relate to intelligence. I don't know how accurate it is, or where to find a summary of apparent effect sizes rather than evidence, which I think would be more interesting.<br /><br />Ryan Carey and I <a href=\"https://dl.dropboxusercontent.com/u/6355797/Gwern%20interview%20writeup.pdf\">talked to</a>&nbsp;<a href=\"http://www.gwern.net/\">Gwern Branwen</a> - an independent researcher with an interest in nootropics - about prospects for substantial intelligence amplification. I was most surprised that Gwern would not be surprised if creatine gave normal people an extra 3 IQ points.</li>\n<li><a style=\"font-weight: bold;\" href=\"http://en.wikipedia.org/wiki/Environment_and_intelligence\">Environmental influences on intelligence</a><br />And some more&nbsp;<a href=\"http://en.wikipedia.org/wiki/Impact_of_health_on_intelligence\">health-specific</a> ones.</li>\n<li><strong>The Flynn Effect</strong><br />People have apparently been getting smarter by about <a href=\"http://www.americanscientist.org/bookshelf/pub/the-domestication-of-the-savage-mind\">3 points per decade</a> for much of the twentieth century, though this trend <a href=\"http://en.wikipedia.org/wiki/Flynn_effect#Possible_end_of_progression\">may be</a> ending.&nbsp;<a href=\"http://en.wikipedia.org/wiki/Flynn_effect\">Several explanations</a> have been proposed.&nbsp;Namesake James Flynn has a <a href=\"https://www.ted.com/talks/james_flynn_why_our_iq_levels_are_higher_than_our_grandparents/transcript?language=en\">TED talk</a> on the phenomenon. It is strangely hard to find a good summary picture of these changes, but here's a table from <a href=\"http://www.jugendsozialarbeit.de/media/raw/flynn1987_What_IQ_tests_really_measure.pdf\">Flynn's classic 1978</a>&nbsp;paper of measured increases at that point:<br /><img src=\"http://images.lesswrong.com/t3_l2n_0.png?v=f9733553a64d2eeb493718c2e30d7709\" alt=\"\" width=\"700\" /><br /><br />Here are changes in IQ test scores over time in a set of <a href=\"http://www.degruyter.com/view/j/ppb.2014.45.issue-2/ppb-2014-0019/ppb-2014-0019.xml\">Polish teenagers</a>, and a set of <a href=\"http://www.mdpi.com/2079-3200/2/3/106/htm\">Norwegian military conscripts</a> respectively:<br /><img src=\"http://images.lesswrong.com/t3_l2n_1.png\" alt=\"\" width=\"500\" /><br /><br /><img src=\"http://images.lesswrong.com/t3_l2n_2.png?v=03fa8dd0a1ac1fd552a96393750645af\" alt=\"\" width=\"500\" /></li>\n<li><strong>Prospects for genetic intelligence enhancement</strong><br /><a href=\"http://www.nature.com/mp/journal/v16/n10/full/mp201185a.html\">This study</a>&nbsp;uses 'Genome-wide Complex Trait Analysis' (<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3014363/\">GCTA</a>)&nbsp;to estimate that about half of variation in fluid intelligence in adults is explained by common genetic variation (childhood intelligence&nbsp;<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/23358156\">may be less heritable</a>). These studies use genetic data to predict 1% of variation in intelligence. <a href=\"http://www.sciencemag.org/content/early/2013/05/29/science.1235488.full.pdf\">This</a>&nbsp;genome-wide association study (<a href=\"http://en.wikipedia.org/wiki/Genome-wide_association_study\">GWAS</a>) allowed prediction of 2% of education and IQ.&nbsp;<a href=\"http://www.pnas.org/content/111/38/13790.abstract\">This study</a> finds several common genetic variants associated with cognitive performance. Stephen Hsu very roughly <a href=\"http://arxiv.org/abs/1408.3421\">estimates</a>&nbsp;that you would need a million samples in order to characterize the relationship between intelligence and genetics. According to <a href=\"https://my.vanderbilt.edu/smpy/files/2013/02/Ferriman_2010.pdf\">Robertson et al</a>, even among students in the top 1% of quantitative ability, cognitive performance predicts differences in occupational outcomes later in life. The <a href=\"http://ssgac.org/Publications.php\">Social Science Genetics Association Consortium</a>&nbsp;(SSGAC) lead research efforts on genetics of education and intelligence, and are also investigating the genetics of other 'social science traits' such as self-employment, happiness and fertility. Carl Shulman and Nick Bostrom provide some <a href=\"http://www.nickbostrom.com/papers/embryo.pdf\">estimates</a>&nbsp;for the feasibility and impact of genetic selection for intelligence, along with a discussion of reproductive technologies that might facilitate more extreme selection. Robert Sparrow writes about <a href=\"http://jme.bmj.com/content/early/2013/02/13/medethics-2012-101200.full\">'in vitro eugenics'</a>. Stephen Hsu also had an interesting <a href=\"http://intelligence.org/2013/08/31/stephen-hsu-on-cognitive-genomics/\">interview</a> with Luke Muehlhauser about several of these topics, and summarizes research on genetics and intelligence in a <a href=\"https://www.youtube.com/watch?v=62jZENi1ed8\">Google Tech Talk</a>.</li>\n<li><strong>Some brain computer interfaces in action</strong><br />For&nbsp;<a href=\"http://youngandshaky.com/?p=405\">Parkinson's disease relief</a>, <a href=\"https://www.youtube.com/watch?v=-QHTjkLluus\">allowing locked in patients to communicate</a>, <a href=\"http://ww2.odu.edu/~dkrusien/\">handwriting</a>, and&nbsp;<a href=\"https://www.youtube.com/watch?v=9cM-2aLHFLg\">controlling</a> <a href=\"https://www.youtube.com/watch?v=WV0bJkk86pw\">robot arms</a>.</li>\n<li><strong>What changes have made human organizations 'smarter' in the past?</strong><br />Big ones I can think of include innovations in using text (writing, printing, digital text editing), communicating better in other ways (faster, further, more reliably), increasing population size (population growth, or connection between disjoint populations),&nbsp;systems for trade (e.g. currency, finance, different kinds of marketplace),&nbsp;innovations in business organization, improvements in governance, and&nbsp;<a href=\"http://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature\">forces</a> leading to reduced conflict.</li>\n</ol></div>\n<h1>In-depth investigations</h1>\n<p>If you are particularly interested in these topics, and want to do further research, these are a few plausible directions, some inspired by Luke Muehlhauser's&nbsp;<a href=\"http://lukemuehlhauser.com/some-studies-which-could-improve-our-strategic-picture-of-superintelligence/\">list</a>, which contains many suggestions related to parts of <em>Superintelligence. </em>These projects could be attempted at various levels of depth.</p>\n<ol>\n<li>How well does IQ predict relevant kinds of success? This is informative about what enhanced humans might achieve, in general and in terms of producing more enhancement. How much better is a person with IQ 150 at programming or doing genetics research than a person with IQ 120? How does IQ relate to philosophical ability, reflectiveness, or the ability to avoid catastrophic errors? (related project guide <a href=\"https://docs.google.com/document/d/18io8lEK-JVl2rWSRjwcRJMHjmJz_ik2cqVCODJFXU7c/edit\">here</a>).</li>\n<li>How promising are nootropics? Bostrom argues 'probably not very', but it seems worth checking more thoroughly. One related curiosity is that on casual inspection, there seem to be quite a few nootropics that appeared promising at some point and then haven't been studied much. This could be explained well by any of publication bias, <a href=\"http://slatestarcodex.com/2014/06/15/fish-now-by-prescription/\">whatever forces</a> are usually blamed for relatively natural drugs receiving little attention, or the casualness of my casual inspection.</li>\n<li>How can we measure intelligence in non-human systems? e.g. What are good ways to track increasing 'intelligence' of social networks, quantitatively? We have the general sense that groups of humans are the level at which everything is a lot better than it was in 1000BC, but it would be nice to have an idea of how this is progressing over time. Is GDP a reasonable metric? &nbsp;</li>\n<li>What are the trends in those things that make groups of humans smarter? e.g. How will world capacity for information communication change over the coming decades? (Hilbert and Lopez's <a href=\"http://www.martinhilbert.net/WorldInfoCapacity.html\">work</a> is probably relevant)</li>\n</ol> <ol> </ol>\n<div>If you are interested in anything like this, you might want to mention it in the comments, and see whether other people have useful thoughts.</div>\n<h1>How to proceed</h1>\n<p>This has been a collection of notes on the chapter.&nbsp;&nbsp;<strong>The most important part of the reading group though is discussion</strong>, which is in the comments section. I pose some questions for you there, and I invite you to add your own. Please remember that this group contains a variety of levels of expertise: if a line of discussion seems too basic or too incomprehensible, look around for one that suits you better!</p>\n<p>Next week, we will talk about 'forms of superintelligence', in the sense of different dimensions in which general intelligence might be scaled up. To prepare,&nbsp;<strong>read</strong>&nbsp;Chapter 3, <em>Forms of Superintelligence </em>(p52-61)<em>.&nbsp;</em>The discussion will go live at 6pm Pacific time next Monday 13 October. Sign up to be notified&nbsp;<a href=\"http://intelligence.us5.list-manage.com/subscribe?u=353906382677fa789a483ba9e&amp;id=28cb982f40\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "tdt83ChxnEgwwKxi6": 1, "jaf5zfcGgCB2REXGw": 1, "5f5c37ee1b5cdee568cfb28c": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SEtkbLgusgtQ9dAzX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "27311", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This is part of a weekly reading group on&nbsp;<a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a>'s book,&nbsp;<a href=\"http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111\">Superintelligence</a>. For more information about the group, and an index of posts so far see the&nbsp;<a href=\"/lw/kw4/superintelligence_reading_group/\">announcement post</a>. For the schedule of future topics, see&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2014/08/Superintelligence-Readers-Guide-early-version.pdf\">MIRI's reading guide</a>.</em></p>\n<hr>\n<p>Welcome. This week we finish chapter 2 with three more routes to superintelligence: enhancement of biological cognition, brain-computer interfaces, and well-organized networks of intelligent agents. This corresponds to the fourth section in the <a href=\"https://intelligence.org/wp-content/uploads/2014/08/Superintelligence-Readers-Guide-early-version.pdf\">reading guide</a>,&nbsp;<em><strong>Biological Cognition, BCIs, Organizations</strong></em>.&nbsp;</p>\n<p>This post summarizes the section, and offers a few relevant notes, and ideas for further investigation. My own thoughts and questions for discussion are in the comments.</p>\n<p>There is no need to proceed in order through this post, or to look at everything. Feel free to jump straight to the discussion. Where applicable and I remember, page numbers indicate the rough part of the chapter that is most related (not necessarily that the chapter is being cited for the specific claim).</p>\n<p><strong>Reading</strong>:&nbsp;<em>\u201cBiological Cognition\u201d&nbsp;</em>and&nbsp;the rest of&nbsp;Chapter 2 (p36-51)</p>\n<hr>\n<h1 id=\"Summary\">Summary</h1>\n<p><strong id=\"Biological_intelligence\">Biological intelligence</strong></p>\n<ol>\n<li>Modest gains to intelligence are available with current interventions such as nutrition.</li>\n<li>Genetic technologies might produce a population whose average is smarter than anyone who has have ever lived.</li>\n<li>Some particularly interesting possibilities are 'iterated embryo selection' where many rounds of selection take place in a single generation, and 'spell-checking' where the genetic mutations which are ubiquitous in current human genomes are removed.</li>\n</ol>\n<p><strong id=\"Brain_computer_interfaces\">Brain-computer interfaces</strong></p>\n<ol>\n<li>It is sometimes suggested that machines interfacing closely with the human brain will greatly enhance human cognition. For instance implants that allow perfect recall and fast arithmetic. (p44-45)&nbsp;</li>\n<li>Brain-computer interfaces seem unlikely to produce superintelligence (p51) This is because they have substantial health risks, because our existing systems for getting information in and out of our brains are hard to compete with, and because our brains are probably bottlenecked in other ways anyway. (p45-6)&nbsp;</li>\n<li>'Downloading' directly from one brain to another seems infeasible because each brain represents concepts idiosyncratically, without a standard format. (p46-7)</li>\n</ol>\n<p><strong id=\"Networks_and_organizations\">Networks and organizations</strong></p>\n<ol>\n<li>A large connected system of people (or something else) might become superintelligent. (p48)&nbsp; </li>\n<li>Systems of connected people become more capable through technological and institutional innovations, such as enhanced communications channels, well-aligned incentives, elimination of bureaucratic failures, and mechanisms for aggregating information. The internet as a whole is a contender for a network of humans that might become superintelligent (p49)&nbsp;</li>\n</ol>\n<p><strong id=\"Summary1\">Summary</strong></p>\n<ol>\n<li>Since there are many possible paths to superintelligence, we can be more confident that we will get there eventually (p50)&nbsp;</li>\n<li>Whole brain emulation and biological enhancement are both likely to succeed after enough incremental progress in existing technologies. Networks and organizations are already improving gradually.&nbsp;</li>\n<li>The path to AI is less clear, and may be discontinuous. Which route we take might matter a lot, even if we end up with similar capabilities anyway. (p50)</li>\n</ol>\n<p><strong id=\"The_book_so_far\">The book so far</strong></p>\n<p>Here's a recap of what we have seen so far, now at the end of Chapter 2:</p>\n<ol>\n<li>Economic history suggests big changes are plausible.</li>\n<li>AI progress is ongoing.</li>\n<li>AI progress is hard to predict, but AI experts tend to expect human-level AI in mid-century.</li>\n<li>Several plausible paths lead to superintelligence: brain emulations, AI, human cognitive enhancement, brain-computer interfaces, and organizations.</li>\n<li>Most of these probably lead to machine superintelligence ultimately.</li>\n<li>That there are several paths suggests we are likely to get there.</li>\n</ol>\n<p>Do you disagree with any of these points? Tell us about it in the comments.</p>\n<h1 id=\"Notes\">Notes</h1>\n<div><ol>\n<li><strong>Nootropics</strong><br><em><a href=\"http://www.informationisbeautiful.net/play/snake-oil-supplements/\">Snake Oil Supplements?</a></em>&nbsp;is a nice illustration of scientific evidence for different supplements, <a href=\"http://www.informationisbeautiful.net/play/snake-oil-supplements/\">here</a> filtered for those with purported mental effects, many of which relate to intelligence. I don't know how accurate it is, or where to find a summary of apparent effect sizes rather than evidence, which I think would be more interesting.<br><br>Ryan Carey and I <a href=\"https://dl.dropboxusercontent.com/u/6355797/Gwern%20interview%20writeup.pdf\">talked to</a>&nbsp;<a href=\"http://www.gwern.net/\">Gwern Branwen</a> - an independent researcher with an interest in nootropics - about prospects for substantial intelligence amplification. I was most surprised that Gwern would not be surprised if creatine gave normal people an extra 3 IQ points.</li>\n<li><a style=\"font-weight: bold;\" href=\"http://en.wikipedia.org/wiki/Environment_and_intelligence\">Environmental influences on intelligence</a><br>And some more&nbsp;<a href=\"http://en.wikipedia.org/wiki/Impact_of_health_on_intelligence\">health-specific</a> ones.</li>\n<li><strong>The Flynn Effect</strong><br>People have apparently been getting smarter by about <a href=\"http://www.americanscientist.org/bookshelf/pub/the-domestication-of-the-savage-mind\">3 points per decade</a> for much of the twentieth century, though this trend <a href=\"http://en.wikipedia.org/wiki/Flynn_effect#Possible_end_of_progression\">may be</a> ending.&nbsp;<a href=\"http://en.wikipedia.org/wiki/Flynn_effect\">Several explanations</a> have been proposed.&nbsp;Namesake James Flynn has a <a href=\"https://www.ted.com/talks/james_flynn_why_our_iq_levels_are_higher_than_our_grandparents/transcript?language=en\">TED talk</a> on the phenomenon. It is strangely hard to find a good summary picture of these changes, but here's a table from <a href=\"http://www.jugendsozialarbeit.de/media/raw/flynn1987_What_IQ_tests_really_measure.pdf\">Flynn's classic 1978</a>&nbsp;paper of measured increases at that point:<br><img src=\"http://images.lesswrong.com/t3_l2n_0.png?v=f9733553a64d2eeb493718c2e30d7709\" alt=\"\" width=\"700\"><br><br>Here are changes in IQ test scores over time in a set of <a href=\"http://www.degruyter.com/view/j/ppb.2014.45.issue-2/ppb-2014-0019/ppb-2014-0019.xml\">Polish teenagers</a>, and a set of <a href=\"http://www.mdpi.com/2079-3200/2/3/106/htm\">Norwegian military conscripts</a> respectively:<br><img src=\"http://images.lesswrong.com/t3_l2n_1.png\" alt=\"\" width=\"500\"><br><br><img src=\"http://images.lesswrong.com/t3_l2n_2.png?v=03fa8dd0a1ac1fd552a96393750645af\" alt=\"\" width=\"500\"></li>\n<li><strong>Prospects for genetic intelligence enhancement</strong><br><a href=\"http://www.nature.com/mp/journal/v16/n10/full/mp201185a.html\">This study</a>&nbsp;uses 'Genome-wide Complex Trait Analysis' (<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3014363/\">GCTA</a>)&nbsp;to estimate that about half of variation in fluid intelligence in adults is explained by common genetic variation (childhood intelligence&nbsp;<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/23358156\">may be less heritable</a>). These studies use genetic data to predict 1% of variation in intelligence. <a href=\"http://www.sciencemag.org/content/early/2013/05/29/science.1235488.full.pdf\">This</a>&nbsp;genome-wide association study (<a href=\"http://en.wikipedia.org/wiki/Genome-wide_association_study\">GWAS</a>) allowed prediction of 2% of education and IQ.&nbsp;<a href=\"http://www.pnas.org/content/111/38/13790.abstract\">This study</a> finds several common genetic variants associated with cognitive performance. Stephen Hsu very roughly <a href=\"http://arxiv.org/abs/1408.3421\">estimates</a>&nbsp;that you would need a million samples in order to characterize the relationship between intelligence and genetics. According to <a href=\"https://my.vanderbilt.edu/smpy/files/2013/02/Ferriman_2010.pdf\">Robertson et al</a>, even among students in the top 1% of quantitative ability, cognitive performance predicts differences in occupational outcomes later in life. The <a href=\"http://ssgac.org/Publications.php\">Social Science Genetics Association Consortium</a>&nbsp;(SSGAC) lead research efforts on genetics of education and intelligence, and are also investigating the genetics of other 'social science traits' such as self-employment, happiness and fertility. Carl Shulman and Nick Bostrom provide some <a href=\"http://www.nickbostrom.com/papers/embryo.pdf\">estimates</a>&nbsp;for the feasibility and impact of genetic selection for intelligence, along with a discussion of reproductive technologies that might facilitate more extreme selection. Robert Sparrow writes about <a href=\"http://jme.bmj.com/content/early/2013/02/13/medethics-2012-101200.full\">'in vitro eugenics'</a>. Stephen Hsu also had an interesting <a href=\"http://intelligence.org/2013/08/31/stephen-hsu-on-cognitive-genomics/\">interview</a> with Luke Muehlhauser about several of these topics, and summarizes research on genetics and intelligence in a <a href=\"https://www.youtube.com/watch?v=62jZENi1ed8\">Google Tech Talk</a>.</li>\n<li><strong>Some brain computer interfaces in action</strong><br>For&nbsp;<a href=\"http://youngandshaky.com/?p=405\">Parkinson's disease relief</a>, <a href=\"https://www.youtube.com/watch?v=-QHTjkLluus\">allowing locked in patients to communicate</a>, <a href=\"http://ww2.odu.edu/~dkrusien/\">handwriting</a>, and&nbsp;<a href=\"https://www.youtube.com/watch?v=9cM-2aLHFLg\">controlling</a> <a href=\"https://www.youtube.com/watch?v=WV0bJkk86pw\">robot arms</a>.</li>\n<li><strong>What changes have made human organizations 'smarter' in the past?</strong><br>Big ones I can think of include innovations in using text (writing, printing, digital text editing), communicating better in other ways (faster, further, more reliably), increasing population size (population growth, or connection between disjoint populations),&nbsp;systems for trade (e.g. currency, finance, different kinds of marketplace),&nbsp;innovations in business organization, improvements in governance, and&nbsp;<a href=\"http://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature\">forces</a> leading to reduced conflict.</li>\n</ol></div>\n<h1 id=\"In_depth_investigations\">In-depth investigations</h1>\n<p>If you are particularly interested in these topics, and want to do further research, these are a few plausible directions, some inspired by Luke Muehlhauser's&nbsp;<a href=\"http://lukemuehlhauser.com/some-studies-which-could-improve-our-strategic-picture-of-superintelligence/\">list</a>, which contains many suggestions related to parts of <em>Superintelligence. </em>These projects could be attempted at various levels of depth.</p>\n<ol>\n<li>How well does IQ predict relevant kinds of success? This is informative about what enhanced humans might achieve, in general and in terms of producing more enhancement. How much better is a person with IQ 150 at programming or doing genetics research than a person with IQ 120? How does IQ relate to philosophical ability, reflectiveness, or the ability to avoid catastrophic errors? (related project guide <a href=\"https://docs.google.com/document/d/18io8lEK-JVl2rWSRjwcRJMHjmJz_ik2cqVCODJFXU7c/edit\">here</a>).</li>\n<li>How promising are nootropics? Bostrom argues 'probably not very', but it seems worth checking more thoroughly. One related curiosity is that on casual inspection, there seem to be quite a few nootropics that appeared promising at some point and then haven't been studied much. This could be explained well by any of publication bias, <a href=\"http://slatestarcodex.com/2014/06/15/fish-now-by-prescription/\">whatever forces</a> are usually blamed for relatively natural drugs receiving little attention, or the casualness of my casual inspection.</li>\n<li>How can we measure intelligence in non-human systems? e.g. What are good ways to track increasing 'intelligence' of social networks, quantitatively? We have the general sense that groups of humans are the level at which everything is a lot better than it was in 1000BC, but it would be nice to have an idea of how this is progressing over time. Is GDP a reasonable metric? &nbsp;</li>\n<li>What are the trends in those things that make groups of humans smarter? e.g. How will world capacity for information communication change over the coming decades? (Hilbert and Lopez's <a href=\"http://www.martinhilbert.net/WorldInfoCapacity.html\">work</a> is probably relevant)</li>\n</ol> <ol> </ol>\n<div>If you are interested in anything like this, you might want to mention it in the comments, and see whether other people have useful thoughts.</div>\n<h1 id=\"How_to_proceed\">How to proceed</h1>\n<p>This has been a collection of notes on the chapter.&nbsp;&nbsp;<strong>The most important part of the reading group though is discussion</strong>, which is in the comments section. I pose some questions for you there, and I invite you to add your own. Please remember that this group contains a variety of levels of expertise: if a line of discussion seems too basic or too incomprehensible, look around for one that suits you better!</p>\n<p>Next week, we will talk about 'forms of superintelligence', in the sense of different dimensions in which general intelligence might be scaled up. To prepare,&nbsp;<strong>read</strong>&nbsp;Chapter 3, <em>Forms of Superintelligence </em>(p52-61)<em>.&nbsp;</em>The discussion will go live at 6pm Pacific time next Monday 13 October. Sign up to be notified&nbsp;<a href=\"http://intelligence.us5.list-manage.com/subscribe?u=353906382677fa789a483ba9e&amp;id=28cb982f40\">here</a>.</p>", "sections": [{"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "Biological intelligence", "anchor": "Biological_intelligence", "level": 2}, {"title": "Brain-computer interfaces", "anchor": "Brain_computer_interfaces", "level": 2}, {"title": "Networks and organizations", "anchor": "Networks_and_organizations", "level": 2}, {"title": "Summary", "anchor": "Summary1", "level": 2}, {"title": "The book so far", "anchor": "The_book_so_far", "level": 2}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"title": "In-depth investigations", "anchor": "In_depth_investigations", "level": 1}, {"title": "How to proceed", "anchor": "How_to_proceed", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "139 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 139, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QDmzDZ9CEHrKQdvcn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-07T01:31:52.880Z", "modifiedAt": null, "url": null, "title": "Meetup : Sydney Meetup - October", "slug": "meetup-sydney-meetup-october", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taryneast", "createdAt": "2010-11-29T20:51:06.328Z", "isAdmin": false, "displayName": "taryneast"}, "userId": "xD8wjhiTvwbXdKirW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PfeRdKX8BfjRkga5e/meetup-sydney-meetup-october", "pageUrlRelative": "/posts/PfeRdKX8BfjRkga5e/meetup-sydney-meetup-october", "linkUrl": "https://www.lesswrong.com/posts/PfeRdKX8BfjRkga5e/meetup-sydney-meetup-october", "postedAtFormatted": "Tuesday, October 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sydney%20Meetup%20-%20October&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sydney%20Meetup%20-%20October%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPfeRdKX8BfjRkga5e%2Fmeetup-sydney-meetup-october%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sydney%20Meetup%20-%20October%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPfeRdKX8BfjRkga5e%2Fmeetup-sydney-meetup-october", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPfeRdKX8BfjRkga5e%2Fmeetup-sydney-meetup-october", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 89, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/15e'>Sydney Meetup - October</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 October 2014 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Sydney City RSL, 565 George St, Sydney, Australia 2000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>6:30 PM for early discussion 7PM general dinner-discussion after dinner we'll have our rationality exercise and a more specific discussion-topic.</p>\n\n<p>We normally meet in the restaurant on level 2. So far they've always put it about 5m to the left of the lift, up against the kitchen area.</p>\n\n<p>Topic TBD, but I'm sure it'll be awesome :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/15e'>Sydney Meetup - October</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PfeRdKX8BfjRkga5e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.0643086479888843e-06, "legacy": true, "legacyId": "27323", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sydney_Meetup___October\">Discussion article for the meetup : <a href=\"/meetups/15e\">Sydney Meetup - October</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 October 2014 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Sydney City RSL, 565 George St, Sydney, Australia 2000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>6:30 PM for early discussion 7PM general dinner-discussion after dinner we'll have our rationality exercise and a more specific discussion-topic.</p>\n\n<p>We normally meet in the restaurant on level 2. So far they've always put it about 5m to the left of the lift, up against the kitchen area.</p>\n\n<p>Topic TBD, but I'm sure it'll be awesome :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sydney_Meetup___October1\">Discussion article for the meetup : <a href=\"/meetups/15e\">Sydney Meetup - October</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sydney Meetup - October", "anchor": "Discussion_article_for_the_meetup___Sydney_Meetup___October", "level": 1}, {"title": "Discussion article for the meetup : Sydney Meetup - October", "anchor": "Discussion_article_for_the_meetup___Sydney_Meetup___October1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-07T18:12:06.259Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow Meetup: Practical rationality", "slug": "meetup-moscow-meetup-practical-rationality", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexander230", "createdAt": "2014-08-27T08:55:16.153Z", "isAdmin": false, "displayName": "Alexander230"}, "userId": "xqoKSJayCCtP5juLh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pYd9iYfQcXwcZ4CcG/meetup-moscow-meetup-practical-rationality", "pageUrlRelative": "/posts/pYd9iYfQcXwcZ4CcG/meetup-moscow-meetup-practical-rationality", "linkUrl": "https://www.lesswrong.com/posts/pYd9iYfQcXwcZ4CcG/meetup-moscow-meetup-practical-rationality", "postedAtFormatted": "Tuesday, October 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%20Meetup%3A%20Practical%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%20Meetup%3A%20Practical%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpYd9iYfQcXwcZ4CcG%2Fmeetup-moscow-meetup-practical-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%20Meetup%3A%20Practical%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpYd9iYfQcXwcZ4CcG%2Fmeetup-moscow-meetup-practical-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpYd9iYfQcXwcZ4CcG%2Fmeetup-moscow-meetup-practical-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 199, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/15f'>Moscow Meetup: Practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 October 2014 02:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Here's our plan:</p>\n\n<ul>\n<li>Integrative psychodiagnostic system with Rorshach's method.</li>\n<li>The valley of harmful rationality, or how being smart will help you to commit follies.</li>\n<li>About white, warm and fluffy, part 1: Analysis of manipulative statements.</li>\n<li>About space and time and why they don't exist.</li>\n<li>Structured discussion.</li>\n</ul>\n\n<p>Details and schedule:</p>\n\n<p><a href=\"https://lesswrong-ru.hackpad.com/-12--BINMqtdCLYt\" rel=\"nofollow\">https://lesswrong-ru.hackpad.com/-12--BINMqtdCLYt</a></p>\n\n<p>Yudcoins, positive reinforcement and pizza will all be present. If you've been to our meetups, you know what I'm talking about, and if you didn't, the best way to find out is to come and see for yourself.</p>\n\n<p>Info for newcomers: We gather in the Yandex office, you need the first revolving door under the archway. Here is a guide how to get there:</p>\n\n<p><a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">http://company.yandex.ru/contacts/redrose/</a></p>\n\n<p>Try to come in time, we will allow latecomers to enter every 15 minutes. Call Slava or send him SMS at +7(926)313-96-42 if you're late. We start at 14:00 and stay until at least 19-20. Please pay attention that we only gather near the entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/15f'>Moscow Meetup: Practical rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pYd9iYfQcXwcZ4CcG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2.0662043317812985e-06, "legacy": true, "legacyId": "27326", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow_Meetup__Practical_rationality\">Discussion article for the meetup : <a href=\"/meetups/15f\">Moscow Meetup: Practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 October 2014 02:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Here's our plan:</p>\n\n<ul>\n<li>Integrative psychodiagnostic system with Rorshach's method.</li>\n<li>The valley of harmful rationality, or how being smart will help you to commit follies.</li>\n<li>About white, warm and fluffy, part 1: Analysis of manipulative statements.</li>\n<li>About space and time and why they don't exist.</li>\n<li>Structured discussion.</li>\n</ul>\n\n<p>Details and schedule:</p>\n\n<p><a href=\"https://lesswrong-ru.hackpad.com/-12--BINMqtdCLYt\" rel=\"nofollow\">https://lesswrong-ru.hackpad.com/-12--BINMqtdCLYt</a></p>\n\n<p>Yudcoins, positive reinforcement and pizza will all be present. If you've been to our meetups, you know what I'm talking about, and if you didn't, the best way to find out is to come and see for yourself.</p>\n\n<p>Info for newcomers: We gather in the Yandex office, you need the first revolving door under the archway. Here is a guide how to get there:</p>\n\n<p><a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">http://company.yandex.ru/contacts/redrose/</a></p>\n\n<p>Try to come in time, we will allow latecomers to enter every 15 minutes. Call Slava or send him SMS at +7(926)313-96-42 if you're late. We start at 14:00 and stay until at least 19-20. Please pay attention that we only gather near the entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow_Meetup__Practical_rationality1\">Discussion article for the meetup : <a href=\"/meetups/15f\">Moscow Meetup: Practical rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow Meetup: Practical rationality", "anchor": "Discussion_article_for_the_meetup___Moscow_Meetup__Practical_rationality", "level": 1}, {"title": "Discussion article for the meetup : Moscow Meetup: Practical rationality", "anchor": "Discussion_article_for_the_meetup___Moscow_Meetup__Practical_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-07T18:20:01.177Z", "modifiedAt": null, "url": null, "title": "October 2014 Bragging thread.", "slug": "october-2014-bragging-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:31.235Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Joshua_Blaine", "createdAt": "2013-07-15T18:37:17.985Z", "isAdmin": false, "displayName": "Joshua_Blaine"}, "userId": "jzvkfAuoy9X7dp9Ma", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3xRQk4J6ggyTmMXqx/october-2014-bragging-thread", "pageUrlRelative": "/posts/3xRQk4J6ggyTmMXqx/october-2014-bragging-thread", "linkUrl": "https://www.lesswrong.com/posts/3xRQk4J6ggyTmMXqx/october-2014-bragging-thread", "postedAtFormatted": "Tuesday, October 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20October%202014%20Bragging%20thread.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOctober%202014%20Bragging%20thread.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3xRQk4J6ggyTmMXqx%2Foctober-2014-bragging-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=October%202014%20Bragging%20thread.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3xRQk4J6ggyTmMXqx%2Foctober-2014-bragging-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3xRQk4J6ggyTmMXqx%2Foctober-2014-bragging-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 199, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">So, to quote myself all those months ago when I first had this idea (I haven't actually posted one of these since the original. funny how that happens):</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify; padding-left: 30px;\">In an attempt to encourage more people to&nbsp;<em>actually do awesome things&nbsp;</em>(a la instrumental rationality), I am proposing a new monthly thread (can be changed to bi-weekly, should that be demanded). Your job, should you choose to accept it, is to comment on this thread explaining&nbsp;<strong>the most awesome thing you've done this month</strong>. You may be as blatantly proud of you self as you feel. You may unabashedly consider yourself&nbsp;<em>the coolest freaking person ever</em>&nbsp;because of that awesome thing you're dying to tell everyone about. This is the place to do just that.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify; padding-left: 30px;\">Remember, however, that this&nbsp;<strong>isn't</strong>&nbsp;any kind of progress thread. Nor is it any kind of proposal thread.<em>This thread is solely for people to talk about the awesomest thing they've done all month. not will do. not are working on</em>.&nbsp;<strong>have already done.</strong>This is to cultivate an environment of object level productivity rather than meta-productivity methods.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">Anything not mentioned in a previous bragging thread is still fair game, even If it didn't actually happen in the last month.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">So, what's the coolest thing you've done this month?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3xRQk4J6ggyTmMXqx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 2.066219345562133e-06, "legacy": true, "legacyId": "27327", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-07T20:40:06.243Z", "modifiedAt": null, "url": null, "title": "Meetup : Warsaw Meetup - This Friday", "slug": "meetup-warsaw-meetup-this-friday", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "LvfE3p4i6uy8Exadr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NyR2LvXE8mDmcf4u6/meetup-warsaw-meetup-this-friday", "pageUrlRelative": "/posts/NyR2LvXE8mDmcf4u6/meetup-warsaw-meetup-this-friday", "linkUrl": "https://www.lesswrong.com/posts/NyR2LvXE8mDmcf4u6/meetup-warsaw-meetup-this-friday", "postedAtFormatted": "Tuesday, October 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Warsaw%20Meetup%20-%20This%20Friday&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Warsaw%20Meetup%20-%20This%20Friday%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNyR2LvXE8mDmcf4u6%2Fmeetup-warsaw-meetup-this-friday%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Warsaw%20Meetup%20-%20This%20Friday%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNyR2LvXE8mDmcf4u6%2Fmeetup-warsaw-meetup-this-friday", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNyR2LvXE8mDmcf4u6%2Fmeetup-warsaw-meetup-this-friday", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/15g'>Warsaw Meetup - This Friday</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 October 2014 06:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Warsaw</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come and meet us in <a href=\"https://www.facebook.com/klubokawiarniaresort\" rel=\"nofollow\">https://www.facebook.com/klubokawiarniaresort</a></p>\n\n<p>If you use Facebook, join our group for better coordination: <a href=\"https://www.facebook.com/groups/lwwarsaw/\" rel=\"nofollow\">https://www.facebook.com/groups/lwwarsaw/</a></p>\n\n<p>4 people already confirmed that they'll arrive.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/15g'>Warsaw Meetup - This Friday</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NyR2LvXE8mDmcf4u6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 2.0664850914147953e-06, "legacy": true, "legacyId": "27328", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Warsaw_Meetup___This_Friday\">Discussion article for the meetup : <a href=\"/meetups/15g\">Warsaw Meetup - This Friday</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 October 2014 06:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Warsaw</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come and meet us in <a href=\"https://www.facebook.com/klubokawiarniaresort\" rel=\"nofollow\">https://www.facebook.com/klubokawiarniaresort</a></p>\n\n<p>If you use Facebook, join our group for better coordination: <a href=\"https://www.facebook.com/groups/lwwarsaw/\" rel=\"nofollow\">https://www.facebook.com/groups/lwwarsaw/</a></p>\n\n<p>4 people already confirmed that they'll arrive.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Warsaw_Meetup___This_Friday1\">Discussion article for the meetup : <a href=\"/meetups/15g\">Warsaw Meetup - This Friday</a></h2>", "sections": [{"title": "Discussion article for the meetup : Warsaw Meetup - This Friday", "anchor": "Discussion_article_for_the_meetup___Warsaw_Meetup___This_Friday", "level": 1}, {"title": "Discussion article for the meetup : Warsaw Meetup - This Friday", "anchor": "Discussion_article_for_the_meetup___Warsaw_Meetup___This_Friday1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-08T21:02:43.338Z", "modifiedAt": null, "url": null, "title": "Questions on Theism", "slug": "questions-on-theism", "viewCount": null, "lastCommentedAt": "2015-04-29T20:40:04.986Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Aiyen", "createdAt": "2013-05-21T06:23:22.095Z", "isAdmin": false, "displayName": "Aiyen"}, "userId": "qsecRTQSuWvxdGpwB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YRL3aLTaTA8vBYRAS/questions-on-theism", "pageUrlRelative": "/posts/YRL3aLTaTA8vBYRAS/questions-on-theism", "linkUrl": "https://www.lesswrong.com/posts/YRL3aLTaTA8vBYRAS/questions-on-theism", "postedAtFormatted": "Wednesday, October 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Questions%20on%20Theism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestions%20on%20Theism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYRL3aLTaTA8vBYRAS%2Fquestions-on-theism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Questions%20on%20Theism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYRL3aLTaTA8vBYRAS%2Fquestions-on-theism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYRL3aLTaTA8vBYRAS%2Fquestions-on-theism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 863, "htmlBody": "<p>Long time lurker, but I've barely posted anything. I'd like to ask Less Wrong for help.</p>\n<p>Reading various articles by the Rationalist Community over the years, here, on Slate Star Codex and a few other websites, I have found that nearly all of it makes sense. Wonderful sense, in fact, the kind of sense you only really find when the author is actually thinking through the implications of what they're saying, and it's been a breath of fresh air. I generally agree, and when I don't it's clear why we're differing, typically due to a dispute in priors.</p>\n<p>Except in theism/atheism.</p>\n<p>In my experience, when atheists make their case, they assume a universe without miracles, i.e. a universe that looks like one would expect if there was no God. Given this assumption, atheism is obviously the rational and correct stance to take. And generally, Christian apologists make the same assumption! They assert miracles in the Bible, but do not point to any accounts of contemporary supernatural activity. And given such assumptions, the only way one can make a case for Christianity is with logical fallacies, which is exactly what most apologists do. The thing is though, there are plenty of contemporary miracle accounts.</p>\n<p>Near death experiences. Answers to prayer that seem to violate the laws of physics. I'm comfortable with dismissing Christian claims that an event was \"more than coincidence\", because given how many people are praying and looking for God's hand in events, and the fact that an unanswered prayer will generally be forgotten while a seemingly-answered one will be remembered, one would expect to see \"more than coincidence\" in any universe with believers, whether or not there was a God. But there are a LOT of people out there claiming to have seen events that one would expect to never occur in a naturalistic universe. I even recall reading an atheist's account of his deconversion (I believe it was Luke Muehlhauser; apologies if I'm misremembering) in which he states that as a Christian, he witnessed healings he could not explain. Now, one could say that these accounts are the result of people lying, but I expect people to be rather more honest than that, and Luke is hardly going to make up evidence for the Christian God in an article promoting unbelief! One could say that \"miracles\" are misunderstood natural events, but there are plenty of accounts that seem pretty unlikely without Divine intervention-I've even read claims by Christians that they had seen people raised from the dead by prayer. And so I'd like to know how atheists respond to the evidence of miracles.</p>\n<p>This isn't just idle curiosity. I am currently a Christian (or maybe an agnostic terrified of ending up on the wrong side of Pascal's Wager), and when you actually take religion seriously, it can be a HUGE drain on quality of life. I find myself being frightened of hell, feeling guilty when I do things that don't hurt anyone but are still considered sins, and feeling guilty when I try to plan out my life, wondering if I should just put my plans in God's hands. To make matters worse, I grew up in a dysfunctional, very Christian family, and my emotions seem to be convinced that being a true Christian means acting like my parents (who were terrible role models; emulating them means losing at life).</p>\n<p>I'm aware of plenty of arguments for non-belief: Occam's Razor giving atheism as one's starting prior in the absence of strong evidence for God, the existence of many contradictory religions proving that humanity tends to generate false gods, claims in Genesis that are simply false (Man created from mud, woman from a rib, etc. have been conclusively debunked by science), commands given by God that seem horrifyingly immoral, no known reason why Christ's death would be needed for human redemption (many apologists try to explain this, but their reasoning never makes sense), no known reason why if belief in Jesus is so important why God wouldn't make himself blatantly obvious, hell seeming like an infinite injustice, the Bible claiming that any prayer prayed in faith will be answered contrasted with the real world where this isn't the case, a study I read about in which praying for the sick didn't improve results at all (and the group that was told they were being prayed for actually had worse results!), etc. All of this, plus the fact that it seems that nearly everyone who's put real effort into their epistemology doesn't believe and moreover is very confident in their nonbelief (I am reminded of Eliezer's comment that he would be less worried about a machine that destroys the universe if the Christian God exists than one that has a one in a trillion chance of destroying us) makes me wonder if there really isn't a God, and in so realizing this, I can put down burdens that have been hurting for nearly my entire life. But the argument from miracles keeps me in faith, keeps me frightened. If there is a good argument against miracles, learning it could be life changing.</p>\n<p>Thank you very much. I do not have words to describe how much this means to me.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YRL3aLTaTA8vBYRAS", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 34, "extendedScore": null, "score": 0.000157, "legacy": true, "legacyId": "27333", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 192, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2014-10-08T21:02:43.338Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-08T23:48:04.250Z", "modifiedAt": "2020-04-26T01:48:03.629Z", "url": null, "title": "Contrarian LW views and their economic implications", "slug": "contrarian-lw-views-and-their-economic-implications", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:29.120Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Larks", "user": {"username": "Larks", "createdAt": "2009-04-28T20:21:45.860Z", "isAdmin": false, "displayName": "Larks"}, "userId": "jQXwiWxFcfyYjytXa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/auRjdC63Y226HeYnG/contrarian-lw-views-and-their-economic-implications", "pageUrlRelative": "/posts/auRjdC63Y226HeYnG/contrarian-lw-views-and-their-economic-implications", "linkUrl": "https://www.lesswrong.com/posts/auRjdC63Y226HeYnG/contrarian-lw-views-and-their-economic-implications", "postedAtFormatted": "Wednesday, October 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Contrarian%20LW%20views%20and%20their%20economic%20implications&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AContrarian%20LW%20views%20and%20their%20economic%20implications%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FauRjdC63Y226HeYnG%2Fcontrarian-lw-views-and-their-economic-implications%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Contrarian%20LW%20views%20and%20their%20economic%20implications%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FauRjdC63Y226HeYnG%2Fcontrarian-lw-views-and-their-economic-implications", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FauRjdC63Y226HeYnG%2Fcontrarian-lw-views-and-their-economic-implications", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 187, "htmlBody": "<p>LW readers have unusual views on many subjects. Efficient Market Hypothesis notwithstanding, many of these are probably alien to most people in finance. So it's plausible they might have implications that are not yet fully integrated into current asset prices. And if you rightfully believe something that most people do not believe, you should be able to make money off that.</p>\n<p>Here's an example for a different group. Feminists believe that women are paid less than men for no good economic reason. If this is the case, feminists should invest in companies that hire many women, and short those which hire few women, to take advantage of the cheaper labour costs. And I can think of examples for groups like Socialists, Neoreactionaries, etc. - cases where their <em>positive </em>beliefs have strong implications for economic predictions. But I struggle to think of such ones for LessWrong, which is why I am asking you. Can you think of any unusual LW-type beliefs that have strong economic implications (say over the next 1-3 years)?</p>\n<p>Wei Dai has previously commented on a <a href=\"/lw/kk5/look_for_the_next_tech_gold_rush/\">similar phenomena</a>, but I'm interested in a wider class of phenomena.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 2, "PDJ6KqJBRzvKPfuS3": 2, "jgcAJnksReZRuvgzp": 4, "pnSDArjzAjkvAF5Jo": 2, "6Qic6PwwBycopJFNN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "auRjdC63Y226HeYnG", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 30, "extendedScore": null, "score": 8.7e-05, "legacy": true, "legacyId": "27335", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 126, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["Jter3YhFBZFYo8vtq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-09T01:50:46.865Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta Beginning of October Meetup - Productive Arguments", "slug": "meetup-atlanta-beginning-of-october-meetup-productive", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Adele_L", "createdAt": "2012-05-25T06:52:13.187Z", "isAdmin": false, "displayName": "Adele_L"}, "userId": "5cAXqfacg2fkQPK8j", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fzzmEZAQ3DNf9xKzx/meetup-atlanta-beginning-of-october-meetup-productive", "pageUrlRelative": "/posts/fzzmEZAQ3DNf9xKzx/meetup-atlanta-beginning-of-october-meetup-productive", "linkUrl": "https://www.lesswrong.com/posts/fzzmEZAQ3DNf9xKzx/meetup-atlanta-beginning-of-october-meetup-productive", "postedAtFormatted": "Thursday, October 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20Beginning%20of%20October%20Meetup%20-%20Productive%20Arguments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20Beginning%20of%20October%20Meetup%20-%20Productive%20Arguments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfzzmEZAQ3DNf9xKzx%2Fmeetup-atlanta-beginning-of-october-meetup-productive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20Beginning%20of%20October%20Meetup%20-%20Productive%20Arguments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfzzmEZAQ3DNf9xKzx%2Fmeetup-atlanta-beginning-of-october-meetup-productive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfzzmEZAQ3DNf9xKzx%2Fmeetup-atlanta-beginning-of-october-meetup-productive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/15h'>Atlanta Beginning of October Meetup - Productive Arguments</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 October 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Unit L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meetup we'll be talking about having productive arguments. We'll be discussing a variety of ways to get the most out of a debate so come ready for good discussion and interesting conversation.</p>\n\n<p>There will be snacks and games.</p>\n\n<p>Please park in a spot marked visitor because parking elsewhere runs the risk of being towed. There are cats at the location.</p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/15h'>Atlanta Beginning of October Meetup - Productive Arguments</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fzzmEZAQ3DNf9xKzx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.0698113041808064e-06, "legacy": true, "legacyId": "27336", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Beginning_of_October_Meetup___Productive_Arguments\">Discussion article for the meetup : <a href=\"/meetups/15h\">Atlanta Beginning of October Meetup - Productive Arguments</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 October 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Unit L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meetup we'll be talking about having productive arguments. We'll be discussing a variety of ways to get the most out of a debate so come ready for good discussion and interesting conversation.</p>\n\n<p>There will be snacks and games.</p>\n\n<p>Please park in a spot marked visitor because parking elsewhere runs the risk of being towed. There are cats at the location.</p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Beginning_of_October_Meetup___Productive_Arguments1\">Discussion article for the meetup : <a href=\"/meetups/15h\">Atlanta Beginning of October Meetup - Productive Arguments</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta Beginning of October Meetup - Productive Arguments", "anchor": "Discussion_article_for_the_meetup___Atlanta_Beginning_of_October_Meetup___Productive_Arguments", "level": 1}, {"title": "Discussion article for the meetup : Atlanta Beginning of October Meetup - Productive Arguments", "anchor": "Discussion_article_for_the_meetup___Atlanta_Beginning_of_October_Meetup___Productive_Arguments1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-09T16:38:06.739Z", "modifiedAt": null, "url": null, "title": "3-day Solstice in Leipzig, Germany: small, nice, very low cost, includes accommodation, 19th-21st Dec", "slug": "3-day-solstice-in-leipzig-germany-small-nice-very-low-cost", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:39.606Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iMG3TiD6PDFE3F88a/3-day-solstice-in-leipzig-germany-small-nice-very-low-cost", "pageUrlRelative": "/posts/iMG3TiD6PDFE3F88a/3-day-solstice-in-leipzig-germany-small-nice-very-low-cost", "linkUrl": "https://www.lesswrong.com/posts/iMG3TiD6PDFE3F88a/3-day-solstice-in-leipzig-germany-small-nice-very-low-cost", "postedAtFormatted": "Thursday, October 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%203-day%20Solstice%20in%20Leipzig%2C%20Germany%3A%20small%2C%20nice%2C%20very%20low%20cost%2C%20includes%20accommodation%2C%2019th-21st%20Dec&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A3-day%20Solstice%20in%20Leipzig%2C%20Germany%3A%20small%2C%20nice%2C%20very%20low%20cost%2C%20includes%20accommodation%2C%2019th-21st%20Dec%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiMG3TiD6PDFE3F88a%2F3-day-solstice-in-leipzig-germany-small-nice-very-low-cost%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=3-day%20Solstice%20in%20Leipzig%2C%20Germany%3A%20small%2C%20nice%2C%20very%20low%20cost%2C%20includes%20accommodation%2C%2019th-21st%20Dec%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiMG3TiD6PDFE3F88a%2F3-day-solstice-in-leipzig-germany-small-nice-very-low-cost", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiMG3TiD6PDFE3F88a%2F3-day-solstice-in-leipzig-germany-small-nice-very-low-cost", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 500, "htmlBody": "<p>Hi everyone,</p>\n<p>like last year, we'll have a Secular Solstice in Leipzig, Germany. You're invited - message me if you'd like to attend.</p>\n<p>We have space for about 25 people. So this isn't a huge event like you'd have in NYC - but it is special in a different way, because it goes Friday to Sunday and involves lots of things to do. We have a big and very nice appartment in the center of Leipzig where lots of people can sleep, so spreading this over several days is easy, and an obvious way to kick it up a notch from last year's event.</p>\n<p>We'll do some of the beautiful ceremonial pieces and songs from Raymonds Hymnal and ride the same general vide. And on top of that, we'll do freestyle, participatory work in groups where we design ways to celebrate the Solstice, using an <a href=\"http://en.wikipedia.org/wiki/Open_Space_Technology\">Open Space Technology</a> inspired method. After all, we're only getting things started, and surely there are many kinds of celebration to explore. Lets find some of them, try them out together and by comparing effects, help optimize Secular Solstices!</p>\n<p>We'll cook together and share the cost for ingredients and drinks - apart from that the event is free. Up to 18 guests can sleep right on the premises - half of them on comfortable beds and mattresses, the rest needs to bring sleeping bags and camping mats. If you really prefer a single or double room, there are fairly cheap hotels and hostels nearby, message me for assistance if necessary.</p>\n<h3>The outline<br /></h3>\n<p>Arrivals are Friday 6pm-7:30. We'll have a welcome round and a few things to get us in the mood, then discuss ideas for Solstice activities to explore together. We'll find the most popular ones and get into groups that design them into something they want to share with everyone. Groups should self-organize fairly fluidly, i.e. you can switch groups, steal ideas from each other etc. and get to know each other in the process. So this will basically be a very social evening of preparation for the next day. Also, cooking.</p>\n<p>On Saturday we will meet in the morning to plan the day, spend some time decorating and cooking, shopping for stuff groups have found they need to do their things, and probably rehearsing. Groups who are done preparing their thing will in some cases probably prepare another, because that is just what happens. We'll have time to chat and get to know each other better. The ceremonial part starts at sunset and is expected to take several hours. After that we'll party - some people will probably want to stay up all night and welcome the sunrise just like last year.</p>\n<p>On Sunday we'll have less cohesion probably, because of high variance in how much people have slept. Still we should be able to come together for feedback discussion, have a nice closing, clean up a bit, and say farewell. If you need more sleep before you get on the road, you're welcome to have it.</p>\n<p>Any questions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iMG3TiD6PDFE3F88a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 17, "extendedScore": null, "score": 6.9e-05, "legacy": true, "legacyId": "27338", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-09T16:41:09.908Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Noticing continued, Creativity", "slug": "meetup-urbana-champaign-noticing-continued-creativity", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6S5u99q379w7njpu3/meetup-urbana-champaign-noticing-continued-creativity", "pageUrlRelative": "/posts/6S5u99q379w7njpu3/meetup-urbana-champaign-noticing-continued-creativity", "linkUrl": "https://www.lesswrong.com/posts/6S5u99q379w7njpu3/meetup-urbana-champaign-noticing-continued-creativity", "postedAtFormatted": "Thursday, October 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Noticing%20continued%2C%20Creativity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Noticing%20continued%2C%20Creativity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6S5u99q379w7njpu3%2Fmeetup-urbana-champaign-noticing-continued-creativity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Noticing%20continued%2C%20Creativity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6S5u99q379w7njpu3%2Fmeetup-urbana-champaign-noticing-continued-creativity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6S5u99q379w7njpu3%2Fmeetup-urbana-champaign-noticing-continued-creativity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/15i'>Urbana-Champaign: Noticing continued, Creativity</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 October 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">206 S. Cedar St, Urbana, IL, 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's reconvene on noticing. We may have to go a little meta, but ah well.</p>\n\n<p>I'd also like to try some group creativity exercises, for great justice.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/15i'>Urbana-Champaign: Noticing continued, Creativity</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6S5u99q379w7njpu3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.0715066323575613e-06, "legacy": true, "legacyId": "27339", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Noticing_continued__Creativity\">Discussion article for the meetup : <a href=\"/meetups/15i\">Urbana-Champaign: Noticing continued, Creativity</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 October 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">206 S. Cedar St, Urbana, IL, 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's reconvene on noticing. We may have to go a little meta, but ah well.</p>\n\n<p>I'd also like to try some group creativity exercises, for great justice.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Noticing_continued__Creativity1\">Discussion article for the meetup : <a href=\"/meetups/15i\">Urbana-Champaign: Noticing continued, Creativity</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Noticing continued, Creativity", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Noticing_continued__Creativity", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Noticing continued, Creativity", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Noticing_continued__Creativity1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-09T19:24:15.861Z", "modifiedAt": null, "url": null, "title": "Happiness Logging: One Year In", "slug": "happiness-logging-one-year-in", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:31.308Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c3dLFn25rPhWEXztX/happiness-logging-one-year-in", "pageUrlRelative": "/posts/c3dLFn25rPhWEXztX/happiness-logging-one-year-in", "linkUrl": "https://www.lesswrong.com/posts/c3dLFn25rPhWEXztX/happiness-logging-one-year-in", "postedAtFormatted": "Thursday, October 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Happiness%20Logging%3A%20One%20Year%20In&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHappiness%20Logging%3A%20One%20Year%20In%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc3dLFn25rPhWEXztX%2Fhappiness-logging-one-year-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Happiness%20Logging%3A%20One%20Year%20In%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc3dLFn25rPhWEXztX%2Fhappiness-logging-one-year-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc3dLFn25rPhWEXztX%2Fhappiness-logging-one-year-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 987, "htmlBody": "<p>I've been logging my happiness for <a href=\"http://www.jefftk.com/p/happiness-logging\">a year now</a>. [1]  My phone notifies me at unpredictable intervals, and I respond with some tags.  For example, if it pinged me now, I would enter \"6 home bed computer blog\".  I always have a numeric tag for my current happiness, and then additional tags for where I am, what I'm doing, and who I'm with.  So: what's working, what's not?</p>\n<p>When I first started rating my happiness on a 1-10 scale I didn't feel like I was very good at it. At the time I thought I might get better with practice, but I think I'm actually getting worse at it.  Instead of really thinking \"how do I feel right now?\" it's really hard not to just think \"in past situations like this I've put down '6' so I should put down '6' now\".</p>\n<p>Being honest to myself like this can also make me less happy. Normally if I'm negative about something I try not to dwell on it.  I don't think about it, and soon I'm thinking about other things and not so negative.  Logging that I'm unhappy makes me own up to being unhappy, which I think doesn't help.  Though it's hard to know because any other sort of measurement would seem to have the same problem.</p>\n<p>There's also a sampling issue.  I don't have my phone ping me during the night, because I don't want it to wake me up.  Before having a kid this worked properly: I'd plug in my phone, which turns off pings, promptly fall asleep, wake up in the morning, unplug my phone.  Now, though, my sleep is generally interrupted several times a night. Time spent waiting to see if the baby falls back asleep on her own, or soothing her back to sleep if she doesn't, or lying awake at 4am because it's hard to fall back asleep when you've had 7hr and just spent an hour walking around and bouncing the baby; none of these are counted.  On the whole, these experiences are much less enjoyable than my average; if the baby started sleeping through the night such that none of these were needed anymore I wouldn't see that as a loss at all.  Which means my data is biased upward.  I'm curious how happiness sampling studies have handled this; people with insomnia would be in a similar situation.</p>\n<p>Another sampling issue is that I don't always notice when I get a ping.  For the brief period when I was <a href=\"http://www.jefftk.com/p/moto-360-review\">wearing a smartwatch</a> I was consistently noticing all my pings but now I'm back to where I sometimes miss the vibration.  I usually fill out these pings retroactively if it's only been a few minutes and I'm confident that I remember how I felt and what I was doing.  I haven't been tagging these pings separately, but now that I think of it I'm going to add an \"r\" tag for retroactive responses.</p>\n<p>Responding to pings when other people are around can also be tricky. For a while there were some people who would try and peek and see what I was writing, and I wasn't sure whether I should let them see.  I ended up deciding that while having all the data eventally end up <a href=\"http://www.jefftk.com/tagtime.log\">public</a> was fine, filling it out in the moment needed to be private so I wouldn't be swayed by wanting to indicate things to the people around me.</p>\n<p>The app I'm using isn't perfect, but it's pretty good.  Entering new tags is a little annoying, and every time I back up the pings it forgets my past tags.  The manual backup step also led to some missing data&mdash;all of September 2014 and some of August&mdash;because my phone died.  This logging data is the only thing on my phone that isn't automatically backed up to the cloud, so when my phone died a few weeks ago I lost the last month of pings. [2] So now there's a <a href=\"http://www.jefftk.com/happiness_graph\">gap in the graph</a>.</p>\n<p>While I'm not that confident in my numeric reports, I'm much more confident in the other tags that indicate what I'm doing at various times.  If I'm on the computer I very reliably tag 'computer', etc. I haven't figured out what to do with this data yet, but it should be interesting for tracking behavior chages over time.  One thing I remember doing is switching from wasting time on my computer to on my phone; let's see what that looked like:</p>\n<p><img src=\"http://www.jefftk.com/timewasting-by-device.png\" alt=\"\" /></p>\n<p>I don't remember why the big drop in computer use at the end of February 2014 happened.  I assumed at first it was having a baby, after which I spent a lot of time reading on my phone while she was curled up on me, but that wasn't until a month later.  I think this may have been when I realized that I didn't hate the facebook app on my phone afterall?  I'm not sure.  The second drop in both phone- and computer-based timewasting, the temporary one in July 2014, was my being in England.  My phone had <a href=\"https://www.facebook.com/jefftk/posts/665944384992\">internet</a> but my computer usually didn't.  And there was generally much more interesting stuff going on around me than my phone.</p>\n<p>Overall my experience with logging has made me put less trust in \"how happy are you right now\" surveys of happiness.  Aside from the practical issues like logging unexpected night wake-time, I mostly don't feel like the numbers I'm recording are very meaningful.  I would rather spend more time in situations I label higher than lower on average, so there is some signal there, but I don't actually have the introspection to accurately report to myself how I'm feeling.</p>\n<p><small><em>I also posted this <a href=\"http://www.jefftk.com/p/happiness-logging-one-year-in\">on my blog</a>.</em></small></p>\n<p><br /> [1] First ping was 2013.10.08 06:31:41, a year ago yesterday.</p>\n<p>[2] Well, it was more my fault than that.  The phone was partly     working and I did a factory reset to see if that would fix it (it     didn't) and I forgot to back up pings first.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c3dLFn25rPhWEXztX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 25, "extendedScore": null, "score": 9.8e-05, "legacy": true, "legacyId": "27341", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-10T00:47:34.969Z", "modifiedAt": null, "url": null, "title": "Meetup : Bangalore Meetup", "slug": "meetup-bangalore-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:03.342Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "anandjeyahar", "createdAt": "2011-08-06T14:22:03.370Z", "isAdmin": false, "displayName": "anandjeyahar"}, "userId": "nRE6w4ErwoFMLnAfg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i6CLvXFoqcpwrYdeL/meetup-bangalore-meetup", "pageUrlRelative": "/posts/i6CLvXFoqcpwrYdeL/meetup-bangalore-meetup", "linkUrl": "https://www.lesswrong.com/posts/i6CLvXFoqcpwrYdeL/meetup-bangalore-meetup", "postedAtFormatted": "Friday, October 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bangalore%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bangalore%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi6CLvXFoqcpwrYdeL%2Fmeetup-bangalore-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bangalore%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi6CLvXFoqcpwrYdeL%2Fmeetup-bangalore-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi6CLvXFoqcpwrYdeL%2Fmeetup-bangalore-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/15j'>Bangalore Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 November 2014 04:10:51PM (+0530)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">cafe coffee day, old airport road ,Bengaluru</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I've registered and created a meetup.com account and a group there. For the first meetup am thinking of Nov.14. (Friday evening). If you would prefer a different date, please post on the meetup group. <a href=\"http://www.meetup.com/Bangalore-LessWrongers-Meetup/events/212529792/\" rel=\"nofollow\">http://www.meetup.com/Bangalore-LessWrongers-Meetup/events/212529792/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/15j'>Bangalore Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i6CLvXFoqcpwrYdeL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 2.0724338297308696e-06, "legacy": true, "legacyId": "27342", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bangalore_Meetup\">Discussion article for the meetup : <a href=\"/meetups/15j\">Bangalore Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 November 2014 04:10:51PM (+0530)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">cafe coffee day, old airport road ,Bengaluru</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I've registered and created a meetup.com account and a group there. For the first meetup am thinking of Nov.14. (Friday evening). If you would prefer a different date, please post on the meetup group. <a href=\"http://www.meetup.com/Bangalore-LessWrongers-Meetup/events/212529792/\" rel=\"nofollow\">http://www.meetup.com/Bangalore-LessWrongers-Meetup/events/212529792/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bangalore_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/15j\">Bangalore Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bangalore Meetup", "anchor": "Discussion_article_for_the_meetup___Bangalore_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Bangalore Meetup", "anchor": "Discussion_article_for_the_meetup___Bangalore_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-10T11:24:41.479Z", "modifiedAt": null, "url": null, "title": "Informing Others: Maximally Efficient?", "slug": "informing-others-maximally-efficient", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:07.468Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Regex", "createdAt": "2014-10-04T05:18:04.115Z", "isAdmin": false, "displayName": "Regex"}, "userId": "ZvShrmxG8GXJCAvP8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SrcHzFrfuxoxtwfkZ/informing-others-maximally-efficient", "pageUrlRelative": "/posts/SrcHzFrfuxoxtwfkZ/informing-others-maximally-efficient", "linkUrl": "https://www.lesswrong.com/posts/SrcHzFrfuxoxtwfkZ/informing-others-maximally-efficient", "postedAtFormatted": "Friday, October 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Informing%20Others%3A%20Maximally%20Efficient%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInforming%20Others%3A%20Maximally%20Efficient%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSrcHzFrfuxoxtwfkZ%2Finforming-others-maximally-efficient%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Informing%20Others%3A%20Maximally%20Efficient%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSrcHzFrfuxoxtwfkZ%2Finforming-others-maximally-efficient", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSrcHzFrfuxoxtwfkZ%2Finforming-others-maximally-efficient", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 689, "htmlBody": "<p>I want to help the world as much as possible: lets define that as reduction of suffering<sup>1</sup> of the human race. <sup></sup></p>\n<p>I am an engineering student. It would be very simple to spend the rest of my life working on engineering problems, thereby making humanity more effective, and so reducing suffering: Pretty acceptable in terms of benefit to the world, and also given my skill set it makes sense. The issue here is that it may be a local maximum.</p>\n<p>Now, what if instead of directly solving problems I focus my efforts on other people? Directing others down a more rational path. This may include things like telling people about Less Wrong, specifically addressing their direction in life, or providing information about effective charities instead of ineffective ones. This seems like a massively more efficient solution space, even if my social skills are weak. Compared to the effectiveness of a single engineer, the life paths of hundreds of people with improved rationality would be enormous.</p>\n<p>Here I make the assumption that more rational thought reduces  suffering. I would argue this to be the case because a less rational  thinker is going to act in more conflicting, arbitrary, and harmful ways  than otherwise. Clearer thinking then means better judgements, and we also  say that on average people's actions improve society as  evidenced by society progressing to this point.</p>\n<p>Still, directing others is not particularly efficient for me considering my skill set. This task should be delegated to another with a skill set more suited: a people person. This would then allow me to get back to my own efficient problems. But here is the question: How many \"people people\" is it efficient to train? Isn't my time still better spent as an ineffective people person even above my direct problem solving even at that point? If so, shouldn't we all be focusing on expanding our numbers?</p>\n<p>By doing so will we increase the quality of our discussions by bringing in many new ideas and faces, or will more people hinder quality? Should we instead simply delve deeper into the depths of rationality growing only as people discover the site organically? What is the ideal community growth rate? (quality versus quantity? If Less Wrong expands too quickly is that an issue?)</p>\n<p>We could also produce some kind of rationality book or other set of materials if we intended to improve rationality without inherently expanding the community by linking them. (Anyone super interested would go looking for more, and we'd definitely want them.)</p>\n<p>I may be overestimating how much other people's lives can be influenced, but it seems a high probability that after even a year of dedicated... irrationality sniping?... on even a bimonthly basis (every other Saturday lets say) would have a profound effect on at least one other individual. Especially given the internet's connective ability it seems that the ability to improve the cognitions of other people is cheap and high impact compared to direct problem solving.</p>\n<p>If I am not solving those engineering problems will someone else be? The question then becomes how much I value the reduction of the world's suffering versus my other values rather than the actual effectiveness of irrationality sniping versus direct problem solving.</p>\n<p>Conclusion: To reduce world suffering I should reduce the irrationality of others, then convince those persons to do the same.</p>\n<p>&nbsp;</p>\n<p>Should there be a lesson plan of sorts for aspiring rationalists? There are the sequences, and people interested enough will find their own way around the site, but I am wondering if there is some more directed thought process we want people to go through, or if perhaps randomly encountering ideas might actually be better, especially if we are trying to expand rationality in all directions.</p>\n<p>Regardless of the answers to these questions I intend to print out flyers and scatter them across my college campus. Is  there already a resource like that I can just print off? I figure even if I drop off the face of the earth I'll have done some good that way.</p>\n<p>&nbsp;</p>\n<p><sub>1. Without uh, killing everyone or tiling smiley faces.</sub></p>\n<p>ps. Sorry if this comes off as rambling. No idea what I am doing. Never stopped me before.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SrcHzFrfuxoxtwfkZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 2.073649381494644e-06, "legacy": true, "legacyId": "27344", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-10T12:24:54.905Z", "modifiedAt": null, "url": null, "title": "Improving the World", "slug": "improving-the-world", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:02.981Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nBEoTQbRsZazLqx39/improving-the-world", "pageUrlRelative": "/posts/nBEoTQbRsZazLqx39/improving-the-world", "linkUrl": "https://www.lesswrong.com/posts/nBEoTQbRsZazLqx39/improving-the-world", "postedAtFormatted": "Friday, October 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Improving%20the%20World&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AImproving%20the%20World%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnBEoTQbRsZazLqx39%2Fimproving-the-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Improving%20the%20World%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnBEoTQbRsZazLqx39%2Fimproving-the-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnBEoTQbRsZazLqx39%2Fimproving-the-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<p>What are we doing to make this world a <a href=\"/lw/etf/firewalling_the_optimal_from_the_rational/\">better</a> (epistemically or instrumentally) place?</p>\n<p>Some answers to this question are already written in <a href=\"http://wiki.lesswrong.com/wiki/Special_threads\">Bragging Threads</a> and other places, but I think they deserve a special emphasis. I think that many smart people are focused on improving themselves, which is a good thing in a long run, but sometimes the world needs some help <em>right now</em>. (Also, there is the failure mode of learning a lot about something, and then actually not applying that knowledge in real life.) Becoming stronger so you can create more good in the future is about the good you will create in the future; but what good are you creating right <em>now</em>?</p>\n<p>&nbsp;</p>\n<p>Rules:</p>\n<p>Top-level comments are the things you are doing right now (not merely planning to do once) to improve the world... or a part of the world... or your neighborhood... or simply any small part of the world <em>other than only yourself</em>.</p>\n<p>Meta debates go under the \"META\" comment.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nBEoTQbRsZazLqx39", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 2.07376434883005e-06, "legacy": true, "legacyId": "27345", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WbLAA8qZQNdbRgKte"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-10T13:34:35.953Z", "modifiedAt": null, "url": null, "title": "Please recommend some audiobooks", "slug": "please-recommend-some-audiobooks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:34.598Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Delta", "createdAt": "2012-08-01T11:37:41.645Z", "isAdmin": false, "displayName": "Delta"}, "userId": "DT22husLNHbuy8TaZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wde7ARbuwec6ndaJW/please-recommend-some-audiobooks", "pageUrlRelative": "/posts/wde7ARbuwec6ndaJW/please-recommend-some-audiobooks", "linkUrl": "https://www.lesswrong.com/posts/wde7ARbuwec6ndaJW/please-recommend-some-audiobooks", "postedAtFormatted": "Friday, October 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Please%20recommend%20some%20audiobooks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APlease%20recommend%20some%20audiobooks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwde7ARbuwec6ndaJW%2Fplease-recommend-some-audiobooks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Please%20recommend%20some%20audiobooks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwde7ARbuwec6ndaJW%2Fplease-recommend-some-audiobooks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwde7ARbuwec6ndaJW%2Fplease-recommend-some-audiobooks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<p>Hi All,</p>\n<p>I've got into audiobooks lately and have been enjoying listening to David Fitzgerald's Nailed! and his Heretics Guide to mormonism, along with Greta Christina's \"Why Are You Atheists So Angry?\" and Laura Bates's \"Everyday Sexism\" which were all very good. I was wondering what other illuminating and engaging books might be recommended, ideally ones available as audiobooks on audible.</p>\n<p>I've already read The Selfish Gene, The God Delusion and God Is Not Great in book form as well, so it might be time for something not specifically religion-related, unless it has some interesting new angle.</p>\n<p>After Nailed and Everyday Sexism were really illuminating I'm now thinking there must be lots of other must-read books out there and wondered what people here might recommend. Any suggestions would be appreciated.</p>\n<p><br />Thanks for your time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wde7ARbuwec6ndaJW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "27346", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-10T14:45:45.573Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington, D.C.: Current Events Schelling Point [To-Do List Hacking Postponed]", "slug": "meetup-washington-d-c-current-events-schelling-point-to-do", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinZ", "createdAt": "2009-07-08T20:34:05.168Z", "isAdmin": false, "displayName": "RobinZ"}, "userId": "eTMojvi4f2z3pDfsc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BzDJxndTBmwonCErc/meetup-washington-d-c-current-events-schelling-point-to-do", "pageUrlRelative": "/posts/BzDJxndTBmwonCErc/meetup-washington-d-c-current-events-schelling-point-to-do", "linkUrl": "https://www.lesswrong.com/posts/BzDJxndTBmwonCErc/meetup-washington-d-c-current-events-schelling-point-to-do", "postedAtFormatted": "Friday, October 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%2C%20D.C.%3A%20Current%20Events%20Schelling%20Point%20%5BTo-Do%20List%20Hacking%20Postponed%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%2C%20D.C.%3A%20Current%20Events%20Schelling%20Point%20%5BTo-Do%20List%20Hacking%20Postponed%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBzDJxndTBmwonCErc%2Fmeetup-washington-d-c-current-events-schelling-point-to-do%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%2C%20D.C.%3A%20Current%20Events%20Schelling%20Point%20%5BTo-Do%20List%20Hacking%20Postponed%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBzDJxndTBmwonCErc%2Fmeetup-washington-d-c-current-events-schelling-point-to-do", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBzDJxndTBmwonCErc%2Fmeetup-washington-d-c-current-events-schelling-point-to-do", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/15k'>Washington, D.C.: Current Events Schelling Point [To-Do List Hacking Postponed]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 October 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><em>Edit: The to-do list hacking is postponed due to illness. The new meeting topic is described below.</em></p>\n\n<p>We will be meeting in the Kogod Courtyard of the National Portrait Gallery (8th and F Sts or 8th and G Sts NW, go straight past the information desk from either entrance). As usual, the period from 3:00 to 3:30 shall be designated for congregating, with conversations to start at 3:30.</p>\n\n<p>Because current news events have not often been the subject of discussion at Less Wrong D.C. meetups, this meeting is designated as a convenient time for people to bring these up. That said, conversation on any topic of interest to attendees is both permitted and encouraged.</p>\n\n<p>Future meetups:</p>\n\n<ul>\n<li>Oct. 19: Mini Talks</li>\n<li>Oct. 26: Create &amp; Complete (tentative)</li>\n<li>Nov. 2: Fun &amp; Games</li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/15k'>Washington, D.C.: Current Events Schelling Point [To-Do List Hacking Postponed]</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BzDJxndTBmwonCErc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.0740332652808323e-06, "legacy": true, "legacyId": "27348", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington__D_C___Current_Events_Schelling_Point__To_Do_List_Hacking_Postponed_\">Discussion article for the meetup : <a href=\"/meetups/15k\">Washington, D.C.: Current Events Schelling Point [To-Do List Hacking Postponed]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 October 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><em>Edit: The to-do list hacking is postponed due to illness. The new meeting topic is described below.</em></p>\n\n<p>We will be meeting in the Kogod Courtyard of the National Portrait Gallery (8th and F Sts or 8th and G Sts NW, go straight past the information desk from either entrance). As usual, the period from 3:00 to 3:30 shall be designated for congregating, with conversations to start at 3:30.</p>\n\n<p>Because current news events have not often been the subject of discussion at Less Wrong D.C. meetups, this meeting is designated as a convenient time for people to bring these up. That said, conversation on any topic of interest to attendees is both permitted and encouraged.</p>\n\n<p>Future meetups:</p>\n\n<ul>\n<li>Oct. 19: Mini Talks</li>\n<li>Oct. 26: Create &amp; Complete (tentative)</li>\n<li>Nov. 2: Fun &amp; Games</li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington__D_C___Current_Events_Schelling_Point__To_Do_List_Hacking_Postponed_1\">Discussion article for the meetup : <a href=\"/meetups/15k\">Washington, D.C.: Current Events Schelling Point [To-Do List Hacking Postponed]</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington, D.C.: Current Events Schelling Point [To-Do List Hacking Postponed]", "anchor": "Discussion_article_for_the_meetup___Washington__D_C___Current_Events_Schelling_Point__To_Do_List_Hacking_Postponed_", "level": 1}, {"title": "Discussion article for the meetup : Washington, D.C.: Current Events Schelling Point [To-Do List Hacking Postponed]", "anchor": "Discussion_article_for_the_meetup___Washington__D_C___Current_Events_Schelling_Point__To_Do_List_Hacking_Postponed_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-10T14:58:20.761Z", "modifiedAt": null, "url": null, "title": "Cryonics in Europe?", "slug": "cryonics-in-europe", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:02.857Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "roland", "createdAt": "2009-02-27T23:03:47.279Z", "isAdmin": false, "displayName": "roland"}, "userId": "p2C9rpg32LHrGwer8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CG2PDqfjwHLWFcgy2/cryonics-in-europe", "pageUrlRelative": "/posts/CG2PDqfjwHLWFcgy2/cryonics-in-europe", "linkUrl": "https://www.lesswrong.com/posts/CG2PDqfjwHLWFcgy2/cryonics-in-europe", "postedAtFormatted": "Friday, October 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryonics%20in%20Europe%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryonics%20in%20Europe%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCG2PDqfjwHLWFcgy2%2Fcryonics-in-europe%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryonics%20in%20Europe%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCG2PDqfjwHLWFcgy2%2Fcryonics-in-europe", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCG2PDqfjwHLWFcgy2%2Fcryonics-in-europe", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<p>What are the best options for cryonics in Europe?</p>\n<p>AFAIK the best option is still to use one of the US providers(e.g. Alcor) and arrange for transportation. There is a problem with this though, in that until you arrive in the US your body will be cooled with dry ice which will cause huge&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">ischemic damage.</span></p>\n<p>Questions:</p>\n<ol>\n<li>How critical is the ischemic damage? If I interpret this <a href=\"/lw/bk6/alcor_vs_cryonics_institute/6dow\">comment</a>&nbsp;by Eliezer correctly we shouldn't worry about this damage if we consider future technology.</li>\n<li>Is there a way to have adequate cooling here in Europe until you arrive at the US for final storage?</li>\n</ol>\n<div><br /></div>\n<div>There is also <a href=\"http://kriorus.ru/en\">KrioRus</a>, a&nbsp;Russian cryonics company, they seem to offer an option of <a href=\"http://kriorus.ru/en/Other-services\">cryo transportation</a> but I don't know how trustworthy they are.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CG2PDqfjwHLWFcgy2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 23, "extendedScore": null, "score": 7.7e-05, "legacy": true, "legacyId": "27347", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-10T17:38:08.666Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-12", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hJeKk666LAm2bgazY/weekly-lw-meetups-12", "pageUrlRelative": "/posts/hJeKk666LAm2bgazY/weekly-lw-meetups-12", "linkUrl": "https://www.lesswrong.com/posts/hJeKk666LAm2bgazY/weekly-lw-meetups-12", "postedAtFormatted": "Friday, October 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhJeKk666LAm2bgazY%2Fweekly-lw-meetups-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhJeKk666LAm2bgazY%2Fweekly-lw-meetups-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhJeKk666LAm2bgazY%2Fweekly-lw-meetups-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 579, "htmlBody": "<p><strong>This summary was posted to LW Main on October 3rd. The following week's summary is <a href=\"/lw/l3p/new_lw_meetup_bangalore/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/14v\">Hasselt Meetup: Brussels moves to Hasselt this month!:&nbsp;<span class=\"date\">11 October 2014 01:00PM</span></a></li>\n</ul>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/zr\">Houston, TX:&nbsp;<span class=\"date\">18 October 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/14l\">Perth, Australia: Games night:&nbsp;<span class=\"date\">07 October 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/154\">Portland, OR: Improv for Rationalists:&nbsp;<span class=\"date\">18 October 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/158\">Urbana-Champaign: Noticing.:&nbsp;<span class=\"date\">05 October 2014 05:30PM</span></a></li>\n<li><a href=\"/meetups/13u\">Utrecht: Effective Altruism and Politics:&nbsp;<span class=\"date\">05 October 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13v\">Utrecht: Artificial Intelligence:&nbsp;<span class=\"date\">19 October 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/13w\">Utrecht: Climate Change:&nbsp;<span class=\"date\">02 November 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/155\">Yale rationality group meeting.:&nbsp;<span class=\"date\">05 October 2014 02:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<div id=\"siteTable\" class=\"sitetable\" style=\"clear: none;\">\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">04 October 2025 01:30PM</span></a></li>\n<li><a href=\"/meetups/157\">[Cambridge MA] Meta Meetup:&nbsp;<span class=\"date\">05 October 2014 03:30AM</span></a></li>\n<li><a href=\"/meetups/156\">Canberra: Contrarianism:&nbsp;<span class=\"date\">11 October 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/150\">[Melbourne] October Rationality Dojo - Non-Violent Communication:&nbsp;<span class=\"date\">05 October 2014 03:30PM</span></a><a href=\"/meetups/14z\"></a></li>\n<li><a href=\"/meetups/151\">Sydney Rationality Dojo - Urge Propagation:&nbsp;<span class=\"date\">05 October 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/159\">Washington, D.C.: Fun &amp; Games:&nbsp;<span class=\"date\">05 October 2014 03:00PM</span></a></li>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Buffalo.2C_NY\">Buffalo</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Moscow.2C_Russia\">Moscow</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong>&nbsp;</strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Sydney\">Sydney</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> </strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tel_Aviv.2C_Israel\">Tel Aviv</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Warsaw.2C_Poland\">Warsaw</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hJeKk666LAm2bgazY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 2.07436248636021e-06, "legacy": true, "legacyId": "27305", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6Jr8CeQGovE7nb7re", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-10T17:55:45.225Z", "modifiedAt": null, "url": null, "title": "Solstice 2014 - Kickstarter and Megameetup", "slug": "solstice-2014-kickstarter-and-megameetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:32.860Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CNGzKea2nbzixpxcd/solstice-2014-kickstarter-and-megameetup", "pageUrlRelative": "/posts/CNGzKea2nbzixpxcd/solstice-2014-kickstarter-and-megameetup", "linkUrl": "https://www.lesswrong.com/posts/CNGzKea2nbzixpxcd/solstice-2014-kickstarter-and-megameetup", "postedAtFormatted": "Friday, October 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Solstice%202014%20-%20Kickstarter%20and%20Megameetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASolstice%202014%20-%20Kickstarter%20and%20Megameetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCNGzKea2nbzixpxcd%2Fsolstice-2014-kickstarter-and-megameetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Solstice%202014%20-%20Kickstarter%20and%20Megameetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCNGzKea2nbzixpxcd%2Fsolstice-2014-kickstarter-and-megameetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCNGzKea2nbzixpxcd%2Fsolstice-2014-kickstarter-and-megameetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1085, "htmlBody": "<p><a href=\"https://www.kickstarter.com/projects/244974495/secular-solstice-2014\"><img src=\"http://photos4.meetupstatic.com/photos/event/c/d/c/0/600_417892672.jpeg\" alt=\"\" width=\"600\" height=\"450\" align=\"center\" /></a></p>\n<pre><br /></pre>\n<h2><strong>Summary:</strong></h2>\n<ul>\n<li>We're running another&nbsp;<a href=\"https://www.kickstarter.com/projects/244974495/secular-solstice-2014\">Winter Solstice kickstarter&nbsp;</a>- this is to fund the venue, musicians, food, drink and decorations for a big event in NYC on December 20th, as well as to record more music and print a larger run of the Solstice Book of Traditions.&nbsp;</li>\n</ul>\n<ul>\n<li>There will also be a Rationality Megameetup the following Sunday. (More generally, people are welcome to come to Highgarden and spend the night starting on Friday - although we request that people <a href=\"https://docs.google.com/forms/d/1QRjjZwklXtPEFNZrSPKjvb2_n8_ytOy5N6ry_lGedro/viewform?usp=send_form\">register their presence via this form so we don't run out of space</a>)</li>\n</ul>\n<ul>\n<li>I'd also like to raise additional money so I can focus full time for the next couple months on helping other communities run their own version of the event, tailored to meet their particular needs while still feeling like part of a cohesive, broader movement - and giving the attendees a genuinely powerful experience.&nbsp;</li>\n</ul>\n<pre><br /></pre>\n<h2><strong>The Beginning</strong></h2>\n<p><em>Four years ago, twenty NYC rationalists gathered in a room to celebrate the Winter Solstice. We sang songs and told stories about things that seemed very important to us. The precariousness of human life. The thousands of years of labor and curiosity that led us from a dangerous stone age to the modern world. The potential to create something even better, if humanity can get our act together and survive long enough.</em></p>\n<p><em>One of the most important ideas we honored was the importance of facing truths, even when they are uncomfortable or make us feel silly or are outright terrifying. Over the evening, we gradually extinguished candles, acknowledging harsher and harsher elements of reality.</em></p>\n<p><em>Until we sat in absolute darkness - aware that humanity is flawed, and alone, in an unforgivingly neutral universe.&nbsp;</em></p>\n<p><em>But also aware that we sit beside people who care deeply about truth, and about our future. Aware that across the world, people are working to give humanity a bright tomorrow, and that we have the power to help. Aware that across history, people have looked impossible situations in the face, and through ingenuity and persperation, made the impossible happen.</em></p>\n<p><em>That seemed worth celebrating.&nbsp;</em></p>\n<pre><br /></pre>\n<h2><strong>The Story So Far</strong></h2>\n<p>As it turned out, this resonated with people outside the rationality community. When we ran the event again in 2012, non-religious but non-Less Wrong attended the event and told me they found it very moving. In 2013, we pushed it much larger - I ran a kickstarter campaign to fund a big event in NYC.&nbsp;</p>\n<p>A hundred and fifty people from various communities attended. From Less Wrong in particular, we had groups from Boston, San Francisco, North Carolina, Ottawa, and Ohio among other places. The following day was one of the largest East Coast Megameetups.&nbsp;</p>\n<p>Meanwhile, in the Bay Area, several people put together an event that gathered around 80 attendees. In Boston and Vancouever and Leipzig Germany, people ran smaller events. This is shaping up to take root as a legitimate holiday, celebrating human history and our potential future.</p>\n<p>This year, we want to do that all again. I also want to dedicate more time to helping other people run their events. Getting people to start celebrating a new holiday is a tricky feat. I've learned a lot about how to go about that and want to help others run polished events that feel connecting and inspirational.</p>\n<pre><br /></pre>\n<h2>So, what's happening, and how can you help?</h2>\n<p>&nbsp;</p>\n<ul>\n<li>The Big Solstice itself will be Saturday, December 20th at 7:00 PM. To fund it, we're aiming to raise $7500 on kickstarter. This is enough to fund the aforementioned venue, food, drink, live musicians, record new music, and print a larger run of the Solstice Book of Traditions. It'll also pay some expenses for the Megameetup. <a href=\"https://www.kickstarter.com/projects/244974495/secular-solstice-2014\">Please consider contributing to the kickstarter</a>.</li>\n</ul>\n<ul>\n<li>If you'd like to host your own Solstice (either a large or a private one) and would like advice, please contact me at raemon777@gmail.com and we'll work something out.</li>\n</ul>\n<ul>\n<li>There will also be Solstices (of varying sizes) run by Less Wrong / EA folk held in the Bay Area, Seattle, Boston and <a href=\"/r/discussion/lw/l3e/3day_solstice_in_leipzig_germany_small_nice_very/\">Leipzig</a>. (There will probably be a larger but non-LW-centered Solstice in Los Angeles and Boston as well). \n<ul>\n<li>If you'd like to be involved with the Seattle solstice, contact Jai at&nbsp;<span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">jai</span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13.3333339691162px;\">@</span><a style=\"color: #1155cc; font-family: arial, sans-serif; font-size: 13.3333339691162px;\" href=\"http://seattlesecularsolstice.com/\" target=\"_blank\">seattlesecularsolstice.com</a>. If you</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>In NYC, there will be a Rationality and EA Megameetup running from Friday, Dec 19th through Sunday evening. \n<ul>\n<li>Friday night and Saturday morning: Arrival, Settling</li>\n<li>Saturday at 2PM - 4:30PM: Unconference (20 minute talks, workshops or discussions)</li>\n<li>Saturday at 7PM: Big Solstice</li>\n<li>Sunday at Noon: Unconference 2</li>\n<li>Sunday at 2PM: Strategic New Years Resolution Planning</li>\n<li>Sunday at 3PM: Discussion of creating private ritual for individual communities</li>\n</ul>\n</li>\n<li>If you're interested in coming to the Megameetup, <a href=\"https://docs.google.com/forms/d/1QRjjZwklXtPEFNZrSPKjvb2_n8_ytOy5N6ry_lGedro/viewform\">please fill out this form</a> saying how many people you're bringing, whether you're interested in giving a talk, and whether you're bringing a vehicle, so we can plan adequately. (We have lots of crash space, but not infinite bedding, so bringing sleeping bags or blankets would be helpful)</li>\n</ul>\n<div><br /></div>\n<h2>Effective Altruism?</h2>\n<p>&nbsp;</p>\n<p>Now, at Less Wrong we like to talk about how to spend money effectively, so I should be clear about a few things. I'm raising non-trivial money for this, but this should be coming out of people's Warm Fuzzies Budgets, not their Effective Altruism budgets. This is a big, end of the year community feel-good festival.&nbsp;</p>\n<p>That said, I do think this is an especially <em>important</em>&nbsp;form of Warm Fuzzies. I've had EA-type folk come to me and tell me the Solstice inspired them to work harder, make life changes, or that it gave them an emotional booster charge to keep going even when things were hard. I hope, eventually, to have this measurable in some fashion such that I can point to it and say \"yes, this was important, and EA folk should definitely consider it important.\"&nbsp;</p>\n<p>But I'm not especially betting on that, and there are some failure modes where the Solstice ends up cannibalizing more resources that could have went towards direct impact. So, please consider that this may be especially <em>valuable</em>&nbsp;entertainment, that pushes culture in a direction where EA ideas can go more mainstream and gives hardcore EAs a motivational boost. But I encourage you to support it with dollars that wouldn't have gone towards direct Effective Altruism.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vtozKm5BZ8gf6zd45": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CNGzKea2nbzixpxcd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 29, "extendedScore": null, "score": 0.000119, "legacy": true, "legacyId": "27329", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"https://www.kickstarter.com/projects/244974495/secular-solstice-2014\"><img src=\"http://photos4.meetupstatic.com/photos/event/c/d/c/0/600_417892672.jpeg\" alt=\"\" width=\"600\" height=\"450\" align=\"center\"></a></p>\n<pre><br></pre>\n<h2 id=\"Summary_\"><strong>Summary:</strong></h2>\n<ul>\n<li>We're running another&nbsp;<a href=\"https://www.kickstarter.com/projects/244974495/secular-solstice-2014\">Winter Solstice kickstarter&nbsp;</a>- this is to fund the venue, musicians, food, drink and decorations for a big event in NYC on December 20th, as well as to record more music and print a larger run of the Solstice Book of Traditions.&nbsp;</li>\n</ul>\n<ul>\n<li>There will also be a Rationality Megameetup the following Sunday. (More generally, people are welcome to come to Highgarden and spend the night starting on Friday - although we request that people <a href=\"https://docs.google.com/forms/d/1QRjjZwklXtPEFNZrSPKjvb2_n8_ytOy5N6ry_lGedro/viewform?usp=send_form\">register their presence via this form so we don't run out of space</a>)</li>\n</ul>\n<ul>\n<li>I'd also like to raise additional money so I can focus full time for the next couple months on helping other communities run their own version of the event, tailored to meet their particular needs while still feeling like part of a cohesive, broader movement - and giving the attendees a genuinely powerful experience.&nbsp;</li>\n</ul>\n<pre><br></pre>\n<h2 id=\"The_Beginning\"><strong>The Beginning</strong></h2>\n<p><em>Four years ago, twenty NYC rationalists gathered in a room to celebrate the Winter Solstice. We sang songs and told stories about things that seemed very important to us. The precariousness of human life. The thousands of years of labor and curiosity that led us from a dangerous stone age to the modern world. The potential to create something even better, if humanity can get our act together and survive long enough.</em></p>\n<p><em>One of the most important ideas we honored was the importance of facing truths, even when they are uncomfortable or make us feel silly or are outright terrifying. Over the evening, we gradually extinguished candles, acknowledging harsher and harsher elements of reality.</em></p>\n<p><em>Until we sat in absolute darkness - aware that humanity is flawed, and alone, in an unforgivingly neutral universe.&nbsp;</em></p>\n<p><em>But also aware that we sit beside people who care deeply about truth, and about our future. Aware that across the world, people are working to give humanity a bright tomorrow, and that we have the power to help. Aware that across history, people have looked impossible situations in the face, and through ingenuity and persperation, made the impossible happen.</em></p>\n<p><em>That seemed worth celebrating.&nbsp;</em></p>\n<pre><br></pre>\n<h2 id=\"The_Story_So_Far\"><strong>The Story So Far</strong></h2>\n<p>As it turned out, this resonated with people outside the rationality community. When we ran the event again in 2012, non-religious but non-Less Wrong attended the event and told me they found it very moving. In 2013, we pushed it much larger - I ran a kickstarter campaign to fund a big event in NYC.&nbsp;</p>\n<p>A hundred and fifty people from various communities attended. From Less Wrong in particular, we had groups from Boston, San Francisco, North Carolina, Ottawa, and Ohio among other places. The following day was one of the largest East Coast Megameetups.&nbsp;</p>\n<p>Meanwhile, in the Bay Area, several people put together an event that gathered around 80 attendees. In Boston and Vancouever and Leipzig Germany, people ran smaller events. This is shaping up to take root as a legitimate holiday, celebrating human history and our potential future.</p>\n<p>This year, we want to do that all again. I also want to dedicate more time to helping other people run their events. Getting people to start celebrating a new holiday is a tricky feat. I've learned a lot about how to go about that and want to help others run polished events that feel connecting and inspirational.</p>\n<pre><br></pre>\n<h2 id=\"So__what_s_happening__and_how_can_you_help_\">So, what's happening, and how can you help?</h2>\n<p>&nbsp;</p>\n<ul>\n<li>The Big Solstice itself will be Saturday, December 20th at 7:00 PM. To fund it, we're aiming to raise $7500 on kickstarter. This is enough to fund the aforementioned venue, food, drink, live musicians, record new music, and print a larger run of the Solstice Book of Traditions. It'll also pay some expenses for the Megameetup. <a href=\"https://www.kickstarter.com/projects/244974495/secular-solstice-2014\">Please consider contributing to the kickstarter</a>.</li>\n</ul>\n<ul>\n<li>If you'd like to host your own Solstice (either a large or a private one) and would like advice, please contact me at raemon777@gmail.com and we'll work something out.</li>\n</ul>\n<ul>\n<li>There will also be Solstices (of varying sizes) run by Less Wrong / EA folk held in the Bay Area, Seattle, Boston and <a href=\"/r/discussion/lw/l3e/3day_solstice_in_leipzig_germany_small_nice_very/\">Leipzig</a>. (There will probably be a larger but non-LW-centered Solstice in Los Angeles and Boston as well). \n<ul>\n<li>If you'd like to be involved with the Seattle solstice, contact Jai at&nbsp;<span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">jai</span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13.3333339691162px;\">@</span><a style=\"color: #1155cc; font-family: arial, sans-serif; font-size: 13.3333339691162px;\" href=\"http://seattlesecularsolstice.com/\" target=\"_blank\">seattlesecularsolstice.com</a>. If you</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>In NYC, there will be a Rationality and EA Megameetup running from Friday, Dec 19th through Sunday evening. \n<ul>\n<li>Friday night and Saturday morning: Arrival, Settling</li>\n<li>Saturday at 2PM - 4:30PM: Unconference (20 minute talks, workshops or discussions)</li>\n<li>Saturday at 7PM: Big Solstice</li>\n<li>Sunday at Noon: Unconference 2</li>\n<li>Sunday at 2PM: Strategic New Years Resolution Planning</li>\n<li>Sunday at 3PM: Discussion of creating private ritual for individual communities</li>\n</ul>\n</li>\n<li>If you're interested in coming to the Megameetup, <a href=\"https://docs.google.com/forms/d/1QRjjZwklXtPEFNZrSPKjvb2_n8_ytOy5N6ry_lGedro/viewform\">please fill out this form</a> saying how many people you're bringing, whether you're interested in giving a talk, and whether you're bringing a vehicle, so we can plan adequately. (We have lots of crash space, but not infinite bedding, so bringing sleeping bags or blankets would be helpful)</li>\n</ul>\n<div><br></div>\n<h2 id=\"Effective_Altruism_\">Effective Altruism?</h2>\n<p>&nbsp;</p>\n<p>Now, at Less Wrong we like to talk about how to spend money effectively, so I should be clear about a few things. I'm raising non-trivial money for this, but this should be coming out of people's Warm Fuzzies Budgets, not their Effective Altruism budgets. This is a big, end of the year community feel-good festival.&nbsp;</p>\n<p>That said, I do think this is an especially <em>important</em>&nbsp;form of Warm Fuzzies. I've had EA-type folk come to me and tell me the Solstice inspired them to work harder, make life changes, or that it gave them an emotional booster charge to keep going even when things were hard. I hope, eventually, to have this measurable in some fashion such that I can point to it and say \"yes, this was important, and EA folk should definitely consider it important.\"&nbsp;</p>\n<p>But I'm not especially betting on that, and there are some failure modes where the Solstice ends up cannibalizing more resources that could have went towards direct impact. So, please consider that this may be especially <em>valuable</em>&nbsp;entertainment, that pushes culture in a direction where EA ideas can go more mainstream and gives hardcore EAs a motivational boost. But I encourage you to support it with dollars that wouldn't have gone towards direct Effective Altruism.</p>", "sections": [{"title": "Summary:", "anchor": "Summary_", "level": 1}, {"title": "The Beginning", "anchor": "The_Beginning", "level": 1}, {"title": "The Story So Far", "anchor": "The_Story_So_Far", "level": 1}, {"title": "So, what's happening, and how can you help?", "anchor": "So__what_s_happening__and_how_can_you_help_", "level": 1}, {"title": "Effective Altruism?", "anchor": "Effective_Altruism_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iMG3TiD6PDFE3F88a"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-10T20:36:31.898Z", "modifiedAt": null, "url": null, "title": "Meetup : Bath: Introduction and PredictionBook", "slug": "meetup-bath-introduction-and-predictionbook", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:32.146Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KnaveOfAllTrades", "createdAt": "2012-07-20T02:08:23.538Z", "isAdmin": false, "displayName": "KnaveOfAllTrades"}, "userId": "FuACexYrBpyrMmz5C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p2tHh5QGZEYRzf4di/meetup-bath-introduction-and-predictionbook", "pageUrlRelative": "/posts/p2tHh5QGZEYRzf4di/meetup-bath-introduction-and-predictionbook", "linkUrl": "https://www.lesswrong.com/posts/p2tHh5QGZEYRzf4di/meetup-bath-introduction-and-predictionbook", "postedAtFormatted": "Friday, October 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bath%3A%20Introduction%20and%20PredictionBook&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bath%3A%20Introduction%20and%20PredictionBook%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp2tHh5QGZEYRzf4di%2Fmeetup-bath-introduction-and-predictionbook%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bath%3A%20Introduction%20and%20PredictionBook%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp2tHh5QGZEYRzf4di%2Fmeetup-bath-introduction-and-predictionbook", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp2tHh5QGZEYRzf4di%2Fmeetup-bath-introduction-and-predictionbook", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 272, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/15l'>Bath: Introduction and PredictionBook</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 October 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">5-10 James St W, Avon, Bath BA1 2BX</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I'll be hosting a meetup for Bath, UK on Sunday 19th October at 14:00. <br />\nThe meetup will be held at the King of Wessex, which is a Wetherspoons pub in the city. Start time is 14:00, and I'll wait at least ninety minutes after that for the first arrivals. I'll put a Less Wrong paperclip print-out on the table so you can identify me.\nIn case you need to contact me (e.g. if the venue is unexpectedly busy and we have to move elsewhere and you can't find us), my mobile number is the product 3 x 3 x 23 x 97 x 375127, preceded by a zero (so eleven digits total).\nSince this is the first meetup, we'll start off with introductions and chit-chat. I will also formulate and bring along (but not check the veracity of) some propositions for us to place probabilities on, as calibration training.\nI recommend you create and play around with a <a href=\"http://predictionbook.com/\" rel=\"nofollow\">PredictionBook</a> account in advance of the meetup to get to grips with it and in case you have any questions about it we can discuss on the day.\n<a href=\"http://predictionbook.com/\" rel=\"nofollow\">(Why not register right now? It only takes one or two minutes.)</a>\nBonus points if you bring a device to log your predictions on PredictionBook as we go along, and as a back-up in case my laptop dies. (The venue has free Wi-Fi that you can register for in a few minutes.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/15l'>Bath: Introduction and PredictionBook</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p2tHh5QGZEYRzf4di", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 2.0747032680374967e-06, "legacy": true, "legacyId": "27350", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bath__Introduction_and_PredictionBook\">Discussion article for the meetup : <a href=\"/meetups/15l\">Bath: Introduction and PredictionBook</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 October 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">5-10 James St W, Avon, Bath BA1 2BX</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I'll be hosting a meetup for Bath, UK on Sunday 19th October at 14:00. <br>\nThe meetup will be held at the King of Wessex, which is a Wetherspoons pub in the city. Start time is 14:00, and I'll wait at least ninety minutes after that for the first arrivals. I'll put a Less Wrong paperclip print-out on the table so you can identify me.\nIn case you need to contact me (e.g. if the venue is unexpectedly busy and we have to move elsewhere and you can't find us), my mobile number is the product 3 x 3 x 23 x 97 x 375127, preceded by a zero (so eleven digits total).\nSince this is the first meetup, we'll start off with introductions and chit-chat. I will also formulate and bring along (but not check the veracity of) some propositions for us to place probabilities on, as calibration training.\nI recommend you create and play around with a <a href=\"http://predictionbook.com/\" rel=\"nofollow\">PredictionBook</a> account in advance of the meetup to get to grips with it and in case you have any questions about it we can discuss on the day.\n<a href=\"http://predictionbook.com/\" rel=\"nofollow\">(Why not register right now? It only takes one or two minutes.)</a>\nBonus points if you bring a device to log your predictions on PredictionBook as we go along, and as a back-up in case my laptop dies. (The venue has free Wi-Fi that you can register for in a few minutes.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bath__Introduction_and_PredictionBook1\">Discussion article for the meetup : <a href=\"/meetups/15l\">Bath: Introduction and PredictionBook</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bath: Introduction and PredictionBook", "anchor": "Discussion_article_for_the_meetup___Bath__Introduction_and_PredictionBook", "level": 1}, {"title": "Discussion article for the meetup : Bath: Introduction and PredictionBook", "anchor": "Discussion_article_for_the_meetup___Bath__Introduction_and_PredictionBook1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-11T04:24:19.594Z", "modifiedAt": null, "url": null, "title": "Meetup : Perth, Australia: Sunday lunch", "slug": "meetup-perth-australia-sunday-lunch-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "4r4aKXsWgDHchNwHh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zz6MzYyQwBHarqu38/meetup-perth-australia-sunday-lunch-1", "pageUrlRelative": "/posts/zz6MzYyQwBHarqu38/meetup-perth-australia-sunday-lunch-1", "linkUrl": "https://www.lesswrong.com/posts/zz6MzYyQwBHarqu38/meetup-perth-australia-sunday-lunch-1", "postedAtFormatted": "Saturday, October 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Perth%2C%20Australia%3A%20Sunday%20lunch&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Perth%2C%20Australia%3A%20Sunday%20lunch%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzz6MzYyQwBHarqu38%2Fmeetup-perth-australia-sunday-lunch-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Perth%2C%20Australia%3A%20Sunday%20lunch%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzz6MzYyQwBHarqu38%2Fmeetup-perth-australia-sunday-lunch-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzz6MzYyQwBHarqu38%2Fmeetup-perth-australia-sunday-lunch-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 114, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/15m'>Perth, Australia: Sunday lunch</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 October 2014 12:00:00PM (+0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Annalakshmi, Barrack Square, Perth, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come have lunch with other Less Wrongians! We'll be at <a href=\"http://www.annalakshmi.com.au/\" rel=\"nofollow\">Annalakshmi</a>, a pay-what-you-want vegetarian restaurant.</p>\n\n<p>We'll discuss <a href=\"https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow\" rel=\"nofollow\">System 1 and System 2</a>, two different ways we think. Very roughly, System 1 is instinctive and System 2 is analytic. When is one style of thinking more helpful than the other? When is one more challenging than the other?</p>\n\n<p>You can RSVP here: <a href=\"http://www.meetup.com/Perth-Less-Wrong/events/212764552/\" rel=\"nofollow\">http://www.meetup.com/Perth-Less-Wrong/events/212764552/</a></p>\n\n<p>How to find us: I'll have a silver water bottle labeled \"CFAR\" in orange. We'll be outside the restaurant until 12 PM, then go inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/15m'>Perth, Australia: Sunday lunch</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zz6MzYyQwBHarqu38", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "27351", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Perth__Australia__Sunday_lunch\">Discussion article for the meetup : <a href=\"/meetups/15m\">Perth, Australia: Sunday lunch</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 October 2014 12:00:00PM (+0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Annalakshmi, Barrack Square, Perth, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come have lunch with other Less Wrongians! We'll be at <a href=\"http://www.annalakshmi.com.au/\" rel=\"nofollow\">Annalakshmi</a>, a pay-what-you-want vegetarian restaurant.</p>\n\n<p>We'll discuss <a href=\"https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow\" rel=\"nofollow\">System 1 and System 2</a>, two different ways we think. Very roughly, System 1 is instinctive and System 2 is analytic. When is one style of thinking more helpful than the other? When is one more challenging than the other?</p>\n\n<p>You can RSVP here: <a href=\"http://www.meetup.com/Perth-Less-Wrong/events/212764552/\" rel=\"nofollow\">http://www.meetup.com/Perth-Less-Wrong/events/212764552/</a></p>\n\n<p>How to find us: I'll have a silver water bottle labeled \"CFAR\" in orange. We'll be outside the restaurant until 12 PM, then go inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Perth__Australia__Sunday_lunch1\">Discussion article for the meetup : <a href=\"/meetups/15m\">Perth, Australia: Sunday lunch</a></h2>", "sections": [{"title": "Discussion article for the meetup : Perth, Australia: Sunday lunch", "anchor": "Discussion_article_for_the_meetup___Perth__Australia__Sunday_lunch", "level": 1}, {"title": "Discussion article for the meetup : Perth, Australia: Sunday lunch", "anchor": "Discussion_article_for_the_meetup___Perth__Australia__Sunday_lunch1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-11T06:39:14.489Z", "modifiedAt": null, "url": null, "title": "2014 Less Wrong Census/Survey - Call For Critiques/Questions", "slug": "2014-less-wrong-census-survey-call-for-critiques-questions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:13.996Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PBhrHw5X8sDmHDWkX/2014-less-wrong-census-survey-call-for-critiques-questions", "pageUrlRelative": "/posts/PBhrHw5X8sDmHDWkX/2014-less-wrong-census-survey-call-for-critiques-questions", "linkUrl": "https://www.lesswrong.com/posts/PBhrHw5X8sDmHDWkX/2014-less-wrong-census-survey-call-for-critiques-questions", "postedAtFormatted": "Saturday, October 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202014%20Less%20Wrong%20Census%2FSurvey%20-%20Call%20For%20Critiques%2FQuestions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2014%20Less%20Wrong%20Census%2FSurvey%20-%20Call%20For%20Critiques%2FQuestions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPBhrHw5X8sDmHDWkX%2F2014-less-wrong-census-survey-call-for-critiques-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2014%20Less%20Wrong%20Census%2FSurvey%20-%20Call%20For%20Critiques%2FQuestions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPBhrHw5X8sDmHDWkX%2F2014-less-wrong-census-survey-call-for-critiques-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPBhrHw5X8sDmHDWkX%2F2014-less-wrong-census-survey-call-for-critiques-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 287, "htmlBody": "<p>It's that time of year again. Actually, a little earlier than that time of year, but I'm pushing it ahead a little to match when Ozy and I expect to have more free time to process the results. <br /><br />The first draft of the 2014 Less Wrong Census/Survey is complete (see 2013 results <a href=\"/lw/jj0/2013_survey_results/\">here</a>) . <br /><br />You can see the survey below if you <em>promise not to try to take the survey because it's not done yet and this is just an example!<br /></em><br /><a href=\" https://docs.google.com/forms/d/1h4IisKq7p8CRRVT_UXMSiKW6RE5U5nl1PLT_MvpbX2I/viewform\"><span style=\"color: #000000;\"><strong>2014 Less Wrong Census/Survey Draft</strong></span></a><br /><br />I want two things from you.<br /><br />First, please critique this draft (it's much the same as last year's). Tell me if any questions are unclear, misleading, offensive, confusing, or stupid. Tell me if the survey is so unbearably long that you would never possibly take it. Tell me if anything needs to be rephrased.<br /><br />Second, I am willing to include any question you want in the Super Extra Bonus Questions section, as long as it is not offensive, super-long-and-involved, or really dumb. Please post any questions you want there. Please be specific - not \"Ask something about taxes\" but give the exact question you want me to ask as well as all answer choices.<br /><br />Try not to add more than a few questions per person, unless you're sure yours are really interesting. Please also don't add any questions that aren't very easily sort-able by a computer program like SPSS unless you can commit to sorting the answers yourself.<br /><br />I will probably post the survey to Main and officially open it for responses sometime early next week.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"kJrjorSx3hXa7q7CJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PBhrHw5X8sDmHDWkX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 29, "extendedScore": null, "score": 0.000125, "legacy": true, "legacyId": "27352", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 274, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pJJdcZgB6mPNWoSWr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-12T06:08:18.000Z", "modifiedAt": "2021-08-12T19:31:01.543Z", "url": null, "title": "Five Planets In Search Of A Sci-Fi Story", "slug": "five-planets-in-search-of-a-sci-fi-story", "viewCount": null, "lastCommentedAt": "2019-12-27T20:50:18.010Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ouSpHCCPgsXkwxAGb/five-planets-in-search-of-a-sci-fi-story", "pageUrlRelative": "/posts/ouSpHCCPgsXkwxAGb/five-planets-in-search-of-a-sci-fi-story", "linkUrl": "https://www.lesswrong.com/posts/ouSpHCCPgsXkwxAGb/five-planets-in-search-of-a-sci-fi-story", "postedAtFormatted": "Sunday, October 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Five%20Planets%20In%20Search%20Of%20A%20Sci-Fi%20Story&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFive%20Planets%20In%20Search%20Of%20A%20Sci-Fi%20Story%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FouSpHCCPgsXkwxAGb%2Ffive-planets-in-search-of-a-sci-fi-story%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Five%20Planets%20In%20Search%20Of%20A%20Sci-Fi%20Story%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FouSpHCCPgsXkwxAGb%2Ffive-planets-in-search-of-a-sci-fi-story", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FouSpHCCPgsXkwxAGb%2Ffive-planets-in-search-of-a-sci-fi-story", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1142, "htmlBody": "<p><b>Gamma Andromeda</b>, where philosophical stoicism went too far. Its inhabitants, tired of the roller coaster ride of daily existence, decided to learn equanimity in the face of gain or misfortune, neither dreading disaster nor taking joy in success.</p>\n<p>But that turned out to be really hard, so instead they just hacked it. Whenever something good happens, the Gammandromedans give themselves an electric shock proportional in strength to its goodness. Whenever something bad happens, the Gammandromedans take an opiate-like drug that directly stimulates the pleasure centers of their brain, in a dose proportional in strength to its badness.</p>\n<p>As a result, every day on Gamma Andromeda is equally good compared to every other day, and its inhabitants need not be jostled about by fear or hope for the future.</p>\n<p>This does sort of screw up their incentives to make good things happen, but luckily they&#8217;re all virtue ethicists.</p>\n<p><b>Zyzzx Prime</b>, inhabited by an alien race descended from a barnacle-like creature. Barnacles are famous for their two stage life-cycle: in the first, they are mobile and curious creatures, cleverly picking out the best spot to make their home. In the second, they root themselves to the spot and, having no further use for their brain, eat it.</p>\n<p>This particular alien race has evolved far beyond that point and does not literally eat its brain. However, once an alien reaches sufficiently high social status, it releases a series of hormones that tell its brain, essentially, that it is now in a safe place and doesn&#8217;t have to waste so much energy on thought and creativity to get ahead. As a result, its mental acuity drops two or three standard deviations.</p>\n<p>The Zyzzxians&#8217; society is marked by a series of experiments with government &#8211; monarchy, democracy, dictatorship &#8211; only to discover that, whether chosen by succession, election, or ruthless conquest, its once brilliant leaders lose their genius immediately upon accession and do a terrible job. Their government is thus marked by a series of perpetual pointless revolutions.</p>\n<p>At one point, a scientific effort was launched to discover the hormones responsible and whether it was possible to block them. Unfortunately, any scientist who showed promise soon lost their genius, and those promoted to be heads of research institutes became stumbling blocks who mismanaged funds and held back their less prestigious co-workers. Suggestions that the institutes eliminate tenure were vetoed by top officials, who said that &#8220;such a drastic step seems unnecessary&#8221;.</p>\n<p><b>K&#8217;th&#8217;ranga V</b>, which has been a global theocracy for thousands of years, ever since its dominant race invented agricultural civilization. This worked out pretty well for a while, until it reached an age of industrialization, globalization, and scientific discovery. Scientists began to uncover truths that contradicted the Sacred Scriptures, and the hectic pace of modern life made the shepherds-and-desert-traders setting of the holy stories look vaguely silly. Worse, the cold logic of capitalism and utilitarianism began to invade the Scriptures&#8217; innocent Stone Age morality.</p>\n<p>The priest-kings tried to turn back the tide of progress, but soon realized this was a losing game. Worse, in order to determine what to suppress, they themselves had to learn the dangerous information, and their mental purity was even more valuable than that of the populace at large.</p>\n<p>So the priest-kings moved en masse to a big island, where they began living an old-timey Bronze Age lifestyle. And the world they ruled sent emissaries to the island, who interfaced with the priest-kings, and sought their guidance, and the priest-kings ruled a world they didn&#8217;t understand as best they could.</p>\n<p>But it soon became clear that the system could not sustain itself indefinitely. For one thing, the priest-kings worried that discussion with the emissaries &#8211; who inevitably wanted to talk about strange things like budgets and interest rates and nuclear armaments &#8211; was contaminating their memetic purity. For another thing, they honestly couldn&#8217;t understand what the emissaries were talking about half the time.</p>\n<p>Luckily, there was a whole chain of islands making an archipelago. So the priest-kings set up ten transitional societies &#8211; themselves in the Bronze Age, another in the Iron Age, another in the Classical Age, and so on to the mainland, who by this point were starting to experiment with nanotech. Mainland society brought its decisions to the first island, who translated it into their own slightly-less-advanced understanding, who brought it to the second island, and so on to the priest-kings, by which point a discussion about global warming might sound like whether we should propitiate the Coal Spirit. The priest-kings would send their decisions to the second-to-last island, and so on back to the mainland.</p>\n<p>Eventually the Kth&#8217; built an AI which achieved superintelligence and set out to conquer the universe. But it was a well-programmed superintelligence coded with Kth&#8217; values. Whenever <i>it</i> wanted a high-level decision made, it would talk to a slightly less powerful superintelligence, who would talk to a slightly less powerful superintelligence, who would talk to the mainlanders, who would talk to the first island&#8230;</p>\n<p><b>Chan X-3</b>, notable for a native species that evolved as fitness-maximizers, not adaptation-executors. Their explicit goal is to maximize the number of copies of their genes. But whatever genetic program they are executing doesn&#8217;t care whether the genes are within a living being capable of expressing them or not. The planet is covered with giant vats full of frozen DNA. There was originally some worry that the species would go extinct, since having children would consume resources that could be used hiring geneticists to make millions of copies of your DNA and stores them in freezers. Luckily, it was realized that children not only provide a useful way to continue the work of copying and storing (half of) your DNA long into the future, but will also work to guard your already-stored DNA against being destroyed. The species has thus continued undiminished, somehow, and their fondest hope is to colonize space and reach the frozen Kuiper Belt objects where their DNA will naturally stay undegraded for all time.</p>\n<p><b>New Capricorn</b>, which contains a previously undiscovered human colony that has achieved a research breakthrough beyond their wildest hopes. A multi-century effort paid off in a fully general cure for death. However, the drug fails to stop aging. Although the Capricornis no longer need fear the grave, after age 100 or so even the hardiest of them get Alzheimers&#8217; or other similar conditions. A hundred years after the breakthrough, more than half of the population is elderly and demented. Two hundred years after, more than 80% are. Capricorni nursing homes quickly became overcrowded and unpleasant, to the dismay of citizens expecting to spend eternity there.</p>\n<p>So another research program was started, and the result were fully immersive, fully life-supporting virtual reality capsules. Stacked in huge warehouses by the millions, the elderly sit in their virtual worlds, vague sunny fields and old gabled houses where it is always the Good Old Days and their grandchildren are always visiting.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ouSpHCCPgsXkwxAGb", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 21, "extendedScore": null, "score": 6e-05, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": "xmDeR64CivZiTAcLx", "canonicalCollectionSlug": "codex", "canonicalBookId": "kcCvSNNZd8pfQvf9E", "canonicalNextPostSlug": "it-was-you-who-made-my-blue-eyes-blue", "canonicalPrevPostSlug": "meditations-on-moloch", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2014-10-12T06:08:18.000Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-12T06:10:01.251Z", "modifiedAt": null, "url": null, "title": "Positive Book and Other Media Recommendations for a Teen Audience", "slug": "positive-book-and-other-media-recommendations-for-a-teen", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:06.020Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Brendon_Wong", "createdAt": "2013-07-18T17:10:38.559Z", "isAdmin": false, "displayName": "Brendon_Wong"}, "userId": "TPaHQ6APqYu44ENNx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o3zmFNhxxLqM4ERdT/positive-book-and-other-media-recommendations-for-a-teen", "pageUrlRelative": "/posts/o3zmFNhxxLqM4ERdT/positive-book-and-other-media-recommendations-for-a-teen", "linkUrl": "https://www.lesswrong.com/posts/o3zmFNhxxLqM4ERdT/positive-book-and-other-media-recommendations-for-a-teen", "postedAtFormatted": "Sunday, October 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Positive%20Book%20and%20Other%20Media%20Recommendations%20for%20a%20Teen%20Audience&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APositive%20Book%20and%20Other%20Media%20Recommendations%20for%20a%20Teen%20Audience%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo3zmFNhxxLqM4ERdT%2Fpositive-book-and-other-media-recommendations-for-a-teen%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Positive%20Book%20and%20Other%20Media%20Recommendations%20for%20a%20Teen%20Audience%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo3zmFNhxxLqM4ERdT%2Fpositive-book-and-other-media-recommendations-for-a-teen", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo3zmFNhxxLqM4ERdT%2Fpositive-book-and-other-media-recommendations-for-a-teen", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>Hi Less Wrong,</p>\n<p>I've got the opportunity to promote books and other forms of content to a largely teenage audience. I'm looking for some good book recommendations and recommendations for a limited amount of other media (websites, movies, etc) that will spread awareness of positive ideas and issues in the world, while still being entertaining to the target audience. HPMOR comes to mind, and although I don't think its main focus is to promote an issue to readers, it is an excellent choice because it is well ranked, promotes rationality which can directly help readers, and is completely free to read online. Recommendations don't have to be free, but that is a very important factor.</p>\n<p>Thanks everyone!</p>\n<p>Edit: Thanks everyone for your contributions. I realize I was being a little too vague, I think content that promotes ethical/altruistic behavior is mainly what I am looking for, either inspiring it with fiction or causing motivation for it with nonfiction. I am looking for free web content, although paid content recommendations are also appreciated. This is for a general youth audience, not a club or any sort of interest group.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o3zmFNhxxLqM4ERdT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 2.0785566137638784e-06, "legacy": true, "legacyId": "27353", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-12T14:37:45.309Z", "modifiedAt": null, "url": null, "title": "[Link] The Coming Plague", "slug": "link-the-coming-plague", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:31.112Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XHdmaxtzjvwoHkYwH/link-the-coming-plague", "pageUrlRelative": "/posts/XHdmaxtzjvwoHkYwH/link-the-coming-plague", "linkUrl": "https://www.lesswrong.com/posts/XHdmaxtzjvwoHkYwH/link-the-coming-plague", "postedAtFormatted": "Sunday, October 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20The%20Coming%20Plague&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20The%20Coming%20Plague%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXHdmaxtzjvwoHkYwH%2Flink-the-coming-plague%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20The%20Coming%20Plague%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXHdmaxtzjvwoHkYwH%2Flink-the-coming-plague", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXHdmaxtzjvwoHkYwH%2Flink-the-coming-plague", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 534, "htmlBody": "<p class=\"entry-title\"><strong>Related to:</strong> <a href=\"/lw/l1w/link_forty_days/\">Forty Days</a> , <a href=\"/lw/iud/link_lowhanging_poop/\">Low Hanging Poop </a></p>\n<p class=\"entry-title\"><a href=\"http://westhunt.wordpress.com/2014/10/11/the-coming-plague/\">From</a> professor <a href=\"http://en.wikipedia.org/wiki/Gregory_Cochran\">Gregory Cochran</a>'s blog West Hunters.</p>\n<blockquote>\n<p>Laurie Garret has an <a href=\"http://www.washingtonpost.com/opinions/five-myths-about-ebola/2014/10/10/6daf70de-4ffe-11e4-babe-e91da079cb8a_story.html\">article</a> out in the Washington Post.&nbsp; She say that there&rsquo;s no point in trying to block the spread of Ebola by travel bans.</p>\n<p>The problem is, she&rsquo;s full of crap.&nbsp; Look, there are two possible scenarios.&nbsp; In both of them, r, the number of new cases generated by each case, is greater than 1 in parts of West Africa &ndash; which is why you get exponential growth, why you have an epidemic.&nbsp; If r &lt; 1.0, the series converges &ndash; a case generates a few extra cases before dying out.</p>\n<p>Everything we know so far suggests that even though it is greater than 1.0,&nbsp; r in West Africa is not all that big (maybe around 2), mostly because of unfortunate local burial customs and incompetent medical personnel.</p>\n<p>It seems highly likely that r in US conditions is well under 1.0 which means you can&rsquo;t get an epidemic. However,&nbsp; r is probably not zero.&nbsp; It doesn&rsquo;t mean that you can&rsquo;t get a few cases per imported case, from immediate contact and hospital mistakes.&nbsp; As an example, suppose that on average each case imported to the US generated a total of two other cases before dying out (counting secondary, tertiary, etc infections).&nbsp; Then, on average, the number of US citizens infected would be twice the number of infected visitors.</p>\n<p>Now suppose that a travel ban blocked 80% of sick people trying to fly here from Liberia.&nbsp; We&rsquo;d have 80% fewer cases in US citizens: and that would be a good thing. Really it would.&nbsp; Does Laurie Garret understand this?&nbsp; Obviously not. She is a senior fellow for global health at the Council on Foreign Relations, but she is incompetent.&nbsp; Totally useless, like virtually everyone else in public life.</p>\n<p>We hear people from the CDC saying that any travel restrictions would backfire, but that&rsquo;s nonsense too.&nbsp; One might wonder why they say such goofy things: I would guess that a major reason is that they were taught in school that quarantines are useless (and worse yet, old-fashioned), just as many biologists were taught that <a href=\"http://westhunt.wordpress.com/2013/09/13/their-lying-eyes/\">parasites </a>are really harmless &ndash; have to be, because evolution!</p>\n<p>In the other scenario, r &gt; 1.0 in US conditions as well, or at least is greater than 1.0 in some subsets of the US population.&nbsp; This is very unlikely- even more unlikely considering we can adjust our behavior to make transmission less likely.&nbsp; But suppose it so, for the sake of argument.&nbsp; Then you would want &ndash; <em>need</em> &ndash; to stop <em>all</em> travelers from the risky regions, because even one infected guy would pose a huge risk.&nbsp; Some say that blocking that spread would be impossible. They&rsquo;re wrong: it is possible*, although it wouldn&rsquo;t happen, because we&rsquo;re too crazy.&nbsp; In fact, in that scenario, we&rsquo;d be justified in shooting down every plane that _might_ carry an infected passenger.&nbsp; This scenario is the one that fits Garrett&rsquo;s remarks, but if she really believed it, she would be frantically buying canned goods and finding a cave in the Rockies to hide her family in.</p>\n<p>*the Atlantic is pretty wide.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XHdmaxtzjvwoHkYwH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 1, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "27354", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rFCwuY5oHu7tWDqAX", "9CGAffRSj4TD7LkcH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-13T00:39:45.207Z", "modifiedAt": null, "url": null, "title": "Baysian conundrum", "slug": "baysian-conundrum", "viewCount": null, "lastCommentedAt": "2018-07-27T07:43:20.187Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jan_Rzymkowski", "createdAt": "2014-05-09T22:34:32.079Z", "isAdmin": false, "displayName": "Jan_Rzymkowski"}, "userId": "kTxmHkNaDGDPpFwci", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rxb46W3okERnTL2EZ/baysian-conundrum", "pageUrlRelative": "/posts/rxb46W3okERnTL2EZ/baysian-conundrum", "linkUrl": "https://www.lesswrong.com/posts/rxb46W3okERnTL2EZ/baysian-conundrum", "postedAtFormatted": "Monday, October 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Baysian%20conundrum&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABaysian%20conundrum%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frxb46W3okERnTL2EZ%2Fbaysian-conundrum%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Baysian%20conundrum%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frxb46W3okERnTL2EZ%2Fbaysian-conundrum", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frxb46W3okERnTL2EZ%2Fbaysian-conundrum", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 239, "htmlBody": "<p>For some time I've been pondering on a certain scenario, which I'll describe shortly. I hope you may help me find a satisfactory&nbsp;answer or at very least be as perplexed by this probabilistic question as me. Feel free to assign any reasonable a priori probabilities as you like. Here's the problem:<br /><br />It's cold cold winter. Radiators are hardly working, but it's not why you're sitting so anxiously in your chair. The real reason is that tomorrow is your assigned upload (and damn, it's just one in million chance you're not gonna get it) and you just can't wait to leave your corporality behind. \"Oh, I'm so sick of having a body, especially now. I'm freezing!\" you think to yourself, \"I wish I were already uploaded and could just pop myself off to a tropical island.\"</p>\n<p>And now it strikes you. It's a weird solution, but it feels so appealing. You make a solemn oath (you'd say one in million chance you'd break it), that soon after upload you will simulate this exact moment thousand times simultaneously and when the clock strikes 11 AM, you're gonna be transposed to a Hawaiian beach, with a fancy drink in your hand.</p>\n<p>It's 10:59 on a clock. What's the probability that you'd be in a tropical paradise in one minute?</p>\n<p>And to make things more paradoxical: What would be said probability, if you wouldn't have made such an oath - just seconds ago?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rxb46W3okERnTL2EZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 17, "extendedScore": null, "score": 2.08068579777099e-06, "legacy": true, "legacyId": "27356", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-13T08:17:22.462Z", "modifiedAt": null, "url": null, "title": "Open thread, Oct. 13 - Oct. 19, 2014", "slug": "open-thread-oct-13-oct-19-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:35.588Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MrMind", "createdAt": "2011-04-19T08:43:22.388Z", "isAdmin": false, "displayName": "MrMind"}, "userId": "LJ4br8GWFXetsXkM8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dLoJKnJxds7Rk9Krm/open-thread-oct-13-oct-19-2014", "pageUrlRelative": "/posts/dLoJKnJxds7Rk9Krm/open-thread-oct-13-oct-19-2014", "linkUrl": "https://www.lesswrong.com/posts/dLoJKnJxds7Rk9Krm/open-thread-oct-13-oct-19-2014", "postedAtFormatted": "Monday, October 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20Oct.%2013%20-%20Oct.%2019%2C%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20Oct.%2013%20-%20Oct.%2019%2C%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdLoJKnJxds7Rk9Krm%2Fopen-thread-oct-13-oct-19-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20Oct.%2013%20-%20Oct.%2019%2C%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdLoJKnJxds7Rk9Krm%2Fopen-thread-oct-13-oct-19-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdLoJKnJxds7Rk9Krm%2Fopen-thread-oct-13-oct-19-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px; font-weight: bold;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one. (<em>Immediately</em>&nbsp;before; refresh the list-of-threads page before posting.)<br /></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">3.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should be posted in Discussion, and not Main.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">4.&nbsp;</span><span style=\"line-height: 19px;\">Open Threads should start on Monday, and end on Sunday.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dLoJKnJxds7Rk9Krm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 2.0815649343501623e-06, "legacy": true, "legacyId": "27357", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 359, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-13T11:07:11.483Z", "modifiedAt": null, "url": null, "title": "Meetup : Australia online meetup", "slug": "meetup-australia-online-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Elo", "createdAt": "2014-04-17T22:36:25.781Z", "isAdmin": false, "displayName": "Elo"}, "userId": "Qad7jGcRgP4BZMa5F", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/okcDP4FQp8mGKG6mv/meetup-australia-online-meetup", "pageUrlRelative": "/posts/okcDP4FQp8mGKG6mv/meetup-australia-online-meetup", "linkUrl": "https://www.lesswrong.com/posts/okcDP4FQp8mGKG6mv/meetup-australia-online-meetup", "postedAtFormatted": "Monday, October 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Australia%20online%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Australia%20online%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FokcDP4FQp8mGKG6mv%2Fmeetup-australia-online-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Australia%20online%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FokcDP4FQp8mGKG6mv%2Fmeetup-australia-online-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FokcDP4FQp8mGKG6mv%2Fmeetup-australia-online-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/15n'>Australia online meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 October 2014 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Sydney</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>See you at the online hangout. From wherever you are.</p>\n\n<p>Link to be posted about 10 minutes before hand because they expire otherwise.</p>\n\n<p>We use google hangouts so make sure you can get into one of those before the meetup or else there is a whole bunch of fluffing around installing things.</p>\n\n<p>bring any fickle puzzles or questions to the floor. or neat group-projects. We created cognitive bias short stories in the past and it was fun!</p>\n\n<p><a href=\"https://www.facebook.com/events/991080387585441/\" rel=\"nofollow\">https://www.facebook.com/events/991080387585441/</a></p>\n\n<p>Usual representation includes; Sydney, Melbourne, Canberra, Brisbane, NZ, This one guy from South America...</p>\n\n<p>time 1900 - 23:30. UTC+11 (Sunday evening)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/15n'>Australia online meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "okcDP4FQp8mGKG6mv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 2.08189133736823e-06, "legacy": true, "legacyId": "27358", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Australia_online_meetup\">Discussion article for the meetup : <a href=\"/meetups/15n\">Australia online meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 October 2014 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Sydney</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>See you at the online hangout. From wherever you are.</p>\n\n<p>Link to be posted about 10 minutes before hand because they expire otherwise.</p>\n\n<p>We use google hangouts so make sure you can get into one of those before the meetup or else there is a whole bunch of fluffing around installing things.</p>\n\n<p>bring any fickle puzzles or questions to the floor. or neat group-projects. We created cognitive bias short stories in the past and it was fun!</p>\n\n<p><a href=\"https://www.facebook.com/events/991080387585441/\" rel=\"nofollow\">https://www.facebook.com/events/991080387585441/</a></p>\n\n<p>Usual representation includes; Sydney, Melbourne, Canberra, Brisbane, NZ, This one guy from South America...</p>\n\n<p>time 1900 - 23:30. UTC+11 (Sunday evening)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Australia_online_meetup1\">Discussion article for the meetup : <a href=\"/meetups/15n\">Australia online meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Australia online meetup", "anchor": "Discussion_article_for_the_meetup___Australia_online_meetup", "level": 1}, {"title": "Discussion article for the meetup : Australia online meetup", "anchor": "Discussion_article_for_the_meetup___Australia_online_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-13T13:51:02.708Z", "modifiedAt": null, "url": null, "title": "[Link] Why Science Is Not Necessarily Self-Correcting", "slug": "link-why-science-is-not-necessarily-self-correcting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:04.202Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChristianKl", "createdAt": "2009-10-13T22:32:16.589Z", "isAdmin": false, "displayName": "ChristianKl"}, "userId": "vbDMpDA5A35329Ju5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/efSuNL59cEzDtqXQ9/link-why-science-is-not-necessarily-self-correcting", "pageUrlRelative": "/posts/efSuNL59cEzDtqXQ9/link-why-science-is-not-necessarily-self-correcting", "linkUrl": "https://www.lesswrong.com/posts/efSuNL59cEzDtqXQ9/link-why-science-is-not-necessarily-self-correcting", "postedAtFormatted": "Monday, October 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Why%20Science%20Is%20Not%20Necessarily%20Self-Correcting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Why%20Science%20Is%20Not%20Necessarily%20Self-Correcting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FefSuNL59cEzDtqXQ9%2Flink-why-science-is-not-necessarily-self-correcting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Why%20Science%20Is%20Not%20Necessarily%20Self-Correcting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FefSuNL59cEzDtqXQ9%2Flink-why-science-is-not-necessarily-self-correcting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FefSuNL59cEzDtqXQ9%2Flink-why-science-is-not-necessarily-self-correcting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 259, "htmlBody": "<blockquote>\n<p>\n<div class=\"contributors\" style=\"margin: 0px; padding: 0px; border: 0px; outline-style: none; font-weight: normal; font-style: normal; font-size: 13px; font-family: Arial, Helvetica, sans-serif; line-height: 16.6399993896484px; text-align: left; vertical-align: baseline; color: #403838; font-variant: normal; letter-spacing: normal; orphans: auto; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: #ffffff;\"></div>\n</p>\n<h1 id=\"article-title-1\" style=\"margin: 10px 0px 0px; padding: 0px; border: 0px; outline-style: none; font-style: normal; font-size: 1.8em; font-family: Arial, Helvetica, sans-serif; line-height: inherit; text-align: left; vertical-align: baseline; color: #403838; font-variant: normal; letter-spacing: normal; orphans: auto; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: #ffffff;\"><a title=\"Why Science is Not Necessarily Self-Correcting\" href=\"http://pps.sagepub.com/content/7/6/645.abstract\">Why Science Is Not Necessarily Self-Correcting</a>&nbsp;<a class=\"name-search\" style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 13px; margin: 0px; padding: 0px; border: 0px; outline-style: none; line-height: 21.7600002288818px; vertical-align: 0px; text-decoration: none; color: #333333; white-space: nowrap; font-weight: normal;\" href=\"http://pps.sagepub.com/search?author1=John+P.+A.+Ioannidis&amp;sortspec=date&amp;submit=Submit\">John P. A. Ioannidis</a></h1>\n<p><span style=\"color: #403838; font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 19.2000007629395px; text-align: justify;\">The ability to self-correct is considered a hallmark of science. However, self-correction does not always happen to scientific evidence by default. The trajectory of scientific credibility can fluctuate over time, both for defined scientific fields and for science at-large. History suggests that major catastrophes in scientific credibility are unfortunately possible and the argument that &ldquo;it is obvious that progress is made&rdquo; is weak. Careful evaluation of the current status of credibility of various scientific fields is important in order to understand any credibility deficits and how one could obtain and establish more trustworthy results. Efficient and unbiased replication mechanisms are essential for maintaining high levels of scientific credibility. Depending on the types of results obtained in the discovery and replication phases, there are different paradigms of research: optimal, self-correcting, false nonreplication, and perpetuated fallacy. In the absence of replication efforts, one is left with unconfirmed (genuine) discoveries and unchallenged fallacies. In several fields of investigation, including many areas of psychological science, perpetuated and unchallenged fallacies may comprise the majority of the circulating evidence. I catalogue a number of impediments to self-correction that have been empirically studied in psychological science. Finally, I discuss some proposed solutions to promote sound replication practices enhancing the credibility of scientific results as well as some potential disadvantages of each of them. Any deviation from the principle that seeking the truth has priority over any other goals may be seriously damaging to the self-correcting functions of science.</span></p>\n</blockquote>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "efSuNL59cEzDtqXQ9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 2.0822063643179105e-06, "legacy": true, "legacyId": "27359", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-14T01:00:46.144Z", "modifiedAt": null, "url": null, "title": "Superintelligence 5: Forms of Superintelligence", "slug": "superintelligence-5-forms-of-superintelligence", "viewCount": null, "lastCommentedAt": "2020-07-17T21:23:00.176Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/semvkn56ZFcXBNc2d/superintelligence-5-forms-of-superintelligence", "pageUrlRelative": "/posts/semvkn56ZFcXBNc2d/superintelligence-5-forms-of-superintelligence", "linkUrl": "https://www.lesswrong.com/posts/semvkn56ZFcXBNc2d/superintelligence-5-forms-of-superintelligence", "postedAtFormatted": "Tuesday, October 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Superintelligence%205%3A%20Forms%20of%20Superintelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuperintelligence%205%3A%20Forms%20of%20Superintelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsemvkn56ZFcXBNc2d%2Fsuperintelligence-5-forms-of-superintelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Superintelligence%205%3A%20Forms%20of%20Superintelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsemvkn56ZFcXBNc2d%2Fsuperintelligence-5-forms-of-superintelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsemvkn56ZFcXBNc2d%2Fsuperintelligence-5-forms-of-superintelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1486, "htmlBody": "<p><em>This is part of a weekly reading group on&nbsp;<a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a>'s book,&nbsp;<a href=\"http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111\">Superintelligence</a>. For more information about the group, and an index of posts so far see the&nbsp;<a href=\"/lw/kw4/superintelligence_reading_group/\">announcement post</a>. For the schedule of future topics, see&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2014/08/Superintelligence-Readers-Guide-early-version.pdf\">MIRI's reading guide</a>.</em></p>\n<hr />\n<p>Welcome. This week we discuss the fifth section in the&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2014/08/Superintelligence-Readers-Guide-early-version.pdf\">reading guide</a>:&nbsp;<em><strong>Forms of superintelligence</strong></em>. This corresponds to Chapter 3, on different ways in which an intelligence can be super.</p>\n<p>This post summarizes the section, and offers a few relevant notes, and ideas for further investigation. Some of my own thoughts and questions for discussion are in the comments.</p>\n<p>There is no need to proceed in order through this post, or to look at everything. Feel free to jump straight to the discussion. Where applicable and I remember, page numbers indicate the rough part of the chapter that is most related (not necessarily that the chapter is being cited for the specific claim).</p>\n<p><strong>Reading</strong>:&nbsp;Chapter 3 (p52-61)</p>\n<hr />\n<h1>Summary</h1>\n<ol>\n<li><strong>A <em>speed superintelligence </em>could do what a human does, but faster</strong>. This would make the outside world seem very slow to it. It might cope with this partially by being very tiny, or virtual. (p53)</li>\n<li><strong>A <em>collective superintelligence </em>is composed of smaller intellects</strong>, interacting in some way. It is especially good at tasks that can be broken into parts and completed in parallel. It can be improved by adding more smaller intellects, or by organizing them better. (p54)</li>\n<li><strong>A <em>quality superintelligence</em>&nbsp;can carry out intellectual tasks that humans just can't in practice</strong>, without&nbsp;necessarily being better or faster at the things humans can do. This can be understood by analogy with the difference between other animals and humans, or the difference between humans with and without certain cognitive capabilities. (p56-7)</li>\n<li>These different kinds of superintelligence are especially good at different kinds of tasks. We might say<strong> they have different 'direct reach'</strong>. Ultimately they could all lead to one another, so can indirectly carry out the same tasks. We might say <strong>their 'indirect reach' is the same</strong>. (p58-9)</li>\n<li>We don't know how smart it is possible for a biological or a synthetic intelligence to be. Nonetheless we can be confident that <strong>synthetic entities can be much more intelligent than biological entities</strong>.&nbsp;<ol>\n<li>Digital intelligences would have <strong>better hardware</strong>: they would be made of components ten million times faster than neurons; the components could communicate about two million times faster than neurons can; they could use many more components while our brains are constrained to our skulls; it looks like better memory should be feasible; and they could be built to be more reliable, long-lasting, flexible, and well suited to their environment.</li>\n<li>Digital intelligences would have <strong>better software</strong>: they could be cheaply and non-destructively 'edited'; they could be duplicated arbitrarily; they could have well aligned goals as a result of this duplication; they could share memories (at least for some forms of AI); and they could have powerful dedicated software (like our vision system) for domains where we have to rely on slow general reasoning.</li>\n</ol></li>\n</ol>\n<h1>Notes</h1>\n<div><ol>\n<li>This chapter is about different kinds of superintelligent entities that could exist. I like to think about the closely related question,<strong>&nbsp;'what kinds of&nbsp;<em>better</em>&nbsp;can intelligence be?'&nbsp;</strong>You can be a better baker if you can bake a cake faster, or bake more cakes, or bake better cakes. Similarly, a system can become more intelligent if it can do the same intelligent things faster, or if it does things that are qualitatively more intelligent. (Collective intelligence seems somewhat different, in that it appears to be a means to be faster or able to do better things, though it may have benefits in dimensions I'm not thinking of.) I think the chapter is getting at different ways intelligence can be better rather than 'forms' in general, which might vary on many other dimensions (e.g. emulation vs AI, goal directed vs. reflexive, nice vs. nasty).</li>\n<li><strong>Some of the hardware and software advantages mentioned would be pretty transformative on their own.</strong> If you haven't before, consider taking a moment to think about what the world would be like if people could be cheaply and perfectly replicated, with their skills intact. Or if people could live arbitrarily long by replacing worn components.&nbsp;</li>\n<li><strong>The main differences between increasing intelligence of a system via speed and via collectiveness</strong>&nbsp;seem to be: (1)&nbsp;the 'collective' route requires that you can break up the task into parallelizable subtasks, (2) it generally has larger costs from communication between those subparts, and (3) it can't produce a single unit as fast as a comparable 'speed-based' system. This suggests that anything a collective intelligence can do, a comparable speed intelligence can do at least as well. One counterexample to this I can think of is that often groups include people with a diversity of knowledge and approaches, and so the group can do a lot more productive thinking than a single person could. It seems wrong to count this as a virtue of collective intelligence in general however, since you could also have a single fast system with varied approaches at different times.</li>\n<li><strong>For each task, we can think of curves for how performance increases as we increase intelligence in these different ways.</strong> For instance, take the task of finding a fact on the internet quickly. It seems to me that a person who ran at 10x speed would get the figure 10x faster. Ten times as many people working in parallel would do it only a bit faster than one, depending on the variance of their individual performance, and whether they found some clever way to complement each other. It's not obvious how to multiply qualitative intelligence by a particular factor, especially as there are different ways to improve the quality of a system. It also seems non-obvious to me how search speed would scale with a particular measure such as IQ.&nbsp;</li>\n<li><strong>How much more intelligent do human systems get as we add more humans?</strong> I can't find much of an answer, but people have investigated the effect of things like <a href=\"http://www.tandfonline.com/doi/abs/10.1080/07399019108964994#.VDxkLdR4p-h\">team size</a>,&nbsp;<a href=\"http://scholar.google.com/scholar?q=productivity+city+size&amp;btnG=&amp;hl=en&amp;as_sdt=0%2C39\">city size</a>, and <a href=\"http://scholar.google.com/scholar?q=productivity+collaboration&amp;btnG=&amp;hl=en&amp;as_sdt=0%2C39\">scientific collaboration</a> on various measures of productivity.</li>\n<li><strong>The things we might think of as collective intelligences - e.g. companies, governments, academic fields - seem notable to me for being slow-moving</strong>, relative to their components. If someone were to steal some chewing gum from Target, Target can respond in the sense that an employee can try to stop them. And this is no slower than an individual human acting to stop their chewing gum from being taken. However it also doesn't involve any extra problem-solving from the organization - to the extent that the organization's intelligence goes into the issue, it has to have already done the thinking ahead of time. Target was probably much smarter than an individual human about setting up the procedures and the incentives to have a person there ready to respond quickly and effectively, but that might have happened over months or years.</li>\n</ol></div>\n<h1>In-depth investigations</h1>\n<p>If you are particularly interested in these topics, and want to do further research, these are a few plausible directions, some inspired by Luke Muehlhauser's&nbsp;<a href=\"http://lukemuehlhauser.com/some-studies-which-could-improve-our-strategic-picture-of-superintelligence/\">list</a>, which contains many suggestions related to parts of&nbsp;<em>Superintelligence.&nbsp;</em>These projects could be attempted at various levels of depth.</p>\n<ol>\n<li>Produce improved measures&nbsp;of (substrate-independent) general intelligence. Build on&nbsp;the ideas of Legg, Yudkowsky, Goertzel, Hernandez-Orallo &amp; Dowe, etc. Differentiate intelligence quality from speed.</li>\n<li>List some feasible&nbsp;but non-realized cognitive talents for humans, and explore what&nbsp;could be achieved if they were given to some humans. </li>\n<li>List and examine some types of problems better solved by a speed superintelligence than by a collective superintelligence, and vice versa. Also, what are the returns on &ldquo;more brains applied to the problem&rdquo; (collective intelligence) for various problems? If there were merely a huge number of human-level agents added to the economy, how much would it speed up economic growth, technological progress, or other relevant metrics? If there were a large number of researchers added to the field of AI, how would it change progress?</li>\n<li>How does intelligence quality improve performance on economically relevant tasks?</li>\n</ol> <ol> </ol>\n<div>If you are interested in anything like this, you might want to mention it in the comments, and see whether other people have useful thoughts.</div>\n<h1>How to proceed</h1>\n<p>This has been a collection of notes on the chapter.&nbsp;&nbsp;<strong>The most important part of the reading group though is discussion</strong>, which is in the comments section. I pose some questions for you there, and I invite you to add your own. Please remember that this group contains a variety of levels of expertise: if a line of discussion seems too basic or too incomprehensible, look around for one that suits you better!</p>\n<p>Next week, we will talk about 'intelligence explosion kinetics', a topic at the center of much contemporary debate over the arrival of machine intelligence. To prepare,&nbsp;<strong>read</strong>&nbsp;Chapter 4,&nbsp;<em>The kinetics of an intelligence explosion&nbsp;</em>(p62-77)<em>.&nbsp;</em>The discussion will go live at 6pm Pacific time next Monday 20 October. Sign up to be notified&nbsp;<a href=\"http://intelligence.us5.list-manage.com/subscribe?u=353906382677fa789a483ba9e&amp;id=28cb982f40\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "tdt83ChxnEgwwKxi6": 1, "5f5c37ee1b5cdee568cfb297": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "semvkn56ZFcXBNc2d", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 22, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "27360", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This is part of a weekly reading group on&nbsp;<a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a>'s book,&nbsp;<a href=\"http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111\">Superintelligence</a>. For more information about the group, and an index of posts so far see the&nbsp;<a href=\"/lw/kw4/superintelligence_reading_group/\">announcement post</a>. For the schedule of future topics, see&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2014/08/Superintelligence-Readers-Guide-early-version.pdf\">MIRI's reading guide</a>.</em></p>\n<hr>\n<p>Welcome. This week we discuss the fifth section in the&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2014/08/Superintelligence-Readers-Guide-early-version.pdf\">reading guide</a>:&nbsp;<em><strong>Forms of superintelligence</strong></em>. This corresponds to Chapter 3, on different ways in which an intelligence can be super.</p>\n<p>This post summarizes the section, and offers a few relevant notes, and ideas for further investigation. Some of my own thoughts and questions for discussion are in the comments.</p>\n<p>There is no need to proceed in order through this post, or to look at everything. Feel free to jump straight to the discussion. Where applicable and I remember, page numbers indicate the rough part of the chapter that is most related (not necessarily that the chapter is being cited for the specific claim).</p>\n<p><strong>Reading</strong>:&nbsp;Chapter 3 (p52-61)</p>\n<hr>\n<h1 id=\"Summary\">Summary</h1>\n<ol>\n<li><strong>A <em>speed superintelligence </em>could do what a human does, but faster</strong>. This would make the outside world seem very slow to it. It might cope with this partially by being very tiny, or virtual. (p53)</li>\n<li><strong>A <em>collective superintelligence </em>is composed of smaller intellects</strong>, interacting in some way. It is especially good at tasks that can be broken into parts and completed in parallel. It can be improved by adding more smaller intellects, or by organizing them better. (p54)</li>\n<li><strong>A <em>quality superintelligence</em>&nbsp;can carry out intellectual tasks that humans just can't in practice</strong>, without&nbsp;necessarily being better or faster at the things humans can do. This can be understood by analogy with the difference between other animals and humans, or the difference between humans with and without certain cognitive capabilities. (p56-7)</li>\n<li>These different kinds of superintelligence are especially good at different kinds of tasks. We might say<strong> they have different 'direct reach'</strong>. Ultimately they could all lead to one another, so can indirectly carry out the same tasks. We might say <strong>their 'indirect reach' is the same</strong>. (p58-9)</li>\n<li>We don't know how smart it is possible for a biological or a synthetic intelligence to be. Nonetheless we can be confident that <strong>synthetic entities can be much more intelligent than biological entities</strong>.&nbsp;<ol>\n<li>Digital intelligences would have <strong>better hardware</strong>: they would be made of components ten million times faster than neurons; the components could communicate about two million times faster than neurons can; they could use many more components while our brains are constrained to our skulls; it looks like better memory should be feasible; and they could be built to be more reliable, long-lasting, flexible, and well suited to their environment.</li>\n<li>Digital intelligences would have <strong>better software</strong>: they could be cheaply and non-destructively 'edited'; they could be duplicated arbitrarily; they could have well aligned goals as a result of this duplication; they could share memories (at least for some forms of AI); and they could have powerful dedicated software (like our vision system) for domains where we have to rely on slow general reasoning.</li>\n</ol></li>\n</ol>\n<h1 id=\"Notes\">Notes</h1>\n<div><ol>\n<li>This chapter is about different kinds of superintelligent entities that could exist. I like to think about the closely related question,<strong>&nbsp;'what kinds of&nbsp;<em>better</em>&nbsp;can intelligence be?'&nbsp;</strong>You can be a better baker if you can bake a cake faster, or bake more cakes, or bake better cakes. Similarly, a system can become more intelligent if it can do the same intelligent things faster, or if it does things that are qualitatively more intelligent. (Collective intelligence seems somewhat different, in that it appears to be a means to be faster or able to do better things, though it may have benefits in dimensions I'm not thinking of.) I think the chapter is getting at different ways intelligence can be better rather than 'forms' in general, which might vary on many other dimensions (e.g. emulation vs AI, goal directed vs. reflexive, nice vs. nasty).</li>\n<li><strong>Some of the hardware and software advantages mentioned would be pretty transformative on their own.</strong> If you haven't before, consider taking a moment to think about what the world would be like if people could be cheaply and perfectly replicated, with their skills intact. Or if people could live arbitrarily long by replacing worn components.&nbsp;</li>\n<li><strong>The main differences between increasing intelligence of a system via speed and via collectiveness</strong>&nbsp;seem to be: (1)&nbsp;the 'collective' route requires that you can break up the task into parallelizable subtasks, (2) it generally has larger costs from communication between those subparts, and (3) it can't produce a single unit as fast as a comparable 'speed-based' system. This suggests that anything a collective intelligence can do, a comparable speed intelligence can do at least as well. One counterexample to this I can think of is that often groups include people with a diversity of knowledge and approaches, and so the group can do a lot more productive thinking than a single person could. It seems wrong to count this as a virtue of collective intelligence in general however, since you could also have a single fast system with varied approaches at different times.</li>\n<li><strong>For each task, we can think of curves for how performance increases as we increase intelligence in these different ways.</strong> For instance, take the task of finding a fact on the internet quickly. It seems to me that a person who ran at 10x speed would get the figure 10x faster. Ten times as many people working in parallel would do it only a bit faster than one, depending on the variance of their individual performance, and whether they found some clever way to complement each other. It's not obvious how to multiply qualitative intelligence by a particular factor, especially as there are different ways to improve the quality of a system. It also seems non-obvious to me how search speed would scale with a particular measure such as IQ.&nbsp;</li>\n<li><strong>How much more intelligent do human systems get as we add more humans?</strong> I can't find much of an answer, but people have investigated the effect of things like <a href=\"http://www.tandfonline.com/doi/abs/10.1080/07399019108964994#.VDxkLdR4p-h\">team size</a>,&nbsp;<a href=\"http://scholar.google.com/scholar?q=productivity+city+size&amp;btnG=&amp;hl=en&amp;as_sdt=0%2C39\">city size</a>, and <a href=\"http://scholar.google.com/scholar?q=productivity+collaboration&amp;btnG=&amp;hl=en&amp;as_sdt=0%2C39\">scientific collaboration</a> on various measures of productivity.</li>\n<li><strong>The things we might think of as collective intelligences - e.g. companies, governments, academic fields - seem notable to me for being slow-moving</strong>, relative to their components. If someone were to steal some chewing gum from Target, Target can respond in the sense that an employee can try to stop them. And this is no slower than an individual human acting to stop their chewing gum from being taken. However it also doesn't involve any extra problem-solving from the organization - to the extent that the organization's intelligence goes into the issue, it has to have already done the thinking ahead of time. Target was probably much smarter than an individual human about setting up the procedures and the incentives to have a person there ready to respond quickly and effectively, but that might have happened over months or years.</li>\n</ol></div>\n<h1 id=\"In_depth_investigations\">In-depth investigations</h1>\n<p>If you are particularly interested in these topics, and want to do further research, these are a few plausible directions, some inspired by Luke Muehlhauser's&nbsp;<a href=\"http://lukemuehlhauser.com/some-studies-which-could-improve-our-strategic-picture-of-superintelligence/\">list</a>, which contains many suggestions related to parts of&nbsp;<em>Superintelligence.&nbsp;</em>These projects could be attempted at various levels of depth.</p>\n<ol>\n<li>Produce improved measures&nbsp;of (substrate-independent) general intelligence. Build on&nbsp;the ideas of Legg, Yudkowsky, Goertzel, Hernandez-Orallo &amp; Dowe, etc. Differentiate intelligence quality from speed.</li>\n<li>List some feasible&nbsp;but non-realized cognitive talents for humans, and explore what&nbsp;could be achieved if they were given to some humans. </li>\n<li>List and examine some types of problems better solved by a speed superintelligence than by a collective superintelligence, and vice versa. Also, what are the returns on \u201cmore brains applied to the problem\u201d (collective intelligence) for various problems? If there were merely a huge number of human-level agents added to the economy, how much would it speed up economic growth, technological progress, or other relevant metrics? If there were a large number of researchers added to the field of AI, how would it change progress?</li>\n<li>How does intelligence quality improve performance on economically relevant tasks?</li>\n</ol> <ol> </ol>\n<div>If you are interested in anything like this, you might want to mention it in the comments, and see whether other people have useful thoughts.</div>\n<h1 id=\"How_to_proceed\">How to proceed</h1>\n<p>This has been a collection of notes on the chapter.&nbsp;&nbsp;<strong>The most important part of the reading group though is discussion</strong>, which is in the comments section. I pose some questions for you there, and I invite you to add your own. Please remember that this group contains a variety of levels of expertise: if a line of discussion seems too basic or too incomprehensible, look around for one that suits you better!</p>\n<p>Next week, we will talk about 'intelligence explosion kinetics', a topic at the center of much contemporary debate over the arrival of machine intelligence. To prepare,&nbsp;<strong>read</strong>&nbsp;Chapter 4,&nbsp;<em>The kinetics of an intelligence explosion&nbsp;</em>(p62-77)<em>.&nbsp;</em>The discussion will go live at 6pm Pacific time next Monday 20 October. Sign up to be notified&nbsp;<a href=\"http://intelligence.us5.list-manage.com/subscribe?u=353906382677fa789a483ba9e&amp;id=28cb982f40\">here</a>.</p>", "sections": [{"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"title": "In-depth investigations", "anchor": "In_depth_investigations", "level": 1}, {"title": "How to proceed", "anchor": "How_to_proceed", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "114 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 114, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QDmzDZ9CEHrKQdvcn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-15T01:59:05.567Z", "modifiedAt": "2020-06-25T22:34:03.591Z", "url": null, "title": "On Caring", "slug": "on-caring", "viewCount": null, "lastCommentedAt": "2021-08-26T17:20:24.056Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "So8res", "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ur9TCRnHJighHmLCW/on-caring", "pageUrlRelative": "/posts/ur9TCRnHJighHmLCW/on-caring", "linkUrl": "https://www.lesswrong.com/posts/ur9TCRnHJighHmLCW/on-caring", "postedAtFormatted": "Wednesday, October 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20Caring&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20Caring%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fur9TCRnHJighHmLCW%2Fon-caring%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20Caring%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fur9TCRnHJighHmLCW%2Fon-caring", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fur9TCRnHJighHmLCW%2Fon-caring", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2895, "htmlBody": "<p style=\"color:#999999;\"><em>This is an essay describing some of my motivation to be an effective altruist. It is <a href=\"http://mindingourway.com/on-caring/\">crossposted</a> from <a href=\"http://mindingourway.com/\">my blog</a>. Many of the ideas here are quite similar to <a href=\"/lw/hw/scope_insensitivity/\">others found in the sequences</a>.&nbsp;I have a slightly different take, and after adjusting for the typical mind fallacy I expect that this post may contain insights that are new to many.</em></p>\n<h1>1</h1>\n<p>I'm not very good at <em>feeling</em> the size of large numbers. Once you start tossing around numbers larger than 1000 (or maybe even 100), the numbers just seem \"big\".</p>\n<p>Consider Sirius, the brightest star in the night sky. If you told me that Sirius is as big as a million earths, I would feel like that's a lot of Earths. If, instead, you told me that you could fit a <em>billion</em> Earths inside Sirius&hellip; I would still just feel like that's a lot of Earths.</p>\n<p>The feelings are almost identical. <em>In context</em>, my brain grudgingly admits that a billion is a lot larger than a million, and puts forth a token effort to feel like a billion-Earth-sized star is bigger than a million-Earth-sized star. But out of context &mdash; if I wasn't anchored at \"a million\" when I heard \"a billion\" &mdash; both these numbers just feel vaguely large.</p>\n<p>I feel a <em>little</em> respect for the bigness of numbers, if you pick really really large numbers. If you say \"one followed by a hundred zeroes\", then this feels <em>a lot</em> bigger than a billion. But it certainly doesn't feel (in my gut) like it's 10 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 times bigger than a billion. Not in the way that four apples <em>internally feels</em> like twice as many as two apples. My brain can't even begin to wrap itself around this sort of magnitude differential.</p>\n<p>This phenomena is related to <a href=\"/lw/hw/scope_insensitivity/\">scope insensitivity</a>, and it's important to me because I live in a world where sometimes the things I care about are really really numerous.</p>\n<p>For example, <a href=\"http://www.globalissues.org/article/26/poverty-facts-and-stats\">billions of people live in squalor</a>, with hundreds of millions of them deprived of basic needs and/or dying from disease. And though most of them are out of my sight, I still care about them.</p>\n<p>The loss of a human life with all is joys and all its sorrows is tragic no matter what the cause, and the tragedy is not reduced simply because I was far away, or because I did not know of it, or because I did not know how to help, or because I was not personally responsible.</p>\n<p>Knowing this, I care about every single individual on this planet. The problem is, my brain is <em>simply incapable</em> of taking the amount of caring I feel for a single person and scaling it up by a billion times. I lack the internal capacity to feel that much. My care-o-meter simply doesn't go up that far.</p>\n<p>And this is a problem.</p>\n<p><a id=\"more\"></a></p>\n<h1>2</h1>\n<p>It's a common trope that courage isn't about being fearless, it's about being afraid but <em>doing the right thing anyway</em>. In the same sense, caring about the world isn't about having a gut feeling that corresponds to the amount of suffering in the world, it's about <em>doing the right thing anyway</em>. Even without the feeling.</p>\n<p>My internal care-o-meter was calibrated to deal with <a href=\"http://en.wikipedia.org/wiki/Dunbar's_number\">about a hundred and fifty people</a>, and it <em>simply can't express</em> the amount of caring that I have for billions of sufferers. The internal care-o-meter just doesn't go up that high.</p>\n<p>Humanity is playing for unimaginably high stakes. At the very least, there are billions of people suffering today. At the worst, there are quadrillions (or more) potential humans, transhumans, or posthumans whose existence depends upon what we do here and now. All the intricate civilizations that the future could hold, the experience and art and beauty that is possible in the future, depends upon the present.</p>\n<p>When you're faced with stakes like these, your internal caring heuristics &mdash; calibrated on numbers like \"ten\" or \"twenty\"&nbsp;&mdash; completely fail to grasp the gravity of the situation.</p>\n<p>Saving a person's life feels <em>great</em>, and <a href=\"/lw/hx/one_life_against_the_world/\">it would probably feel just about as good to save one life as it would feel to save the world</a>. It surely wouldn't be <em>many billion times</em> more of a high to save the world, because your hardware can't express a feeling a billion times bigger than the feeling of saving a person's life. But even though the altruistic high from saving someone's life would be shockingly similar to the altruistic high from saving the world, always remember that <em>behind</em> those similar feelings there is a whole world of difference.</p>\n<p>Our internal care-feelings are woefully inadequate for deciding how to act in a world with big problems.</p>\n<h1>3</h1>\n<p>There's a mental shift that happened to me when I first started internalizing scope insensitivity. It is a little difficult to articulate, so I'm going to start with a few stories.</p>\n<p>Consider Alice, a software engineer at Amazon in Seattle. Once a month or so, those college students will show up on street corners with clipboards, looking ever more disillusioned as they struggle to convince people to donate to <a href=\"http://www.doctorswithoutborders.org/\">Doctors Without Borders</a>. Usually, Alice avoids eye contact and goes about her day, but this month they finally manage to corner her. They explain Doctors Without Borders, and she actually has to admit that it sounds like a pretty good cause. She ends up handing them $20 through a combination of guilt, social pressure, and altruism, and then rushes back to work. (Next month, when they show up again, she avoids eye contact.)</p>\n<p>Now consider Bob, who has been given the <a href=\"http://en.wikipedia.org/wiki/Ice_Bucket_Challenge\">Ice Bucket Challenge</a> by a friend on facebook. He feels too busy to do the ice bucket challenge, and instead just donates $100 to <a href=\"http://www.alsa.org/\">ALSA</a>.</p>\n<p>Now consider Christine, who is in the college sorority &Alpha;&Delta;&Pi;. &Alpha;&Delta;&Pi; is engaged in a competition with &Pi;&Beta;&Phi; (another sorority) to see who can raise the most money for the National Breast Cancer Foundation in a week. Christine has a competitive spirit and gets engaged in fund-raising, and gives a few hundred dollars herself over the course of the week (especially at times when &Alpha;&Delta;&Pi; is especially behind).</p>\n<p>All three of these people are donating money to charitable organizations&hellip; and that's great. But notice that there's something similar in these three stories: these donations are largely motivated by a <em>social context</em>. Alice feels obligation and social pressure. Bob feels social pressure and maybe a bit of camaraderie. Christine feels camaraderie and competitiveness. These are all fine motivations, but notice that these motivations are related to the <em>social setting</em>, and only tangentially to the <em>content</em> of the charitable donation.</p>\n<p>If you took any of Alice or Bob or Christine and asked them why they aren't donating <em>all</em> of their time and money to these causes that they apparently believe are worthwhile, they'd look at you funny and they'd probably think you were being rude (with good reason!). If you pressed, they might tell you that money is a little tight right now, or that they would donate more if they were a better person.</p>\n<p>But the question would still feel kind of <em>wrong</em>. Giving all your money away is just not what you do with money. We can all <em>say out loud</em> that people who give all their possessions away are really great, but behind closed doors we all know that people are crazy. (Good crazy, perhaps, but crazy all the same.)</p>\n<p>This is a mindset that I inhabited for a while. There's an alternative mindset that can hit you like a freight train when you start internalizing scope insensitivity.</p>\n<h1>4</h1>\n<p>Consider Daniel, a college student shortly after the <a href=\"http://en.wikipedia.org/wiki/Deepwater_Horizon_oil_spill\">Deepwater Horizon</a> BP oil spill. He encounters one of those college students with the clipboards on the street corners, soliciting donations to the <a href=\"http://www.worldwildlife.org/\">World Wildlife Foundation</a>. They're trying to save as many oiled birds as possible. Normally, Daniel would simply dismiss the charity as Not The Most Important Thing, or Not Worth His Time Right Now, or Somebody Else's Problem, but this time Daniel has been thinking about how his brain is bad at numbers and decides to do a quick sanity check.</p>\n<p>He pictures himself walking along the beach after the oil spill, and encountering a group of people cleaning birds as fast as they can. They simply don't have the resources to clean all the available birds. A pathetic young bird flops towards his feet, slick with oil, eyes barely able to open. He kneels down to pick it up and help it onto the table. One of the bird-cleaners informs him that they won't have time to get to that bird themselves, but he could pull on some gloves and could probably save the bird with three minutes of washing.</p>\n<p style=\"text-align:center\"><img src=\"http://4.bp.blogspot.com/_Bv2wRceCYKA/S-whZ1sSJ3I/AAAAAAAABjo/cgXz7Npz_W0/s320/Dawn_IBRRC_2010.05.07_MG_7082_240.jpg\" alt=\"blog.bird-rescue.org\" /></p>\n<p>Daniel decides that he <em>would</em> spend three minutes of his time to save the bird, and that he would <em>also</em> be happy to pay at least $3 to have someone else spend a few minutes cleaning the bird. He introspects and finds that this is not just because he imagined a bird right in front of him: he feels that it is <em>worth</em> at least three minutes of his time (or $3) to save an oiled bird in some vague platonic sense.</p>\n<p>And, because he's been thinking about scope insensitivity, he <em>expects</em> his brain to misreport how much he actually cares about large numbers of birds: the internal feeling of caring can't be expected to line up with the actual importance of the situation. So instead of just <em>asking his gut</em> how much he cares about de-oiling lots of birds, he shuts up and multiplies.</p>\n<p><a href=\"http://dailydeadbirds.com/\">Thousands and thousands</a> of birds were oiled by the BP spill alone. After shutting up and multiplying, Daniel realizes (with growing horror) that the amount he <em>acutally</em> cares about oiled birds is lower bounded by two months of hard work and/or fifty thousand dollars. And that's not even counting wildlife threatened by <a href=\"http://en.wikipedia.org/wiki/List_of_oil_spills\">other oil spills</a>.</p>\n<p>And if he cares that much about <em>de-oiling birds</em>, then how much does he actually care about factory farming, nevermind hunger, or poverty, or sickness? How much does he actually care about wars that ravage nations? About neglected, deprived children? About the future of humanity? He <em>actually</em> cares about these things to the tune of much more money than he has, and much more time than he has.</p>\n<p>For the first time, Daniel sees a glimpse of of how much he actually cares, and how poor a state the world is in.</p>\n<p>This has the strange effect that Daniel's reasoning goes full-circle, and he realizes that he actually <em>can't</em> care about oiled birds to the tune of 3 minutes or $3: not because the birds aren't <em>worth</em> the time and money (and, in fact, he thinks that the economy produces things priced at $3 which are worth less than the bird's survival), but because he can't spend <em>his</em> time or money on saving the birds. The opportunity cost suddenly seems far too high: there is <em>too much else to do!</em> People are sick and starving and dying! The very future of our civilization is at stake!</p>\n<p>Daniel doesn't wind up giving $50k to the WWF, and he also doesn't donate to ALSA or NBCF. But if you ask <em>Daniel</em> why he's not donating all his money, he won't look at you funny or think you're rude. He's left the place where you don't care far behind, and has realized that <em>his mind was lying to him the whole time</em> about the gravity of the real problems.</p>\n<p>Now he realizes that he <em>can't possibly do enough</em>. After adjusting for his scope insensitivity (and the fact that his brain lies about the size of large numbers), even the \"less important\" causes like the WWF suddenly seem worthy of dedicating a life to. Wildlife destruction and ALS and breast cancer are suddenly all problems that he would <em>move mountains</em> to solve &mdash; except he's finally understood that there are just too many mountains, and ALS isn't the bottleneck, and AHHH HOW DID ALL THESE MOUNTAINS GET HERE?</p>\n<p>In the original mindstate, the reason he didn't drop everything to work on ALS was because it just didn't seem&hellip; pressing enough. Or tractable enough. Or important enough. Kind of. These are sort of the reason, but the real reason is more that the concept of \"dropping everything to address ALS\" never even <em>crossed his mind</em> as a real possibility. The idea was too much of a break from the standard narrative. It wasn't his problem.</p>\n<p>In the new mindstate, <em>everything</em> is his problem. The only reason he's not dropping everything to work on ALS is because there are far too many things to do first.</p>\n<p>Alice and Bob and Christine usually aren't spending time solving all the world's problems because they forget to see them. If you remind them &mdash; put them in a social context where they remember how much they care (hopefully without guilt or pressure) &mdash; then they'll likely donate a little money.</p>\n<p>By contrast, Daniel and others who have undergone the mental shift aren't spending time solving all the world's problems because there are <em>just too many problems</em>. (Daniel hopefully goes on to discover movements like <a href=\"http://effectivealtruism.org/\">effective altruism</a> and starts contributing towards fixing the world's most pressing problems.)</p>\n<h1>5</h1>\n<p>I'm not trying to preach here about how to be a good person. You don't need to share my viewpoint to be a good person (obviously).</p>\n<p>Rather, I'm trying to point at a shift in perspective. Many of us go through life understanding that we <em>should</em> care about people suffering far away from us, but failing to. I think that this attitude is tied, at least in part, to the fact that most of us implicitly trust our internal care-o-meters.</p>\n<p>The \"care feeling\" isn't usually strong enough to compel us to frantically save everyone dying. So while we acknowledge that it would be <em>virtuous</em> to do more for the world, we think that we <em>can't</em>, because we weren't gifted with that virtuous extra-caring that prominent altruists must have.</p>\n<p>But this is an error &mdash; prominent altruists aren't the people who have a larger care-o-meter, they're the people who have <em>learned not to trust their care-o-meters</em>.</p>\n<p>Our care-o-meters are broken. They don't work on large numbers. Nobody has one capable of faithfully representing the scope of the world's problems. But the fact that you can't <em>feel</em> the caring doesn't mean that you can't <em>do</em> the caring.</p>\n<p>You don't get to feel the appropriate amount of \"care\", in your body. Sorry &mdash; the world's problems are just too large, and your body is not built to respond appropriately to problems of this magnitude. But if you choose to do so, you can still <em>act</em> like the world's problems are as big as they are. You can stop trusting the internal feelings to guide your actions and switch over to manual control.</p>\n<h1>6</h1>\n<p>This, of course, leads us to the question of \"what the hell do you then?\"</p>\n<p>And I don't really know yet. (Though I'll plug the <a href=\"http://givingwhatwecan.org\">Giving What We Can pledge</a>, <a href=\"http://givewell.org\">GiveWell</a>, <a href=\"http://intelligence.org\">MIRI</a>, and <a href=\"http://www.fhi.ox.ac.uk\">The Future of Humanity Institute</a> as a good start).</p>\n<p>I think that at least part of it comes from a certain sort of desperate perspective. It's not enough to think you <em>should</em> change the world &mdash; you also need the sort of desperation that comes from realizing that you would dedicate your entire life to solving the world's 100th biggest problem if you could, but you can't, because there are 99 bigger problems you have to address first.</p>\n<p>I'm not trying to guilt you into giving more money away &mdash; becoming a philanthropist is <em>really really hard</em>. (If you're <em>already</em> a philanthropist, then you have my acclaim and my affection.) First it requires you to have money, which is uncommon, and then it requires you to <em>throw that money at distant invisible problems</em>, which is not an easy sell to a human brain. <a href=\"http://en.wikipedia.org/wiki/Akrasia\">Akrasia</a> is a formidable enemy. And most importantly, guilt doesn't seem like a good long-term motivator: if you want to join the ranks of people saving the world, I would rather you join them proudly. There are many trials and tribulations ahead, and we'd do better to face them with our heads held high.</p>\n<h1>7</h1>\n<p>Courage isn't about being fearless, it's about being able to do the right thing even if you're afraid.</p>\n<p>And similarly, addressing the major problems of our time isn't about feeling a strong compulsion to do so. It's about doing it anyway, even when internal compulsion utterly fails to capture the scope of the problems we face.</p>\n<p>It's easy to look at especially virtuous people &mdash; Gandhi, Mother Theresa, Nelson Mandela &mdash; and conclude that they must have cared more than we do. But I don't think that's the case.</p>\n<p>Nobody gets to comprehend the scope of these problems. The closest we can get is doing the multiplication: finding something we care about, putting a number on it, and multiplying. And then trusting the numbers more than we trust our feelings.</p>\n<p>Because our feelings lie to us.</p>\n<p>When you do the multiplication, you realize that addressing global poverty and building a brighter future deserve more resources than currently exist. There is not enough money, time, or effort in the world to do what we need to do.</p>\n<p>There is only you, and me, and everyone else who is trying anyway.</p>\n<h1>8</h1>\n<p>You can't actually feel the weight of the world. The human mind is not capable of that feat.</p>\n<p>But sometimes, you can catch a glimpse.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"iP2X4jQNHMWHRNPne": 12, "xexCWMyds6QLWognu": 12, "9DmA84e4ZvYoYu6q8": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ur9TCRnHJighHmLCW", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 148, "baseScore": 192, "extendedScore": null, "score": 0.000542, "legacy": true, "legacyId": "27324", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "pFatcKW3JJhTSxqAF", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "the-value-of-a-life", "canonicalPrevPostSlug": "how-we-will-be-measured", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 192, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"color:#999999;\"><em>This is an essay describing some of my motivation to be an effective altruist. It is <a href=\"http://mindingourway.com/on-caring/\">crossposted</a> from <a href=\"http://mindingourway.com/\">my blog</a>. Many of the ideas here are quite similar to <a href=\"/lw/hw/scope_insensitivity/\">others found in the sequences</a>.&nbsp;I have a slightly different take, and after adjusting for the typical mind fallacy I expect that this post may contain insights that are new to many.</em></p>\n<h1 id=\"1\">1</h1>\n<p>I'm not very good at <em>feeling</em> the size of large numbers. Once you start tossing around numbers larger than 1000 (or maybe even 100), the numbers just seem \"big\".</p>\n<p>Consider Sirius, the brightest star in the night sky. If you told me that Sirius is as big as a million earths, I would feel like that's a lot of Earths. If, instead, you told me that you could fit a <em>billion</em> Earths inside Sirius\u2026 I would still just feel like that's a lot of Earths.</p>\n<p>The feelings are almost identical. <em>In context</em>, my brain grudgingly admits that a billion is a lot larger than a million, and puts forth a token effort to feel like a billion-Earth-sized star is bigger than a million-Earth-sized star. But out of context \u2014 if I wasn't anchored at \"a million\" when I heard \"a billion\" \u2014 both these numbers just feel vaguely large.</p>\n<p>I feel a <em>little</em> respect for the bigness of numbers, if you pick really really large numbers. If you say \"one followed by a hundred zeroes\", then this feels <em>a lot</em> bigger than a billion. But it certainly doesn't feel (in my gut) like it's 10 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 times bigger than a billion. Not in the way that four apples <em>internally feels</em> like twice as many as two apples. My brain can't even begin to wrap itself around this sort of magnitude differential.</p>\n<p>This phenomena is related to <a href=\"/lw/hw/scope_insensitivity/\">scope insensitivity</a>, and it's important to me because I live in a world where sometimes the things I care about are really really numerous.</p>\n<p>For example, <a href=\"http://www.globalissues.org/article/26/poverty-facts-and-stats\">billions of people live in squalor</a>, with hundreds of millions of them deprived of basic needs and/or dying from disease. And though most of them are out of my sight, I still care about them.</p>\n<p>The loss of a human life with all is joys and all its sorrows is tragic no matter what the cause, and the tragedy is not reduced simply because I was far away, or because I did not know of it, or because I did not know how to help, or because I was not personally responsible.</p>\n<p>Knowing this, I care about every single individual on this planet. The problem is, my brain is <em>simply incapable</em> of taking the amount of caring I feel for a single person and scaling it up by a billion times. I lack the internal capacity to feel that much. My care-o-meter simply doesn't go up that far.</p>\n<p>And this is a problem.</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"2\">2</h1>\n<p>It's a common trope that courage isn't about being fearless, it's about being afraid but <em>doing the right thing anyway</em>. In the same sense, caring about the world isn't about having a gut feeling that corresponds to the amount of suffering in the world, it's about <em>doing the right thing anyway</em>. Even without the feeling.</p>\n<p>My internal care-o-meter was calibrated to deal with <a href=\"http://en.wikipedia.org/wiki/Dunbar's_number\">about a hundred and fifty people</a>, and it <em>simply can't express</em> the amount of caring that I have for billions of sufferers. The internal care-o-meter just doesn't go up that high.</p>\n<p>Humanity is playing for unimaginably high stakes. At the very least, there are billions of people suffering today. At the worst, there are quadrillions (or more) potential humans, transhumans, or posthumans whose existence depends upon what we do here and now. All the intricate civilizations that the future could hold, the experience and art and beauty that is possible in the future, depends upon the present.</p>\n<p>When you're faced with stakes like these, your internal caring heuristics \u2014 calibrated on numbers like \"ten\" or \"twenty\"&nbsp;\u2014 completely fail to grasp the gravity of the situation.</p>\n<p>Saving a person's life feels <em>great</em>, and <a href=\"/lw/hx/one_life_against_the_world/\">it would probably feel just about as good to save one life as it would feel to save the world</a>. It surely wouldn't be <em>many billion times</em> more of a high to save the world, because your hardware can't express a feeling a billion times bigger than the feeling of saving a person's life. But even though the altruistic high from saving someone's life would be shockingly similar to the altruistic high from saving the world, always remember that <em>behind</em> those similar feelings there is a whole world of difference.</p>\n<p>Our internal care-feelings are woefully inadequate for deciding how to act in a world with big problems.</p>\n<h1 id=\"3\">3</h1>\n<p>There's a mental shift that happened to me when I first started internalizing scope insensitivity. It is a little difficult to articulate, so I'm going to start with a few stories.</p>\n<p>Consider Alice, a software engineer at Amazon in Seattle. Once a month or so, those college students will show up on street corners with clipboards, looking ever more disillusioned as they struggle to convince people to donate to <a href=\"http://www.doctorswithoutborders.org/\">Doctors Without Borders</a>. Usually, Alice avoids eye contact and goes about her day, but this month they finally manage to corner her. They explain Doctors Without Borders, and she actually has to admit that it sounds like a pretty good cause. She ends up handing them $20 through a combination of guilt, social pressure, and altruism, and then rushes back to work. (Next month, when they show up again, she avoids eye contact.)</p>\n<p>Now consider Bob, who has been given the <a href=\"http://en.wikipedia.org/wiki/Ice_Bucket_Challenge\">Ice Bucket Challenge</a> by a friend on facebook. He feels too busy to do the ice bucket challenge, and instead just donates $100 to <a href=\"http://www.alsa.org/\">ALSA</a>.</p>\n<p>Now consider Christine, who is in the college sorority \u0391\u0394\u03a0. \u0391\u0394\u03a0 is engaged in a competition with \u03a0\u0392\u03a6 (another sorority) to see who can raise the most money for the National Breast Cancer Foundation in a week. Christine has a competitive spirit and gets engaged in fund-raising, and gives a few hundred dollars herself over the course of the week (especially at times when \u0391\u0394\u03a0 is especially behind).</p>\n<p>All three of these people are donating money to charitable organizations\u2026 and that's great. But notice that there's something similar in these three stories: these donations are largely motivated by a <em>social context</em>. Alice feels obligation and social pressure. Bob feels social pressure and maybe a bit of camaraderie. Christine feels camaraderie and competitiveness. These are all fine motivations, but notice that these motivations are related to the <em>social setting</em>, and only tangentially to the <em>content</em> of the charitable donation.</p>\n<p>If you took any of Alice or Bob or Christine and asked them why they aren't donating <em>all</em> of their time and money to these causes that they apparently believe are worthwhile, they'd look at you funny and they'd probably think you were being rude (with good reason!). If you pressed, they might tell you that money is a little tight right now, or that they would donate more if they were a better person.</p>\n<p>But the question would still feel kind of <em>wrong</em>. Giving all your money away is just not what you do with money. We can all <em>say out loud</em> that people who give all their possessions away are really great, but behind closed doors we all know that people are crazy. (Good crazy, perhaps, but crazy all the same.)</p>\n<p>This is a mindset that I inhabited for a while. There's an alternative mindset that can hit you like a freight train when you start internalizing scope insensitivity.</p>\n<h1 id=\"4\">4</h1>\n<p>Consider Daniel, a college student shortly after the <a href=\"http://en.wikipedia.org/wiki/Deepwater_Horizon_oil_spill\">Deepwater Horizon</a> BP oil spill. He encounters one of those college students with the clipboards on the street corners, soliciting donations to the <a href=\"http://www.worldwildlife.org/\">World Wildlife Foundation</a>. They're trying to save as many oiled birds as possible. Normally, Daniel would simply dismiss the charity as Not The Most Important Thing, or Not Worth His Time Right Now, or Somebody Else's Problem, but this time Daniel has been thinking about how his brain is bad at numbers and decides to do a quick sanity check.</p>\n<p>He pictures himself walking along the beach after the oil spill, and encountering a group of people cleaning birds as fast as they can. They simply don't have the resources to clean all the available birds. A pathetic young bird flops towards his feet, slick with oil, eyes barely able to open. He kneels down to pick it up and help it onto the table. One of the bird-cleaners informs him that they won't have time to get to that bird themselves, but he could pull on some gloves and could probably save the bird with three minutes of washing.</p>\n<p style=\"text-align:center\"><img src=\"http://4.bp.blogspot.com/_Bv2wRceCYKA/S-whZ1sSJ3I/AAAAAAAABjo/cgXz7Npz_W0/s320/Dawn_IBRRC_2010.05.07_MG_7082_240.jpg\" alt=\"blog.bird-rescue.org\"></p>\n<p>Daniel decides that he <em>would</em> spend three minutes of his time to save the bird, and that he would <em>also</em> be happy to pay at least $3 to have someone else spend a few minutes cleaning the bird. He introspects and finds that this is not just because he imagined a bird right in front of him: he feels that it is <em>worth</em> at least three minutes of his time (or $3) to save an oiled bird in some vague platonic sense.</p>\n<p>And, because he's been thinking about scope insensitivity, he <em>expects</em> his brain to misreport how much he actually cares about large numbers of birds: the internal feeling of caring can't be expected to line up with the actual importance of the situation. So instead of just <em>asking his gut</em> how much he cares about de-oiling lots of birds, he shuts up and multiplies.</p>\n<p><a href=\"http://dailydeadbirds.com/\">Thousands and thousands</a> of birds were oiled by the BP spill alone. After shutting up and multiplying, Daniel realizes (with growing horror) that the amount he <em>acutally</em> cares about oiled birds is lower bounded by two months of hard work and/or fifty thousand dollars. And that's not even counting wildlife threatened by <a href=\"http://en.wikipedia.org/wiki/List_of_oil_spills\">other oil spills</a>.</p>\n<p>And if he cares that much about <em>de-oiling birds</em>, then how much does he actually care about factory farming, nevermind hunger, or poverty, or sickness? How much does he actually care about wars that ravage nations? About neglected, deprived children? About the future of humanity? He <em>actually</em> cares about these things to the tune of much more money than he has, and much more time than he has.</p>\n<p>For the first time, Daniel sees a glimpse of of how much he actually cares, and how poor a state the world is in.</p>\n<p>This has the strange effect that Daniel's reasoning goes full-circle, and he realizes that he actually <em>can't</em> care about oiled birds to the tune of 3 minutes or $3: not because the birds aren't <em>worth</em> the time and money (and, in fact, he thinks that the economy produces things priced at $3 which are worth less than the bird's survival), but because he can't spend <em>his</em> time or money on saving the birds. The opportunity cost suddenly seems far too high: there is <em>too much else to do!</em> People are sick and starving and dying! The very future of our civilization is at stake!</p>\n<p>Daniel doesn't wind up giving $50k to the WWF, and he also doesn't donate to ALSA or NBCF. But if you ask <em>Daniel</em> why he's not donating all his money, he won't look at you funny or think you're rude. He's left the place where you don't care far behind, and has realized that <em>his mind was lying to him the whole time</em> about the gravity of the real problems.</p>\n<p>Now he realizes that he <em>can't possibly do enough</em>. After adjusting for his scope insensitivity (and the fact that his brain lies about the size of large numbers), even the \"less important\" causes like the WWF suddenly seem worthy of dedicating a life to. Wildlife destruction and ALS and breast cancer are suddenly all problems that he would <em>move mountains</em> to solve \u2014 except he's finally understood that there are just too many mountains, and ALS isn't the bottleneck, and AHHH HOW DID ALL THESE MOUNTAINS GET HERE?</p>\n<p>In the original mindstate, the reason he didn't drop everything to work on ALS was because it just didn't seem\u2026 pressing enough. Or tractable enough. Or important enough. Kind of. These are sort of the reason, but the real reason is more that the concept of \"dropping everything to address ALS\" never even <em>crossed his mind</em> as a real possibility. The idea was too much of a break from the standard narrative. It wasn't his problem.</p>\n<p>In the new mindstate, <em>everything</em> is his problem. The only reason he's not dropping everything to work on ALS is because there are far too many things to do first.</p>\n<p>Alice and Bob and Christine usually aren't spending time solving all the world's problems because they forget to see them. If you remind them \u2014 put them in a social context where they remember how much they care (hopefully without guilt or pressure) \u2014 then they'll likely donate a little money.</p>\n<p>By contrast, Daniel and others who have undergone the mental shift aren't spending time solving all the world's problems because there are <em>just too many problems</em>. (Daniel hopefully goes on to discover movements like <a href=\"http://effectivealtruism.org/\">effective altruism</a> and starts contributing towards fixing the world's most pressing problems.)</p>\n<h1 id=\"5\">5</h1>\n<p>I'm not trying to preach here about how to be a good person. You don't need to share my viewpoint to be a good person (obviously).</p>\n<p>Rather, I'm trying to point at a shift in perspective. Many of us go through life understanding that we <em>should</em> care about people suffering far away from us, but failing to. I think that this attitude is tied, at least in part, to the fact that most of us implicitly trust our internal care-o-meters.</p>\n<p>The \"care feeling\" isn't usually strong enough to compel us to frantically save everyone dying. So while we acknowledge that it would be <em>virtuous</em> to do more for the world, we think that we <em>can't</em>, because we weren't gifted with that virtuous extra-caring that prominent altruists must have.</p>\n<p>But this is an error \u2014 prominent altruists aren't the people who have a larger care-o-meter, they're the people who have <em>learned not to trust their care-o-meters</em>.</p>\n<p>Our care-o-meters are broken. They don't work on large numbers. Nobody has one capable of faithfully representing the scope of the world's problems. But the fact that you can't <em>feel</em> the caring doesn't mean that you can't <em>do</em> the caring.</p>\n<p>You don't get to feel the appropriate amount of \"care\", in your body. Sorry \u2014 the world's problems are just too large, and your body is not built to respond appropriately to problems of this magnitude. But if you choose to do so, you can still <em>act</em> like the world's problems are as big as they are. You can stop trusting the internal feelings to guide your actions and switch over to manual control.</p>\n<h1 id=\"6\">6</h1>\n<p>This, of course, leads us to the question of \"what the hell do you then?\"</p>\n<p>And I don't really know yet. (Though I'll plug the <a href=\"http://givingwhatwecan.org\">Giving What We Can pledge</a>, <a href=\"http://givewell.org\">GiveWell</a>, <a href=\"http://intelligence.org\">MIRI</a>, and <a href=\"http://www.fhi.ox.ac.uk\">The Future of Humanity Institute</a> as a good start).</p>\n<p>I think that at least part of it comes from a certain sort of desperate perspective. It's not enough to think you <em>should</em> change the world \u2014 you also need the sort of desperation that comes from realizing that you would dedicate your entire life to solving the world's 100th biggest problem if you could, but you can't, because there are 99 bigger problems you have to address first.</p>\n<p>I'm not trying to guilt you into giving more money away \u2014 becoming a philanthropist is <em>really really hard</em>. (If you're <em>already</em> a philanthropist, then you have my acclaim and my affection.) First it requires you to have money, which is uncommon, and then it requires you to <em>throw that money at distant invisible problems</em>, which is not an easy sell to a human brain. <a href=\"http://en.wikipedia.org/wiki/Akrasia\">Akrasia</a> is a formidable enemy. And most importantly, guilt doesn't seem like a good long-term motivator: if you want to join the ranks of people saving the world, I would rather you join them proudly. There are many trials and tribulations ahead, and we'd do better to face them with our heads held high.</p>\n<h1 id=\"7\">7</h1>\n<p>Courage isn't about being fearless, it's about being able to do the right thing even if you're afraid.</p>\n<p>And similarly, addressing the major problems of our time isn't about feeling a strong compulsion to do so. It's about doing it anyway, even when internal compulsion utterly fails to capture the scope of the problems we face.</p>\n<p>It's easy to look at especially virtuous people \u2014 Gandhi, Mother Theresa, Nelson Mandela \u2014 and conclude that they must have cared more than we do. But I don't think that's the case.</p>\n<p>Nobody gets to comprehend the scope of these problems. The closest we can get is doing the multiplication: finding something we care about, putting a number on it, and multiplying. And then trusting the numbers more than we trust our feelings.</p>\n<p>Because our feelings lie to us.</p>\n<p>When you do the multiplication, you realize that addressing global poverty and building a brighter future deserve more resources than currently exist. There is not enough money, time, or effort in the world to do what we need to do.</p>\n<p>There is only you, and me, and everyone else who is trying anyway.</p>\n<h1 id=\"8\">8</h1>\n<p>You can't actually feel the weight of the world. The human mind is not capable of that feat.</p>\n<p>But sometimes, you can catch a glimpse.</p>", "sections": [{"title": "1", "anchor": "1", "level": 1}, {"title": "2", "anchor": "2", "level": 1}, {"title": "3", "anchor": "3", "level": 1}, {"title": "4", "anchor": "4", "level": 1}, {"title": "5", "anchor": "5", "level": 1}, {"title": "6", "anchor": "6", "level": 1}, {"title": "7", "anchor": "7", "level": 1}, {"title": "8", "anchor": "8", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "275 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 275, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["2ftJ38y9SRBCBsCzy", "xiHy3kFni8nsxfdcP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 18, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-15T02:44:54.054Z", "modifiedAt": null, "url": null, "title": "What math is essential to the art of rationality?", "slug": "what-math-is-essential-to-the-art-of-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:07.487Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Capla", "createdAt": "2014-08-13T03:07:43.569Z", "isAdmin": false, "displayName": "Capla"}, "userId": "KSqXxC5yBv52Aaj4Z", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4D4QJ6F2d2tCkXkHR/what-math-is-essential-to-the-art-of-rationality", "pageUrlRelative": "/posts/4D4QJ6F2d2tCkXkHR/what-math-is-essential-to-the-art-of-rationality", "linkUrl": "https://www.lesswrong.com/posts/4D4QJ6F2d2tCkXkHR/what-math-is-essential-to-the-art-of-rationality", "postedAtFormatted": "Wednesday, October 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20math%20is%20essential%20to%20the%20art%20of%20rationality%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20math%20is%20essential%20to%20the%20art%20of%20rationality%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4D4QJ6F2d2tCkXkHR%2Fwhat-math-is-essential-to-the-art-of-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20math%20is%20essential%20to%20the%20art%20of%20rationality%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4D4QJ6F2d2tCkXkHR%2Fwhat-math-is-essential-to-the-art-of-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4D4QJ6F2d2tCkXkHR%2Fwhat-math-is-essential-to-the-art-of-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 293, "htmlBody": "<p>I have started to put together a sort of curriculum for learning the subjects that lend themselves to rationality. It includes things like experimental methodology and cognitive psychology (obviously), along with \"support disciplines\" like computer science and economics. I think (though maybe I'm wrong) that mathematics is one of the most important things to understand.</p>\n<p>Eliezer said in <a href=\"/lw/l7/the_simple_math_of_everything/\" target=\"_blank\">the simple math of everything</a>:</p>\n<blockquote>\n<p>It seems to me that there's a substantial advantage in knowing the <em>drop-dead basic fundamental embarrassingly simple</em> mathematics in as many different subjects as you can manage.&nbsp; Not,  necessarily, the high-falutin' complicated damn math that appears in the  latest journal articles.&nbsp; Not unless you plan to become a professional  in the field.&nbsp; But for people who can read calculus, and sometimes just  plain algebra, the drop-dead basic mathematics of a field may not take  that long to learn.&nbsp; And it's likely to change your outlook on life more  than the math-free popularizations <em>or</em> the highly technical math.</p>\n</blockquote>\n<p>I want to have access to outlook-changing insights. So, what math do I need to know? What are the generally applicable mathematical principles that are <em>most</em> worth learning? The above quote seems to indicate <em>at least</em> calculus, and everyone is a fan of Bayesian statistics (which I know little about).&nbsp;</p>\n<p>Secondarily, what are some of the most important of that \"<em>drop-dead basic fundamental embarrassingly simple</em> mathematics\" from different fields? What fields are mathematically based, other than physics and evolutionary biology, and economics?</p>\n<p>What is the most important math for an educated person to be familiar with?</p>\n<p>As someone who took an honors calculus class in high school, liked it, and did alright in the class, but who has probably forgotten most of it by now and needs to relearn it, how should I go about learning that math?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4D4QJ6F2d2tCkXkHR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 23, "extendedScore": null, "score": 2.0864710294421102e-06, "legacy": true, "legacyId": "27362", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 63, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HnPEpu5eQWkbyAJCT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-15T04:39:13.533Z", "modifiedAt": null, "url": null, "title": "Meetup : Boston Meetup - New Location", "slug": "meetup-boston-meetup-new-location", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anders_H", "createdAt": "2013-07-28T20:46:58.747Z", "isAdmin": false, "displayName": "Anders_H"}, "userId": "jfdosp4Hn7tFNbf9k", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ki6aH88QobnjYWvWF/meetup-boston-meetup-new-location", "pageUrlRelative": "/posts/ki6aH88QobnjYWvWF/meetup-boston-meetup-new-location", "linkUrl": "https://www.lesswrong.com/posts/ki6aH88QobnjYWvWF/meetup-boston-meetup-new-location", "postedAtFormatted": "Wednesday, October 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boston%20Meetup%20-%20New%20Location&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boston%20Meetup%20-%20New%20Location%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fki6aH88QobnjYWvWF%2Fmeetup-boston-meetup-new-location%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boston%20Meetup%20-%20New%20Location%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fki6aH88QobnjYWvWF%2Fmeetup-boston-meetup-new-location", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fki6aH88QobnjYWvWF%2Fmeetup-boston-meetup-new-location", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 214, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/15o'>Boston Meetup - New Location</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 October 2014 03:30:37PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Harvard Science Center Room 109, 1 Oxford Street, Cambridge MA 02138</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>IMPORTANT LOCATION CHANGE!</p>\n\n<p>Rather than meeting at MIT or at the Citadel, we'll be meeting at Harvard this week. Specifically, room 109 of the Science Center.\nHarvard Science Center 1 Oxford Street Cambridge MA 02138 Room 109\nRon will be speaking about active trading and financial options. This will discuss economics and markets more than our previous talk about personal finance.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 3:30pm, and have an alternating location:</p>\n\n<ul>\n<li>1st Sunday meetups are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville. </li>\n<li>3rd Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.\n(We also have last Wednesday meetups at Citadel at 7pm.)\nOur default schedule is as follows:</li>\n</ul>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.\n\u2014Phase 2: The headline event. This starts promptly at 4pm, and lasts 30-60 minutes. \n\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.\n\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/15o'>Boston Meetup - New Location</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ki6aH88QobnjYWvWF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2.086691678481015e-06, "legacy": true, "legacyId": "27363", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boston_Meetup___New_Location\">Discussion article for the meetup : <a href=\"/meetups/15o\">Boston Meetup - New Location</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 October 2014 03:30:37PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Harvard Science Center Room 109, 1 Oxford Street, Cambridge MA 02138</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>IMPORTANT LOCATION CHANGE!</p>\n\n<p>Rather than meeting at MIT or at the Citadel, we'll be meeting at Harvard this week. Specifically, room 109 of the Science Center.\nHarvard Science Center 1 Oxford Street Cambridge MA 02138 Room 109\nRon will be speaking about active trading and financial options. This will discuss economics and markets more than our previous talk about personal finance.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 3:30pm, and have an alternating location:</p>\n\n<ul>\n<li>1st Sunday meetups are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville. </li>\n<li>3rd Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.\n(We also have last Wednesday meetups at Citadel at 7pm.)\nOur default schedule is as follows:</li>\n</ul>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.\n\u2014Phase 2: The headline event. This starts promptly at 4pm, and lasts 30-60 minutes. \n\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.\n\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Boston_Meetup___New_Location1\">Discussion article for the meetup : <a href=\"/meetups/15o\">Boston Meetup - New Location</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boston Meetup - New Location", "anchor": "Discussion_article_for_the_meetup___Boston_Meetup___New_Location", "level": 1}, {"title": "Discussion article for the meetup : Boston Meetup - New Location", "anchor": "Discussion_article_for_the_meetup___Boston_Meetup___New_Location1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-10-15T12:29:02.681Z", "modifiedAt": null, "url": null, "title": "How to write an academic paper, according to me", "slug": "how-to-write-an-academic-paper-according-to-me", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:08.748Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PPoTcRcaEhkin4pcN/how-to-write-an-academic-paper-according-to-me", "pageUrlRelative": "/posts/PPoTcRcaEhkin4pcN/how-to-write-an-academic-paper-according-to-me", "linkUrl": "https://www.lesswrong.com/posts/PPoTcRcaEhkin4pcN/how-to-write-an-academic-paper-according-to-me", "postedAtFormatted": "Wednesday, October 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20write%20an%20academic%20paper%2C%20according%20to%20me&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20write%20an%20academic%20paper%2C%20according%20to%20me%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPPoTcRcaEhkin4pcN%2Fhow-to-write-an-academic-paper-according-to-me%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20write%20an%20academic%20paper%2C%20according%20to%20me%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPPoTcRcaEhkin4pcN%2Fhow-to-write-an-academic-paper-according-to-me", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPPoTcRcaEhkin4pcN%2Fhow-to-write-an-academic-paper-according-to-me", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 950, "htmlBody": "<p><em>Disclaimer: this is entirely a personal viewpoint, formed by a few years of publication in a few academic fields.</em>&nbsp;<strong>EDIT:</strong>&nbsp;Many of the comments are very worth reading as well.</p>\n<p>Having recently finished a very rushed submission (turns out you can write a novel paper in a day and half, if you're willing to sacrifice quality and sanity), I've been thinking about how academic papers are structured - and more importantly, how they should be structured.</p>\n<p>It seems to me that the key is to consider the audience. Or, more precisely, to consider the audiences - because different people will read you paper to different depths, and you should cater to all of them. An example of this is the \"<a href=\"https://en.wikipedia.org/wiki/Inverted_pyramid\">inverted pyramid</a>\" structure for many news articles - start with the salient facts, then the most important details, then fill in the other details. The idea is to ensure that a reader who stops reading at any point (which happens often) will nevertheless have got the most complete impression that it was possible to convey in the bit that they did read.</p>\n<p>So, with that model in mind, lets consider the different levels of audience for a general academic paper (of course, some papers just can't fit into this mould, but many can):</p>\n<p>&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<h2>Title readers</h2>\n<p>The least important audience. An interesting title may draw casual browsers in, but those likely aren't very valuable readers. Most people encountering an academic article will either be looking for it, or will have had it referred to them from some source. They will likely read more of it. So the main role of the title is to not put off these readers, and to clarify what the paper is about, and what field it belongs in. Witty titles are perfectly acceptable, as long as it fulfils those criteria. So in-jokes for the whole academic field are perfectly acceptable, in-jokes for a narrow subfield are not - unless you're not aiming beyond that subfield.</p>\n<p>&nbsp;</p>\n<h2>Abstract readers</h2>\n<p>The most important audience of all. Most people reading a paper will only read the abstract, and will then proceed to dismiss the paper or accept it and move on. The abstract thus plays three roles:</p>\n<ol>\n<li>It presents the paper's results. The abstract must be crystal-clear on what the paper says; abstract readers must be able to describe the results correctly.</li>\n<li>It establishes the credibility of the result. It can do this by briefly outlying the methods used, and by its general tone. It must thus be serious, and use the correct vocabulary for the field. No room for impressive rhetoric here - dry and descriptive is the model of the abstract.</li>\n<li>It can draw the reader into looking into the paper proper. Because of the first two points, it cannot achieve this by teasers or rhetoric. Instead it must present strong results that cause the reader to want to read more.</li>\n</ol>\n<p>&nbsp;</p>\n<h2>Skimmers</h2>\n<p>This audience will skim through the paper to see what it says. Most crucial for them is the introduction and, depending on the field, possibly the conclusion or discussion section. These must tell the skimmers everything there is to know about the paper - what the problem is, what the results are, what methods were used, why these results are valid, why they are important. As long as all these points are covered, rhetoric and wit can be used, in moderation, to make the reading more enjoyable and salient. But be careful to use these in moderation, lest you give the impression that the paper's results depend on rhetorical tricks. Rhetoric is the flavouring, giving out the information above is the main goal.</p>\n<p>&nbsp;</p>\n<h2>Full readers</h2>\n<p>These are those readers who will go through the whole paper, though they may skim some parts along the way. The important thing here is to get the structure absolutely clear - it must be easy for them to see what the crucial steps or arguments are, what implies what, what relies on what. To do this, lay out the structure of the argument and of the paper clearly in the introduction or in the second section. Emphasise the important results through the paper (consider the layout for this, it can often be used to draw attention to the main points), and connect them together (\"combining this with the results of section 2.3x.iii...\"). Some rhetoric can be used around these important results, especially if it emphasises their importance.</p>\n<p>&nbsp;</p>\n<h2>Deep readers</h2>\n<p>These are your greatest fans or your more hated critics. They will go through the whole paper, taking your argument apart to understand it completely and figure out how it ticks. No fancy rhetoric for them, just careful attention to detail, clarity, and rigour. In mathematical terms, these are the people who will be reading the proofs of your minor lemmas. Don't waste space with anything that doesn't help you establish your argument or your results. These are the lawyers among your readers, looking for the tiniest of flaws. Don't give them any of these, and don't try to hide them with weak arguments.</p>\n<p>&nbsp;</p>\n<h2>Writing the paper</h2>\n<p>The different audiences above give a structure to the paper, but they can also give a structure to writing process. Looking back, I realise that I start by writing for the full readers, getting the important points and structure correct. Then I fill in the details for the deep readers. I then write the introduction (and conclusion, if appropriate) for the skimmers, and conclude with the abstract for the most important audience. The title can be chosen at any point in this process.</p>\n<p>Hope this helps! I think I've been following this advice implicitly for a long time, and it's got me a few publications. Feel free to ignore it, of course, or to post your own preferred approach.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"7mTviCYysGmLqiHai": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PPoTcRcaEhkin4pcN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 45, "extendedScore": null, "score": 0.000253, "legacy": true, "legacyId": "27364", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Disclaimer: this is entirely a personal viewpoint, formed by a few years of publication in a few academic fields.</em>&nbsp;<strong>EDIT:</strong>&nbsp;Many of the comments are very worth reading as well.</p>\n<p>Having recently finished a very rushed submission (turns out you can write a novel paper in a day and half, if you're willing to sacrifice quality and sanity), I've been thinking about how academic papers are structured - and more importantly, how they should be structured.</p>\n<p>It seems to me that the key is to consider the audience. Or, more precisely, to consider the audiences - because different people will read you paper to different depths, and you should cater to all of them. An example of this is the \"<a href=\"https://en.wikipedia.org/wiki/Inverted_pyramid\">inverted pyramid</a>\" structure for many news articles - start with the salient facts, then the most important details, then fill in the other details. The idea is to ensure that a reader who stops reading at any point (which happens often) will nevertheless have got the most complete impression that it was possible to convey in the bit that they did read.</p>\n<p>So, with that model in mind, lets consider the different levels of audience for a general academic paper (of course, some papers just can't fit into this mould, but many can):</p>\n<p>&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Title_readers\">Title readers</h2>\n<p>The least important audience. An interesting title may draw casual browsers in, but those likely aren't very valuable readers. Most people encountering an academic article will either be looking for it, or will have had it referred to them from some source. They will likely read more of it. So the main role of the title is to not put off these readers, and to clarify what the paper is about, and what field it belongs in. Witty titles are perfectly acceptable, as long as it fulfils those criteria. So in-jokes for the whole academic field are perfectly acceptable, in-jokes for a narrow subfield are not - unless you're not aiming beyond that subfield.</p>\n<p>&nbsp;</p>\n<h2 id=\"Abstract_readers\">Abstract readers</h2>\n<p>The most important audience of all. Most people reading a paper will only read the abstract, and will then proceed to dismiss the paper or accept it and move on. The abstract thus plays three roles:</p>\n<ol>\n<li>It presents the paper's results. The abstract must be crystal-clear on what the paper says; abstract readers must be able to describe the results correctly.</li>\n<li>It establishes the credibility of the result. It can do this by briefly outlying the methods used, and by its general tone. It must thus be serious, and use the correct vocabulary for the field. No room for impressive rhetoric here - dry and descriptive is the model of the abstract.</li>\n<li>It can draw the reader into looking into the paper proper. Because of the first two points, it cannot achieve this by teasers or rhetoric. Instead it must present strong results that cause the reader to want to read more.</li>\n</ol>\n<p>&nbsp;</p>\n<h2 id=\"Skimmers\">Skimmers</h2>\n<p>This audience will skim through the paper to see what it says. Most crucial for them is the introduction and, depending on the field, possibly the conclusion or discussion section. These must tell the skimmers everything there is to know about the paper - what the problem is, what the results are, what methods were used, why these results are valid, why they are important. As long as all these points are covered, rhetoric and wit can be used, in moderation, to make the reading more enjoyable and salient. But be careful to use these in moderation, lest you give the impression that the paper's results depend on rhetorical tricks. Rhetoric is the flavouring, giving out the information above is the main goal.</p>\n<p>&nbsp;</p>\n<h2 id=\"Full_readers\">Full readers</h2>\n<p>These are those readers who will go through the whole paper, though they may skim some parts along the way. The important thing here is to get the structure absolutely clear - it must be easy for them to see what the crucial steps or arguments are, what implies what, what relies on what. To do this, lay out the structure of the argument and of the paper clearly in the introduction or in the second section. Emphasise the important results through the paper (consider the layout for this, it can often be used to draw attention to the main points), and connect them together (\"combining this with the results of section 2.3x.iii...\"). Some rhetoric can be used around these important results, especially if it emphasises their importance.</p>\n<p>&nbsp;</p>\n<h2 id=\"Deep_readers\">Deep readers</h2>\n<p>These are your greatest fans or your more hated critics. They will go through the whole paper, taking your argument apart to understand it completely and figure out how it ticks. No fancy rhetoric for them, just careful attention to detail, clarity, and rigour. In mathematical terms, these are the people who will be reading the proofs of your minor lemmas. Don't waste space with anything that doesn't help you establish your argument or your results. These are the lawyers among your readers, looking for the tiniest of flaws. Don't give them any of these, and don't try to hide them with weak arguments.</p>\n<p>&nbsp;</p>\n<h2 id=\"Writing_the_paper\">Writing the paper</h2>\n<p>The different audiences above give a structure to the paper, but they can also give a structure to writing process. Looking back, I realise that I start by writing for the full readers, getting the important points and structure correct. Then I fill in the details for the deep readers. I then write the introduction (and conclusion, if appropriate) for the skimmers, and conclude with the abstract for the most important audience. The title can be chosen at any point in this process.</p>\n<p>Hope this helps! I think I've been following this advice implicitly for a long time, and it's got me a few publications. Feel free to ignore it, of course, or to post your own preferred approach.</p>", "sections": [{"title": "Title readers", "anchor": "Title_readers", "level": 1}, {"title": "Abstract readers", "anchor": "Abstract_readers", "level": 1}, {"title": "Skimmers", "anchor": "Skimmers", "level": 1}, {"title": "Full readers", "anchor": "Full_readers", "level": 1}, {"title": "Deep readers", "anchor": "Deep_readers", "level": 1}, {"title": "Writing the paper", "anchor": "Writing_the_paper", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "27 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}