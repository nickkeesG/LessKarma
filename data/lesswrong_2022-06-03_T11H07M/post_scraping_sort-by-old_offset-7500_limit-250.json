{"results": [{"createdAt": null, "postedAt": "2012-09-26T02:30:53.785Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Dark Side Epistemology", "slug": "seq-rerun-dark-side-epistemology", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:55.993Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3mTT6TXrsC8zhZWru/seq-rerun-dark-side-epistemology", "pageUrlRelative": "/posts/3mTT6TXrsC8zhZWru/seq-rerun-dark-side-epistemology", "linkUrl": "https://www.lesswrong.com/posts/3mTT6TXrsC8zhZWru/seq-rerun-dark-side-epistemology", "postedAtFormatted": "Wednesday, September 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Dark%20Side%20Epistemology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Dark%20Side%20Epistemology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3mTT6TXrsC8zhZWru%2Fseq-rerun-dark-side-epistemology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Dark%20Side%20Epistemology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3mTT6TXrsC8zhZWru%2Fseq-rerun-dark-side-epistemology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3mTT6TXrsC8zhZWru%2Fseq-rerun-dark-side-epistemology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p>Today's post, <a href=\"/lw/uy/dark_side_epistemology/\">Dark Side Epistemology</a> was originally published on 17 October 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Dark_Side_Epistemology\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If you want to tell a truly convincing lie, to someone who knows what they're talking about, you either have to lie about lots of specific object level facts, or about more general laws, or about the laws of thought. Lots of the memes out there about how you learn things originally came from people who were trying to convince other people to believe false statements.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/enc/seq_rerun_traditional_capitalist_values/\">Traditional Capitalist Values</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3mTT6TXrsC8zhZWru", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.95131444843923e-07, "legacy": true, "legacyId": "19006", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XTWkjCJScy2GFAgDt", "EcH9sEFykS95En7YL", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-26T03:53:48.227Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham NC HPMoR Discussion, chapters 4-7", "slug": "meetup-durham-nc-hpmor-discussion-chapters-4-7", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:00.629Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PEThBaizCwbokHqja/meetup-durham-nc-hpmor-discussion-chapters-4-7", "pageUrlRelative": "/posts/PEThBaizCwbokHqja/meetup-durham-nc-hpmor-discussion-chapters-4-7", "linkUrl": "https://www.lesswrong.com/posts/PEThBaizCwbokHqja/meetup-durham-nc-hpmor-discussion-chapters-4-7", "postedAtFormatted": "Wednesday, September 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20NC%20HPMoR%20Discussion%2C%20chapters%204-7&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20NC%20HPMoR%20Discussion%2C%20chapters%204-7%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPEThBaizCwbokHqja%2Fmeetup-durham-nc-hpmor-discussion-chapters-4-7%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20NC%20HPMoR%20Discussion%2C%20chapters%204-7%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPEThBaizCwbokHqja%2Fmeetup-durham-nc-hpmor-discussion-chapters-4-7", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPEThBaizCwbokHqja%2Fmeetup-durham-nc-hpmor-discussion-chapters-4-7", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/eb\">HPMoR Discussion, chapters 4-7</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">06 October 2012 11:00:00AM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Guglhupf bakery, 2706 durham-chapel hill blvd. suite #1 durham, NC</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We will discuss HPMoR chapters 4-7 over brunch. Discussion will be informal, and probably ramble off topic after we go over the chapters. Attendees are encouraged to read all the chapters, but also encouraged to come if you haven't read all of them. We'll do chapter summaries as we discuss.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/eb\">HPMoR Discussion, chapters 4-7</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PEThBaizCwbokHqja", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 9.951746233105934e-07, "legacy": true, "legacyId": "19007", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___HPMoR_Discussion__chapters_4_7\">Discussion article for the meetup : <a href=\"/meetups/eb\">HPMoR Discussion, chapters 4-7</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">06 October 2012 11:00:00AM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Guglhupf bakery, 2706 durham-chapel hill blvd. suite #1 durham, NC</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We will discuss HPMoR chapters 4-7 over brunch. Discussion will be informal, and probably ramble off topic after we go over the chapters. Attendees are encouraged to read all the chapters, but also encouraged to come if you haven't read all of them. We'll do chapter summaries as we discuss.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___HPMoR_Discussion__chapters_4_71\">Discussion article for the meetup : <a href=\"/meetups/eb\">HPMoR Discussion, chapters 4-7</a></h2>", "sections": [{"title": "Discussion article for the meetup : HPMoR Discussion, chapters 4-7", "anchor": "Discussion_article_for_the_meetup___HPMoR_Discussion__chapters_4_7", "level": 1}, {"title": "Discussion article for the meetup : HPMoR Discussion, chapters 4-7", "anchor": "Discussion_article_for_the_meetup___HPMoR_Discussion__chapters_4_71", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-26T12:25:48.899Z", "modifiedAt": "2020-07-03T04:08:41.516Z", "url": null, "title": "[Poll] Less Wrong and Mainstream Philosophy: How Different are We?", "slug": "poll-less-wrong-and-mainstream-philosophy-how-different-are", "viewCount": null, "lastCommentedAt": "2019-04-03T02:29:26.111Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mPBuTuqShRotscGSP/poll-less-wrong-and-mainstream-philosophy-how-different-are", "pageUrlRelative": "/posts/mPBuTuqShRotscGSP/poll-less-wrong-and-mainstream-philosophy-how-different-are", "linkUrl": "https://www.lesswrong.com/posts/mPBuTuqShRotscGSP/poll-less-wrong-and-mainstream-philosophy-how-different-are", "postedAtFormatted": "Wednesday, September 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BPoll%5D%20Less%20Wrong%20and%20Mainstream%20Philosophy%3A%20How%20Different%20are%20We%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BPoll%5D%20Less%20Wrong%20and%20Mainstream%20Philosophy%3A%20How%20Different%20are%20We%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmPBuTuqShRotscGSP%2Fpoll-less-wrong-and-mainstream-philosophy-how-different-are%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BPoll%5D%20Less%20Wrong%20and%20Mainstream%20Philosophy%3A%20How%20Different%20are%20We%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmPBuTuqShRotscGSP%2Fpoll-less-wrong-and-mainstream-philosophy-how-different-are", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmPBuTuqShRotscGSP%2Fpoll-less-wrong-and-mainstream-philosophy-how-different-are", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p>Despite being (IMO) a philosophy blog, many Less Wrongers tend to disparage mainstream philosophy and emphasize the divergence between our beliefs and theirs. But, how different are we really? My intention with this post is to quantify this difference.</p>\n<p>The questions I will post\u00a0as comments to this article are from the 2009 <a href=\"http://philpapers.org/surveys/\">PhilPapers Survey</a>. If you answer \"other\" on any of the questions, then please reply to that comment in order to elaborate your answer. Later, I'll post another article comparing the answers I obtain from Less Wrongers\u00a0with those given by the professional philosophers. This should give us some indication about the differences in belief\u00a0between Less Wrong and mainstream philosophy.</p>\n<p><strong>Glossary<br /></strong></p>\n<p><a href=\"http://plato.stanford.edu/entries/analytic-synthetic/\">analytic-synthetic distinction</a>, <a href=\"http://plato.stanford.edu/entries/time/#TheBThe\">A-theory and B-theory</a>, <a href=\"http://plato.stanford.edu/entries/atheism-agnosticism/#1\">atheism</a>, <a href=\"http://plato.stanford.edu/entries/compatibilism/\">compatibilism</a>, <a href=\"http://plato.stanford.edu/entries/consequentialism/\">consequentialism</a>, <a href=\"http://plato.stanford.edu/entries/contextualism-epistemology/\">contextualism</a>, <a href=\"http://plato.stanford.edu/entries/truth-correspondence/\">correspondence theory of truth</a>, <a href=\"http://plato.stanford.edu/entries/ethics-deontological/\">deontology</a>, <a href=\"http://wiki.lesswrong.com/wiki/Egalitarianism\">egalitarianism</a>, <a href=\"http://plato.stanford.edu/entries/rationalism-empiricism/#1.2\">empiricism</a>, <a href=\"http://plato.stanford.edu/entries/laws-of-nature/#HumSup\">Humeanism</a>, <a href=\"http://plato.stanford.edu/entries/libertarianism/\">libertarianism</a>, <a href=\"http://plato.stanford.edu/entries/content-externalism/\">mental content externalism</a>, <a href=\"http://plato.stanford.edu/entries/moral-realism/\">moral realism</a>, <a href=\"http://plato.stanford.edu/entries/moral-motivation/#IntVExt\">moral motivation internalism and externalism</a>, <a href=\"http://plato.stanford.edu/entries/naturalism/\">naturalism</a>, <a href=\"http://plato.stanford.edu/entries/nominalism-metaphysics/#AbsObj\">nominalism</a>, <a href=\"http://en.wikipedia.org/wiki/Newcomb%27s_problem\">Newcomb's problem</a>, <a href=\"http://plato.stanford.edu/entries/physicalism/\">physicalism</a>, <a href=\"http://plato.stanford.edu/entries/platonism/\">Platonism</a>, <a href=\"http://plato.stanford.edu/entries/rationalism-empiricism/#1.1\">rationalism</a>, <a href=\"http://plato.stanford.edu/entries/relativism/#2.4\">relativism</a>, <a href=\"http://plato.stanford.edu/entries/scientific-realism/\">scientific realism</a>, <a href=\"http://en.wikipedia.org/wiki/Trolley_problem\">trolley problem</a>, <a href=\"http://en.wikipedia.org/wiki/Theism\">theism</a>, <a href=\"http://plato.stanford.edu/entries/ethics-virtue/\">virtue ethics</a></p>\n<p><strong>Note</strong></p>\n<p>Thanks <a href=\"/user/pragmatist/\">pragmatist</a>, for attaching short (mostly accurate) descriptions of the philosophical positions under the poll comments.</p>\n<p><strong>Post Script</strong></p>\n<p>The polls stopped rendering correctly after the migration to LW 2.0, but the raw data can be found in <a href=\"https://github.com/jaysonvirissimo/primary-data/tree/master/impersonal\">this</a> repo.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GLykb6NukBeBQtDvQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mPBuTuqShRotscGSP", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 55, "extendedScore": null, "score": 0.0005863017106431277, "legacy": true, "legacyId": "18955", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Despite being (IMO) a philosophy blog, many Less Wrongers tend to disparage mainstream philosophy and emphasize the divergence between our beliefs and theirs. But, how different are we really? My intention with this post is to quantify this difference.</p>\n<p>The questions I will post&nbsp;as comments to this article are from the 2009 <a href=\"http://philpapers.org/surveys/\">PhilPapers Survey</a>. If you answer \"other\" on any of the questions, then please reply to that comment in order to elaborate your answer. Later, I'll post another article comparing the answers I obtain from Less Wrongers&nbsp;with those given by the professional philosophers. This should give us some indication about the differences in belief&nbsp;between Less Wrong and mainstream philosophy.</p>\n<p><strong id=\"Glossary\">Glossary<br></strong></p>\n<p><a href=\"http://plato.stanford.edu/entries/analytic-synthetic/\">analytic-synthetic distinction</a>, <a href=\"http://plato.stanford.edu/entries/time/#TheBThe\">A-theory and B-theory</a>, <a href=\"http://plato.stanford.edu/entries/atheism-agnosticism/#1\">atheism</a>, <a href=\"http://plato.stanford.edu/entries/compatibilism/\">compatibilism</a>, <a href=\"http://plato.stanford.edu/entries/consequentialism/\">consequentialism</a>, <a href=\"http://plato.stanford.edu/entries/contextualism-epistemology/\">contextualism</a>, <a href=\"http://plato.stanford.edu/entries/truth-correspondence/\">correspondence theory of truth</a>, <a href=\"http://plato.stanford.edu/entries/ethics-deontological/\">deontology</a>, <a href=\"http://wiki.lesswrong.com/wiki/Egalitarianism\">egalitarianism</a>, <a href=\"http://plato.stanford.edu/entries/rationalism-empiricism/#1.2\">empiricism</a>, <a href=\"http://plato.stanford.edu/entries/laws-of-nature/#HumSup\">Humeanism</a>, <a href=\"http://plato.stanford.edu/entries/libertarianism/\">libertarianism</a>, <a href=\"http://plato.stanford.edu/entries/content-externalism/\">mental content externalism</a>, <a href=\"http://plato.stanford.edu/entries/moral-realism/\">moral realism</a>, <a href=\"http://plato.stanford.edu/entries/moral-motivation/#IntVExt\">moral motivation internalism and externalism</a>, <a href=\"http://plato.stanford.edu/entries/naturalism/\">naturalism</a>, <a href=\"http://plato.stanford.edu/entries/nominalism-metaphysics/#AbsObj\">nominalism</a>, <a href=\"http://en.wikipedia.org/wiki/Newcomb%27s_problem\">Newcomb's problem</a>, <a href=\"http://plato.stanford.edu/entries/physicalism/\">physicalism</a>, <a href=\"http://plato.stanford.edu/entries/platonism/\">Platonism</a>, <a href=\"http://plato.stanford.edu/entries/rationalism-empiricism/#1.1\">rationalism</a>, <a href=\"http://plato.stanford.edu/entries/relativism/#2.4\">relativism</a>, <a href=\"http://plato.stanford.edu/entries/scientific-realism/\">scientific realism</a>, <a href=\"http://en.wikipedia.org/wiki/Trolley_problem\">trolley problem</a>, <a href=\"http://en.wikipedia.org/wiki/Theism\">theism</a>, <a href=\"http://plato.stanford.edu/entries/ethics-virtue/\">virtue ethics</a></p>\n<p><strong id=\"Note\">Note</strong></p>\n<p>Thanks <a href=\"/user/pragmatist/\">pragmatist</a>, for attaching short (mostly accurate) descriptions of the philosophical positions under the poll comments.</p>\n<p><strong id=\"Post_Script\">Post Script</strong></p>\n<p>The polls stopped rendering correctly after the migration to LW 2.0, but the raw data can be found in <a href=\"https://github.com/jaysonvirissimo/primary-data/tree/master/impersonal\">this</a> repo.</p>", "sections": [{"title": "Glossary", "anchor": "Glossary", "level": 1}, {"title": "Note", "anchor": "Note", "level": 1}, {"title": "Post Script", "anchor": "Post_Script", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "629 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 631, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-26T16:55:32.611Z", "modifiedAt": null, "url": null, "title": "Causality: a chapter by chapter review", "slug": "causality-a-chapter-by-chapter-review", "viewCount": null, "lastCommentedAt": "2015-10-07T06:30:29.543Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jnjjzkH8Fdzg4D6EK/causality-a-chapter-by-chapter-review", "pageUrlRelative": "/posts/jnjjzkH8Fdzg4D6EK/causality-a-chapter-by-chapter-review", "linkUrl": "https://www.lesswrong.com/posts/jnjjzkH8Fdzg4D6EK/causality-a-chapter-by-chapter-review", "postedAtFormatted": "Wednesday, September 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Causality%3A%20a%20chapter%20by%20chapter%20review&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACausality%3A%20a%20chapter%20by%20chapter%20review%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjnjjzkH8Fdzg4D6EK%2Fcausality-a-chapter-by-chapter-review%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Causality%3A%20a%20chapter%20by%20chapter%20review%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjnjjzkH8Fdzg4D6EK%2Fcausality-a-chapter-by-chapter-review", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjnjjzkH8Fdzg4D6EK%2Fcausality-a-chapter-by-chapter-review", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3698, "htmlBody": "<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://bayes.cs.ucla.edu/BOOK-09/causality2-cover.jpg\" alt=\"\" width=\"170\" height=\"240\" />This is a chapter by chapter review of <a href=\"http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/052189560X/ref=nosim?tag=vglnk-c319-20\">Causality</a> (2nd ed.) by Judea Pearl (<a href=\"http://bayes.cs.ucla.edu/jp_home.html\">UCLA</a>, <a href=\"http://www.mii.ucla.edu/causality/\">blog</a>). Like my <a href=\"/lw/cb1/thinking_and_deciding_a_chapter_by_chapter_review/\">previous review</a>, the intention is not to summarize but to help readers determine whether or not they should read the book (and if they do, what parts to read). Reading the review is in no way a substitute for reading the book.<br /><br />I'll state my basic impression of the book up front, with detailed comments after the chapter discussions: this book is monumentally important to anyone interested in procuring knowledge (especially causal knowledge) from statistical data, but it is a heavily technical book primarily suitable for experts. The mathematics involved is not particularly difficult, but its presentation requires dedicated reading and clarity of thought. Only the epilogue, <a href=\"http://singapore.cs.ucla.edu/LECTURE/lecture_sec1.htm\">this lecture</a>, is suitable for the general audience, and that will be the highest value portion for most readers of LW.</p>\n<h2 class=\"Standard\"><a id=\"more\"></a>1. Introduction to Probabilities, Graphs, and Causal Models</h2>\n<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:DoNotShowComments /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <w:DoNotOptimizeForBrowser /> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--></p>\n<p><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:none; mso-hyphenate:none; text-autospace:ideograph-other; font-size:12.0pt; font-family:\"Liberation Serif\",\"serif\"; mso-font-kerning:1.5pt; mso-fareast-language:ZH-CN; mso-bidi-language:HI;} --> <!--[endif] --></p>\n<p class=\"Standard\">While the descriptions are complete, this chapter may be more useful as a refresher than as an introduction. The three sections are detailed<span style=\"mso-spacerun:yes\"> in </span>inverse proportion to the expected reader's familiarity.</p>\n<p class=\"Standard\">For the reader who's seen probability calculus before, Pearl's description of it in 12 pages is short, sweet, and complete. For the reader that hasn't seen it, that's just enough space to list the definitions and give a few examples. Compare <a href=\"http://yudkowsky.net/rational/bayes\">Eliezer's explanation of Bayes' Rule</a> (almost 50 pages) to Pearl's (around 2).</p>\n<p class=\"Standard\">The section on graphs moves a little less quickly, but even so don't be afraid to find an online tutorial on d-separation if Pearl's explanation is too fast. For some reason, he does not mention here that section 11.1.2 (p 335-337 in my copy) is a gentler introduction to d-separation. [edit] His blog also linked to <a href=\"http://ftp.cs.ucla.edu/pub/stat_ser/uai12-mohan-pearl.pdf\">this presentation</a>, which is an even gentler introduction to graphs, causal networks, and d-separation.</p>\n<p class=\"Standard\">The section on causal models is the most detailed, as it will be new to most readers, and closely follows the section on graphs. Pearl uses an example to demonstrate the use of counterfactuals, which is a potent first glance at the usefulness of causal models.</p>\n<p class=\"Standard\">He also draws an important distinction between probabilistic, statistical, and causal parameters. Probabilistic parameters are quantities defined in terms of a joint distribution. Statistical parameters are quantities defined in terms of <em>observed</em> variables drawn from a joint distribution. Causal parameters are quantities defined in terms of a causal model, and are not statistical. (I'll leave the explanation of the full implications of the distinctions to the chapter.)</p>\n<h2 class=\"Standard\">2. A Theory of Inferred Causation</h2>\n<p class=\"Standard\">Philosophers have long grappled with the challenge of identifying causal information from data, especially non-experimental data. This chapter details an algorithm to attack that problem.</p>\n<p class=\"Standard\">The key conceptual leap is the use of a third variable in the model as a control. Suppose X and Y are correlated; if there is a third variable Z that is correlated with Y but not with X, the natural interpretation is that X and Z both cause Y. That is not the unique interpretation, which causes quite a bit of philosophical trouble, which Pearl addresses with stability. Only one of the multiple consistent interpretations is stable.</p>\n<p class=\"Standard\">Pearl gives the example of a photo of a chair. There are two primary hypotheses: first, that the underlying scenario was a single chair, and second, that the underlying scenario was two chairs, placed so that the first chair hides the second. While both scenarios predict the observed data, the first scenario is not just simpler, but more stable. If the camera position moved slightly, the second chair might not be hidden anymore- and so we should expect two chairs to be visible in most photos of two chair scenarios.</p>\n<p class=\"Standard\">Pearl also calls on Occam's Razor, in the form of preferring candidate models and distributions which cannot be overfit to those which can be overfit. With those two reasonable criteria, we can move from an infinite set of possible causal models that could explain the data to a single equivalency class of causal models which <em>most frugally</em> explain the data.</p>\n<p class=\"Standard\">The chapter describes the algorithm, its functionality, and some implementation details, which I won't discuss here.</p>\n<p class=\"Standard\">Pearl also discusses how to differentiate between potential causes, genuine causes, and spurious associations given the output of the causal inference algorithm.</p>\n<p class=\"Standard\">The chapter concludes with some philosophical discussion of the influence of time and variable choice, as well as defending the three core assumptions (of minimality, the Markovian structure of causal models, and stability).</p>\n<h2 class=\"Standard\">3. Causal Diagrams and the Identification of Causal Effects</h2>\n<p class=\"Standard\">While we can infer causal relationships from data, that task is far easier when we allow ourselves to assume some sensible causal relationships. This step is necessary, and desirable even though making it transparent is sometimes controversial.</p>\n<p class=\"Standard\">Interesting situations are often very complex, and Pearl shows how causal graphs- even ones where not all nodes may be measured- make it possible to navigate the complexity of those situations. The chapter focuses primarily on identifiability- that is, if we fix X to be some value x, can we determine p(y|do(x))? For an arbitrarily large and complex graph, the answer is non-obvious.</p>\n<p class=\"Standard\">The answer is non-obvious enough that there is massive controversy between statisticians and econometricians, which Pearl attempted to defuse (and describes how well that went at the end of the chapter), because there is a subtle difference between <em>observing</em> that X=x and <em>setting</em> X=x. If we see that the price of corn is $1 a bushel, that implies a very different world than one where we set the price of corn at $1 a bushel. In applications where we want to control a system, we're interested in the second- but normal updating based on Bayes' Rule will give us the first. That is, from a statistical perspective, we can always determine the joint probability distribution, and condition on X=x to get p(y|X=x); but from a causal perspective this generally won't give us the information we want. Different causal models can have the same joint probability distribution- and thus look statistically indistinguishable- but give very different results when X is fixed to a particular value.</p>\n<p class=\"Standard\">When everything is observable (and thus deterministic), there's no challenge in figuring out what will happen when X is fixed to a particular value. When there is uncertainty- that is, only some variables are observable- then we need to determine if we know <em>enough</em> to still be able to determine the effects of fixing X.</p>\n<p class=\"Standard\">His 'intervention calculus'<span style=\"mso-spacerun:yes\"> </span>describes how to fix a variable by modifying the graph, and then what you can get out of the new, modified graph. It takes a necessary detour through the impacts of confounding variables (or, more precisely, what graph structure that represents and how to determine identifiability in the light of that graph structure). This is what lets us describe and calculate p(y|do(x)).</p>\n<p class=\"Standard\">I should comment I feel badly reviewing a technical book like this; my summary of forty pages of math is half a page long, because I leave all of the math to the book itself, and just describe the motivation for the math.</p>\n<h2 class=\"Standard\">4. Actions, Plans, and Direct Effects</h2>\n<p class=\"Standard\">This chapter begins with a distinction between acts and actions, very similar to the distinction discussed in the previous chapter. He treats acts as events or reactions to stimuli; because they are caused by the environment, they give evidence about the environment. Actions are treated as deliberative- they can't be used as evidence because they haven't happened yet, and are the result of deliberation. They become acts once performed- they're actions from the inside, but acts from the outside. (Link to algorithm feels like on the inside.) Pearl describes the controversy over Newcomb's Problem as a confusion over the distinction between acts and actions. Evidential Decision Theory, often called EDT, is discussed and dismissed; because it doesn't respect this distinction (or, really, any causal information), it gives nonsensical results. Commuters shouldn't rush to work, because if they did, that would increase the probability that they've overslept.</p>\n<p class=\"Standard\">Pearl gives a brief description of the relationship between influence diagrams, used in decision analysis, and the causal diagrams he describes here; basically, they're very similar, although the ID literature purposefully sidesteps causal implications which are at the forefront here.</p>\n<p class=\"Standard\">Much of the chapter is spent describing the math that determines when an action's or plan's effects are identifiable.</p>\n<p class=\"Standard\">Of particular interest is the section on direct effects, which walks through the famous Berkeley Admissions example of Simpson's Paradox. Pearl presents a modified version in which the school admits students solely based on qualifications, but appears to discriminate on a department-by-department basis, to demonstrate the necessity of using a full causal model, rather than simple adjusting.</p>\n<h2 class=\"Standard\">5. Causality and Structural Models in Social Science and Economics</h2>\n<p class=\"Standard\">This chapter will be much more significant to readers with experience doing economic or social science modeling, but is still worthwhile to other readers as a demonstration of the power of causal graphs as a language.</p>\n<p class=\"Standard\">The part of the chapter that is interesting outside of the context of structural models is the part that discusses testing of models. Every missing link in a causal graph is the strong prediction that those two variables are independent (if properly conditioned). This presents a ready test of a causal graph- compute the covariance for every missing link (after proper conditioning), and confirm that those links are not necessary. As a statistical practice, this significantly aids in the debugging of models because it makes local errors obvious, even when they might be obscured in global error tests.</p>\n<p class=\"Standard\">That said, I found it mildly disconcerting that Pearl did not mention there the rationale for using global tests. That is, if there are twenty missing links in your causal diagram, and you collect real data and calculate covariances, on average you should expect the covariance of one missing link to be statistically significantly different from zero if you're using a local test for each link independently. A global test will look at one statistically significant red flag and ignore it as expected given the number of coefficients.</p>\n<p class=\"Standard\">In the context of structural models, most of the interesting parts of the chapter deal with determining the identifiability of parameters in the structural models, and then how to interpret those parameters. Pearl's approach is clear, easily understandable, and soundly superior to alternatives that he quotes (primarily to demonstrate his superiority to them).</p>\n<h2 class=\"Standard\">6. Simpson's Paradox, Confounding, and Collapsibility</h2>\n<p class=\"Standard\">This chapter begins by dissolving Simpson's Paradox, which is more precisely called a reversal effect. Pearl gives a simple example: suppose 80 subjects have a disease and take a drug to treat it. 50%<span style=\"mso-spacerun:yes\"> </span>(20) of those who take the drug recover, and 40% (16) of those who do not take the drug recover. By itself, this seems to suggest that the drug increases the recovery rate.</p>\n<p class=\"Standard\">The effect is reversed, though, when you take gender into account. Of the men, 30 decided to take the drug- and only 60% (18) of them recovered, compared to 70% (7) of the 10 that decided to not take the drug. Of the women, 10 decided to take the drug- and only 20% (2) of them recovered, compared to 30% (9) of the 30 who did not decide to take the drug.</p>\n<p class=\"Standard\">Depicting the issue causally, the effect is clear: sex impacts both the proportion of subjects who take the drug and the base recovery rate, and the positive impact of sex on recovery is masking the negative impact of the drug on recovery. Simply calculating p(recovery|drug)-p(recovery|~drug) does not tell us if the drug is helpful. The parameter we need for that is p(recovery|do(drug))-p(recovery|do(~drug)). With causal diagrams and a clear conceptual difference between observing and fixing events, that's not a mistake one would make, and so there's no paradox to avoid and nothing interesting to see.</p>\n<p class=\"Standard\">The rest of the chapter discusses confounding, presenting a definition of stable no-confounding between variables and showing why other definitions are less useful or rigorous. For readers who haven't heard of those alternatives before, the comparisons will not be particularly interesting or enlightening (as compared to the previous chapter, where the discussion of structural models seems readily intelligible to someone with little experience with them), though they do provide some insight into the issue of confounding.</p>\n<h2 class=\"Standard\">7. The Logic of Structure-Based Counterfactuals</h2>\n<p class=\"Standard\">Pearl returns to the topic of counterfactuals, briefly introduced before, and gives them a firm mathematical foundation and linguistic interpretation, then makes their usefulness clear. Counterfactuals are the basis of interventions in complex systems- they encode the knowledge of what consequences a particular change would have. They also represent a convenient way to store and test causal information.</p>\n<p class=\"Standard\">This power to predict the consequences of changes is what makes causal models superior to non-causal models. Pearl gives a great example of a basic econometric situation:</p>\n<p class=\"Standard\" style=\"padding-left: 30px;\">q=b<sub>1</sub>p+d<sub>1</sub>i+u<sub>1</sub></p>\n<p class=\"Standard\" style=\"padding-left: 30px;\">p=b<sub>2</sub>q+d<sub>2</sub>w+u<sub>2</sub></p>\n<p class=\"Standard\">where q is the quantity demanded, I is the household income, p is the price level, w is the wage rate, and the u<sub>i</sub>s are uncorrelated error terms. The equilibrium level of price and quantity demanded is determined by the feedback between those two equations.</p>\n<p class=\"Standard\" style=\"margin-left:0in;text-indent:0in;mso-list:l0 level1 lfo1\">Pearl identifies three quantities of interest:</p>\n<ol>\n<li>What is the expected value of the demand Q if the price is <em>controlled at p=p<sub>0</sub></em>?</li>\n<li>What is the expected value of the demand Q if the price is <em>reported to be p=p<sub>0</sub></em>?</li>\n<li><span style=\"mso-fareast-font-family:&quot;Liberation Serif&quot;;mso-bidi-font-family:&quot;Liberation Serif&quot;\"><span style=\"mso-list:Ignore\"><span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;</span></span></span>Given that the price is currently p=p<sub>0</sub>, what is the expected value of the demand Q if <em>we were to control</em> the price at p=p<sub>1</sub>?</li>\n</ol>\n<p class=\"Standard\">The second is the only quantity available from standard econometric analysis; the causal analysis Pearl describes easily calculates all three quantities. Again, I leave all of the actual math to the book, but this example was vivid enough that I had to reprint it.</p>\n<p class=\"Standard\">The chapter continues with a set of axioms that describe structural counterfactuals, which then allows Pearl to compare structural counterfactuals with formulations attempted by others. Again, for the reader only interested in Pearl's approach, the comparisons are more tedious than enlightening. There are enough possibly non-obvious implications to reward the dedicated reader, and the reader familiar with the object of the comparison will find the comparison far more meaningful, but the hurried reader would be forgiven for skipping a few sections.</p>\n<p class=\"Standard\">The discussion of exogeneity is valuable for all readers, though, as it elucidates a hierarchy between graphical criteria, error-based criteria, and counterfactual criteria. Each of those criteria implies the one that follows it, but the implications do not flow in the reverse direction; another example of how the language of graphs is more powerful than alternative languages.</p>\n<h2 class=\"Standard\">8. Imperfect Experiments: Bounding Effects and Counterfactuals</h2>\n<p class=\"Standard\">This chapter describes how to extract useful information (through bounds) from imperfect experiments. For experiments where all observed variables are binary (and, if they aren't, they can be binarized through partitioning), but unobserved variables are free to be monstrously complicated, that complexity can be partitioned into four classes of responses, to match the four possible functional forms between binary variables.</p>\n<p class=\"Standard\">Pearl uses the example of medical drug testing- patients are encouraged to take the drug (experimental group) or not (control group), but compliance may be imperfect, as patients may not take medication given to them or patients not given medication may procure it by other means. Patients can be classed as either never taking the drug, complying with instructions, defying instructions, or always taking the drug. Similarly, the drug's effect on patient recovery can be classified as never recovering, helping, hurting, or always recovering. The two could obviously be related, and so the full joint distribution has 15 degrees of freedom- but we can pin down enough of those degrees of freedom with the observations that we make (of encouragement, treatment, and then recovery) to establish an upper and lower bound for the effect that the treatment has on recovery.</p>\n<p class=\"Standard\">The examples in this chapter are much more detailed and numerical; it also includes a section on Bayesian estimation of the parameters as a complement to or substitute for bounding.</p>\n<h2 class=\"Standard\">9. Probability of Causation: Interpretation and Identification</h2>\n<p class=\"Standard\">This chapter defines and differentiates between three types of causation: necessary causes, sufficient causes, and necessary and sufficient causes. When starting a fire, oxygen is a necessary cause, but not a sufficient cause (given the lack of spontaneous combustion). Striking a match is both a necessary cause and a sufficient cause, as the fire would not occur without the match and striking a match is likely to start a fire. These intuitive terms are given formal mathematical definitions using counterfactuals, and much of the chapter is devoted to determining when those counterfactuals can be uniquely measured (i.e. when they're identifiable). Simply knowing the joint probability distribution is insufficient, but is sufficient to establish lower and upper bounds for those quantities. In the presence of certain assumptions or causal graphs, those quantities are identifiable.</p>\n<h2 class=\"Standard\">10. The Actual Cause</h2>\n<p class=\"Standard\">This chapter provides a formal definition of the concept of an &ldquo;actual cause,&rdquo; useful primarily for determining legal liability. For other contexts, the concept of a sufficient cause may be more natural. Pearl introduces the concepts of \"sustenance,\" which is (informally) that a variable's current setting is enough to cause the outcome, regardless of other configurations of the system, and \"causal beams,\" which are structures used to determine sustenance from causal graphs. The chapter provides more examples of causal diagrams, and a bit more intuition about the various kinds of causation, but is primarily useful for retrospective rather than predictive analysis.</p>\n<h2 class=\"Standard\">11. Reflections, Elaborations, and Discussions with Readers</h2>\n<p class=\"Standard\">This chapter bounces from topic to topic, and (perhaps unsurprisingly, given the title) elaborates on many sections of the book. It may be worthwhile to read through chapter 11 in parallel with the rest of the book, as many responses to letters are Pearl clearing up a (presumably common) confusion with a concept. Indeed, the section numbers of this chapter match the chapter numbers of the rest of the book, and so 11.3 is a companion to 3.</p>\n<p class=\"Standard\">The first response, 11.1.1 is worth reading in full. One paragraph in particular stands out:</p>\n<blockquote>\n<p class=\"Standard\">These considerations imply that the slogan \"correlation does not imply causation\" can be translated into a useful principle: behind every causal conclusion there must lie some causal assumption that is not discernible from the distribution function.</p>\n</blockquote>\n<h2 class=\"Standard\">Epilogue. The Art and Science of Cause and Effect</h2>\n<p class=\"Standard\">The book concludes with a public lecture given in 1996. The lecture swiftly introduces concepts and their informal relationships, as well as some of the historical context of the scientific understanding of causality. The lecture moves swiftly but focuses on the narrative and motivation over the mathematics.</p>\n<p class=\"Standard\">&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>The preface to the second edition states that Pearl's \"main audience is the students\" but the book is <a href=\"/lw/kh/explainers_shoot_high_aim_low/\">actually</a> well-suited to be a reference text for experts. There are no exercises, and axioms, theorems, and definitions outweigh the examples. (As a side note, if you find yourself stating that the proof of a theorem can be found in a paper you reference, you are not targeting introductory statistics students.)</p>\n<p>I would recommend reading the lecture first, then the rest of the book (reading the corresponding section of chapter 11 after each chapter), then the lecture again. The first reading of the lecture will motivate many of the concepts involved in the book, then the book will formalize those concepts, and then a second reading of the epilogue will be useful as a comparative exercise. Indeed, the lay reader is likely to find the lecture engaging and informative and the rest of the book impenetrable, so they should read only it (again, it's <a href=\"http://singapore.cs.ucla.edu/LECTURE/lecture_sec1.htm\">available online</a>). When finding links for this post, I discovered that the two most helpful Amazon reviews also suggested to read the epilogue first.</p>\n<p>There are many sections of the book that compare Pearl's approach to other approaches. For readers familiar with those other approaches, I imagine those sections are well worth reading as they provide a clearer picture of what Pearl's approach actually is, why it's necessary, and also make misunderstandings less likely. For the reader who is not familiar with those other approaches, reading the comparisons will sometimes provide deeper intuition, but often just provides historical context. For the reader who has already bought into Pearl's approach, this can get frustrating- particularly when his treatment of alternatives grows combative.</p>\n<p>Chapter 5 is where this becomes significantly noticeable, although I found the comparisons in that chapter helpful; they seemed directly informative about Pearl's approach. In subsequent chapters, though, the convinced reader may skip entire sections with little loss. Unfortunately, the separation is not always clean. For example, in section 6.1: 6.1.1 is definitely worth reading, 6.1.2 probably not, but 6.1.3 is a mixture of relevant and irrelevant; figure 6.2 (around a third of the way through that section) is helpful for understanding the causal graph approach, but is introduced solely to poke holes in competing approaches! 6.1.4 begins comparatively, with the first bit repeating that other approaches have problems with this situation, but then rapidly shifts to the mechanics of navigating the situation where other approaches founder.</p>\n<p>In my previous review of Thinking and Deciding, it seemed natural to recommend different sections to different readers, as the book served many purposes. Here, the mathematical development builds upon itself, so attempting to read chapter 4 without reading chapter 3 seems like a bad idea. Later chapters may be irrelevant to some readers- chapters 9 and 10 are primarily useful for making retrospective and not predictive statements, though they still provide some intuition about and experience with manipulating causal graphs.</p>\n<p>All in all, the book seems deeply important. Causal graphs, interventions, and counterfactuals are all very significant concepts, and the book serves well as a reference for them but perhaps not as an introduction to them. It is probably best at explaining counterfactuals, both what they are and why they are powerful, but I would feel far more confident recommending a less defensive volume which focused on the motivations, basics, and practice for those concepts, rather than their mathematical and theoretical underpinnings.</p>\n<p>&nbsp;</p>\n<p>On a more parochial note, much of the more recent work referenced in the book was done by one of Pearl's former graduate students whose name <a href=\"/user/IlyaShpitser/\">LWers may recognize</a>, and a question by EY prompts an example in 11.3.7.</p>\n<p>The first edition of the book is online for free <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2Femc%2Fcausality_a_chapter_by_chapter_review%2F&amp;v=1&amp;libid=1352480348549&amp;out=http%3A%2F%2Fbayes.cs.ucla.edu%2FBOOK-99%2Fbook-toc.html&amp;ref=http%3A%2F%2Flesswrong.com%2Flw%2Femc%2Fcausality_a_chapter_by_chapter_review%2F&amp;title=Causality%3A%20a%20chapter%20by%20chapter%20review%20-%20Less%20Wrong&amp;txt=here&amp;jsonp=vglnk_jsonp_13524805096432\">here</a>.</p>\n<p class=\"Standard\">Many thanks to <a href=\"/user/majus/\">majus</a>, who lent me his copy of Causality, without which this review would have occurred much later.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"cq69M9ceLNA35ShTR": 2, "4Kcm4etxAJjmeDkHP": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jnjjzkH8Fdzg4D6EK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 60, "baseScore": 100, "extendedScore": null, "score": 0.00022, "legacy": true, "legacyId": "18948", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 100, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><img style=\"float: right; margin-left: 10px; margin-right: 10px;\" src=\"http://bayes.cs.ucla.edu/BOOK-09/causality2-cover.jpg\" alt=\"\" width=\"170\" height=\"240\">This is a chapter by chapter review of <a href=\"http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/052189560X/ref=nosim?tag=vglnk-c319-20\">Causality</a> (2nd ed.) by Judea Pearl (<a href=\"http://bayes.cs.ucla.edu/jp_home.html\">UCLA</a>, <a href=\"http://www.mii.ucla.edu/causality/\">blog</a>). Like my <a href=\"/lw/cb1/thinking_and_deciding_a_chapter_by_chapter_review/\">previous review</a>, the intention is not to summarize but to help readers determine whether or not they should read the book (and if they do, what parts to read). Reading the review is in no way a substitute for reading the book.<br><br>I'll state my basic impression of the book up front, with detailed comments after the chapter discussions: this book is monumentally important to anyone interested in procuring knowledge (especially causal knowledge) from statistical data, but it is a heavily technical book primarily suitable for experts. The mathematics involved is not particularly difficult, but its presentation requires dedicated reading and clarity of thought. Only the epilogue, <a href=\"http://singapore.cs.ucla.edu/LECTURE/lecture_sec1.htm\">this lecture</a>, is suitable for the general audience, and that will be the highest value portion for most readers of LW.</p>\n<h2 class=\"Standard\" id=\"1__Introduction_to_Probabilities__Graphs__and_Causal_Models\"><a id=\"more\"></a>1. Introduction to Probabilities, Graphs, and Causal Models</h2>\n<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:DoNotShowComments /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <w:DoNotOptimizeForBrowser /> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--></p>\n<p><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:none; mso-hyphenate:none; text-autospace:ideograph-other; font-size:12.0pt; font-family:\"Liberation Serif\",\"serif\"; mso-font-kerning:1.5pt; mso-fareast-language:ZH-CN; mso-bidi-language:HI;} --> <!--[endif] --></p>\n<p class=\"Standard\">While the descriptions are complete, this chapter may be more useful as a refresher than as an introduction. The three sections are detailed<span style=\"mso-spacerun:yes\"> in </span>inverse proportion to the expected reader's familiarity.</p>\n<p class=\"Standard\">For the reader who's seen probability calculus before, Pearl's description of it in 12 pages is short, sweet, and complete. For the reader that hasn't seen it, that's just enough space to list the definitions and give a few examples. Compare <a href=\"http://yudkowsky.net/rational/bayes\">Eliezer's explanation of Bayes' Rule</a> (almost 50 pages) to Pearl's (around 2).</p>\n<p class=\"Standard\">The section on graphs moves a little less quickly, but even so don't be afraid to find an online tutorial on d-separation if Pearl's explanation is too fast. For some reason, he does not mention here that section 11.1.2 (p 335-337 in my copy) is a gentler introduction to d-separation. [edit] His blog also linked to <a href=\"http://ftp.cs.ucla.edu/pub/stat_ser/uai12-mohan-pearl.pdf\">this presentation</a>, which is an even gentler introduction to graphs, causal networks, and d-separation.</p>\n<p class=\"Standard\">The section on causal models is the most detailed, as it will be new to most readers, and closely follows the section on graphs. Pearl uses an example to demonstrate the use of counterfactuals, which is a potent first glance at the usefulness of causal models.</p>\n<p class=\"Standard\">He also draws an important distinction between probabilistic, statistical, and causal parameters. Probabilistic parameters are quantities defined in terms of a joint distribution. Statistical parameters are quantities defined in terms of <em>observed</em> variables drawn from a joint distribution. Causal parameters are quantities defined in terms of a causal model, and are not statistical. (I'll leave the explanation of the full implications of the distinctions to the chapter.)</p>\n<h2 class=\"Standard\" id=\"2__A_Theory_of_Inferred_Causation\">2. A Theory of Inferred Causation</h2>\n<p class=\"Standard\">Philosophers have long grappled with the challenge of identifying causal information from data, especially non-experimental data. This chapter details an algorithm to attack that problem.</p>\n<p class=\"Standard\">The key conceptual leap is the use of a third variable in the model as a control. Suppose X and Y are correlated; if there is a third variable Z that is correlated with Y but not with X, the natural interpretation is that X and Z both cause Y. That is not the unique interpretation, which causes quite a bit of philosophical trouble, which Pearl addresses with stability. Only one of the multiple consistent interpretations is stable.</p>\n<p class=\"Standard\">Pearl gives the example of a photo of a chair. There are two primary hypotheses: first, that the underlying scenario was a single chair, and second, that the underlying scenario was two chairs, placed so that the first chair hides the second. While both scenarios predict the observed data, the first scenario is not just simpler, but more stable. If the camera position moved slightly, the second chair might not be hidden anymore- and so we should expect two chairs to be visible in most photos of two chair scenarios.</p>\n<p class=\"Standard\">Pearl also calls on Occam's Razor, in the form of preferring candidate models and distributions which cannot be overfit to those which can be overfit. With those two reasonable criteria, we can move from an infinite set of possible causal models that could explain the data to a single equivalency class of causal models which <em>most frugally</em> explain the data.</p>\n<p class=\"Standard\">The chapter describes the algorithm, its functionality, and some implementation details, which I won't discuss here.</p>\n<p class=\"Standard\">Pearl also discusses how to differentiate between potential causes, genuine causes, and spurious associations given the output of the causal inference algorithm.</p>\n<p class=\"Standard\">The chapter concludes with some philosophical discussion of the influence of time and variable choice, as well as defending the three core assumptions (of minimality, the Markovian structure of causal models, and stability).</p>\n<h2 class=\"Standard\" id=\"3__Causal_Diagrams_and_the_Identification_of_Causal_Effects\">3. Causal Diagrams and the Identification of Causal Effects</h2>\n<p class=\"Standard\">While we can infer causal relationships from data, that task is far easier when we allow ourselves to assume some sensible causal relationships. This step is necessary, and desirable even though making it transparent is sometimes controversial.</p>\n<p class=\"Standard\">Interesting situations are often very complex, and Pearl shows how causal graphs- even ones where not all nodes may be measured- make it possible to navigate the complexity of those situations. The chapter focuses primarily on identifiability- that is, if we fix X to be some value x, can we determine p(y|do(x))? For an arbitrarily large and complex graph, the answer is non-obvious.</p>\n<p class=\"Standard\">The answer is non-obvious enough that there is massive controversy between statisticians and econometricians, which Pearl attempted to defuse (and describes how well that went at the end of the chapter), because there is a subtle difference between <em>observing</em> that X=x and <em>setting</em> X=x. If we see that the price of corn is $1 a bushel, that implies a very different world than one where we set the price of corn at $1 a bushel. In applications where we want to control a system, we're interested in the second- but normal updating based on Bayes' Rule will give us the first. That is, from a statistical perspective, we can always determine the joint probability distribution, and condition on X=x to get p(y|X=x); but from a causal perspective this generally won't give us the information we want. Different causal models can have the same joint probability distribution- and thus look statistically indistinguishable- but give very different results when X is fixed to a particular value.</p>\n<p class=\"Standard\">When everything is observable (and thus deterministic), there's no challenge in figuring out what will happen when X is fixed to a particular value. When there is uncertainty- that is, only some variables are observable- then we need to determine if we know <em>enough</em> to still be able to determine the effects of fixing X.</p>\n<p class=\"Standard\">His 'intervention calculus'<span style=\"mso-spacerun:yes\"> </span>describes how to fix a variable by modifying the graph, and then what you can get out of the new, modified graph. It takes a necessary detour through the impacts of confounding variables (or, more precisely, what graph structure that represents and how to determine identifiability in the light of that graph structure). This is what lets us describe and calculate p(y|do(x)).</p>\n<p class=\"Standard\">I should comment I feel badly reviewing a technical book like this; my summary of forty pages of math is half a page long, because I leave all of the math to the book itself, and just describe the motivation for the math.</p>\n<h2 class=\"Standard\" id=\"4__Actions__Plans__and_Direct_Effects\">4. Actions, Plans, and Direct Effects</h2>\n<p class=\"Standard\">This chapter begins with a distinction between acts and actions, very similar to the distinction discussed in the previous chapter. He treats acts as events or reactions to stimuli; because they are caused by the environment, they give evidence about the environment. Actions are treated as deliberative- they can't be used as evidence because they haven't happened yet, and are the result of deliberation. They become acts once performed- they're actions from the inside, but acts from the outside. (Link to algorithm feels like on the inside.) Pearl describes the controversy over Newcomb's Problem as a confusion over the distinction between acts and actions. Evidential Decision Theory, often called EDT, is discussed and dismissed; because it doesn't respect this distinction (or, really, any causal information), it gives nonsensical results. Commuters shouldn't rush to work, because if they did, that would increase the probability that they've overslept.</p>\n<p class=\"Standard\">Pearl gives a brief description of the relationship between influence diagrams, used in decision analysis, and the causal diagrams he describes here; basically, they're very similar, although the ID literature purposefully sidesteps causal implications which are at the forefront here.</p>\n<p class=\"Standard\">Much of the chapter is spent describing the math that determines when an action's or plan's effects are identifiable.</p>\n<p class=\"Standard\">Of particular interest is the section on direct effects, which walks through the famous Berkeley Admissions example of Simpson's Paradox. Pearl presents a modified version in which the school admits students solely based on qualifications, but appears to discriminate on a department-by-department basis, to demonstrate the necessity of using a full causal model, rather than simple adjusting.</p>\n<h2 class=\"Standard\" id=\"5__Causality_and_Structural_Models_in_Social_Science_and_Economics\">5. Causality and Structural Models in Social Science and Economics</h2>\n<p class=\"Standard\">This chapter will be much more significant to readers with experience doing economic or social science modeling, but is still worthwhile to other readers as a demonstration of the power of causal graphs as a language.</p>\n<p class=\"Standard\">The part of the chapter that is interesting outside of the context of structural models is the part that discusses testing of models. Every missing link in a causal graph is the strong prediction that those two variables are independent (if properly conditioned). This presents a ready test of a causal graph- compute the covariance for every missing link (after proper conditioning), and confirm that those links are not necessary. As a statistical practice, this significantly aids in the debugging of models because it makes local errors obvious, even when they might be obscured in global error tests.</p>\n<p class=\"Standard\">That said, I found it mildly disconcerting that Pearl did not mention there the rationale for using global tests. That is, if there are twenty missing links in your causal diagram, and you collect real data and calculate covariances, on average you should expect the covariance of one missing link to be statistically significantly different from zero if you're using a local test for each link independently. A global test will look at one statistically significant red flag and ignore it as expected given the number of coefficients.</p>\n<p class=\"Standard\">In the context of structural models, most of the interesting parts of the chapter deal with determining the identifiability of parameters in the structural models, and then how to interpret those parameters. Pearl's approach is clear, easily understandable, and soundly superior to alternatives that he quotes (primarily to demonstrate his superiority to them).</p>\n<h2 class=\"Standard\" id=\"6__Simpson_s_Paradox__Confounding__and_Collapsibility\">6. Simpson's Paradox, Confounding, and Collapsibility</h2>\n<p class=\"Standard\">This chapter begins by dissolving Simpson's Paradox, which is more precisely called a reversal effect. Pearl gives a simple example: suppose 80 subjects have a disease and take a drug to treat it. 50%<span style=\"mso-spacerun:yes\"> </span>(20) of those who take the drug recover, and 40% (16) of those who do not take the drug recover. By itself, this seems to suggest that the drug increases the recovery rate.</p>\n<p class=\"Standard\">The effect is reversed, though, when you take gender into account. Of the men, 30 decided to take the drug- and only 60% (18) of them recovered, compared to 70% (7) of the 10 that decided to not take the drug. Of the women, 10 decided to take the drug- and only 20% (2) of them recovered, compared to 30% (9) of the 30 who did not decide to take the drug.</p>\n<p class=\"Standard\">Depicting the issue causally, the effect is clear: sex impacts both the proportion of subjects who take the drug and the base recovery rate, and the positive impact of sex on recovery is masking the negative impact of the drug on recovery. Simply calculating p(recovery|drug)-p(recovery|~drug) does not tell us if the drug is helpful. The parameter we need for that is p(recovery|do(drug))-p(recovery|do(~drug)). With causal diagrams and a clear conceptual difference between observing and fixing events, that's not a mistake one would make, and so there's no paradox to avoid and nothing interesting to see.</p>\n<p class=\"Standard\">The rest of the chapter discusses confounding, presenting a definition of stable no-confounding between variables and showing why other definitions are less useful or rigorous. For readers who haven't heard of those alternatives before, the comparisons will not be particularly interesting or enlightening (as compared to the previous chapter, where the discussion of structural models seems readily intelligible to someone with little experience with them), though they do provide some insight into the issue of confounding.</p>\n<h2 class=\"Standard\" id=\"7__The_Logic_of_Structure_Based_Counterfactuals\">7. The Logic of Structure-Based Counterfactuals</h2>\n<p class=\"Standard\">Pearl returns to the topic of counterfactuals, briefly introduced before, and gives them a firm mathematical foundation and linguistic interpretation, then makes their usefulness clear. Counterfactuals are the basis of interventions in complex systems- they encode the knowledge of what consequences a particular change would have. They also represent a convenient way to store and test causal information.</p>\n<p class=\"Standard\">This power to predict the consequences of changes is what makes causal models superior to non-causal models. Pearl gives a great example of a basic econometric situation:</p>\n<p class=\"Standard\" style=\"padding-left: 30px;\">q=b<sub>1</sub>p+d<sub>1</sub>i+u<sub>1</sub></p>\n<p class=\"Standard\" style=\"padding-left: 30px;\">p=b<sub>2</sub>q+d<sub>2</sub>w+u<sub>2</sub></p>\n<p class=\"Standard\">where q is the quantity demanded, I is the household income, p is the price level, w is the wage rate, and the u<sub>i</sub>s are uncorrelated error terms. The equilibrium level of price and quantity demanded is determined by the feedback between those two equations.</p>\n<p class=\"Standard\" style=\"margin-left:0in;text-indent:0in;mso-list:l0 level1 lfo1\">Pearl identifies three quantities of interest:</p>\n<ol>\n<li>What is the expected value of the demand Q if the price is <em>controlled at p=p<sub>0</sub></em>?</li>\n<li>What is the expected value of the demand Q if the price is <em>reported to be p=p<sub>0</sub></em>?</li>\n<li><span style=\"mso-fareast-font-family:&quot;Liberation Serif&quot;;mso-bidi-font-family:&quot;Liberation Serif&quot;\"><span style=\"mso-list:Ignore\"><span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;</span></span></span>Given that the price is currently p=p<sub>0</sub>, what is the expected value of the demand Q if <em>we were to control</em> the price at p=p<sub>1</sub>?</li>\n</ol>\n<p class=\"Standard\">The second is the only quantity available from standard econometric analysis; the causal analysis Pearl describes easily calculates all three quantities. Again, I leave all of the actual math to the book, but this example was vivid enough that I had to reprint it.</p>\n<p class=\"Standard\">The chapter continues with a set of axioms that describe structural counterfactuals, which then allows Pearl to compare structural counterfactuals with formulations attempted by others. Again, for the reader only interested in Pearl's approach, the comparisons are more tedious than enlightening. There are enough possibly non-obvious implications to reward the dedicated reader, and the reader familiar with the object of the comparison will find the comparison far more meaningful, but the hurried reader would be forgiven for skipping a few sections.</p>\n<p class=\"Standard\">The discussion of exogeneity is valuable for all readers, though, as it elucidates a hierarchy between graphical criteria, error-based criteria, and counterfactual criteria. Each of those criteria implies the one that follows it, but the implications do not flow in the reverse direction; another example of how the language of graphs is more powerful than alternative languages.</p>\n<h2 class=\"Standard\" id=\"8__Imperfect_Experiments__Bounding_Effects_and_Counterfactuals\">8. Imperfect Experiments: Bounding Effects and Counterfactuals</h2>\n<p class=\"Standard\">This chapter describes how to extract useful information (through bounds) from imperfect experiments. For experiments where all observed variables are binary (and, if they aren't, they can be binarized through partitioning), but unobserved variables are free to be monstrously complicated, that complexity can be partitioned into four classes of responses, to match the four possible functional forms between binary variables.</p>\n<p class=\"Standard\">Pearl uses the example of medical drug testing- patients are encouraged to take the drug (experimental group) or not (control group), but compliance may be imperfect, as patients may not take medication given to them or patients not given medication may procure it by other means. Patients can be classed as either never taking the drug, complying with instructions, defying instructions, or always taking the drug. Similarly, the drug's effect on patient recovery can be classified as never recovering, helping, hurting, or always recovering. The two could obviously be related, and so the full joint distribution has 15 degrees of freedom- but we can pin down enough of those degrees of freedom with the observations that we make (of encouragement, treatment, and then recovery) to establish an upper and lower bound for the effect that the treatment has on recovery.</p>\n<p class=\"Standard\">The examples in this chapter are much more detailed and numerical; it also includes a section on Bayesian estimation of the parameters as a complement to or substitute for bounding.</p>\n<h2 class=\"Standard\" id=\"9__Probability_of_Causation__Interpretation_and_Identification\">9. Probability of Causation: Interpretation and Identification</h2>\n<p class=\"Standard\">This chapter defines and differentiates between three types of causation: necessary causes, sufficient causes, and necessary and sufficient causes. When starting a fire, oxygen is a necessary cause, but not a sufficient cause (given the lack of spontaneous combustion). Striking a match is both a necessary cause and a sufficient cause, as the fire would not occur without the match and striking a match is likely to start a fire. These intuitive terms are given formal mathematical definitions using counterfactuals, and much of the chapter is devoted to determining when those counterfactuals can be uniquely measured (i.e. when they're identifiable). Simply knowing the joint probability distribution is insufficient, but is sufficient to establish lower and upper bounds for those quantities. In the presence of certain assumptions or causal graphs, those quantities are identifiable.</p>\n<h2 class=\"Standard\" id=\"10__The_Actual_Cause\">10. The Actual Cause</h2>\n<p class=\"Standard\">This chapter provides a formal definition of the concept of an \u201cactual cause,\u201d useful primarily for determining legal liability. For other contexts, the concept of a sufficient cause may be more natural. Pearl introduces the concepts of \"sustenance,\" which is (informally) that a variable's current setting is enough to cause the outcome, regardless of other configurations of the system, and \"causal beams,\" which are structures used to determine sustenance from causal graphs. The chapter provides more examples of causal diagrams, and a bit more intuition about the various kinds of causation, but is primarily useful for retrospective rather than predictive analysis.</p>\n<h2 class=\"Standard\" id=\"11__Reflections__Elaborations__and_Discussions_with_Readers\">11. Reflections, Elaborations, and Discussions with Readers</h2>\n<p class=\"Standard\">This chapter bounces from topic to topic, and (perhaps unsurprisingly, given the title) elaborates on many sections of the book. It may be worthwhile to read through chapter 11 in parallel with the rest of the book, as many responses to letters are Pearl clearing up a (presumably common) confusion with a concept. Indeed, the section numbers of this chapter match the chapter numbers of the rest of the book, and so 11.3 is a companion to 3.</p>\n<p class=\"Standard\">The first response, 11.1.1 is worth reading in full. One paragraph in particular stands out:</p>\n<blockquote>\n<p class=\"Standard\">These considerations imply that the slogan \"correlation does not imply causation\" can be translated into a useful principle: behind every causal conclusion there must lie some causal assumption that is not discernible from the distribution function.</p>\n</blockquote>\n<h2 class=\"Standard\" id=\"Epilogue__The_Art_and_Science_of_Cause_and_Effect\">Epilogue. The Art and Science of Cause and Effect</h2>\n<p class=\"Standard\">The book concludes with a public lecture given in 1996. The lecture swiftly introduces concepts and their informal relationships, as well as some of the historical context of the scientific understanding of causality. The lecture moves swiftly but focuses on the narrative and motivation over the mathematics.</p>\n<p class=\"Standard\">&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p>The preface to the second edition states that Pearl's \"main audience is the students\" but the book is <a href=\"/lw/kh/explainers_shoot_high_aim_low/\">actually</a> well-suited to be a reference text for experts. There are no exercises, and axioms, theorems, and definitions outweigh the examples. (As a side note, if you find yourself stating that the proof of a theorem can be found in a paper you reference, you are not targeting introductory statistics students.)</p>\n<p>I would recommend reading the lecture first, then the rest of the book (reading the corresponding section of chapter 11 after each chapter), then the lecture again. The first reading of the lecture will motivate many of the concepts involved in the book, then the book will formalize those concepts, and then a second reading of the epilogue will be useful as a comparative exercise. Indeed, the lay reader is likely to find the lecture engaging and informative and the rest of the book impenetrable, so they should read only it (again, it's <a href=\"http://singapore.cs.ucla.edu/LECTURE/lecture_sec1.htm\">available online</a>). When finding links for this post, I discovered that the two most helpful Amazon reviews also suggested to read the epilogue first.</p>\n<p>There are many sections of the book that compare Pearl's approach to other approaches. For readers familiar with those other approaches, I imagine those sections are well worth reading as they provide a clearer picture of what Pearl's approach actually is, why it's necessary, and also make misunderstandings less likely. For the reader who is not familiar with those other approaches, reading the comparisons will sometimes provide deeper intuition, but often just provides historical context. For the reader who has already bought into Pearl's approach, this can get frustrating- particularly when his treatment of alternatives grows combative.</p>\n<p>Chapter 5 is where this becomes significantly noticeable, although I found the comparisons in that chapter helpful; they seemed directly informative about Pearl's approach. In subsequent chapters, though, the convinced reader may skip entire sections with little loss. Unfortunately, the separation is not always clean. For example, in section 6.1: 6.1.1 is definitely worth reading, 6.1.2 probably not, but 6.1.3 is a mixture of relevant and irrelevant; figure 6.2 (around a third of the way through that section) is helpful for understanding the causal graph approach, but is introduced solely to poke holes in competing approaches! 6.1.4 begins comparatively, with the first bit repeating that other approaches have problems with this situation, but then rapidly shifts to the mechanics of navigating the situation where other approaches founder.</p>\n<p>In my previous review of Thinking and Deciding, it seemed natural to recommend different sections to different readers, as the book served many purposes. Here, the mathematical development builds upon itself, so attempting to read chapter 4 without reading chapter 3 seems like a bad idea. Later chapters may be irrelevant to some readers- chapters 9 and 10 are primarily useful for making retrospective and not predictive statements, though they still provide some intuition about and experience with manipulating causal graphs.</p>\n<p>All in all, the book seems deeply important. Causal graphs, interventions, and counterfactuals are all very significant concepts, and the book serves well as a reference for them but perhaps not as an introduction to them. It is probably best at explaining counterfactuals, both what they are and why they are powerful, but I would feel far more confident recommending a less defensive volume which focused on the motivations, basics, and practice for those concepts, rather than their mathematical and theoretical underpinnings.</p>\n<p>&nbsp;</p>\n<p>On a more parochial note, much of the more recent work referenced in the book was done by one of Pearl's former graduate students whose name <a href=\"/user/IlyaShpitser/\">LWers may recognize</a>, and a question by EY prompts an example in 11.3.7.</p>\n<p>The first edition of the book is online for free <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2Femc%2Fcausality_a_chapter_by_chapter_review%2F&amp;v=1&amp;libid=1352480348549&amp;out=http%3A%2F%2Fbayes.cs.ucla.edu%2FBOOK-99%2Fbook-toc.html&amp;ref=http%3A%2F%2Flesswrong.com%2Flw%2Femc%2Fcausality_a_chapter_by_chapter_review%2F&amp;title=Causality%3A%20a%20chapter%20by%20chapter%20review%20-%20Less%20Wrong&amp;txt=here&amp;jsonp=vglnk_jsonp_13524805096432\">here</a>.</p>\n<p class=\"Standard\">Many thanks to <a href=\"/user/majus/\">majus</a>, who lent me his copy of Causality, without which this review would have occurred much later.</p>", "sections": [{"title": "1. Introduction to Probabilities, Graphs, and Causal Models", "anchor": "1__Introduction_to_Probabilities__Graphs__and_Causal_Models", "level": 1}, {"title": "2. A Theory of Inferred Causation", "anchor": "2__A_Theory_of_Inferred_Causation", "level": 1}, {"title": "3. Causal Diagrams and the Identification of Causal Effects", "anchor": "3__Causal_Diagrams_and_the_Identification_of_Causal_Effects", "level": 1}, {"title": "4. Actions, Plans, and Direct Effects", "anchor": "4__Actions__Plans__and_Direct_Effects", "level": 1}, {"title": "5. Causality and Structural Models in Social Science and Economics", "anchor": "5__Causality_and_Structural_Models_in_Social_Science_and_Economics", "level": 1}, {"title": "6. Simpson's Paradox, Confounding, and Collapsibility", "anchor": "6__Simpson_s_Paradox__Confounding__and_Collapsibility", "level": 1}, {"title": "7. The Logic of Structure-Based Counterfactuals", "anchor": "7__The_Logic_of_Structure_Based_Counterfactuals", "level": 1}, {"title": "8. Imperfect Experiments: Bounding Effects and Counterfactuals", "anchor": "8__Imperfect_Experiments__Bounding_Effects_and_Counterfactuals", "level": 1}, {"title": "9. Probability of Causation: Interpretation and Identification", "anchor": "9__Probability_of_Causation__Interpretation_and_Identification", "level": 1}, {"title": "10. The Actual Cause", "anchor": "10__The_Actual_Cause", "level": 1}, {"title": "11. Reflections, Elaborations, and Discussions with Readers", "anchor": "11__Reflections__Elaborations__and_Discussions_with_Readers", "level": 1}, {"title": "Epilogue. The Art and Science of Cause and Effect", "anchor": "Epilogue__The_Art_and_Science_of_Cause_and_Effect", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "20 comments"}], "headingsCount": 14}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mn3WdvME7CJqSaRar", "2TPph4EGZ6trEbtku"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-26T23:46:39.597Z", "modifiedAt": null, "url": null, "title": "Why aren't we pooling our knowledge-resources yet?", "slug": "why-aren-t-we-pooling-our-knowledge-resources-yet", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:57.961Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamisom", "createdAt": "2011-06-13T03:19:15.520Z", "isAdmin": false, "displayName": "adamisom"}, "userId": "eT8NPFmc2GDb5QFfc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rs5MeAvCWjst8d7DZ/why-aren-t-we-pooling-our-knowledge-resources-yet", "pageUrlRelative": "/posts/rs5MeAvCWjst8d7DZ/why-aren-t-we-pooling-our-knowledge-resources-yet", "linkUrl": "https://www.lesswrong.com/posts/rs5MeAvCWjst8d7DZ/why-aren-t-we-pooling-our-knowledge-resources-yet", "postedAtFormatted": "Wednesday, September 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20aren't%20we%20pooling%20our%20knowledge-resources%20yet%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20aren't%20we%20pooling%20our%20knowledge-resources%20yet%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frs5MeAvCWjst8d7DZ%2Fwhy-aren-t-we-pooling-our-knowledge-resources-yet%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20aren't%20we%20pooling%20our%20knowledge-resources%20yet%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frs5MeAvCWjst8d7DZ%2Fwhy-aren-t-we-pooling-our-knowledge-resources-yet", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frs5MeAvCWjst8d7DZ%2Fwhy-aren-t-we-pooling-our-knowledge-resources-yet", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 514, "htmlBody": "<p>Why aren't we systematically pooling many excellent and relevant academic articles yet?</p>\n<p>I'm well aware of the current thread on the best textbooks for any subject, but why only share \"the best\" \"textbooks\"? I understand the desire for vetted quality, but I think we're missing an awesome opportunity here.&nbsp;I, for one, have a long (to me) list of fascinating academic articles I'm probably not going to read, but which I think other LessWrongers might very much like to be pointed to. For example:</p>\n<ul>\n<li><em>Formalizing Trust as a Computational Concept</em>, a CS dissertation by Stephen Marsh, 1994. From the abstract: \"This thesis provides a clarification of trust. We present a formalism for trust which provides us with a tool for precise discussion. The formalism is implementable: it can be embedded in an artificial agent, enabling the agent to make trust-based decisions.\"</li>\n<li><em>Mechanisms of Social Cognition</em>, Frith and Frith, 2012, Annual Review of Psychology, which includes a spectacular table on pages 4 and 5: column 1, the broad areas considered (e.g. reward learning, tracking intentions); column 2, the mechanisms involved; column 3, the brain regions involved; and column 4, the social processes to which the mechanism and/or brain region apply.</li>\n<li><em>Peering into the Bias Blind Spot</em>, in which we learn that part of the reason people think themselves to be less biased than the average is because they rely on introspections while looking at behavior in others.</li>\n<li><em>Information processing, computation, and cognition</em>&nbsp;(Piccinini and Scarantino, 2009), which argues that there are a number of myths concerning these topics that stand in the way of theoretical progress, myths like 'computation is the same as information processing' or 'the Church-Turing thesis entails that cognition is computation'.</li>\n<li>Review articles on operant conditioning;&nbsp;implementation intentions,&nbsp;a major new scientific approach to forming habits';&nbsp;the planning fallacy (and it's \"cognitive, motivational, and social origins\"); and procrastination (the last one has already been mentioned here in lukeprog's main procrastination article).</li>\n</ul>\n<p>We could have a database of some kind. And it could include&nbsp;articles that have already been well-spread here, like the seminal Kahneman &amp; Tversky papers. Or the Singularity Institute's work and related work, such as Asimov's 3 laws of robotics and machine meta-ethics (S.L. Anderson), or the Transhumanist FAQ (Nick Bostrom).</p>\n<p>I propose that we do so, and that we seek a balance between (a) quality and quantity and (b) organization (into topics) and simply adding, but that we also not worry too much about that right now and just start something. I envision adding article entries to some kind of simple database with (i)&nbsp;the bibliographic information, (ii) a link to a non-gated pdf if possible, (iii) a brief description detailing why it's bound to be very interesting to some LessWrongers, and (iv) a vote-up/down mechanism to allow fellow LessWrongers to agree that 'yes, this article title+description does seem exceptionally interesting'.</p>\n<p><em>Main issue: determining the best way to share articles with each other, after determining that it's something we'd like to do.</em></p>\n<p>P.S. I am unfamiliar with tagging on LW articles (having not written one before), should I add a tag/tags?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rs5MeAvCWjst8d7DZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 12, "extendedScore": null, "score": 9.957962342487259e-07, "legacy": true, "legacyId": "19020", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-27T02:30:11.465Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC: Choice Blindness", "slug": "meetup-washington-dc-choice-blindness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:58.621Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ncws4Fwr7Tg3WQ7Kd/meetup-washington-dc-choice-blindness", "pageUrlRelative": "/posts/Ncws4Fwr7Tg3WQ7Kd/meetup-washington-dc-choice-blindness", "linkUrl": "https://www.lesswrong.com/posts/Ncws4Fwr7Tg3WQ7Kd/meetup-washington-dc-choice-blindness", "postedAtFormatted": "Thursday, September 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%3A%20Choice%20Blindness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%3A%20Choice%20Blindness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNcws4Fwr7Tg3WQ7Kd%2Fmeetup-washington-dc-choice-blindness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%3A%20Choice%20Blindness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNcws4Fwr7Tg3WQ7Kd%2Fmeetup-washington-dc-choice-blindness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNcws4Fwr7Tg3WQ7Kd%2Fmeetup-washington-dc-choice-blindness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ec'>Washington DC: Choice Blindness</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 September 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll spend some time discussing the recent findings on choice blindness, then revert to general conversation.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ec'>Washington DC: Choice Blindness</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ncws4Fwr7Tg3WQ7Kd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 9.958815057390502e-07, "legacy": true, "legacyId": "19022", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Choice_Blindness\">Discussion article for the meetup : <a href=\"/meetups/ec\">Washington DC: Choice Blindness</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 September 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll spend some time discussing the recent findings on choice blindness, then revert to general conversation.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Choice_Blindness1\">Discussion article for the meetup : <a href=\"/meetups/ec\">Washington DC: Choice Blindness</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC: Choice Blindness", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Choice_Blindness", "level": 1}, {"title": "Discussion article for the meetup : Washington DC: Choice Blindness", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Choice_Blindness1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-27T04:25:52.580Z", "modifiedAt": null, "url": null, "title": "Could evolution have selected for moral realism?", "slug": "could-evolution-have-selected-for-moral-realism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:04.334Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3bEQ36Yrxwiquh7MK/could-evolution-have-selected-for-moral-realism", "pageUrlRelative": "/posts/3bEQ36Yrxwiquh7MK/could-evolution-have-selected-for-moral-realism", "linkUrl": "https://www.lesswrong.com/posts/3bEQ36Yrxwiquh7MK/could-evolution-have-selected-for-moral-realism", "postedAtFormatted": "Thursday, September 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Could%20evolution%20have%20selected%20for%20moral%20realism%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACould%20evolution%20have%20selected%20for%20moral%20realism%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3bEQ36Yrxwiquh7MK%2Fcould-evolution-have-selected-for-moral-realism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Could%20evolution%20have%20selected%20for%20moral%20realism%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3bEQ36Yrxwiquh7MK%2Fcould-evolution-have-selected-for-moral-realism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3bEQ36Yrxwiquh7MK%2Fcould-evolution-have-selected-for-moral-realism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 782, "htmlBody": "<p>I was surprised to see the <a href=\"/r/all/lw/emj/poll_less_wrong_and_mainstream_philosophy_how/7iag\">high number</a> of moral realists on Less Wrong, so I thought I would bring up a (probably unoriginal) point that occurred to me a while ago.</p>\n<p>Let's say that all your thoughts either seem factual or fictional.&nbsp; Memories seem factual, stories seem fictional.&nbsp; Dreams seem factual, daydreams seem fictional (though they might seem factual if you're a <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/21959201\">compulsive fantasizer</a>).&nbsp; Although the things that seem factual match up reasonably well to the things that actually are factional, this isn't the case axiomatically.&nbsp; If deviating from this pattern is adaptive, evolution will select for it.&nbsp; This could result in situations like: the rule that pieces move diagonally in checkers seems fictional, while the rule that you can't kill people seems factual, even though they're both just conventions.&nbsp; (Yes, the rule that you can't kill people is a very <em>good</em> convention, and it makes sense to have heavy default punishments for breaking it.&nbsp; But I don't think it's different in kind from the rule that you must move diagonally in checkers.)</p>\n<p>I'm not an expert, but it definitely seems as though this could actually be the case.&nbsp; Humans are fairly conformist social animals, and it seems plausible that evolution would've selected for taking the rules seriously, even if it meant using the fact-processing system for things that were really just conventions.</p>\n<p>Another spin on this: We could see philosophy as the discipline of measuring, collating, and making internally consistent our intuitions on various philosophical issues.&nbsp; Katja Grace has <a href=\"http://www.overcomingbias.com/2012/09/signaling-bias-in-philosophical-intuition.html\">suggested</a> that the measurement of philosophical intuitions may be corrupted by the desire to signal on the part of the philosophy enthusiasts.&nbsp; Could evolutionary pressure be an additional source of corruption?&nbsp; Taking this idea even further, what do our intuitions amount to at <em>all</em> aside from a composite of evolved and encultured notions?&nbsp; If we're talking about a <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">question of fact</a>, one can overcome evolution/enculturation by improving one's model of the world, performing experiments, etc.&nbsp; (I was encultured to believe in God by my parents.&nbsp; God didn't drop proverbial bowling balls from the sky when I prayed for them, so I eventually noticed the contradiction in my model and deconverted.&nbsp; It wasn't trivial--there was a high degree of enculturation to overcome.)&nbsp; But if the question has no basis in fact, like the question of whether morals are \"real\", then genes and enculturation will wholly determine your answer to it.&nbsp; Right?</p>\n<p>Yes, you can think about your moral intuitions, weigh them against each other, and make them internally consistent.&nbsp; But this is kind of like trying to add resolution back in to an extremely pixelated photo--just because it's no longer obviously \"wrong\" doesn't guarantee that it's \"right\".&nbsp; And there's the possibility of path-dependence--the parts of the photo you try to improve initially could have a very significant effect on the final product.&nbsp; Even if you <em>think</em> you're willing to discard your initial philosophical conclusions, there's still the possibility of accidentally destroying your initial intuitional data or enculturing yourself with your early results.</p>\n<p>To avoid this possibility of path-dependence, you could carefully document your initial intuitions, pursue lots of different paths to making them consistent in parallel, and maybe even choose a \"best match\".&nbsp; But it's not obvious to me that your initial mix of evolved and encultured values even deserves this preferential treatment.</p>\n<p>Currently, I disagree with what seems to be the prevailing view on Less Wrong that achieving a Really Good Consistent Match for our morality is Really Darn Important.&nbsp; I'm not sure that randomness from evolution and enculturation should be treated differently from random factors in the intuition-squaring process.&nbsp; It's randomness all the way through either way, right?&nbsp; The main reason \"bad\" consistent matches are considered so \"bad\", I suspect, is that they engender cognitive dissonance (e.g. maybe my current ethics says I should hack Osama Bin Laden to death in his sleep with a knife if I get the chance, but this is an extremely bad match for my evolved/encultured intuitions, so I experience a ton of cognitive dissonance actually doing this).&nbsp; But cognitive dissonance seems to me like just another aversive experience to factor in to my utility calculations.</p>\n<p>Now that you've read this, maybe your intuition has changed and you're a moral anti-realist.&nbsp; But in what sense has your intuition \"improved\" or become more accurate?</p>\n<p>I really have zero expertise on any of this, so if you have relevant links please share them.&nbsp; But also, who's to say that matters?&nbsp; In what sense could philosophers have \"better\" philosophical intuition?&nbsp; The only way I can think of for theirs to be \"better\" is if they've seen a larger part of the landscape of philosophical questions, and are therefore better equipped to build consistent philosophical models (<a href=\"http://commonsenseatheism.com/?p=6113\">example</a>).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2, "exZi6Bing5AiM4ZQB": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3bEQ36Yrxwiquh7MK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 7, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "19028", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a7n8GdKiAZRX86T5A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-27T06:01:11.175Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Protected From Myself", "slug": "seq-rerun-protected-from-myself", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:57.933Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h4J37gH5xr5w7tuKo/seq-rerun-protected-from-myself", "pageUrlRelative": "/posts/h4J37gH5xr5w7tuKo/seq-rerun-protected-from-myself", "linkUrl": "https://www.lesswrong.com/posts/h4J37gH5xr5w7tuKo/seq-rerun-protected-from-myself", "postedAtFormatted": "Thursday, September 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Protected%20From%20Myself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Protected%20From%20Myself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh4J37gH5xr5w7tuKo%2Fseq-rerun-protected-from-myself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Protected%20From%20Myself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh4J37gH5xr5w7tuKo%2Fseq-rerun-protected-from-myself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh4J37gH5xr5w7tuKo%2Fseq-rerun-protected-from-myself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p>Today's post, <a href=\"/lw/uz/protected_from_myself/\">Protected From Myself</a> was originally published on 19 October 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Protected_From_Myself\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Ethics can protect you from your own mistakes, especially when your mistakes are about really fundamental things.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/eny/seq_rerun_dark_side_epistemology/\">Dark Side Epistemology</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h4J37gH5xr5w7tuKo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 9.9599154570081e-07, "legacy": true, "legacyId": "19038", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yz2btSaCLHmLWgWD5", "3mTT6TXrsC8zhZWru", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-27T14:35:11.395Z", "modifiedAt": null, "url": null, "title": "[Link] Social Desirability Bias vs. Intelligence Research", "slug": "link-social-desirability-bias-vs-intelligence-research", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:57.798Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7R3eAEDvW7a5EySA5/link-social-desirability-bias-vs-intelligence-research", "pageUrlRelative": "/posts/7R3eAEDvW7a5EySA5/link-social-desirability-bias-vs-intelligence-research", "linkUrl": "https://www.lesswrong.com/posts/7R3eAEDvW7a5EySA5/link-social-desirability-bias-vs-intelligence-research", "postedAtFormatted": "Thursday, September 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Social%20Desirability%20Bias%20vs.%20Intelligence%20Research&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Social%20Desirability%20Bias%20vs.%20Intelligence%20Research%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7R3eAEDvW7a5EySA5%2Flink-social-desirability-bias-vs-intelligence-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Social%20Desirability%20Bias%20vs.%20Intelligence%20Research%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7R3eAEDvW7a5EySA5%2Flink-social-desirability-bias-vs-intelligence-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7R3eAEDvW7a5EySA5%2Flink-social-desirability-bias-vs-intelligence-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 421, "htmlBody": "<p>From <a href=\"http://econlog.econlib.org/archives/2012/09/social_desirabi.html\">EconLog</a> by <a href=\"http://en.wikipedia.org/wiki/Bryan_Caplan\">Bryan Caplan</a>.</p>\n<blockquote>\n<p>When lies sound better than truth, people tend to lie.&nbsp; That's <a href=\"http://econlog.econlib.org/archives/2012/09/the_public_good.html\">Social Desirability Bias</a> for you.&nbsp; Take the truth, \"Half the population is below the 50th percentile of intelligence.\"&nbsp; It's unequivocally true - and sounds <em>awful</em>.&nbsp; Nice people don't call others stupid - even privately.<br /><br />The 2000 <a href=\"http://sda.berkeley.edu/cgi-bin/hsda?harcsda+nes2000\">American National Election Study</a> elegantly confirms this claim.&nbsp; One of the interviewers' tasks was to rate respondents' \"apparent intelligence.\"&nbsp; Possible answers (reverse coded by me for clarity):<br /><br />0= Very Low<br />1= Fairly Low<br />2= Average<br />3= Fairly High<br />4= Very High<br /><br />Objectively measured intelligence famously <a href=\"http://www.amazon.com/Bell-Curve-Intelligence-Structure-Paperbacks/dp/0684824299/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1347371463&amp;sr=1-1&amp;keywords=bell+curve\">fits a bell curve</a>.&nbsp; Subjectively assessed intelligence does not.&nbsp; At all.&nbsp; Check out the ANES distribution.<br /><br /><span class=\"mt-enclosure mt-enclosure-image\" style=\"display: inline;\"><img class=\"mt-image-none\" src=\"http://econlog.econlib.org/iqanes.jpg\" alt=\"iqanes.jpg\" width=\"341\" height=\"261\" /></span><br /> <br />The ANES is supposed to be a representative national sample.&nbsp; Yet according to interviewers, only 6.1% of respondents are \"below average\"!&nbsp; The median respondent is \"fairly high.\"&nbsp; Over 20% are \"very high.\"&nbsp; Social Desirability Bias - interviewers' reluctance to impugn anyone's intelligence - practically has to be the explanation.<br /><br />You could just call this as an amusing curiosity and move on.&nbsp; But wait.&nbsp; Stare at the ANES results for a minute.&nbsp; Savor the data.&nbsp; Question: Are you starting to see the true face of widespread hostility to intelligence research?&nbsp; I sure think I do.<br /><br />Suppose intelligence research were impeccable.&nbsp; How would psychologically normal humans react?&nbsp; Probably just as they do in the ANES: With denial.&nbsp; How can stupidity be a major cause of personal failure and social ills?&nbsp; Only if the world is full of stupid people.&nbsp; What kind of a person believes the world is full of stupid people?&nbsp; \"A realist\"?&nbsp; No!&nbsp; A jerk.&nbsp; A big meanie.<br /><br />My point is not that intelligence research is impeccable.&nbsp; My point, rather, is that <strong>hostility to intelligence research is all out of proportion to its flaws</strong> - and Social Desirability Bias is the best explanation.&nbsp; Intelligence research tells the world what it doesn't want to hear.&nbsp; It says what people aren't supposed to say.&nbsp; <strong>On reflection, the amazing thing isn't that intelligence research has failed to vanquish its angry critics.&nbsp; The amazing thing is that the angry critics have failed to vanquish intelligence research.</strong>&nbsp; <a href=\"http://www.amazon.com/Factor-Science-Evolution-Behavior-Intelligence/dp/0275961036/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1347373751&amp;sr=1-1&amp;keywords=g+factor\">Everything we've learned</a> about human intelligence is a triumph of mankind's rationality over mankind's Social Desirability Bias.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7R3eAEDvW7a5EySA5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 11, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "19040", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-27T19:04:14.872Z", "modifiedAt": null, "url": null, "title": "From First Principles", "slug": "from-first-principles", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:58.601Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W6oJz6eiQ3q5t84Z3/from-first-principles", "pageUrlRelative": "/posts/W6oJz6eiQ3q5t84Z3/from-first-principles", "linkUrl": "https://www.lesswrong.com/posts/W6oJz6eiQ3q5t84Z3/from-first-principles", "postedAtFormatted": "Thursday, September 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20From%20First%20Principles&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFrom%20First%20Principles%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW6oJz6eiQ3q5t84Z3%2Ffrom-first-principles%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=From%20First%20Principles%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW6oJz6eiQ3q5t84Z3%2Ffrom-first-principles", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW6oJz6eiQ3q5t84Z3%2Ffrom-first-principles", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1490, "htmlBody": "<p>Related: <a href=\"/lw/la/truly_part_of_you/\">Truly a Part of You</a>, <a href=\"/lw/5fu/what_data_generated_that_thought/\">What Data Generated That Thought</a></p>\n<h2>Some Case Studies</h2>\n<p>The other day my friend was learning to solder and he asked an experienced hacker for advice. The hacker told him that because heat rises, you should apply the soldering iron underneath the work to maximize heat transfer. Seems reasonable, logically inescapable, even. When I heard of this, I thought through to why heat rises and when, and saw that it was not so. I don't remember the conversation, but the punchline is that hot things become less dense, and less dense things float, and if you're not in a fluid, hot fluids can't float. In the case of soldering, the primary mode of heat transfer is conduction through the liquid metal, so to maximize heat transfer, get the tip wet before you stick it in, and don't worry about position.</p>\n<p>This is a case of surface reasoning failing because the heuristic (heat rises) was not <a href=\"/lw/la/truly_part_of_you/\">truly a part</a> of my friend or the random hacker. I want to focus on the actual 5-second skill of going back To First Principles that catches those failures.</p>\n<p>Here's another; watch for the 5 second cues and responses: A few years ago, I was building a robot submarine for a school project. We were in the initial concept design phase, wondering what it should look like. My friend Peter said, \"It should be wide, because stability is important\". I noticed the heuristic \"low and wide is stable\" and thought to myself \"Where does that come from? When is it valid?\". In the case of catamarans or sports cars, wide is stable because it increases the lever arm between restoring force (gravity) and support point (wheel or hull), and low makes the tipping point harder to reach. Under water, there is no tipping point, and things are better modeled as hanging from their center of volume. In other words, underwater, the stability criteria is vertical separation, instead of horizontal separation. (More precisely, you can model the submarine as a damped pendulum, and notice that you want to tune the parameters for approximately <a href=\"https://en.wikipedia.org/wiki/Damping\">critical damping</a>). We went back to First Principles and figured out what actually mattered, then went on to build an awesome robot.</p>\n<p>Let's review what happened. We noticed a heuristic or bit of qualitative knowledge (wide is stable), and asked \"Why? When? How much?\", which led us to the <em>quantitative</em> answer, which told us much more precisely exactly <em>what</em> matters (critical damping) and what does not matter (width, maximizing restoring force, etc).</p>\n<p>A more Rationality-related example: I recently thought about Courage, and the fact that most people are too afraid of risk (beyond just utility concavity), and as a heuristic we should be failing more. Around the same time, I'd been hounding Michael Vassar (at minicamp) for advice. One piece that stuck with me was \"use decision theory\". Ok, Courage is about decisions; let's go.</p>\n<p>\"You should be failing more\", they say. You notice the heuristic, and immediately ask yourself \"Why? How much more? Prove it from first principles!\" \"Ok\", your forked copy says. \"We want to take all actions with positive expected utility. By the law of large numbers, in (non-black-swan) games we play a lot of, observed utility should approximate expected utility, which means you should be observing just as much fail as win on the edge of what you're willing to do. Courage is being well calibrated on risk; If your craziest plans are systematically succeeding, you are not well calibrated and you need to take more risks.\" That's approximately quantitative, and you can pull out the equations to verify if you like.</p>\n<p>Notice all the subtle qualifications that you may not have guessed from the initial advice; (non-pascalian/lln applies, you can observe utility, your craziest plans, <em>just as much</em> fail as win (not just as many, not more)). (example application: one of the best matches for those conditions is social interaction) Those of you who actually busted out the equations and saw the math of it, notice how much more you understand than I am able to communicate with just words.</p>\n<p>Ok, now I've <a href=\"/lw/bc3/sotw_be_specific/\">named three</a>, so we can play the generalization game without angering the gods.</p>\n<h2><a href=\"/lw/5kz/the_5second_level/\">On the Five-Second Level</a></h2>\n<p><strong>Trigger:</strong> Notice an attempt to use some bit of knowledge or a heuristic. Something qualitative, something with unclear domain, something that affects what you are doing, something where you can't <em>see</em> the truth.</p>\n<p><strong>Action:</strong> Ask yourself: What problem does it try to solve (what's its interface, type signature, domain, etc)? What's the <a href=\"/lw/bc3/sotw_be_specific/\">specific mechanism</a> of its truth when it is true? In what situations does that hold? Is this one of those? If not, can we derive what the correct result would be in this case? Basically \"prove it\". Sometimes it will take 2 seconds, sometimes a day or two; if it looks like you can't immediately see it, come up with whatever quick approximation you can and update towards \"I don't know what's going on here\". Come back later for practice.</p>\n<p>It doesn't have to be a formal proof that would convince even the most skeptical mathematician or outsmart even the most powerful demon, but be sure to <em>see</em> the truth.</p>\n<p>Without this skill of going back to First Principles, I think you would not fully get the point of <a href=\"/lw/la/truly_part_of_you/\">truly a part of you</a>. Why is being able to regenerate your knowledge useful? What are the hidden qualifications on that? How does it work? (See what I'm doing here?) Once you see many examples of the kind of expanded and formidably precise knowledge you get from having performed a derivation, and the vague and confusing state of having only a theorem, you will notice the difference. What the difference is, in terms of a derivation From First Principles, is left as an exercise for the reader (ie. I don't know). Even without that, though, having <em>seen</em> the difference is a huge step up.</p>\n<p>From having seen the difference between derived and taught knowledge, I notice that one of the caveats of making knowledge Truly a Part of You is that just being <em>able</em> to get it From First Principles is not enough; Actually having done the proof tells you a <em>lot</em> more than simply what the correct theorem is. Do not take my word for it; go do some proofs;&nbsp;<em>see</em> the difference.</p>\n<p>So far I've just <em>described</em> something that has been unusually valuable <em>for me</em>. Can it be taught? Will others gain as much? I don't know; I got this one more or less by intellectual lottery. It can probably be tested, though:</p>\n<h2>Testing the \"Prove It\" Habit</h2>\n<p>In school, we had this awesome teacher for thermodynamics and fluid dynamics. He was usually voted best in faculty. His teaching and testing style fit perfectly with my \"learn first principles and derive on the fly\" approach that I've just outlined above, so I did very well in his classes.</p>\n<p>In the lectures and homework, we'd learn all the equations, where they came from (with derivations), how they are used, etc. He'd get us to practice and be good at straightforward application of them. Some of the questions required a bit of creativity.</p>\n<p>On the exams, the questions were substantially easier, but they <em>all</em> required creativity and really understanding the first principles. \"Curve Balls\", we called them. Otherwise smart people found his tests very hard; I got all my marks from them. It's fair to say I did well because I had a very efficient and practiced From First Principles groove in my mind. (This was fair, because actually studying for the test was a reasonable substitute.)</p>\n<p>So basically, I think a good discriminator would be to throw people difficult problems that can be solved with standard procedure and surface heuristics, and then some easier problems that require creative application of first principles, or don't quite work with standard heuristics (but seem to).</p>\n<p>If your subjects have consistent scores between the two types, they are doing it From First Principles. If they get the standard problems right, but not the curve balls, they aren't.</p>\n<p>Examples:</p>\n<p><strong>Straight:</strong> Bayesian cancer test. <strong>Curve:</strong> Here's the base rate and positive rate, how good is the test (liklihood ratio)?</p>\n<p><strong>Straight:</strong> Sunk cost on some bad investment.&nbsp;<strong>Curve:</strong> Something where switching costs, opportunity for experience make staying the correct thing.</p>\n<p><strong>Straight:</strong> Monty Hall. <strong>Curve:</strong> Ignorant Monty Hall.</p>\n<p>Etc.</p>\n<h2>Exercises</h2>\n<p>Again, maybe this can't be taught, but here's some practice ideas just in case it can. I got substantial value from figuring these out From First Principles. Some may be correct, others incorrect, or correct in a limited range. The point is to use them to point you to a problem to solve; once you know the actual problem, ignore the heuristic and just go for truth:</p>\n<p>Science says good theories make bold predictions.</p>\n<p>Deriving From First Principles is a good habit.</p>\n<p>Boats go where you point them, so just sail with the bow pointed to the island.</p>\n<p>People who do bad things should feel guilty.</p>\n<p>I don't have to feel responsible for people getting tortured in Syria.</p>\n<p>If it's broken, fix it.</p>\n<p>(post more in comments)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"SJFsFfFhE6m2ThAYJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W6oJz6eiQ3q5t84Z3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 50, "baseScore": 78, "extendedScore": null, "score": 0.000178, "legacy": true, "legacyId": "19041", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 78, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Related: <a href=\"/lw/la/truly_part_of_you/\">Truly a Part of You</a>, <a href=\"/lw/5fu/what_data_generated_that_thought/\">What Data Generated That Thought</a></p>\n<h2 id=\"Some_Case_Studies\">Some Case Studies</h2>\n<p>The other day my friend was learning to solder and he asked an experienced hacker for advice. The hacker told him that because heat rises, you should apply the soldering iron underneath the work to maximize heat transfer. Seems reasonable, logically inescapable, even. When I heard of this, I thought through to why heat rises and when, and saw that it was not so. I don't remember the conversation, but the punchline is that hot things become less dense, and less dense things float, and if you're not in a fluid, hot fluids can't float. In the case of soldering, the primary mode of heat transfer is conduction through the liquid metal, so to maximize heat transfer, get the tip wet before you stick it in, and don't worry about position.</p>\n<p>This is a case of surface reasoning failing because the heuristic (heat rises) was not <a href=\"/lw/la/truly_part_of_you/\">truly a part</a> of my friend or the random hacker. I want to focus on the actual 5-second skill of going back To First Principles that catches those failures.</p>\n<p>Here's another; watch for the 5 second cues and responses: A few years ago, I was building a robot submarine for a school project. We were in the initial concept design phase, wondering what it should look like. My friend Peter said, \"It should be wide, because stability is important\". I noticed the heuristic \"low and wide is stable\" and thought to myself \"Where does that come from? When is it valid?\". In the case of catamarans or sports cars, wide is stable because it increases the lever arm between restoring force (gravity) and support point (wheel or hull), and low makes the tipping point harder to reach. Under water, there is no tipping point, and things are better modeled as hanging from their center of volume. In other words, underwater, the stability criteria is vertical separation, instead of horizontal separation. (More precisely, you can model the submarine as a damped pendulum, and notice that you want to tune the parameters for approximately <a href=\"https://en.wikipedia.org/wiki/Damping\">critical damping</a>). We went back to First Principles and figured out what actually mattered, then went on to build an awesome robot.</p>\n<p>Let's review what happened. We noticed a heuristic or bit of qualitative knowledge (wide is stable), and asked \"Why? When? How much?\", which led us to the <em>quantitative</em> answer, which told us much more precisely exactly <em>what</em> matters (critical damping) and what does not matter (width, maximizing restoring force, etc).</p>\n<p>A more Rationality-related example: I recently thought about Courage, and the fact that most people are too afraid of risk (beyond just utility concavity), and as a heuristic we should be failing more. Around the same time, I'd been hounding Michael Vassar (at minicamp) for advice. One piece that stuck with me was \"use decision theory\". Ok, Courage is about decisions; let's go.</p>\n<p>\"You should be failing more\", they say. You notice the heuristic, and immediately ask yourself \"Why? How much more? Prove it from first principles!\" \"Ok\", your forked copy says. \"We want to take all actions with positive expected utility. By the law of large numbers, in (non-black-swan) games we play a lot of, observed utility should approximate expected utility, which means you should be observing just as much fail as win on the edge of what you're willing to do. Courage is being well calibrated on risk; If your craziest plans are systematically succeeding, you are not well calibrated and you need to take more risks.\" That's approximately quantitative, and you can pull out the equations to verify if you like.</p>\n<p>Notice all the subtle qualifications that you may not have guessed from the initial advice; (non-pascalian/lln applies, you can observe utility, your craziest plans, <em>just as much</em> fail as win (not just as many, not more)). (example application: one of the best matches for those conditions is social interaction) Those of you who actually busted out the equations and saw the math of it, notice how much more you understand than I am able to communicate with just words.</p>\n<p>Ok, now I've <a href=\"/lw/bc3/sotw_be_specific/\">named three</a>, so we can play the generalization game without angering the gods.</p>\n<h2 id=\"On_the_Five_Second_Level\"><a href=\"/lw/5kz/the_5second_level/\">On the Five-Second Level</a></h2>\n<p><strong>Trigger:</strong> Notice an attempt to use some bit of knowledge or a heuristic. Something qualitative, something with unclear domain, something that affects what you are doing, something where you can't <em>see</em> the truth.</p>\n<p><strong>Action:</strong> Ask yourself: What problem does it try to solve (what's its interface, type signature, domain, etc)? What's the <a href=\"/lw/bc3/sotw_be_specific/\">specific mechanism</a> of its truth when it is true? In what situations does that hold? Is this one of those? If not, can we derive what the correct result would be in this case? Basically \"prove it\". Sometimes it will take 2 seconds, sometimes a day or two; if it looks like you can't immediately see it, come up with whatever quick approximation you can and update towards \"I don't know what's going on here\". Come back later for practice.</p>\n<p>It doesn't have to be a formal proof that would convince even the most skeptical mathematician or outsmart even the most powerful demon, but be sure to <em>see</em> the truth.</p>\n<p>Without this skill of going back to First Principles, I think you would not fully get the point of <a href=\"/lw/la/truly_part_of_you/\">truly a part of you</a>. Why is being able to regenerate your knowledge useful? What are the hidden qualifications on that? How does it work? (See what I'm doing here?) Once you see many examples of the kind of expanded and formidably precise knowledge you get from having performed a derivation, and the vague and confusing state of having only a theorem, you will notice the difference. What the difference is, in terms of a derivation From First Principles, is left as an exercise for the reader (ie. I don't know). Even without that, though, having <em>seen</em> the difference is a huge step up.</p>\n<p>From having seen the difference between derived and taught knowledge, I notice that one of the caveats of making knowledge Truly a Part of You is that just being <em>able</em> to get it From First Principles is not enough; Actually having done the proof tells you a <em>lot</em> more than simply what the correct theorem is. Do not take my word for it; go do some proofs;&nbsp;<em>see</em> the difference.</p>\n<p>So far I've just <em>described</em> something that has been unusually valuable <em>for me</em>. Can it be taught? Will others gain as much? I don't know; I got this one more or less by intellectual lottery. It can probably be tested, though:</p>\n<h2 id=\"Testing_the__Prove_It__Habit\">Testing the \"Prove It\" Habit</h2>\n<p>In school, we had this awesome teacher for thermodynamics and fluid dynamics. He was usually voted best in faculty. His teaching and testing style fit perfectly with my \"learn first principles and derive on the fly\" approach that I've just outlined above, so I did very well in his classes.</p>\n<p>In the lectures and homework, we'd learn all the equations, where they came from (with derivations), how they are used, etc. He'd get us to practice and be good at straightforward application of them. Some of the questions required a bit of creativity.</p>\n<p>On the exams, the questions were substantially easier, but they <em>all</em> required creativity and really understanding the first principles. \"Curve Balls\", we called them. Otherwise smart people found his tests very hard; I got all my marks from them. It's fair to say I did well because I had a very efficient and practiced From First Principles groove in my mind. (This was fair, because actually studying for the test was a reasonable substitute.)</p>\n<p>So basically, I think a good discriminator would be to throw people difficult problems that can be solved with standard procedure and surface heuristics, and then some easier problems that require creative application of first principles, or don't quite work with standard heuristics (but seem to).</p>\n<p>If your subjects have consistent scores between the two types, they are doing it From First Principles. If they get the standard problems right, but not the curve balls, they aren't.</p>\n<p>Examples:</p>\n<p><strong>Straight:</strong> Bayesian cancer test. <strong>Curve:</strong> Here's the base rate and positive rate, how good is the test (liklihood ratio)?</p>\n<p><strong>Straight:</strong> Sunk cost on some bad investment.&nbsp;<strong>Curve:</strong> Something where switching costs, opportunity for experience make staying the correct thing.</p>\n<p><strong>Straight:</strong> Monty Hall. <strong>Curve:</strong> Ignorant Monty Hall.</p>\n<p>Etc.</p>\n<h2 id=\"Exercises\">Exercises</h2>\n<p>Again, maybe this can't be taught, but here's some practice ideas just in case it can. I got substantial value from figuring these out From First Principles. Some may be correct, others incorrect, or correct in a limited range. The point is to use them to point you to a problem to solve; once you know the actual problem, ignore the heuristic and just go for truth:</p>\n<p>Science says good theories make bold predictions.</p>\n<p>Deriving From First Principles is a good habit.</p>\n<p>Boats go where you point them, so just sail with the bow pointed to the island.</p>\n<p>People who do bad things should feel guilty.</p>\n<p>I don't have to feel responsible for people getting tortured in Syria.</p>\n<p>If it's broken, fix it.</p>\n<p>(post more in comments)</p>", "sections": [{"title": "Some Case Studies", "anchor": "Some_Case_Studies", "level": 1}, {"title": "On the Five-Second Level", "anchor": "On_the_Five_Second_Level", "level": 1}, {"title": "Testing the \"Prove It\" Habit", "anchor": "Testing_the__Prove_It__Habit", "level": 1}, {"title": "Exercises", "anchor": "Exercises", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "46 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fg9fXrHpeaDD6pEPL", "rKNuyxFK85yKtwP6G", "NgtYDP3ZtLJaM248W", "JcpzFpPBSmzuksmWM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-27T19:08:22.194Z", "modifiedAt": null, "url": null, "title": "Probability of Cryonic Success?", "slug": "probability-of-cryonic-success", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:58.941Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZZRxFmpph4B4H3RBT/probability-of-cryonic-success", "pageUrlRelative": "/posts/ZZRxFmpph4B4H3RBT/probability-of-cryonic-success", "linkUrl": "https://www.lesswrong.com/posts/ZZRxFmpph4B4H3RBT/probability-of-cryonic-success", "postedAtFormatted": "Thursday, September 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Probability%20of%20Cryonic%20Success%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProbability%20of%20Cryonic%20Success%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZZRxFmpph4B4H3RBT%2Fprobability-of-cryonic-success%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Probability%20of%20Cryonic%20Success%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZZRxFmpph4B4H3RBT%2Fprobability-of-cryonic-success", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZZRxFmpph4B4H3RBT%2Fprobability-of-cryonic-success", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 139, "htmlBody": "<p>While browsing some of the websites on cryonics, I've come across <a href=\"https://public.sheet.zoho.com/public/brooknorton/quick-look-calculator\">this page</a>, a spreadsheet which performs a quick analysis of the odds of a successful cryonic revival. It allows a user to enter their estimates of various events happening in a given period, such as the failure of the cryonics facility due to various causes, when revival technology will be developed, and so forth. (There's also a more advanced calculator <a href=\"https://public.sheet.zoho.com/public/brooknorton/advanced-calculator\">here</a>, which I'm not going to worry about at the moment.)</p>\n<p>What are your best estimates of the relevant factors?</p>\n<p>&nbsp;</p>\n<p>(Or, in case it might save a step: my own current age is 35, and my current estimate of my mean time of death across all my futures is when I'm 78. Given that, what is your best estimate of the probability that I'll be successfully revived from cryonic suspension?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZZRxFmpph4B4H3RBT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 3, "extendedScore": null, "score": 9.964022750455026e-07, "legacy": true, "legacyId": "19042", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-27T19:48:04.828Z", "modifiedAt": null, "url": null, "title": "EY \"Politics is the Mind Killer\" sighting at Washington Examiner and Reason.com", "slug": "ey-politics-is-the-mind-killer-sighting-at-washington", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:59.002Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "buybuydandavis", "createdAt": "2011-09-04T00:02:35.971Z", "isAdmin": false, "displayName": "buybuydandavis"}, "userId": "sQqaueY24d7xa3PXD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hzZaS4CZAHz2piD9A/ey-politics-is-the-mind-killer-sighting-at-washington", "pageUrlRelative": "/posts/hzZaS4CZAHz2piD9A/ey-politics-is-the-mind-killer-sighting-at-washington", "linkUrl": "https://www.lesswrong.com/posts/hzZaS4CZAHz2piD9A/ey-politics-is-the-mind-killer-sighting-at-washington", "postedAtFormatted": "Thursday, September 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20EY%20%22Politics%20is%20the%20Mind%20Killer%22%20sighting%20at%20Washington%20Examiner%20and%20Reason.com&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEY%20%22Politics%20is%20the%20Mind%20Killer%22%20sighting%20at%20Washington%20Examiner%20and%20Reason.com%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhzZaS4CZAHz2piD9A%2Fey-politics-is-the-mind-killer-sighting-at-washington%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=EY%20%22Politics%20is%20the%20Mind%20Killer%22%20sighting%20at%20Washington%20Examiner%20and%20Reason.com%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhzZaS4CZAHz2piD9A%2Fey-politics-is-the-mind-killer-sighting-at-washington", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhzZaS4CZAHz2piD9A%2Fey-politics-is-the-mind-killer-sighting-at-washington", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<p>Original at Washington Examiner</p>\n<p><a href=\"http://washingtonexaminer.com/down-with-politics/article/2508882#.UGSscI0iYZm\">http://washingtonexaminer.com/down-with-politics/article/2508882#.UGSscI0iYZm</a></p>\n<blockquote>\n<p>...</p>\n<p>Politics makes us worse because \"politics is the mindkiller,\" as intelligence theorist Eliezer Yudkowsky puts it. \"Evolutionary psychology produces strange echoes in time,\" he writes, \"as adaptations continue to execute long after they cease to maximize fitness.\" We gorge ourselves sick on sugar and fat, and we indulge our tribal hard-wiring by picking a political \"team\" and denouncing the \"enemy.\"</p>\n<p>But our atavistic Red/Blue tribalism plays to the interests of \"individual politicians in getting you to identify with them instead of judging them,\" Yudkowsky writes.</p>\n<p>...</p>\n<p><em>Examiner Columnist Gene Healy is a vice president at the Cato Institute and the author of \"The Cult of the Presidency.\"</em></p>\n</blockquote>\n<p>Repost at Reason.com</p>\n<p><a href=\"http://reason.com/archives/2012/09/25/why-politics-are-bad-for-us\">http://reason.com/archive/2012/09/25/why-politics-are-bad-for-us</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hzZaS4CZAHz2piD9A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 25, "extendedScore": null, "score": 7.3e-05, "legacy": true, "legacyId": "19043", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-28T01:27:19.040Z", "modifiedAt": null, "url": null, "title": "Confabulation Bias", "slug": "confabulation-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:57.628Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EricHerboso", "createdAt": "2012-03-23T19:25:39.720Z", "isAdmin": false, "displayName": "EricHerboso"}, "userId": "Ax5XirLrGLSxHgGnN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oiujyJAFx6evycHqQ/confabulation-bias", "pageUrlRelative": "/posts/oiujyJAFx6evycHqQ/confabulation-bias", "linkUrl": "https://www.lesswrong.com/posts/oiujyJAFx6evycHqQ/confabulation-bias", "postedAtFormatted": "Friday, September 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Confabulation%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConfabulation%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoiujyJAFx6evycHqQ%2Fconfabulation-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Confabulation%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoiujyJAFx6evycHqQ%2Fconfabulation-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoiujyJAFx6evycHqQ%2Fconfabulation-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 530, "htmlBody": "<p><em>(Edit: <a href=\"/user/gwern/\">Gwern</a> points out in the comments that there is previous discussion on this study at <a href=\"/lw/elg/new_study_on_choice_blindness_in_moral_positions/\">New study on choice blindness in moral positions</a>.)</em></p>\n<p>Earlier this month,&nbsp;a group of Swedish scientists&nbsp;<a href=\"http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0045457\">published a study</a>&nbsp;that describes a new type of bias that I haven't seen listed in any of the sequences or on the <a href=\"http://wiki.lesswrong.com/wiki/Bias\">wiki</a>. Their methodology:</p>\n<blockquote>\n<p>We created a self-transforming paper survey of moral opinions, covering both foundational principles, and current dilemmas hotly debated in the media. This survey used a magic trick to expose participants to a reversal of their previously stated attitudes, allowing us to record whether they were prepared to endorse and argue for the opposite view of what they had stated only moments ago.</p>\n</blockquote>\n<p>In other words, people were surveyed on their beliefs and were immediately asked to defend them after finishing the survey. Despite having <em>just</em> written down how they felt, 69% did not even notice that at least one of their answers were surreptitiously changed.&nbsp;Amazingly, a majority of people actually \"argued unequivocally for the opposite of their original attitude\".</p>\n<p>Perhaps this type of effect is already discussed here on LessWrong, but, if so, I have not yet run across any such discussion. (It is not on the <a href=\"http://wiki.lesswrong.com/wiki/Bias\">LessWrong wiki</a>&nbsp;nor the <a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\">other wiki</a>, for example.) This appears to be some kind of confabulation bias, where invented positions thrust upon people result in confabulated reasons for believing them.</p>\n<p>Some people might object to my calling this a bias. (After all, the experimenters themselves did not use that word.) But I'm trying to refer less to the trick involved in the experiment and more toward the bias this experiment shows that we have toward our own views. This is a fine distinction to make, but I feel it is important for us to recognize.</p>\n<p>When I say we prefer our own opinions, this is obvious on its face.&nbsp;Of course we think our own positions are correct; they're the result of our previously reasoned thought. We have <em>reason</em> to believe they are correct. But this study shows that our preference for our own views goes even further than this. We actually are biased toward our own positions to such a degree that we will actually verbally defend them <em>even when we were tricked into thinking we held those positions</em>. This is what I mean when I call it confabulation bias.</p>\n<p>Of particular interest to the LessWrong community is the fact that this bias apparently is more susceptible to those of us that are more capable of good argumentation. This puts confabulation bias in the same category as&nbsp;the&nbsp;<a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">sophistication&nbsp;effect</a>&nbsp;in that well informed people should take special care to not fall for it. (The idea that confabulation bias is more likely to occur with those of us that argue better is not shown in this study, but it seems like a reasonable hypothesis to make.)</p>\n<p>As a final minor point, I just want to point out that the effect did not disappear when the changed opinion was extreme. The options available to participants involved agreeing or disagreeing on a 1-9 scale; a full 31% of respondents who chose an extreme position (like 1 or 9) did not even notice when they were shown to have said the opposite extreme.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oiujyJAFx6evycHqQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -1, "extendedScore": null, "score": 9.966001055652524e-07, "legacy": true, "legacyId": "19044", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ekkB4qRZ5yFnMwZGr", "AdYdLP2sRqPMoe8fb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-28T06:06:01.347Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Ethical Inhibitions", "slug": "seq-rerun-ethical-inhibitions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:58.389Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wsCNNZ7YoBMWpjHkF/seq-rerun-ethical-inhibitions", "pageUrlRelative": "/posts/wsCNNZ7YoBMWpjHkF/seq-rerun-ethical-inhibitions", "linkUrl": "https://www.lesswrong.com/posts/wsCNNZ7YoBMWpjHkF/seq-rerun-ethical-inhibitions", "postedAtFormatted": "Friday, September 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Ethical%20Inhibitions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Ethical%20Inhibitions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwsCNNZ7YoBMWpjHkF%2Fseq-rerun-ethical-inhibitions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Ethical%20Inhibitions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwsCNNZ7YoBMWpjHkF%2Fseq-rerun-ethical-inhibitions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwsCNNZ7YoBMWpjHkF%2Fseq-rerun-ethical-inhibitions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 169, "htmlBody": "<p>Today's post, <a href=\"/lw/v0/ethical_inhibitions/\">Ethical Inhibitions</a> was originally published on 19 October 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Ethical_Inhibitions\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Humans may have a sense of ethical inhibition because various ancestors, who didn't follow ethical norms when they thought they could get away with it, nevertheless got caught.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/eou/seq_rerun_protected_from_myself/\">Protected From Myself</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wsCNNZ7YoBMWpjHkF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "19055", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cyRpNbPsW8HzsxhRK", "h4J37gH5xr5w7tuKo", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-28T15:03:41.993Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Berlin, Portland, Urbana-Champaign IL, Vienna", "slug": "weekly-lw-meetups-austin-berlin-portland-urbana-champaign-il", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:54.167Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8Lz9awP2W8RyoSBFZ/weekly-lw-meetups-austin-berlin-portland-urbana-champaign-il", "pageUrlRelative": "/posts/8Lz9awP2W8RyoSBFZ/weekly-lw-meetups-austin-berlin-portland-urbana-champaign-il", "linkUrl": "https://www.lesswrong.com/posts/8Lz9awP2W8RyoSBFZ/weekly-lw-meetups-austin-berlin-portland-urbana-champaign-il", "postedAtFormatted": "Friday, September 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Portland%2C%20Urbana-Champaign%20IL%2C%20Vienna&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Portland%2C%20Urbana-Champaign%20IL%2C%20Vienna%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Lz9awP2W8RyoSBFZ%2Fweekly-lw-meetups-austin-berlin-portland-urbana-champaign-il%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Portland%2C%20Urbana-Champaign%20IL%2C%20Vienna%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Lz9awP2W8RyoSBFZ%2Fweekly-lw-meetups-austin-berlin-portland-urbana-champaign-il", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Lz9awP2W8RyoSBFZ%2Fweekly-lw-meetups-austin-berlin-portland-urbana-champaign-il", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 428, "htmlBody": "<p><strong>This summary was posted to LW Main on September 21st, and has now been moved to discussion. The more recent summary is <a href=\"/lw/epd/weekly_lw_meetups_austin_berlin_melbourne_moscow/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/dx\">Urbana-Champaign, IL:&nbsp;<span class=\"date\">22 September 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/dv\">Berlin Meetup:&nbsp;<span class=\"date\">28 September 2012 07:30PM</span></a></li>\n<li><a href=\"/meetups/e0\">Vienna meetup:&nbsp;<span class=\"date\">28 September 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/dm\">Moscow: applied rationality and web resources:&nbsp;<span class=\"date\">29 September 2012 04:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/do\">Portland Oregon:&nbsp;<span class=\"date\">22 September 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">22 September 2018 01:30PM</span></a></li>\n<li><a href=\"/meetups/df\">Cambridge (MA) first-Sundays meetup:&nbsp;<span class=\"date\">07 October 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/dg\">Cambridge (MA) third-Sundays Meetup:&nbsp;<span class=\"date\">21 October 2012 02:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong>&nbsp;</strong><strong></strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8Lz9awP2W8RyoSBFZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.970265362788888e-07, "legacy": true, "legacyId": "18937", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dTMhmeyyLwrrpQdTj", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-28T17:43:27.803Z", "modifiedAt": null, "url": null, "title": "Yale Creates First Self-Aware Robot?", "slug": "yale-creates-first-self-aware-robot", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:01.732Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JQuinton", "createdAt": "2011-04-29T16:29:16.319Z", "isAdmin": false, "displayName": "JQuinton"}, "userId": "edHNo35soNo2w6ZwN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4AexFBGid7TXuiPib/yale-creates-first-self-aware-robot", "pageUrlRelative": "/posts/4AexFBGid7TXuiPib/yale-creates-first-self-aware-robot", "linkUrl": "https://www.lesswrong.com/posts/4AexFBGid7TXuiPib/yale-creates-first-self-aware-robot", "postedAtFormatted": "Friday, September 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Yale%20Creates%20First%20Self-Aware%20Robot%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYale%20Creates%20First%20Self-Aware%20Robot%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4AexFBGid7TXuiPib%2Fyale-creates-first-self-aware-robot%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Yale%20Creates%20First%20Self-Aware%20Robot%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4AexFBGid7TXuiPib%2Fyale-creates-first-self-aware-robot", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4AexFBGid7TXuiPib%2Fyale-creates-first-self-aware-robot", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 222, "htmlBody": "<p>Apparently a PhD candidate at the Social Robotics Lab at Yale <a href=\"http://www.yaledailynews.com/news/2012/sep/25/first-self-aware-robot-created/\">created a self-aware robot</a>:&nbsp;</p>\r\n<blockquote>\r\n<p>In the mirror test, developed by Gordon Gallup in 1970, a mirror is placed in an animal&rsquo;s enclosure, allowing the animal to acclimatize to it. At first, the animal will behave socially with the mirror, assuming its reflection to be another animal, but eventually most animals recognize the image to be their own reflections. After this, researchers remove the mirror, sedate the animal and place an ink dot on its frontal region, and then replace the mirror. If the animal inspects the ink dot on itself, it is said to have self-awareness, because it recognized the change in its physical appearance.</p>\r\n<p>[...]</p>\r\n<p>To adapt the traditional mirror test to a robot subject, computer science Ph.D. candidate Justin Hart said he would run a program that would have Nico, a robot that looks less like R2D2 and more like a jumble of wires with eyes and a smile, learn a three-dimensional model of its body and coloring. He would then change an aspect of the robot&rsquo;s physical appearance and have Nico, by looking at a reflective surface, &ldquo;identify where [his body] is different.&rdquo;</p>\r\n</blockquote>\r\n<p>&nbsp;What do Less Wrongians think? Is this \"cheating\" traditional concepts of self-awareness, or is self-awareness \"self-awareness\" regardless of the path taken to get there?</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fuZZ64fNz24BLrXnY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4AexFBGid7TXuiPib", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 3, "extendedScore": null, "score": 9.971100252408752e-07, "legacy": true, "legacyId": "19058", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-28T19:11:19.414Z", "modifiedAt": null, "url": null, "title": "[LINK] Law Goes Meta", "slug": "link-law-goes-meta", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:58.129Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TimS", "createdAt": "2011-10-11T12:16:35.235Z", "isAdmin": false, "displayName": "TimS"}, "userId": "pewD8vNSS3LGCvE4t", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s22LaKiR5KjA2wrNN/link-law-goes-meta", "pageUrlRelative": "/posts/s22LaKiR5KjA2wrNN/link-law-goes-meta", "linkUrl": "https://www.lesswrong.com/posts/s22LaKiR5KjA2wrNN/link-law-goes-meta", "postedAtFormatted": "Friday, September 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Law%20Goes%20Meta&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Law%20Goes%20Meta%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs22LaKiR5KjA2wrNN%2Flink-law-goes-meta%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Law%20Goes%20Meta%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs22LaKiR5KjA2wrNN%2Flink-law-goes-meta", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs22LaKiR5KjA2wrNN%2Flink-law-goes-meta", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 145, "htmlBody": "<p>Some legal background:</p>\n<ul>\n<li>In the United States, there are several courts of appeals, called Circuit Courts. They can disagree about legal points - this is called a circuit split. One of the purposes of the Supreme Court is to resolve circuit splits.</li>\n<li>Sometimes, laws are ruled to be ambiguous. If so, the relevant agency regulations interpreting the law are determinative, unless the regulations are an obviously stupid interpretation. This is called <a href=\"http://en.wikipedia.org/wiki/Chevron_U.S.A.,_Inc._v._Natural_Resources_Defense_Council,_Inc.\">Chevron deference</a>.</li>\n</ul>\n<p>One would think that disagreement between Circuits about the meaning of a law would be legally relevant evidence about whether the law was ambiguous. Instead, there appears to be a circuit split on the meaning of circuit splits.</p>\n<p>More available <a href=\"http://www.volokh.com/2012/09/27/anything-you-can-do-i-can-do-meta-2/\">here</a>, for the amusement of those on this site who like to think meta. Also a bit of a lesson on the limits of meta-style analysis in solving actual problems.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s22LaKiR5KjA2wrNN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "19059", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-28T22:17:05.782Z", "modifiedAt": null, "url": null, "title": "How can a high school student learn physics and math while coping with high school?", "slug": "how-can-a-high-school-student-learn-physics-and-math-while", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:58.278Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "nLPjP3PWrHDRy89ys", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4sbqtavbWJdjjJnps/how-can-a-high-school-student-learn-physics-and-math-while", "pageUrlRelative": "/posts/4sbqtavbWJdjjJnps/how-can-a-high-school-student-learn-physics-and-math-while", "linkUrl": "https://www.lesswrong.com/posts/4sbqtavbWJdjjJnps/how-can-a-high-school-student-learn-physics-and-math-while", "postedAtFormatted": "Friday, September 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20can%20a%20high%20school%20student%20learn%20physics%20and%20math%20while%20coping%20with%20high%20school%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20can%20a%20high%20school%20student%20learn%20physics%20and%20math%20while%20coping%20with%20high%20school%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4sbqtavbWJdjjJnps%2Fhow-can-a-high-school-student-learn-physics-and-math-while%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20can%20a%20high%20school%20student%20learn%20physics%20and%20math%20while%20coping%20with%20high%20school%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4sbqtavbWJdjjJnps%2Fhow-can-a-high-school-student-learn-physics-and-math-while", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4sbqtavbWJdjjJnps%2Fhow-can-a-high-school-student-learn-physics-and-math-while", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": null, "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4sbqtavbWJdjjJnps", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 3, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "19060", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": null, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-29T01:07:29.100Z", "modifiedAt": null, "url": null, "title": "Meetup : Zendo in the West Loop", "slug": "meetup-zendo-in-the-west-loop", "viewCount": null, "lastCommentedAt": "2012-09-29T01:07:29.100Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nic_Smith", "createdAt": "2009-10-23T03:32:46.312Z", "isAdmin": false, "displayName": "Nic_Smith"}, "userId": "XP9GcTgRGLBCnf9ih", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BJ4ezRGC8xBfK6BbG/meetup-zendo-in-the-west-loop", "pageUrlRelative": "/posts/BJ4ezRGC8xBfK6BbG/meetup-zendo-in-the-west-loop", "linkUrl": "https://www.lesswrong.com/posts/BJ4ezRGC8xBfK6BbG/meetup-zendo-in-the-west-loop", "postedAtFormatted": "Saturday, September 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Zendo%20in%20the%20West%20Loop&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Zendo%20in%20the%20West%20Loop%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBJ4ezRGC8xBfK6BbG%2Fmeetup-zendo-in-the-west-loop%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Zendo%20in%20the%20West%20Loop%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBJ4ezRGC8xBfK6BbG%2Fmeetup-zendo-in-the-west-loop", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBJ4ezRGC8xBfK6BbG%2Fmeetup-zendo-in-the-west-loop", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ed'>Zendo in the West Loop</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 October 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">545 W. Madison Street, Chicago</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's have a game of Zendo near the train stations on Friday; we'll be meeting at the Potbelly Sandwich Shop at Presidential Towers, directly  across the intersection from Ogilvie. Be sure to check out the Meetup page for this event and let us know if you'll be there: <a href=\"http://www.meetup.com/Less-Wrong-Chicago/events/83939402/\" rel=\"nofollow\">http://www.meetup.com/Less-Wrong-Chicago/events/83939402/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ed'>Zendo in the West Loop</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BJ4ezRGC8xBfK6BbG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.973421263289263e-07, "legacy": true, "legacyId": "19061", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Zendo_in_the_West_Loop\">Discussion article for the meetup : <a href=\"/meetups/ed\">Zendo in the West Loop</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 October 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">545 W. Madison Street, Chicago</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's have a game of Zendo near the train stations on Friday; we'll be meeting at the Potbelly Sandwich Shop at Presidential Towers, directly  across the intersection from Ogilvie. Be sure to check out the Meetup page for this event and let us know if you'll be there: <a href=\"http://www.meetup.com/Less-Wrong-Chicago/events/83939402/\" rel=\"nofollow\">http://www.meetup.com/Less-Wrong-Chicago/events/83939402/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Zendo_in_the_West_Loop1\">Discussion article for the meetup : <a href=\"/meetups/ed\">Zendo in the West Loop</a></h2>", "sections": [{"title": "Discussion article for the meetup : Zendo in the West Loop", "anchor": "Discussion_article_for_the_meetup___Zendo_in_the_West_Loop", "level": 1}, {"title": "Discussion article for the meetup : Zendo in the West Loop", "anchor": "Discussion_article_for_the_meetup___Zendo_in_the_West_Loop1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 0, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-29T04:06:24.018Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Ethical Injunctions", "slug": "seq-rerun-ethical-injunctions", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nuYzKYDF633d7x22A/seq-rerun-ethical-injunctions", "pageUrlRelative": "/posts/nuYzKYDF633d7x22A/seq-rerun-ethical-injunctions", "linkUrl": "https://www.lesswrong.com/posts/nuYzKYDF633d7x22A/seq-rerun-ethical-injunctions", "postedAtFormatted": "Saturday, September 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Ethical%20Injunctions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Ethical%20Injunctions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnuYzKYDF633d7x22A%2Fseq-rerun-ethical-injunctions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Ethical%20Injunctions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnuYzKYDF633d7x22A%2Fseq-rerun-ethical-injunctions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnuYzKYDF633d7x22A%2Fseq-rerun-ethical-injunctions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 167, "htmlBody": "<p>Today's post, <a href=\"/lw/v1/ethical_injunctions/\">Ethical Injunctions</a> was originally published on 20 October 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A look at the peculiar properties of decision theories that include rules about not doing certain things, even when they seem like the right thing to do.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/epb/seq_rerun_ethical_inhibitions/\">Ethical Inhibitions</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nuYzKYDF633d7x22A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "19062", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dWTEtgBfFaz6vjwQf", "wsCNNZ7YoBMWpjHkF", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-29T10:21:45.153Z", "modifiedAt": null, "url": null, "title": "Cryonic Revival Mutual Assistance Pact?", "slug": "cryonic-revival-mutual-assistance-pact", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:00.606Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YJgJRy39J2x3eeyhB/cryonic-revival-mutual-assistance-pact", "pageUrlRelative": "/posts/YJgJRy39J2x3eeyhB/cryonic-revival-mutual-assistance-pact", "linkUrl": "https://www.lesswrong.com/posts/YJgJRy39J2x3eeyhB/cryonic-revival-mutual-assistance-pact", "postedAtFormatted": "Saturday, September 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryonic%20Revival%20Mutual%20Assistance%20Pact%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryonic%20Revival%20Mutual%20Assistance%20Pact%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYJgJRy39J2x3eeyhB%2Fcryonic-revival-mutual-assistance-pact%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryonic%20Revival%20Mutual%20Assistance%20Pact%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYJgJRy39J2x3eeyhB%2Fcryonic-revival-mutual-assistance-pact", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYJgJRy39J2x3eeyhB%2Fcryonic-revival-mutual-assistance-pact", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 347, "htmlBody": "<p>The odds of a successful cryonic revival may me one in several thousand, or five percent, or ninety percent; the error bars on the various sub-parts of the question are very broad.</p>\n<p>But <em>if</em> those assumptions work out, and <em>if</em> at least some people placed in suspension in the near future will be successfully revived in the far future...</p>\n<p>... then are there any useful arrangements which can be made now, which have little-to-no present cost (beyond the cryonic arrangements themselves)?</p>\n<p>&nbsp;</p>\n<p>For example, if someone were to make an announcement along the lines of, \"If anyone makes a promise to try to assist in my cryonic revival, and to assist me in getting myself established thereafter; then I promise to try to assist those people with their cryonic revivals, and assisting them, ahead of anyone who hasn't made such a promise.\", then what downsides would there be to having made it? Would making it create any perverse incentives, which could be avoided? Do the potential benefits, especially the benefit of a potential increase in the odds of being revived, outweigh the potential costs?</p>\n<p>Would it be better to make promises to specific people while one is alive, instead of making an open-ended promise? That is, I might try to convince EYudkowsky to make a mutual-assistance agreement with me personally, in hopes that one of us will one day be able to help the other; or I might make the agreement so broad that people can make their promise to help me even after I'm dead.</p>\n<p>How large would the benefits be of unilaterally promising to help someone else, without even asking for a reciprocal promise? Or, put another way, how big would the costs be if I were to simply announce that, if it's ever in my power, I'll try to assist in EYudkowsky's revival?</p>\n<p>&nbsp;</p>\n<p>Does anyone care to try figuring out the Prisoner's-Dilemma-like aspects of this, such as the probability that someone in such a pact would renege on their end of it, and how the terms could be adjusted to minimize the benefits and maximize the costs of such anti-social behavior?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YJgJRy39J2x3eeyhB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 9, "extendedScore": null, "score": 9.976319897181112e-07, "legacy": true, "legacyId": "19072", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-29T15:54:16.358Z", "modifiedAt": null, "url": null, "title": "MIT Challenge complete", "slug": "mit-challenge-complete", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:00.983Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "komponisto", "createdAt": "2009-03-01T21:10:23.585Z", "isAdmin": false, "displayName": "komponisto"}, "userId": "h48TMtPzfimsEobTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ihZkuKz4wBiy2rzkx/mit-challenge-complete", "pageUrlRelative": "/posts/ihZkuKz4wBiy2rzkx/mit-challenge-complete", "linkUrl": "https://www.lesswrong.com/posts/ihZkuKz4wBiy2rzkx/mit-challenge-complete", "postedAtFormatted": "Saturday, September 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MIT%20Challenge%20complete&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMIT%20Challenge%20complete%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FihZkuKz4wBiy2rzkx%2Fmit-challenge-complete%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MIT%20Challenge%20complete%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FihZkuKz4wBiy2rzkx%2Fmit-challenge-complete", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FihZkuKz4wBiy2rzkx%2Fmit-challenge-complete", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 232, "htmlBody": "<p><strong>Followup to:</strong> <a href=\"/lw/7u6/mit_challenge_blogger_to_attempt_cs_curriculum_on/\">MIT Challenge: blogger to attempt CS curriculum on own</a></p>\n<p>A year after he announced it, productivity blogger <a href=\"http://www.scotthyoung.com/blog/\">Scott Young</a> has <a href=\"http://www.scotthyoung.com/blog/2012/09/27/mit-challenge-done/\">successfully completed</a> his \"MIT Challenge\", an attempt to work through that institution's undergraduate computer science curriculum independently, using the <a href=\"http://ocw.mit.edu/index.htm\">large selection of online material</a> that it has made available.</p>\n<p>His three pieces of advice for independent learners are worth noting:</p>\n<p>&nbsp;</p>\n<blockquote><ol>\n<li>Create an exciting, but specific, mission. I couldn&rsquo;t have learned the content of this challenge if I hadn&rsquo;t wrapped it into a compelling mission. Even calling it the &ldquo;MIT Challenge&rdquo; helped me make the goal more specific and real. Too many self-education quests begin as vague ideas and fall apart without any constraints.</li>\n<li>Build a curriculum or find one. For small projects, taking an individual course will do. For bigger ones, try creating an actual curriculum. MIT (and other universities) offer many free courses, and also have outlines of their undergraduate and graduate programs. Having a preexisting curriculum forced me to be consistent and not avoid topics just because they were hard.</li>\n<li>Be public in your quest. Self-ed has a harder time obtaining legitimacy, in part, because nobody holds you accountable to that. Being public about my challenge made me accountable and gave me discipline I wouldn&rsquo;t have had in a private quest. Consider starting a blog about your mission, even if you do it anonymously.</li>\n</ol></blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ihZkuKz4wBiy2rzkx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 29, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "19074", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iNAAtrpjL4A65Ym2D"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-29T23:14:04.329Z", "modifiedAt": null, "url": null, "title": "Nozick's proposed rules for extrapolating your preferences", "slug": "nozick-s-proposed-rules-for-extrapolating-your-preferences", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:00.874Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wML432troCaMquidk/nozick-s-proposed-rules-for-extrapolating-your-preferences", "pageUrlRelative": "/posts/wML432troCaMquidk/nozick-s-proposed-rules-for-extrapolating-your-preferences", "linkUrl": "https://www.lesswrong.com/posts/wML432troCaMquidk/nozick-s-proposed-rules-for-extrapolating-your-preferences", "postedAtFormatted": "Saturday, September 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nozick's%20proposed%20rules%20for%20extrapolating%20your%20preferences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANozick's%20proposed%20rules%20for%20extrapolating%20your%20preferences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwML432troCaMquidk%2Fnozick-s-proposed-rules-for-extrapolating-your-preferences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nozick's%20proposed%20rules%20for%20extrapolating%20your%20preferences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwML432troCaMquidk%2Fnozick-s-proposed-rules-for-extrapolating-your-preferences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwML432troCaMquidk%2Fnozick-s-proposed-rules-for-extrapolating-your-preferences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4376, "htmlBody": "<p>You have desires. You also have desires <em>about your desires</em>: perhaps you desire cake but you also desire that you <em>didn't</em>&nbsp;desire cake. You also have desires about <em>the processes which produce your desires</em>: perhaps you desire X and Y but only because of a weird evolutionary turn and you wish the processes which created your desires weren't so far beyond your own control.</p>\n<p>But what should you do, when these different kinds of desires are in conflict with each other? If you could reflect upon and then rewrite your own desires, how should you choose to resolve those conflicts?</p>\n<p><a href=\"http://www.amazon.com/Nature-Rationality-Princeton-Paperbacks-ebook/dp/B002SQKR8E/\">Nozick (1993)</a> proposes 23 constraints on rational preferences, which one could also interpret as 23 constraints on the process of resolving conflicts among one's preferences. I reproduce this passage below, for those who are interested:</p>\n<blockquote>\n<p>Let me emphasize that my purpose&nbsp;is not to endorse the particular conditions I shall put forward or to&nbsp;defend their particular details. Rather, I hope to show what promising&nbsp;room there is for conditions of the sort that I discuss...</p>\n<p>Some of these conditions are justified&nbsp;by instrumental considerations, such as the &ldquo;money pump&rdquo; argument&nbsp;that preferences be transitive, while others are presented as normatively&nbsp;appealing on their face. (Unless these latter can be given an instrumental&nbsp;justification also, isn&rsquo;t this already a step beyond instrumental&nbsp;rationality?) Contemporary decision theory takes this one step&nbsp;beyond Hume: group of them together can be. Let us&nbsp;suppose that there are normative principles specifying the structure of&nbsp;several preferences together and that these principles are conditions of&nbsp;rationality. (The literature contains putative counterexamples and objections&nbsp;to some of the Von Neumann&ndash;Morgenstern conditions; the&nbsp;point here is not to use those particular ones but some such appropriate&nbsp;set of conditions.)</p>\n<p style=\"padding-left: 30px;\"><strong>I.</strong> The person satisfies the Von Neumann&ndash;Morgenstern or some other&nbsp;specified appropriate set of conditions upon preferences and their relations&nbsp;to probabilities.</p>\n<p>This suggests at least one further condition that a person&rsquo;s preferences&nbsp;must satisfy in order to be rational, namely, that she must prefer satisfying&nbsp;the normative conditions to not satisfying them. Indeed, for any&nbsp;valid structural condition C of rationality, whether rationality of preference,&nbsp;of action, or of belief:</p>\n<p style=\"padding-left: 30px;\"><strong>II.</strong> The person prefers satisfying the rationality condition C to not satisfying&nbsp;C.</p>\n<p>(This condition should be stated as a prima facie one or with a ceteris&nbsp;paribus clause, as should many of the ones below. The person who&nbsp;knows that he will be killed if he always satisfies the condition that&nbsp;indifference be transitive, or the condition that he not believe any&nbsp;statement whose credibility is less than that of an incompatible statement,&nbsp;may well prefer not to.) Since the person is, let us assume, instrumentally&nbsp;rational,</p>\n<p style=\"padding-left: 30px;\"><strong>III.</strong> The person will, all other things being equal, desire the means and&nbsp;preconditions to satisfying rationality conditions C.</p>\n<p>These rationality conditions C not only concern the structure of preferences&nbsp;but also include whatever the appropriate structural conditions&nbsp;are on the rationality of belief. Hence the person will desire the means&nbsp;and preconditions of rational belief, she will desire the means and preconditions&nbsp;for the effective assignment of credibility values (and for&nbsp;deciding about the utility of holding a particular belief).</p>\n<p>A person lacks rational integration when he prefers some alternative&nbsp;x to another alternative y, yet prefers that he did not have this preference,&nbsp;that is, when he also prefers not preferring x to y to preferring x&nbsp;to y. When such a second-order preference conflicts with a first-order&nbsp;one, it is an open question which of these preferences should be&nbsp;changed. What is clear is that they do not hang together well, and a&nbsp;rational person would prefer that this not (continue to) be the case. We&nbsp;thus have a requirement that a person have a particular third-order&nbsp;preference, namely, preferring that the conflict of preferences not obtain.&nbsp;Let S stand for this conflict situation, where the person prefers x&nbsp;to y yet prefers not having this preference, that is, let S stand for: xPy&nbsp;&amp; [not-(xPy) P (xPy)]. Then</p>\n<p style=\"padding-left: 30px;\"><strong>IV.</strong> For every x and y, the person prefers not-S to S, all other things&nbsp;being equal.&nbsp;</p>\n<p>This does not mean the person must choose not-S over S no matter&nbsp;what. An addict who desires not to desire heroin may know that he&nbsp;cannot feasibly obliterate his first-order desire for heroin, and thus&nbsp;know that the only way to resolve the conflict of preferences is to drop&nbsp;his second-order desire not to have that first-order desire. Still, he may&nbsp;prefer to keep the conflict among desires because, with it, the addiction&nbsp;will be less completely pursued or his addictive desire less of a flaw.</p>\n<p>Hume claims that all preferences are equally rational. But an under- standing of what a preference is, and what preferences are for, might make further conditions appropriate. In recent theories, a preference has been understood as a disposition to choose one thing over an- other.8 The function of preferences, the reason evolution instilled the capacity for them within us, is to eventuate in preferential choice. But one can make preferential choices only in some situations: being alive, having the capacity to know of alternatives, having the capacity to make a choice, being able to effectuate an action toward a chosen alter- native, facing no interference with these capacities that makes it im- possible to exercise them. These are preconditions (means) for prefer- ential choice. Now, one does not have to prefer that these conditions continue; some people might have reason to prefer being dead. But they need a reason, I think; the mere preference for being dead, for no reason at all, is irrational. There is a presumption that the person will prefer that the necessary conditions for preferential choice, for making any preferential choice at all, be satisfied; she need not actually have the preference, but she needs a reason for not having it.</p>\n<p style=\"padding-left: 30px;\"><strong>V.</strong> The person prefers that each of the preconditions (means) for her making any preferential choices be satisfied, in the absence of any particular reason for not preferring this.</p>\n<p>So a person prefers being alive and not dying, having a capacity to know of alternatives and not having this capacity removed, having the capacity to effectuate a choice and not having this capacity destroyed, and so on. Again, we might add</p>\n<p style=\"padding-left: 30px;\"><strong>VI.</strong> The person prefers, all other things being equal, that the capacities that are the preconditions for preferential choice not be interfered with by a penalty (= a much unpreferred alternative) that makes him prefer never to exercise these capacities in other situations.</p>\n<p>There is something more to be said about reasons, I think. (I propose this very tentatively; more work is needed to get this matter right.)</p>\n<p>Suppose I simply prefer x to y for no reason at all. Then I will be willing, and should be willing, to reverse my preference to gain something else that I prefer having. I should be willing, were it in my power, to reverse my preference, to now start preferring y to x, in order to receive 25 cents. I then would move from a situation of preferring x to y to one of preferring y to x and having 25 additional cents. And won't I prefer the latter to the former? Perhaps not, perhaps I strongly prefer x to y, and do so for no reason at all. Having a strong preference for no reason at all is, I think, anomalous. Given that I have it, I will act upon it; but it is irrational to be wedded to it, paying the cost of pursuing it or keeping it when I have no reason to hold it. Or perhaps I prefer preferring x to y to not having this preference, and I prefer that strongly enough to outweigh 25 cents. So this second-order preference for preferring x to y might make me unwilling to give up that preference. But why do I have this second-order preference? I want to say that, unlike any arbitrary first-order preference, a second-order preference requires a reason behind it. A second-order preference for preferring x to y is irrational unless the person has some reason for preferring x to y. That is, he must have a reason for preferring to have that first-order preference-perhaps his mother told him to, or perhaps that preference now has become part of his identity and hence something he would not wish to change-or have a direct reason for preferring x to y, a reason concerning the attributes of x and y. But what is a direct reason? Must a reason in this context be anything more than another preference? It must at least be another preference that functions like a reason, that is, one that is general, though defeasible. To have a reason for preferring x to y is standardly thought to involve knowing some feature F of x such that, in general, all other things being equal, you prefer things with F to things without them, among things of the type that x is. (Preferring cold drinks to warm does not require preferring cold rooms to warm ones.)</p>\n<p style=\"padding-left: 30px;\"><strong>VII.</strong> If the person prefers x to y, either: (a) the person is willing to switch to preferring y to x for a small gain, or (b) the person has some reason to prefer x to y, or (c) the person has some reason to prefer preferring x to y to not doing that.</p>\n<p>I don't say that all of a person's preferences require reasons for them- it is unclear what to say about ones that are topmost; perhaps they are anchored by ones under them-but first-level ones do require reasons when the person is not willing to shift them. Once we are launched within the domain of reasons for preferences, we can consider how more general reasons relate to less general ones, we can impose consistency conditions among the reasons, and so forth. The way becomes open for further normative conditions upon preferences, at least for those preferences a person is not willing to switch at the drop of a hat. Especially in the case of preferences that go against the preconditions for preferential choice mentioned above, a person will need not just any reasons but reasons of a certain weight, where this means at least that the reasons must intertwine with many of the person's other preferences, perhaps at various levels.</p>\n<p>We also might want to add that the desires and preferences are in equilibrium, in that knowing the causes of your having them does not lead you to (want to) stop having them. The desires and preferences withstand a knowledge of their causes.</p>\n<p style=\"padding-left: 30px;\"><strong>VIII.</strong> The person's desires and preferences are in equilibrium (with his beliefs about their causes).</p>\n<p>Since preferences and desires are to be realized or satisfied, a person whose preferences were so structured that he always wanted to be in the other situation-preferring y to x when he has x and preferring x to y when he has y-would be doomed to dissatisfaction, to more dissatisfaction than is inherent in the human condition. The grass shouldn't always be greener in the other place. So</p>\n<p style=\"padding-left: 30px;\"><strong>IX.</strong> For no x and y does the person always prefer x to y when y is the case and y to x when x is the case. (His conditional preferences are not such that for some x and y he prefers x to y/given that y is the case, and prefers y to x/given that x is the case.)</p>\n<p>Desires are not simply preferences. A level of filtering or processing takes place in the step from preferences to desires-as (we shall see) another does in the step from desires to goals. We might say that rational desires are those it is possible to fulfill, or at least those you believe it is possible to fulfill, or at least those you don't believe it is impossible to fulfill. Let us be most cautious and say</p>\n<p style=\"padding-left: 30px;\"><strong>X.</strong> The person does not have desires that she knows are impossible to fulfill.</p>\n<p>Perhaps it is all right to prefer to fly unaided, but it is not rational for a person to desire this. (It might be rational, though, to wish it were possible.) Desires, unlike mere preferences, will feed into some decision process. They must pass some feasibility tests, and not simply in isolation: your desires must be jointly copossible to satisfy. And when it is discovered they are not, the desires must get changed, although a desire that is altered or dropped may remain as a preference.</p>\n<p>Goals, in turn, are different from preferences or desires. To have or accept goals is to use them to filter from consideration in choice situations those actions that don't serve these goals well enough or at all. For beings of limited capacity who cannot at each moment consider and evaluate every possible action available to them-try to list all of the actions available to you now-such a filtering device is crucial. Moreover, we can use goals to generate actions for serious consideration, actions that do serve these goals. And the goals provide salient dimensions of the outcomes, dimensions that will get weight in assessing the utility of these outcomes. Given these multiple and important functions of goals, one would expect that for an important goal that is stable over time we would devote one of our few channels of alertness to it, to noticing promising routes to its achievement, monitoring how we currently are doing, and so on.</p>\n<p>How do our goals arise? How are they selected? It seems plausible to think that they arise out of a matrix of preferences, desires, and beliefs about probabilities, possibilities, and feasibilities. (And then goals reorganize our desires and preferences, giving more prominence to some and reversing others because that reversed preference fits or advances the goal.) One possibility is that goals arise in an application of expected utility theory. For each goal Gi, treat pursuing goal Gi as an action with its own probability distribution over outcomes, and compute the expected utility of this \"action.\" Adopt that goal with the maximum expected utility, and then use it to generate options, exclude others, and so forth.</p>\n<p>There is an objection to this easy way of fitting goals within an expected utility framework. The effect of making something Gi a goal is a large one. Now Gi functions as an exclusionary device and has a status very different from another possible goal Gj that came very close but just missed having maximum expected utility. A marginal difference now makes a very great difference. It seems that large differences, such as one thing setting the framework whereby other things are excluded, should be based upon pre-existing differences that are significant. Consider the descriptive finding a dominant structure, and she uses mechanisms such as combining and altering attributes and collapsing alternatives in order to get one action weakly dominating all others on all (considered) attributes. Thereby, conflict is avoided, for one action clearly is best; there is no reason for doing another. Will such dominance always set up a gulf between actions that is significant enough to make a qualitative difference with large effects and so be applicable to the formation of goals? Yet one action can weakly dominate another when there are six dimensions, the two actions tying on five of these while the first action is (only) slightly better on the sixth. Even in this framework, we seem to need more than simply weak dominance; perhaps we need strong winning on one dimension or winning on many of them.</p>\n<p>Returning to the expected utility framework, we might say that goal Gi is to be chosen not simply when it has maximum expected utility but when it beats the other candidate goals decisively. For each j, EU(Gi) &minus; EU(Gj) is greater than or equal to some fixed positive specified quantity q. (There remains a similar but smaller problem, though. Gi beats the other goals decisively, yet there is no decisive difference between beating decisively and not doing so; the difference EU(Gi) &minus; EU(Gj) might barely reach, or just fail to reach, q.) To make something a goal is, in part, to adopt a desire to find a feasible route from where you are to the achievement of that goal. Therefore,</p>\n<p style=\"padding-left: 30px;\"><strong>XI.</strong> A person will not have a goal for which he knows that there is no feasible route, however long, from his current situation to the achievement of that goal.</p>\n<p>Moreover, we might say that a rational person will have some goals toward which she will search for feasible routes and not just have merely preferences and desires. She will filter out actions that cannot reach these goals, generate for consideration actions that might reach them, and so on. And some of these goals will have some stability, so that they can be pursued over time with some prospect of success.</p>\n<p style=\"padding-left: 30px;\"><strong>XII.</strong> A person will have some stable goals.</p>\n<p>A rational person will consider not only particular (external) outcomes but also what he himself is like, and he will have some preferences among the different ways he might be. Let Wp be the way the person believes he will be when p is the case; let Wq be the way he believes he will be when q is the case. (These include the ways that p, or q, will cause or shape or prompt him to be.) There is a presumption, which can be overriden by reasons, that preferences among ways of being will take precedence over lower-level preferences that are personal ones. (Personal preferences are ones derived solely from estimates of benefits to himself.)</p>\n<p style=\"padding-left: 30px;\"><strong>XIII.</strong> If the person prefers Wp to Wq, then (all things being equal) he does not hold the (personal) preference of q to p.</p>\n<p>Condition XIII holds that the way the person is, what kind of person he is, will have greater weight in his preferences than (what otherwise would be) his personal preferences. (Is this condition culture-bound and plausible only to people in certain kinds of cultures?)</p>\n<p>The dutch book argument that someone's probability beliefs should satisfy the axioms of probability theory says that if they do not, and if she is willing always to bet upon such probability beliefs, then someone can so arrange things so that she is sure to lose money and hence reach a less preferred alternative. This argument says that if her (probabilistic) beliefs are irrational, she can be guaranteed to end up worse off on her utility scale. We might try the dual of this argument, imposing as a condition:</p>\n<p style=\"padding-left: 30px;\"><strong>XIV.</strong> A person's desires are not such that acting upon them guarantees that she will end up with irrational beliefs or probabilities.</p>\n<p>Various things might come under the ban of this condition: desiring to believe something no matter what the evidence; desiring to spend time with a known liar without any safeguards; desiring to place oneself in a state-through alcohol, drugs, or whatever-that will have continuing effects on the rationality of one's beliefs. But this requirement is too strong as stated; perhaps acting upon the desire will bring her something she (legitimately) values more than avoiding some particular irrational beliefs or probabilities. Similarly, the dutch book requirement is too strong as usually stated, for perhaps some situation holds in the world so that having incoherent probabilities will bring a far greater benefit-someone will bestow a large prize upon you for those incoherent probabilities-than the loss to be incurred in the bets. The dutch book argument points out that loss can be guaranteed, but still it might be counterbalanced; so too the irrational beliefs or probabilities you have through violating condition XIV might be counterbalanced. To avoid this, the moral of the dutch book argument must not be put too strongly, and similarly for condition XIV.</p>\n<p>These fourteen conditions can take us some considerable distance past Hume toward substantive constraints upon preferences and desires. Empirical information about the actual preconditions of satisfying the conditions of rationality, and of making preferential choices- mandated by conditions III, V, and VI-might require quite specific substantive content to one's preferences and desires, the more so when combined with the constraints of the other conditions.</p>\n<p>Can we proceed further to specific content? One intriguing route is to attempt to parallel with desire what we want to say about the rationality of belief. For example, people have held that a belief is rational if it is formed by a reliable process whose operation yields a high percentage of true beliefs. To be sure, the details are more complicated, but we might hope to parallel these complications also. A rational desire, then, would be one formed by a process that reliably yields a high percentage of dominant desires. But how are we to fill in that blank? What, for desires, corresponds to truth in the case of beliefs? For now, I have no independent substantive criterion to propose.</p>\n<p>We can, however, use our previous conditions, and any additional similar ones, to specify the goal of that process: a desire or preference is rational only if it was formed by a process that reliably yields desires and preferences that satisfy the previous conditions on how preferences are to be structured, namely, conditions I&ndash;XIV. This says more than just that these fourteen conditions are to be satisfied, for any process (we can follow) that reliably yields the satisfaction of these conditions may also further constrain a person's desires and preferences.</p>\n<p style=\"padding-left: 30px;\"><strong>XV.</strong> A particular preference or desire is rational only if there is a process P for arriving at desires and preferences, and (a) that preference or desire was arrived at through that process P, and (b) that process P reliably yields desires and preferences that satisfy the above normative structural conditions I&ndash;XIV, and (c) there is no narrower process P&prime; such that the desire or preference was arrived at through P&prime;, and P&prime; tends to produce desires and preferences that fail to satisfy conditions I&ndash;XIV.</p>\n<p>If we say that preferences and desires are rationally coherent when they satisfy conditions I&ndash;XIV (and similar conditions), then condition XV says that a preference or desire is rational only if (it is rationally coherent and) it is arrived at by a process that yields rationally coherent preferences and desires.</p>\n<p>Not only can that process P reliably yield rationally coherent preferences and desires, it can aim at such preferences and desires, it can shape and guide preferences and desires into rational coherence. The process P can be a homeostatic mechanism, one of whose goal-states is that preferences and desires be rationally coherent. In that case, a function of preferences and desires is to be rationally coherent. (Similarly, if the belief-forming mechanism B aims at beliefs being approximately true, then one function of beliefs is to be approximately true.)</p>\n<p>We therefore might add the following condition.</p>\n<p style=\"padding-left: 30px;\"><strong>XVI.</strong> The process P that yields preferences and desires aims at their being rationally coherent; it is a homeostatic mechanism, one of whose goal-states is that preferences and desires be rationally coherent.</p>\n<p>And similarly,</p>\n<p style=\"padding-left: 30px;\"><strong>XVII.</strong> The cognitive mechanism B that yields beliefs aims at these beliefs satisfying particular cognitive goals, such as these beliefs being (approximately) true, having explanatory power, and so on. B is a homeostatic mechanism, one of whose goal-states is that the beliefs meet the cognitive goals.</p>\n<p>A function of preferences and desires is to be rationally coherent; a function of beliefs is to meet the cognitive goals. That follows from our earlier account of function, if these mechanisms P and B are indeed such homeostatic mechanisms. Suppose these homeostatic mechanisms do produce beliefs and desires with these functions. Is it their function to do so? That depends upon what other mechanisms and processes produce and maintain those desireand belief-forming mechanisms. If those preference and cognitive mechanisms P and B were themselves designed, produced, or altered and maintained by homeostatic devices whose goals included aiming P and B at being devices that produced rationally coherent preferences and approximately true beliefs, then we have a double functionality. It is a function of the preferences and beliefs to be rationally coherent and approximately true, and it also is a function of the mechanisms that produce such beliefs and preferences to produce things like that, with those functions.</p>\n<p style=\"padding-left: 30px;\"><strong>XVIII.</strong> There is a homeostatic mechanism M1 whose goal-state is that the preference mechanism P yield rationally coherent preferences, and P is produced or maintained by M1 (through M1's pursuit of this goalstate).</p>\n<p style=\"padding-left: 30px;\"><strong>XIX.</strong> There is a homeostatic mechanism M2, whose goal-state is that the belief mechanism B yield beliefs that fulfill cognitive goals, and B is produced or maintained by M2 (through M2's pursuit of this goal-state).</p>\n<p>It is plausible to think that our desireand belief-forming mechanisms have undergone evolutionary and social shaping that in some signifi-cant part aimed at their having these functions. There is more. Once people become self-conscious about their preferences and beliefs, they can guide them, monitor them for deviations from rational coherence and truth, and make appropriate corrections. Conscious awareness becomes a part of the processes P and B, and consciously aims them at the goals of rational coherence and truth.</p>\n<p style=\"padding-left: 30px;\"><strong>XX.</strong> One component of the homeostatic preferenceand desire-forming process P is the person's consciously aiming at rationally coherent preferences and desires.</p>\n<p style=\"padding-left: 30px;\"><strong>XXI.</strong> One component of the homeostatic belief-forming process B is the person's consciously aiming at beliefs that fulfill cognitive goals.</p>\n<p>This self-awareness and monitoring gives us a fuller rationality. (Some might suggest that only when these conditions are satisfied do we have any rationality at all.)</p>\n<p>Self-conscious awareness can monitor not just preferences and beliefs but also the processes by which these are formed, P and B themselves. It can alter and improve these processes; it can reshape them. Conscious awareness thus becomes a part of the mechanisms M1 and M2 and so comes to play a role in determining the functions of the preferenceand belief-forming mechanisms themselves.</p>\n<p style=\"padding-left: 30px;\"><strong>XXII.</strong> One component of the homeostatic mechanism M1 that maintains P is the person's consciously aiming at P's yielding rationally coherent preferences.</p>\n<p style=\"padding-left: 30px;\"><strong>XXIII.</strong> One component of the homeostatic mechanism M2 that maintains B is the person's consciously aiming at B's yielding beliefs that satisfy cognitive goals.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wML432troCaMquidk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 30, "extendedScore": null, "score": 9.980361358816352e-07, "legacy": true, "legacyId": "19075", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-29T23:45:43.916Z", "modifiedAt": null, "url": null, "title": "LINK \"Politics is the Mindkiller\" a la Cracked.com", "slug": "link-politics-is-the-mindkiller-a-la-cracked-com", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:58.678Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atorm", "createdAt": "2011-03-30T16:41:30.635Z", "isAdmin": false, "displayName": "atorm"}, "userId": "PvazkPKLZs5LNujcL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/stEk8HKCM5bYMyFrg/link-politics-is-the-mindkiller-a-la-cracked-com", "pageUrlRelative": "/posts/stEk8HKCM5bYMyFrg/link-politics-is-the-mindkiller-a-la-cracked-com", "linkUrl": "https://www.lesswrong.com/posts/stEk8HKCM5bYMyFrg/link-politics-is-the-mindkiller-a-la-cracked-com", "postedAtFormatted": "Saturday, September 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK%20%22Politics%20is%20the%20Mindkiller%22%20a%20la%20Cracked.com&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK%20%22Politics%20is%20the%20Mindkiller%22%20a%20la%20Cracked.com%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FstEk8HKCM5bYMyFrg%2Flink-politics-is-the-mindkiller-a-la-cracked-com%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK%20%22Politics%20is%20the%20Mindkiller%22%20a%20la%20Cracked.com%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FstEk8HKCM5bYMyFrg%2Flink-politics-is-the-mindkiller-a-la-cracked-com", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FstEk8HKCM5bYMyFrg%2Flink-politics-is-the-mindkiller-a-la-cracked-com", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 19, "htmlBody": "<p>A Cracked.com columnist gives a witty explanation of some of the same problems EY points out with political teams.</p>\n<p>http://www.cracked.com/blog/5-ways-internet-convinced-me-not-to-vote/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "stEk8HKCM5bYMyFrg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "19077", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-30T04:24:44.098Z", "modifiedAt": null, "url": null, "title": "Introduction to Connectionist Modelling of Cognitive Processes: a chapter by chapter review ", "slug": "introduction-to-connectionist-modelling-of-cognitive", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:58.537Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9zZBZtk8AYYPpxgnH/introduction-to-connectionist-modelling-of-cognitive", "pageUrlRelative": "/posts/9zZBZtk8AYYPpxgnH/introduction-to-connectionist-modelling-of-cognitive", "linkUrl": "https://www.lesswrong.com/posts/9zZBZtk8AYYPpxgnH/introduction-to-connectionist-modelling-of-cognitive", "postedAtFormatted": "Sunday, September 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Introduction%20to%20Connectionist%20Modelling%20of%20Cognitive%20Processes%3A%20a%20chapter%20by%20chapter%20review%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntroduction%20to%20Connectionist%20Modelling%20of%20Cognitive%20Processes%3A%20a%20chapter%20by%20chapter%20review%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9zZBZtk8AYYPpxgnH%2Fintroduction-to-connectionist-modelling-of-cognitive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Introduction%20to%20Connectionist%20Modelling%20of%20Cognitive%20Processes%3A%20a%20chapter%20by%20chapter%20review%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9zZBZtk8AYYPpxgnH%2Fintroduction-to-connectionist-modelling-of-cognitive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9zZBZtk8AYYPpxgnH%2Fintroduction-to-connectionist-modelling-of-cognitive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1940, "htmlBody": "<p style=\"padding-left: 30px;\"><em>This chapter by chapter review was inspired by <a href=\"/lw/emc/causality_a_chapter_by_chapter_review/\">Vaniver's recent chapter by chapter review of Causality</a>. Like with that review, the intention is not so much to summarize but to help readers determine whether or not they should read the book. Reading the review is in no way a substitute for reading the book.</em></p>\n<p>I first read <a href=\"http://www.amazon.com/Introduction-Connectionist-Modelling-Cognitive-Processes/dp/0198524269\">Introduction to Connectionist Modelling of Cognitive Processes</a> (ICMCP) as part of an undergraduate course on cognitive modelling. We were assigned one half of the book to read: I ended up reading every page. Recently I felt like I should read it again, so I bought a used copy off Amazon. That was money well spent: the book was just as good as I remembered.</p>\n<p>By their nature, artificial neural networks (referred to as connectionist networks in the book) are a very mathy topic, and it would be easy to write a textbook that was nothing but formulas and very hard to understand. And while ICMCP also spends a lot of time talking about the math behind the various kinds of neural nets, it does its best to explain things as intuitively as possible, sticking to elementary mathematics and elaborating on the reasons of why the equations are what they are. At this, it succeeds &ndash; it can be easily understood by someone knowing only high school math. I haven't personally studied ANNs at a more advanced level, but I would imagine that anybody who intended to do so would greatly benefit from the strong conceptual and historical understanding ICMCP provided.</p>\n<p>The book also comes with a floppy disk containing a <strong>tlearn</strong> simulator which can be used to run various exercises given in the book. I haven't tried using this program, so I won't comment on it, nor on the exercises.</p>\n<p>The book has 15 chapters, and it is divided into two sections: principles and applications.</p>\n<p><strong>Principles</strong></p>\n<p><strong>1: &rdquo;The basics of connectionist information processing&rdquo;</strong> provides a general overview of how ANNs work. The chapter begins by providing a verbal summary of five assumptions of connectionist modelling: that 1) neurons integrate information, 2) neurons pass information about the level of their input, 3) brain structure is layered, 4) the influence of one neuron on another depends on the strength of the connection between them, and 5) learning is achieved by changing the strengths of connections between neurons. After this verbal introduction, the basic symbols and equations relating to ANNs are introduced simultaneously with an explanation of how the &rdquo;neurons&rdquo; in an ANN model work.</p>\n<p><a id=\"more\"></a></p>\n<p><strong>2: &rdquo;The attraction of parallel distributed processing for modelling cognition&rdquo;</strong> explains why we're supposed to be interested in these kinds of models in the first place. It elaborates on some interesting characteristics of ANNs: the representation of knowledge is distributed over the whole network, they are damage resistant and fault tolerant, and they allow memory access by content. All of these properties show up in the human brain, but not in classical computer programs. After briefly explaining these properties, there is an extended example of an ANN-based distributed database storing information about various gang members. In addition to being content addressable, it also shows typicality effects &ndash; it can be asked a question like &rdquo;what are the members of the gang 'Sharks' like&rdquo;, and it will naturally retrieve information about their typical ages, occupations, educational backgrounds, and so forth. Likewise, if asked to return the name of a pusher, it will suggest the name of the most typical pusher. In addition to explaining what the model is like, this chapter also explains the reasons for why it works the way it does.</p>\n<p><strong>3: &rdquo;Pattern association&rdquo; </strong>describes a specific kind of an ANN, a pattern associator, and a particular kind of learning rule, the Hebb rule. Pattern associators are networks which are presented with a certain kind of pattern as input and a certain kind of pattern as output, after which they will learn to transform the input pattern to the output pattern. They are capable of generalization: if they encounter an input which is similar to ones they have encountered before, they will produce a similar output. They are also fault tolerant, in that they can produce good results even if parts of the network are destroyed. They also automatically perform prototype extraction. Suppose that there is a prototypical &rdquo;average&rdquo; apple, and all other apples are noisy versions of the prototype. Pattern associators presented with several different patterns representing apples will learn to react the most strongly to an apple which is closest to the prototype, even if they have never actually seen the prototype itself.</p>\n<p><strong>4: &rdquo;Autoassociation&rdquo;</strong> deals with autoassociator networks, and explains how the Delta learning rule works. Autoassociators are a special case of pattern associators &ndash; they are taught to reproduce the same pattern at output that was present at input. While this may seem pointless at first, autoassociators are an effective way of implementing a kind of memory: once trained, they can reproduce a complex pattern merely from seeing a small fragment of the original pattern. This has an obvious connection to the human brain, which can e.g. recall a complicated past memory from simply picking up a smell that formed a minor part of the original memory. Autoassociators are also capable of forming categories and prototypes from individual experiences, such as forming a category corresponding to the concept of a dog from seeing several dogs, without explicitly being told that they all belong to the same category. (Or, to put it in Less Wrong jargon, they learn to recognize <a href=\"/lw/nl/the_cluster_structure_of_thingspace/\">clusters in thingspace</a>.)</p>\n<p><strong>5: &rdquo;Training a multi-layer network with an error signal: hidden units and backpropagation&rdquo;</strong> deals with the limitations of single-layered networks and how those limitations can be overcome by using more complex networks that require new kinds of training rules.</p>\n<p><strong>6: &rdquo;Competitive networks&rdquo; </strong>differ from previous networks in that they can carry out unsupervised learning: while the previous nets had an explicit teacher signal, competitive networks learn to categorize input patterns into related sets on their own. They can perform both categorization, transforming related inputs into more similar outputs, and orthogonalization, transforming similar inputs into less similar outputs.</p>\n<p><strong>7: &rdquo;Recurrent networks&rdquo;</strong> are capable of doing more than just simple transformations: they have feedback loops and more complicated internal states than non-recurrent networks can. This can be used to create sequences of actions, or to do things like predicting the next letter in a string of words and to identify word boundaries.</p>\n<p><strong>Applications</strong></p>\n<p><strong>8: &rdquo;Reading aloud&rdquo;</strong> can be difficult, especially in a highly irregular language like English, where most rules of how to transform a spelling to sounds have frequent exceptions. A child has to try to discover the regularities in an environment where there are both regularities and many exceptions. This chapter first briefly discusses traditional &rdquo;2-route&rdquo; models of reading aloud, which presume that the brain has one route that uses pronounciation rules to read aloud regular words, and another route which memorizes specific knowledge about the pronounciation of exception words. These are then contrasted with connectionist models, in which there is no distinction between specific information and general rules. &rdquo;There is only one kind of knowledge &ndash; the weights of the connections which the model has acquired as a result of its experiences during training &ndash; and this is all stored in a common network.&rdquo; The chapter then discusses several connectionist models which are successful in reading words aloud correctly, and which produce novel predictions and close matches to experimental psychological data.</p>\n<p><strong>9: &rdquo;Language acquisition&rdquo;</strong> &rdquo;examines three aspects of language learning by children &ndash; learning the past tense, the sudden growth in vocabulary which occurs towards the end of the second year, and the acquisition of syntactic rules&rdquo;. It describes and discusses various connectionist models which reproduce various pecularities of children's language learning. For example, some young children initially correctly learn to produce the past tense of the word &rdquo;go&rdquo; as &rdquo;went&rdquo;, then later on overgeneralize and treat it as a regular verb, saying &rdquo;goed&rdquo;, until they finally re-learn the correct form of &rdquo;went&rdquo;. As in the previous chapter, models are discussed which are capable of reproducing this and other pecularities, as well as providing novel predictions of human language learning, at least some of which were later confirmed in psychological studies. The various reasons for such peculiarities are also discussed.</p>\n<p>This chapter discusses many interesting issues, among others the fact that vocabulary spurts &ndash; dramatic increases in a child's vocabulary that commonly happen around the end of age two &ndash; have been taken as evidence of the emergence of a new kind of cognitive mechanism. Experiments with connectionist models show that this isn't necessarily the case &ndash; vocabulary spurts can also be produced without new mechanisms, as learning in an old mechanism reaches a threshold level which allows it to integrate information from different sources better than before.</p>\n<p><strong>10: &rdquo;Connectionism and cognitive development&rdquo;</strong> elaborates on the issue of new mechanisms, discussing the fact that children's learning appears to advance in stages. Traditionally, such qualitative changes in behavior have been presumed to be due to qualitative changes in the brain's architecture. This chapter discusses connectionist models simulating the apperance of object permanence &ndash; the realization that objects continue to exist even when you don't see them &ndash; and the balance beam problem, in which children are asked to judge the direction in which a balance beam loaded with various weights will tilt. It is shown that as the models are trained, they undergo stage-like change like a child, even though their basic architecture remains constant and the same training rule is used.</p>\n<p><strong>11: &rdquo;Connectionist neuropsychology &ndash; lesioning networks&rdquo;</strong> shows that selectively damaging trained networks can closely mimic the performance of various brain damaged patients. Models of damaged performance are examined in the fields of reading, semantic memory, and attention.</p>\n<p><strong>12: &rdquo;Mental representation: rules, symbols and connectionist networks&rdquo; </strong>discusses and counters claims of connectionist networks never being able to learn some kinds of rules which require one to use rules or symbols, due to having no explicit representation of them.</p>\n<p><strong>13: &rdquo;Network models of brain function&rdquo;</strong> discusses two models which attempt to replicate brain functionality and experimental data about the brain: a model of the hippocampus, and a model of the visual cortex. Both are shown to be effective. The hippocampus model in good in storing and recalling patterns of data. The visual cortex model, on the other hand, succeeds in identifying faces regardless of their position in the visual field, and regardless of whether the face is seen from the front or from the side.</p>\n<p><strong>14: &rdquo;Evolutionary connectionism&rdquo;</strong> has a brief discussion of connectionist networks that are trained using evolutionary algorithms. <strong>15: &rdquo;A selective history of connectionism before 1986&rdquo;</strong> is pretty much what it sounds like.</p>\n<p>-----------</p>\n<p>In addition to providing a nice analysis of various models originally published in journal papers, the book also provides a bit of a historical perspective. Several of the chapters start with analyzing an early model that produced promising but imperfect results, and then move on to a later model which built on the previous work and developed more realistic results. References are provided to all the original papers, and readers who are interested in learning more about the various topics discussed in the book are frequently referred to sources which discuss them in more depth.</p>\n<p>I would recommend this book to anyone who has an interest in psychology, can handle a bit of math, and who isn't yet familar with all the various and fascinating things that connectionist models are capable of doing. Although the models discussed in the book are all very simple as compared to the ones in the brain, they do make the idea of the brain being successfully reverse-engineered during this century feel a lot more plausible.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9zZBZtk8AYYPpxgnH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 18, "extendedScore": null, "score": 9.981987839168096e-07, "legacy": true, "legacyId": "19073", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"padding-left: 30px;\"><em>This chapter by chapter review was inspired by <a href=\"/lw/emc/causality_a_chapter_by_chapter_review/\">Vaniver's recent chapter by chapter review of Causality</a>. Like with that review, the intention is not so much to summarize but to help readers determine whether or not they should read the book. Reading the review is in no way a substitute for reading the book.</em></p>\n<p>I first read <a href=\"http://www.amazon.com/Introduction-Connectionist-Modelling-Cognitive-Processes/dp/0198524269\">Introduction to Connectionist Modelling of Cognitive Processes</a> (ICMCP) as part of an undergraduate course on cognitive modelling. We were assigned one half of the book to read: I ended up reading every page. Recently I felt like I should read it again, so I bought a used copy off Amazon. That was money well spent: the book was just as good as I remembered.</p>\n<p>By their nature, artificial neural networks (referred to as connectionist networks in the book) are a very mathy topic, and it would be easy to write a textbook that was nothing but formulas and very hard to understand. And while ICMCP also spends a lot of time talking about the math behind the various kinds of neural nets, it does its best to explain things as intuitively as possible, sticking to elementary mathematics and elaborating on the reasons of why the equations are what they are. At this, it succeeds \u2013 it can be easily understood by someone knowing only high school math. I haven't personally studied ANNs at a more advanced level, but I would imagine that anybody who intended to do so would greatly benefit from the strong conceptual and historical understanding ICMCP provided.</p>\n<p>The book also comes with a floppy disk containing a <strong>tlearn</strong> simulator which can be used to run various exercises given in the book. I haven't tried using this program, so I won't comment on it, nor on the exercises.</p>\n<p>The book has 15 chapters, and it is divided into two sections: principles and applications.</p>\n<p><strong id=\"Principles\">Principles</strong></p>\n<p><strong>1: \u201dThe basics of connectionist information processing\u201d</strong> provides a general overview of how ANNs work. The chapter begins by providing a verbal summary of five assumptions of connectionist modelling: that 1) neurons integrate information, 2) neurons pass information about the level of their input, 3) brain structure is layered, 4) the influence of one neuron on another depends on the strength of the connection between them, and 5) learning is achieved by changing the strengths of connections between neurons. After this verbal introduction, the basic symbols and equations relating to ANNs are introduced simultaneously with an explanation of how the \u201dneurons\u201d in an ANN model work.</p>\n<p><a id=\"more\"></a></p>\n<p><strong>2: \u201dThe attraction of parallel distributed processing for modelling cognition\u201d</strong> explains why we're supposed to be interested in these kinds of models in the first place. It elaborates on some interesting characteristics of ANNs: the representation of knowledge is distributed over the whole network, they are damage resistant and fault tolerant, and they allow memory access by content. All of these properties show up in the human brain, but not in classical computer programs. After briefly explaining these properties, there is an extended example of an ANN-based distributed database storing information about various gang members. In addition to being content addressable, it also shows typicality effects \u2013 it can be asked a question like \u201dwhat are the members of the gang 'Sharks' like\u201d, and it will naturally retrieve information about their typical ages, occupations, educational backgrounds, and so forth. Likewise, if asked to return the name of a pusher, it will suggest the name of the most typical pusher. In addition to explaining what the model is like, this chapter also explains the reasons for why it works the way it does.</p>\n<p><strong>3: \u201dPattern association\u201d </strong>describes a specific kind of an ANN, a pattern associator, and a particular kind of learning rule, the Hebb rule. Pattern associators are networks which are presented with a certain kind of pattern as input and a certain kind of pattern as output, after which they will learn to transform the input pattern to the output pattern. They are capable of generalization: if they encounter an input which is similar to ones they have encountered before, they will produce a similar output. They are also fault tolerant, in that they can produce good results even if parts of the network are destroyed. They also automatically perform prototype extraction. Suppose that there is a prototypical \u201daverage\u201d apple, and all other apples are noisy versions of the prototype. Pattern associators presented with several different patterns representing apples will learn to react the most strongly to an apple which is closest to the prototype, even if they have never actually seen the prototype itself.</p>\n<p><strong>4: \u201dAutoassociation\u201d</strong> deals with autoassociator networks, and explains how the Delta learning rule works. Autoassociators are a special case of pattern associators \u2013 they are taught to reproduce the same pattern at output that was present at input. While this may seem pointless at first, autoassociators are an effective way of implementing a kind of memory: once trained, they can reproduce a complex pattern merely from seeing a small fragment of the original pattern. This has an obvious connection to the human brain, which can e.g. recall a complicated past memory from simply picking up a smell that formed a minor part of the original memory. Autoassociators are also capable of forming categories and prototypes from individual experiences, such as forming a category corresponding to the concept of a dog from seeing several dogs, without explicitly being told that they all belong to the same category. (Or, to put it in Less Wrong jargon, they learn to recognize <a href=\"/lw/nl/the_cluster_structure_of_thingspace/\">clusters in thingspace</a>.)</p>\n<p><strong>5: \u201dTraining a multi-layer network with an error signal: hidden units and backpropagation\u201d</strong> deals with the limitations of single-layered networks and how those limitations can be overcome by using more complex networks that require new kinds of training rules.</p>\n<p><strong>6: \u201dCompetitive networks\u201d </strong>differ from previous networks in that they can carry out unsupervised learning: while the previous nets had an explicit teacher signal, competitive networks learn to categorize input patterns into related sets on their own. They can perform both categorization, transforming related inputs into more similar outputs, and orthogonalization, transforming similar inputs into less similar outputs.</p>\n<p><strong>7: \u201dRecurrent networks\u201d</strong> are capable of doing more than just simple transformations: they have feedback loops and more complicated internal states than non-recurrent networks can. This can be used to create sequences of actions, or to do things like predicting the next letter in a string of words and to identify word boundaries.</p>\n<p><strong id=\"Applications\">Applications</strong></p>\n<p><strong>8: \u201dReading aloud\u201d</strong> can be difficult, especially in a highly irregular language like English, where most rules of how to transform a spelling to sounds have frequent exceptions. A child has to try to discover the regularities in an environment where there are both regularities and many exceptions. This chapter first briefly discusses traditional \u201d2-route\u201d models of reading aloud, which presume that the brain has one route that uses pronounciation rules to read aloud regular words, and another route which memorizes specific knowledge about the pronounciation of exception words. These are then contrasted with connectionist models, in which there is no distinction between specific information and general rules. \u201dThere is only one kind of knowledge \u2013 the weights of the connections which the model has acquired as a result of its experiences during training \u2013 and this is all stored in a common network.\u201d The chapter then discusses several connectionist models which are successful in reading words aloud correctly, and which produce novel predictions and close matches to experimental psychological data.</p>\n<p><strong>9: \u201dLanguage acquisition\u201d</strong> \u201dexamines three aspects of language learning by children \u2013 learning the past tense, the sudden growth in vocabulary which occurs towards the end of the second year, and the acquisition of syntactic rules\u201d. It describes and discusses various connectionist models which reproduce various pecularities of children's language learning. For example, some young children initially correctly learn to produce the past tense of the word \u201dgo\u201d as \u201dwent\u201d, then later on overgeneralize and treat it as a regular verb, saying \u201dgoed\u201d, until they finally re-learn the correct form of \u201dwent\u201d. As in the previous chapter, models are discussed which are capable of reproducing this and other pecularities, as well as providing novel predictions of human language learning, at least some of which were later confirmed in psychological studies. The various reasons for such peculiarities are also discussed.</p>\n<p>This chapter discusses many interesting issues, among others the fact that vocabulary spurts \u2013 dramatic increases in a child's vocabulary that commonly happen around the end of age two \u2013 have been taken as evidence of the emergence of a new kind of cognitive mechanism. Experiments with connectionist models show that this isn't necessarily the case \u2013 vocabulary spurts can also be produced without new mechanisms, as learning in an old mechanism reaches a threshold level which allows it to integrate information from different sources better than before.</p>\n<p><strong>10: \u201dConnectionism and cognitive development\u201d</strong> elaborates on the issue of new mechanisms, discussing the fact that children's learning appears to advance in stages. Traditionally, such qualitative changes in behavior have been presumed to be due to qualitative changes in the brain's architecture. This chapter discusses connectionist models simulating the apperance of object permanence \u2013 the realization that objects continue to exist even when you don't see them \u2013 and the balance beam problem, in which children are asked to judge the direction in which a balance beam loaded with various weights will tilt. It is shown that as the models are trained, they undergo stage-like change like a child, even though their basic architecture remains constant and the same training rule is used.</p>\n<p><strong>11: \u201dConnectionist neuropsychology \u2013 lesioning networks\u201d</strong> shows that selectively damaging trained networks can closely mimic the performance of various brain damaged patients. Models of damaged performance are examined in the fields of reading, semantic memory, and attention.</p>\n<p><strong>12: \u201dMental representation: rules, symbols and connectionist networks\u201d </strong>discusses and counters claims of connectionist networks never being able to learn some kinds of rules which require one to use rules or symbols, due to having no explicit representation of them.</p>\n<p><strong>13: \u201dNetwork models of brain function\u201d</strong> discusses two models which attempt to replicate brain functionality and experimental data about the brain: a model of the hippocampus, and a model of the visual cortex. Both are shown to be effective. The hippocampus model in good in storing and recalling patterns of data. The visual cortex model, on the other hand, succeeds in identifying faces regardless of their position in the visual field, and regardless of whether the face is seen from the front or from the side.</p>\n<p><strong>14: \u201dEvolutionary connectionism\u201d</strong> has a brief discussion of connectionist networks that are trained using evolutionary algorithms. <strong>15: \u201dA selective history of connectionism before 1986\u201d</strong> is pretty much what it sounds like.</p>\n<p>-----------</p>\n<p>In addition to providing a nice analysis of various models originally published in journal papers, the book also provides a bit of a historical perspective. Several of the chapters start with analyzing an early model that produced promising but imperfect results, and then move on to a later model which built on the previous work and developed more realistic results. References are provided to all the original papers, and readers who are interested in learning more about the various topics discussed in the book are frequently referred to sources which discuss them in more depth.</p>\n<p>I would recommend this book to anyone who has an interest in psychology, can handle a bit of math, and who isn't yet familar with all the various and fascinating things that connectionist models are capable of doing. Although the models discussed in the book are all very simple as compared to the ones in the brain, they do make the idea of the brain being successfully reverse-engineered during this century feel a lot more plausible.</p>", "sections": [{"title": "Principles", "anchor": "Principles", "level": 1}, {"title": "Applications", "anchor": "Applications", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jnjjzkH8Fdzg4D6EK", "WBw8dDkAWohFjWQSk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-30T05:07:14.754Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Prices or Bindings?", "slug": "seq-rerun-prices-or-bindings", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:59.064Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RsSMzTMqTezYMKZ5M/seq-rerun-prices-or-bindings", "pageUrlRelative": "/posts/RsSMzTMqTezYMKZ5M/seq-rerun-prices-or-bindings", "linkUrl": "https://www.lesswrong.com/posts/RsSMzTMqTezYMKZ5M/seq-rerun-prices-or-bindings", "postedAtFormatted": "Sunday, September 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Prices%20or%20Bindings%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Prices%20or%20Bindings%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRsSMzTMqTezYMKZ5M%2Fseq-rerun-prices-or-bindings%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Prices%20or%20Bindings%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRsSMzTMqTezYMKZ5M%2Fseq-rerun-prices-or-bindings", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRsSMzTMqTezYMKZ5M%2Fseq-rerun-prices-or-bindings", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 169, "htmlBody": "<p>Today's post, <a href=\"/lw/v2/prices_or_bindings/\">Prices or Bindings?</a> was originally published on 21 October 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Prices_or_Bindings.3F\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Are ethical rules simply actions that have a high cost associated with them? Or are they bindings, expected to hold in all situations, no matter the cost otherwise?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/epi/seq_rerun_ethical_injunctions/\">Ethical Injunctions</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RsSMzTMqTezYMKZ5M", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "19078", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K2c3dkKErsqFd28Dh", "nuYzKYDF633d7x22A", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-30T05:13:00.302Z", "modifiedAt": null, "url": null, "title": "Female Test Subject - Convince Me To Get Cryo", "slug": "female-test-subject-convince-me-to-get-cryo", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:38.309Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Epiphany", "createdAt": "2012-08-12T03:33:21.256Z", "isAdmin": false, "displayName": "Epiphany"}, "userId": "BbbFp6hQzKF4YX8em", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dgFKZvdv46vbixrje/female-test-subject-convince-me-to-get-cryo", "pageUrlRelative": "/posts/dgFKZvdv46vbixrje/female-test-subject-convince-me-to-get-cryo", "linkUrl": "https://www.lesswrong.com/posts/dgFKZvdv46vbixrje/female-test-subject-convince-me-to-get-cryo", "postedAtFormatted": "Sunday, September 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Female%20Test%20Subject%20-%20Convince%20Me%20To%20Get%20Cryo&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFemale%20Test%20Subject%20-%20Convince%20Me%20To%20Get%20Cryo%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdgFKZvdv46vbixrje%2Ffemale-test-subject-convince-me-to-get-cryo%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Female%20Test%20Subject%20-%20Convince%20Me%20To%20Get%20Cryo%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdgFKZvdv46vbixrje%2Ffemale-test-subject-convince-me-to-get-cryo", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdgFKZvdv46vbixrje%2Ffemale-test-subject-convince-me-to-get-cryo", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 442, "htmlBody": "<p>I heard that women are difficult to convince when it comes to signing up for cryo.&nbsp; In <a href=\"/lw/e0z/mentioning_cryonics_to_a_dying_person/\">mentioning cryonics to a dying person</a>, there seems to be a consensus that it's not going to happen. I encountered a post: <a href=\"/lw/6xk/years_saved_cryonics_vs_villagereach/\">Years saved: Cryonics vs VillageReach</a>, which addressed my main objection (that the amount of money spent on cryo may be better spent on saving starving children, especially considering that you could save multiple children for that amount of money with high probability whereas you save only one life with low probability by paying for cryo).&nbsp; Now I'm open to being persuaded.</p>\n<p>My first instinct was to go read a lot about cryo, but it dawned on me that there are a lot of people here who will want to convince family members, some of them female, to sign up - and these people may appreciate the opportunity to practice on somebody.&nbsp; It has been argued that \"Brilliant and creative minds have explored the argument territory quite thoroughly.\" but if we already know all of the objections and have working rebuttals for each, why is it still thought of as extra difficult to get through to women?&nbsp; If there were a solution to this, it would not be seen as difficult.&nbsp; There must be something that pro-cryo people need for persuading women that they either haven't figured out or aren't good enough at yet.</p>\n<p>So, I decided to offer myself for experiments in attempting to convince a woman to sign up for cryo and took a <a href=\"/lw/ein/open_thread_september_1530_2012/7j1g\">poll in an open thread</a> to see whether there was interest.&nbsp; I don't claim to be perfectly representative of the female population, but I assume that I will have at least some objections in common with them and that persuading me would still be good practice for anyone planning to convince family members in the future.&nbsp; Having a study on persuading women would be more scientific but how do you come up with hypotheses to test for such a study if you have no actual experience persuading women?</p>\n<p>So, here is your opportunity to try whatever methods of persuasion you feel like with no guilt, explore my full list of objections without worrying about it being socially awkward, (I will even share cached religious thoughts, as annoyed as I am that I still have them.), and I will document as many of my impressions and objections as I can before I forget them.</p>\n<p>I am putting each objection / impression into a new comment for organization.&nbsp; Also, I have decided to avoid reading anything further on cryo, until/unless it is suggested by one of my persuaders.&nbsp;</p>\n<p>Well, have fun getting inside my head.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dgFKZvdv46vbixrje", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 8, "extendedScore": null, "score": 9.98224059970557e-07, "legacy": true, "legacyId": "19079", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 176, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sSYmGhBpjTG88Q4FW", "EEofqZQsMKJfNTDKJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-30T05:35:05.221Z", "modifiedAt": null, "url": null, "title": "Morale management for entrepreneurs", "slug": "morale-management-for-entrepreneurs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:00.159Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rX72LBqpixWacszit/morale-management-for-entrepreneurs", "pageUrlRelative": "/posts/rX72LBqpixWacszit/morale-management-for-entrepreneurs", "linkUrl": "https://www.lesswrong.com/posts/rX72LBqpixWacszit/morale-management-for-entrepreneurs", "postedAtFormatted": "Sunday, September 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Morale%20management%20for%20entrepreneurs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMorale%20management%20for%20entrepreneurs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrX72LBqpixWacszit%2Fmorale-management-for-entrepreneurs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Morale%20management%20for%20entrepreneurs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrX72LBqpixWacszit%2Fmorale-management-for-entrepreneurs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrX72LBqpixWacszit%2Fmorale-management-for-entrepreneurs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 507, "htmlBody": "<p>One of the odd things about the <a href=\"/lw/3w3/how_to_beat_procrastination/\">procrastination equation</a> is that part of it resembles an expected value calculation: value * expectancy.&nbsp; Why does the equation's numerator present a problem at all then, if it's just the expected value of what you're trying to do?&nbsp; Shouldn't that be the main factor in your motivation anyway?</p>\n<p>One answer: In lukeprog's post, he conflates the \"value\" that task presents intrinsically (how much you enjoy doing it), and possible extrinsic motivators (some reward you hope to achieve after the task is completed).&nbsp; So part of the reason your motivation system is miscalibrated is because not all valuable tasks are proportionately enjoyable.</p>\n<p>But today I thought of another answer: Your subconscious expected value calculation may be falling prey to biases that aren't affecting your conscious expected value calculation.&nbsp; Thus you correctly assign the task a high value consciously, but subconsciously, a particular bias may be bringing your estimate off.</p>\n<p>Paul Graham <a href=\"http://www.paulgraham.com/good.html\">writes</a>:</p>\n<blockquote>\n<p>Morale is tremendously important to a startup&mdash;so important that morale alone is almost enough to determine success. Startups are often described as emotional roller-coasters. One minute you're going to take over the world, and the next you're doomed. The problem with feeling you're doomed is not just that it makes you unhappy, but that it makes you <em>stop working</em>.</p>\n</blockquote>\n<p>Let's pretend that we were running a betting market for your startup's chance of success.&nbsp; If you and your cofounders are the only people in the market, you could picture the value of a contract in this market fluctuating up and down wildly.&nbsp; But if you let others play in the market, there's an obvious money-making strategy: take the average of recent fluctuations.&nbsp; Whenever the price fluctuates below that average, buy.&nbsp; Whenever it fluctuates above that average, sell.&nbsp; You and your cofounders can expect to lose a lot of money playing this market, at least early on in your startup's life.</p>\n<p>The point I'm trying to make here is that this \"emotional roller coaster\" represents a kind of irrationality on the part of entrepreneurs.&nbsp; And fixing this irrationality, especially in a way that hooks in to your motivation system and changes the numerator of your internal procrastination equation, could be very valuable for them.</p>\n<p>One idea for a bias that contributes to this effect is the <a href=\"http://wiki.lesswrong.com/wiki/Availability_heuristic\">availability heuristic</a>.&nbsp; This suggests that your subconscious rates very recent, \"available\" events related to your startup higher than earlier less \"available\" events.&nbsp; To fix this, you might be able to try to bring to mind older, less \"available\" data that suggests your startup will be successful and make it more salient.</p>\n<p>Another possible bias is simple overconfidence.&nbsp; It's really very difficult to know in advance whether your startup should succeed, so if you're either very bullish or very bearish, you're probably overconfident.&nbsp; A common path to startup success seems to be discovering some fact about the market you're in that lets you re-make your business as something much better.&nbsp; Since it's hard to predict the discovery of such facts in advance, it's hard to say much about how you will do.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8sh6iLwYWDJ7z3fPo": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rX72LBqpixWacszit", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 15, "extendedScore": null, "score": 9.982356233179162e-07, "legacy": true, "legacyId": "19080", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RWo4LwFzpHNQCTcYt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-30T21:33:50.919Z", "modifiedAt": null, "url": null, "title": "Meetup : NYC Humanist Open Mic (New Location, people under 21 now allowed)", "slug": "meetup-nyc-humanist-open-mic-new-location-people-under-21", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Jtsyqb7TXJPCmkEiK/meetup-nyc-humanist-open-mic-new-location-people-under-21", "pageUrlRelative": "/posts/Jtsyqb7TXJPCmkEiK/meetup-nyc-humanist-open-mic-new-location-people-under-21", "linkUrl": "https://www.lesswrong.com/posts/Jtsyqb7TXJPCmkEiK/meetup-nyc-humanist-open-mic-new-location-people-under-21", "postedAtFormatted": "Sunday, September 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20NYC%20Humanist%20Open%20Mic%20(New%20Location%2C%20people%20under%2021%20now%20allowed)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20NYC%20Humanist%20Open%20Mic%20(New%20Location%2C%20people%20under%2021%20now%20allowed)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJtsyqb7TXJPCmkEiK%2Fmeetup-nyc-humanist-open-mic-new-location-people-under-21%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20NYC%20Humanist%20Open%20Mic%20(New%20Location%2C%20people%20under%2021%20now%20allowed)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJtsyqb7TXJPCmkEiK%2Fmeetup-nyc-humanist-open-mic-new-location-people-under-21", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJtsyqb7TXJPCmkEiK%2Fmeetup-nyc-humanist-open-mic-new-location-people-under-21", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 210, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ee'>NYC Humanist Open Mic (New Location, people under 21 now allowed)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 October 2012 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">138 West Houston St. New York, NY 10012</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Third meetup of the Open Mind Open Mic, co-hosted by New York Center for Inquiry.</p>\n\n<p>The Open Mic is an opportunity for rationally-minded people to share performance art related to things they value. Have a piece to share about rationality, science, ethics or human achievement? It can be a song, story, poem, comedy act... anything that excites you. Covers of existing songs that you find appropriate are also encouraged.</p>\n\n<p>If you haven't performed before and aren't sure you're ready, you might consider bringing a short poem you like (last time we had a few people who originally came, intending to just listen but ended up finding a short work online to share).</p>\n\n<p>And of course, if you just want to come and listen, that's great too.</p>\n\n<p>We're at a new venue now, the Ivy Bakery at 138 West Houston St. New York, NY 10012. (Previously we were at a bar and unfortunately had to turn away people under 21, this is no longer the case). 7:00 PM.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ee'>NYC Humanist Open Mic (New Location, people under 21 now allowed)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Jtsyqb7TXJPCmkEiK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "19085", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___NYC_Humanist_Open_Mic__New_Location__people_under_21_now_allowed_\">Discussion article for the meetup : <a href=\"/meetups/ee\">NYC Humanist Open Mic (New Location, people under 21 now allowed)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 October 2012 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">138 West Houston St. New York, NY 10012</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Third meetup of the Open Mind Open Mic, co-hosted by New York Center for Inquiry.</p>\n\n<p>The Open Mic is an opportunity for rationally-minded people to share performance art related to things they value. Have a piece to share about rationality, science, ethics or human achievement? It can be a song, story, poem, comedy act... anything that excites you. Covers of existing songs that you find appropriate are also encouraged.</p>\n\n<p>If you haven't performed before and aren't sure you're ready, you might consider bringing a short poem you like (last time we had a few people who originally came, intending to just listen but ended up finding a short work online to share).</p>\n\n<p>And of course, if you just want to come and listen, that's great too.</p>\n\n<p>We're at a new venue now, the Ivy Bakery at 138 West Houston St. New York, NY 10012. (Previously we were at a bar and unfortunately had to turn away people under 21, this is no longer the case). 7:00 PM.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___NYC_Humanist_Open_Mic__New_Location__people_under_21_now_allowed_1\">Discussion article for the meetup : <a href=\"/meetups/ee\">NYC Humanist Open Mic (New Location, people under 21 now allowed)</a></h2>", "sections": [{"title": "Discussion article for the meetup : NYC Humanist Open Mic (New Location, people under 21 now allowed)", "anchor": "Discussion_article_for_the_meetup___NYC_Humanist_Open_Mic__New_Location__people_under_21_now_allowed_", "level": 1}, {"title": "Discussion article for the meetup : NYC Humanist Open Mic (New Location, people under 21 now allowed)", "anchor": "Discussion_article_for_the_meetup___NYC_Humanist_Open_Mic__New_Location__people_under_21_now_allowed_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-09-30T23:01:20.260Z", "modifiedAt": null, "url": null, "title": "Meetup : DC Meetup: Boardgames", "slug": "meetup-dc-meetup-boardgames-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MshmYP5WT6JTCC9FH/meetup-dc-meetup-boardgames-0", "pageUrlRelative": "/posts/MshmYP5WT6JTCC9FH/meetup-dc-meetup-boardgames-0", "linkUrl": "https://www.lesswrong.com/posts/MshmYP5WT6JTCC9FH/meetup-dc-meetup-boardgames-0", "postedAtFormatted": "Sunday, September 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20DC%20Meetup%3A%20Boardgames&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20DC%20Meetup%3A%20Boardgames%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMshmYP5WT6JTCC9FH%2Fmeetup-dc-meetup-boardgames-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20DC%20Meetup%3A%20Boardgames%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMshmYP5WT6JTCC9FH%2Fmeetup-dc-meetup-boardgames-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMshmYP5WT6JTCC9FH%2Fmeetup-dc-meetup-boardgames-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ef'>DC Meetup: Boardgames</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 October 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery Courtyard</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to play boardgames! Please post here or email the list if you can bring games, and hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ef'>DC Meetup: Boardgames</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MshmYP5WT6JTCC9FH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.987837705067987e-07, "legacy": true, "legacyId": "19086", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___DC_Meetup__Boardgames\">Discussion article for the meetup : <a href=\"/meetups/ef\">DC Meetup: Boardgames</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 October 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery Courtyard</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to play boardgames! Please post here or email the list if you can bring games, and hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___DC_Meetup__Boardgames1\">Discussion article for the meetup : <a href=\"/meetups/ef\">DC Meetup: Boardgames</a></h2>", "sections": [{"title": "Discussion article for the meetup : DC Meetup: Boardgames", "anchor": "Discussion_article_for_the_meetup___DC_Meetup__Boardgames", "level": 1}, {"title": "Discussion article for the meetup : DC Meetup: Boardgames", "anchor": "Discussion_article_for_the_meetup___DC_Meetup__Boardgames1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-01T01:44:31.169Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign, IL", "slug": "meetup-urbana-champaign-il-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:02.979Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "eJgCe9t2yKiCNAZxF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qKaLrR9aajyDwiW6H/meetup-urbana-champaign-il-0", "pageUrlRelative": "/posts/qKaLrR9aajyDwiW6H/meetup-urbana-champaign-il-0", "linkUrl": "https://www.lesswrong.com/posts/qKaLrR9aajyDwiW6H/meetup-urbana-champaign-il-0", "postedAtFormatted": "Monday, October 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%2C%20IL&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%2C%20IL%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqKaLrR9aajyDwiW6H%2Fmeetup-urbana-champaign-il-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%2C%20IL%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqKaLrR9aajyDwiW6H%2Fmeetup-urbana-champaign-il-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqKaLrR9aajyDwiW6H%2Fmeetup-urbana-champaign-il-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/eg\">Urbana-Champaign, IL</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">07 October 2012 02:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">The Starbucks by Green and 5th (503 Green St., Champaign, IL)</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We'll be getting to know each other! The goal is to decide what we want to do in the next meetups. Bring ideas for projects, discussion topics, anything you'd like to see going forward. Also, Manfred will be there with a fun improv game! We should all leave knowing some specifics for what we plan to do the next weekend. I will be there a bit early with an \"LW\" sign.</p>\n<p>As we are still just getting started, please post to this thread or to the google group if you plan on coming.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/eg\">Urbana-Champaign, IL</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qKaLrR9aajyDwiW6H", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 9.988693120309893e-07, "legacy": true, "legacyId": "19087", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__IL\">Discussion article for the meetup : <a href=\"/meetups/eg\">Urbana-Champaign, IL</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">07 October 2012 02:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">The Starbucks by Green and 5th (503 Green St., Champaign, IL)</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We'll be getting to know each other! The goal is to decide what we want to do in the next meetups. Bring ideas for projects, discussion topics, anything you'd like to see going forward. Also, Manfred will be there with a fun improv game! We should all leave knowing some specifics for what we plan to do the next weekend. I will be there a bit early with an \"LW\" sign.</p>\n<p>As we are still just getting started, please post to this thread or to the google group if you plan on coming.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__IL1\">Discussion article for the meetup : <a href=\"/meetups/eg\">Urbana-Champaign, IL</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign, IL", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__IL", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign, IL", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__IL1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-01T04:32:43.023Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Ethics Notes", "slug": "seq-rerun-ethics-notes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:58.981Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PrbgQaeZHjQ4v4XAb/seq-rerun-ethics-notes", "pageUrlRelative": "/posts/PrbgQaeZHjQ4v4XAb/seq-rerun-ethics-notes", "linkUrl": "https://www.lesswrong.com/posts/PrbgQaeZHjQ4v4XAb/seq-rerun-ethics-notes", "postedAtFormatted": "Monday, October 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Ethics%20Notes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Ethics%20Notes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPrbgQaeZHjQ4v4XAb%2Fseq-rerun-ethics-notes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Ethics%20Notes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPrbgQaeZHjQ4v4XAb%2Fseq-rerun-ethics-notes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPrbgQaeZHjQ4v4XAb%2Fseq-rerun-ethics-notes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<p>Today's post, <a href=\"/lw/v3/ethics_notes/\">Ethics Notes</a> was originally published on 21 October 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Ethics_Notes\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Some responses to comments about the idea of Ethical Injunctions.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/epy/seq_rerun_prices_or_bindings/\">Prices or Bindings?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PrbgQaeZHjQ4v4XAb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "19089", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hXuB8BCyyiYuzij3F", "RsSMzTMqTezYMKZ5M", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-01T05:54:52.611Z", "modifiedAt": null, "url": null, "title": " Open Thread, October 1-15, 2012", "slug": "open-thread-october-1-15-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:04.406Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mAHabNaAhAubgvXKc/open-thread-october-1-15-2012", "pageUrlRelative": "/posts/mAHabNaAhAubgvXKc/open-thread-october-1-15-2012", "linkUrl": "https://www.lesswrong.com/posts/mAHabNaAhAubgvXKc/open-thread-october-1-15-2012", "postedAtFormatted": "Monday, October 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%20Open%20Thread%2C%20October%201-15%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%20Open%20Thread%2C%20October%201-15%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmAHabNaAhAubgvXKc%2Fopen-thread-october-1-15-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%20Open%20Thread%2C%20October%201-15%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmAHabNaAhAubgvXKc%2Fopen-thread-october-1-15-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmAHabNaAhAubgvXKc%2Fopen-thread-october-1-15-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mAHabNaAhAubgvXKc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "19090", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 483, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-01T15:16:36.161Z", "modifiedAt": null, "url": null, "title": "October 2012 Media Thread", "slug": "october-2012-media-thread", "viewCount": null, "lastCommentedAt": "2012-11-02T04:43:47.856Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Gxtm7HfSbv3tEgCqk/october-2012-media-thread", "pageUrlRelative": "/posts/Gxtm7HfSbv3tEgCqk/october-2012-media-thread", "linkUrl": "https://www.lesswrong.com/posts/Gxtm7HfSbv3tEgCqk/october-2012-media-thread", "postedAtFormatted": "Monday, October 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20October%202012%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOctober%202012%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGxtm7HfSbv3tEgCqk%2Foctober-2012-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=October%202012%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGxtm7HfSbv3tEgCqk%2Foctober-2012-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGxtm7HfSbv3tEgCqk%2Foctober-2012-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 220, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is the monthly thread for posting media of various types that you've found that you enjoy. I find that exposure to LW ideas makes me less likely to enjoy some entertainment media that is otherwise quite popular, and finding media recommended by LWers is a good way to mitigate this. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the&nbsp;</span><a style=\"color: #8a8a8b;\" href=\"/r/discussion/tag/media_thread/\">older threads</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Rules:</p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you have a thread to add, such as a video game thread or an Anime thread, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Gxtm7HfSbv3tEgCqk", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 9.992952069487217e-07, "legacy": true, "legacyId": "19092", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 97, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-10-01T15:16:36.161Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-01T18:14:44.970Z", "modifiedAt": null, "url": null, "title": "[LINK] Higher intelligence correlates with greater cooperation", "slug": "link-higher-intelligence-correlates-with-greater-cooperation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:00.431Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XY3rko4TqYC894kC4/link-higher-intelligence-correlates-with-greater-cooperation", "pageUrlRelative": "/posts/XY3rko4TqYC894kC4/link-higher-intelligence-correlates-with-greater-cooperation", "linkUrl": "https://www.lesswrong.com/posts/XY3rko4TqYC894kC4/link-higher-intelligence-correlates-with-greater-cooperation", "postedAtFormatted": "Monday, October 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Higher%20intelligence%20correlates%20with%20greater%20cooperation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Higher%20intelligence%20correlates%20with%20greater%20cooperation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXY3rko4TqYC894kC4%2Flink-higher-intelligence-correlates-with-greater-cooperation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Higher%20intelligence%20correlates%20with%20greater%20cooperation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXY3rko4TqYC894kC4%2Flink-higher-intelligence-correlates-with-greater-cooperation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXY3rko4TqYC894kC4%2Flink-higher-intelligence-correlates-with-greater-cooperation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<p>The <a href=\"http://mason.gmu.edu/~gjonesb/iqcoop1\">result</a> is from 2008, but it's new to me. Abstract:</p>\n<blockquote>\n<p>A meta-study of repeated prisoner&rsquo;s dilemma experiments run at numerous universities suggests that students cooperate 5% to 8% more often for every 100 point increase in the school&rsquo;s average SAT score.</p>\n</blockquote>\n<p>Some obvious points from my first five minutes of thinking about it:</p>\n<p>&nbsp;</p>\n<ul>\n<li>Meta-study or not, the sample still only covers humans. No implications for Friendly AI or intelligent aliens, which don't have our motivations. </li>\n<li>Even among humans the sample is WEIRD, and a subset of WEIRD at that; although there is obviously variation between universities, it's smaller than what you'd get if you extended the sample down into the working class. I also wonder what would happen if the PD was played between students and non-students.&nbsp; </li>\n<li>Probably a point in favour of the Machiavellian intelligence hypothesis, in that we see those of higher intelligence doing better on a social problem. </li>\n<li>Presumably this implies that your best move, whatever your level of intelligence, is to surround yourself with the smartest people you can find, and then cooperate to ensure they don't throw you out. </li>\n<li>I'd like to know some details: Does intelligence also correlate with effective retaliation? With probing for suckers? What about cooperation in single games? (The study mentions one, in a footnote, which apparently did find higher intelligence correlated with greater cooperation even in one-shot games; but there's no metastudy.) </li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XY3rko4TqYC894kC4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "19093", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-02T03:49:35.424Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Which Parts Are \"Me\"?", "slug": "seq-rerun-which-parts-are-me", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:59.366Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qkkFvMd6dZBrARPCF/seq-rerun-which-parts-are-me", "pageUrlRelative": "/posts/qkkFvMd6dZBrARPCF/seq-rerun-which-parts-are-me", "linkUrl": "https://www.lesswrong.com/posts/qkkFvMd6dZBrARPCF/seq-rerun-which-parts-are-me", "postedAtFormatted": "Tuesday, October 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Which%20Parts%20Are%20%22Me%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Which%20Parts%20Are%20%22Me%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqkkFvMd6dZBrARPCF%2Fseq-rerun-which-parts-are-me%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Which%20Parts%20Are%20%22Me%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqkkFvMd6dZBrARPCF%2Fseq-rerun-which-parts-are-me", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqkkFvMd6dZBrARPCF%2Fseq-rerun-which-parts-are-me", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 169, "htmlBody": "<p>Today's post, <a href=\"/lw/v4/which_parts_are_me/\">Which Parts Are \"Me\"?</a> was originally published on 22 October 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Which_Parts_Are_.22Me.22.3F\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Everything you are, is inside your brain. But not everything inside your brain is you. You can draw mental separation lines, which can make you more reflective.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/eq9/seq_rerun_ethics_notes/\">Ethics Notes</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qkkFvMd6dZBrARPCF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 9.996903964663372e-07, "legacy": true, "legacyId": "19102", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vjmw8tW6wZAtNJMKo", "PrbgQaeZHjQ4v4XAb", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-02T09:15:30.156Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 10/1/12", "slug": "group-rationality-diary-10-1-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:04.153Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EjEBNhNoeKBEoqXy6/group-rationality-diary-10-1-12", "pageUrlRelative": "/posts/EjEBNhNoeKBEoqXy6/group-rationality-diary-10-1-12", "linkUrl": "https://www.lesswrong.com/posts/EjEBNhNoeKBEoqXy6/group-rationality-diary-10-1-12", "postedAtFormatted": "Tuesday, October 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%2010%2F1%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%2010%2F1%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEjEBNhNoeKBEoqXy6%2Fgroup-rationality-diary-10-1-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%2010%2F1%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEjEBNhNoeKBEoqXy6%2Fgroup-rationality-diary-10-1-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEjEBNhNoeKBEoqXy6%2Fgroup-rationality-diary-10-1-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">T</span><span style=\"color: #333333;\">his is the public group instrumental rationality diary for the week of October 1st. It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<div id=\"entry_t3_drj\" class=\"content clear\" style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; font-size: 12px; line-height: 18px;\">\n<div class=\"md\">\n<ul style=\"padding: 0px; line-height: 19px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Thanks to everyone who contributes!</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\"><a style=\"color: #8a8a8b;\" href=\"/lw/eku/group_rationality_diary_91712/\">Previous diary</a>;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">archive of prior diaries</a>.</p>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EjEBNhNoeKBEoqXy6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9.998615302101882e-07, "legacy": true, "legacyId": "19111", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rvMPfd4xh3oe5ZLLZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-02T11:09:21.661Z", "modifiedAt": null, "url": null, "title": "Meetup : Bielefeld Meetup, October 9th ", "slug": "meetup-bielefeld-meetup-october-9th", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:04.353Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Just_existing", "createdAt": "2012-01-16T18:40:07.392Z", "isAdmin": false, "displayName": "Just_existing"}, "userId": "o89m5G8Hk2tbpKTxg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3m33CTRp3BvmrB2vY/meetup-bielefeld-meetup-october-9th", "pageUrlRelative": "/posts/3m33CTRp3BvmrB2vY/meetup-bielefeld-meetup-october-9th", "linkUrl": "https://www.lesswrong.com/posts/3m33CTRp3BvmrB2vY/meetup-bielefeld-meetup-october-9th", "postedAtFormatted": "Tuesday, October 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bielefeld%20Meetup%2C%20October%209th%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bielefeld%20Meetup%2C%20October%209th%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3m33CTRp3BvmrB2vY%2Fmeetup-bielefeld-meetup-october-9th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bielefeld%20Meetup%2C%20October%209th%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3m33CTRp3BvmrB2vY%2Fmeetup-bielefeld-meetup-october-9th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3m33CTRp3BvmrB2vY%2Fmeetup-bielefeld-meetup-october-9th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/eh'>Bielefeld Meetup, October 9th </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 October 2012 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Grill/Bar Verve, Klosterplatz 13, Bielefeld</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The first meetup in this area ever.</p>\n\n<p>There is lots of rationality stuff to talk about, and I will prepare one or two activities in addition.</p>\n\n<p>I will have a sign.</p>\n\n<p>Everybody is welcome!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/eh'>Bielefeld Meetup, October 9th </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3m33CTRp3BvmrB2vY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 9.999213285425384e-07, "legacy": true, "legacyId": "19112", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bielefeld_Meetup__October_9th_\">Discussion article for the meetup : <a href=\"/meetups/eh\">Bielefeld Meetup, October 9th </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 October 2012 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Grill/Bar Verve, Klosterplatz 13, Bielefeld</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The first meetup in this area ever.</p>\n\n<p>There is lots of rationality stuff to talk about, and I will prepare one or two activities in addition.</p>\n\n<p>I will have a sign.</p>\n\n<p>Everybody is welcome!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bielefeld_Meetup__October_9th_1\">Discussion article for the meetup : <a href=\"/meetups/eh\">Bielefeld Meetup, October 9th </a></h2>", "sections": [{"title": "Discussion article for the meetup : Bielefeld Meetup, October 9th ", "anchor": "Discussion_article_for_the_meetup___Bielefeld_Meetup__October_9th_", "level": 1}, {"title": "Discussion article for the meetup : Bielefeld Meetup, October 9th ", "anchor": "Discussion_article_for_the_meetup___Bielefeld_Meetup__October_9th_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-02T14:56:15.094Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Place*", "slug": "meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:01.241Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bNMbiEogqWwf4yPBp/meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-1", "pageUrlRelative": "/posts/bNMbiEogqWwf4yPBp/meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-1", "linkUrl": "https://www.lesswrong.com/posts/bNMbiEogqWwf4yPBp/meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-1", "postedAtFormatted": "Tuesday, October 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Place*&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Place*%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbNMbiEogqWwf4yPBp%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-place-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Place*%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbNMbiEogqWwf4yPBp%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-place-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbNMbiEogqWwf4yPBp%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-place-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ei'>Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 October 2012 07:00:00AM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>And we're back to the Bean Cycle on Thursday, because of the reality of more choices of restaurants.</p>\n\n<p>What was the last experiment you did?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ei'>Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bNMbiEogqWwf4yPBp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.000040510148188e-06, "legacy": true, "legacyId": "19113", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_\">Discussion article for the meetup : <a href=\"/meetups/ei\">Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 October 2012 07:00:00AM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>And we're back to the Bean Cycle on Thursday, because of the reality of more choices of restaurants.</p>\n\n<p>What was the last experiment you did?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_1\">Discussion article for the meetup : <a href=\"/meetups/ei\">Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Place*", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Place*", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-02T15:10:26.749Z", "modifiedAt": null, "url": null, "title": "The difficulty in predicting AI, in three lines", "slug": "the-difficulty-in-predicting-ai-in-three-lines", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:01.075Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/srQFeGqgrZZXto8Ht/the-difficulty-in-predicting-ai-in-three-lines", "pageUrlRelative": "/posts/srQFeGqgrZZXto8Ht/the-difficulty-in-predicting-ai-in-three-lines", "linkUrl": "https://www.lesswrong.com/posts/srQFeGqgrZZXto8Ht/the-difficulty-in-predicting-ai-in-three-lines", "postedAtFormatted": "Tuesday, October 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20difficulty%20in%20predicting%20AI%2C%20in%20three%20lines&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20difficulty%20in%20predicting%20AI%2C%20in%20three%20lines%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsrQFeGqgrZZXto8Ht%2Fthe-difficulty-in-predicting-ai-in-three-lines%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20difficulty%20in%20predicting%20AI%2C%20in%20three%20lines%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsrQFeGqgrZZXto8Ht%2Fthe-difficulty-in-predicting-ai-in-three-lines", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsrQFeGqgrZZXto8Ht%2Fthe-difficulty-in-predicting-ai-in-three-lines", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<p>An over-simplification, but an evocative one:</p>\n<ul>\n<li>The social sciences are contentious, their predictions questionable.</li>\n<li>And yet social sciences use the scientific method; AI predictions generally don't.</li>\n<li>Hence predictions involving human-level AI should be treated as less certain than any prediction in the social sciences.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "srQFeGqgrZZXto8Ht", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 2, "extendedScore": null, "score": 1.0000479669552699e-06, "legacy": true, "legacyId": "19114", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-02T18:16:12.303Z", "modifiedAt": "2021-02-01T20:20:12.580Z", "url": null, "title": "The Useful Idea of Truth", "slug": "the-useful-idea-of-truth", "viewCount": null, "lastCommentedAt": "2022-03-10T16:03:59.759Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XqvnWFtRD2keJdwjX/the-useful-idea-of-truth", "pageUrlRelative": "/posts/XqvnWFtRD2keJdwjX/the-useful-idea-of-truth", "linkUrl": "https://www.lesswrong.com/posts/XqvnWFtRD2keJdwjX/the-useful-idea-of-truth", "postedAtFormatted": "Tuesday, October 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Useful%20Idea%20of%20Truth&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Useful%20Idea%20of%20Truth%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXqvnWFtRD2keJdwjX%2Fthe-useful-idea-of-truth%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Useful%20Idea%20of%20Truth%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXqvnWFtRD2keJdwjX%2Fthe-useful-idea-of-truth", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXqvnWFtRD2keJdwjX%2Fthe-useful-idea-of-truth", "socialPreviewImageUrl": "https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4796c21df0a6970726273b4463edc825263ff28f8e3a497e.png", "question": false, "authorIsUnreviewed": false, "wordCount": 3553, "htmlBody": "<p><i>(This is the first post of a new Sequence, <u>Highly Advanced Epistemology 101 for Beginners,</u>&nbsp;setting up the Sequence <u>Open Problems in Friendly AI.</u>&nbsp; For experienced readers, this first post may seem somewhat elementary; but it serves as a basis for what follows. &nbsp;And though it may be conventional in standard philosophy, the world at large does not know it, and it is useful to know a compact explanation. &nbsp;Kudos to Alex Altair for helping in the production and editing of this post and Sequence!)</i></p><hr><p>I remember this paper I wrote on existentialism. My teacher gave it back with an F. She\u2019d underlined true and truth wherever it appeared in the essay, probably about twenty times, with a question mark beside each. She wanted to know what I meant by truth.<br>-- Danielle Egan</p><p>I understand what it means for a hypothesis to be elegant, or falsifiable, or compatible with the evidence. It sounds to me like calling a belief \u2018true\u2019 or \u2018real\u2019 or \u2018actual\u2019 is merely the difference between saying you believe something, and saying you really really believe something.<br>-- Dale Carrico</p><p>What then is truth? A movable host of metaphors, metonymies, and; anthropomorphisms: in short, a sum of human relations which have been poetically and rhetorically intensified, transferred, and embellished, and which, after long usage, seem to a people to be fixed, canonical, and binding.<br>-- Friedrich Nietzche</p><hr><p>The Sally-Anne False-Belief task is an experiment used to tell whether a child understands the difference between belief and reality. It goes as follows:</p><p>The child sees Sally hide a marble inside a covered basket, as Anne looks on.</p><p>Sally leaves the room, and Anne takes the marble out of the basket and hides it inside a lidded box.</p><p>Anne leaves the room, and Sally returns.</p><p>The experimenter asks the child where Sally will look for her marble.</p><p>Children under the age of four say that Sally will look for her marble inside the box. Children over the age of four say that Sally will look for her marble inside the basket.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4796c21df0a6970726273b4463edc825263ff28f8e3a497e.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4796c21df0a6970726273b4463edc825263ff28f8e3a497e.png/w_121 121w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4796c21df0a6970726273b4463edc825263ff28f8e3a497e.png/w_201 201w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4796c21df0a6970726273b4463edc825263ff28f8e3a497e.png/w_281 281w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4796c21df0a6970726273b4463edc825263ff28f8e3a497e.png/w_361 361w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4796c21df0a6970726273b4463edc825263ff28f8e3a497e.png/w_441 441w\"></figure><p>(Attributed to: &nbsp;Baron-Cohen, S., Leslie, L. and Frith, U. (1985) \u2018Does the autistic child have a \u201ctheory of mind\u201d?\u2019, Cognition, vol. 21, pp. 37\u201346.)</p><p>Human children over the age of (typically) four, first begin to understand what it means for Sally to lose her marbles - for Sally's beliefs to stop corresponding to reality. A three-year-old has a model only of&nbsp;<i>where the marble is.</i>&nbsp;A four-year old is developing a theory of mind; they separately model&nbsp;<i>where the marble is</i>&nbsp;and&nbsp;<i>where Sally believes the marble is,</i>&nbsp;so they can notice when the two conflict - when Sally has a false belief.</p><figure class=\"image\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/6/67/A.jpg\"></figure><p>Any meaningful belief has a&nbsp;<i>truth-condition,</i>&nbsp;some way reality can be which can make that belief true, or alternatively false. If Sally's brain holds a mental image of a marble inside the basket, then, in reality itself, the marble can actually be inside the basket - in which case Sally's belief is called 'true', since reality falls inside its truth-condition. Or alternatively, Anne may have taken out the marble and hidden it in the box, in which case Sally's belief is termed 'false', since reality falls outside the belief's truth-condition.</p><figure class=\"image\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/77/B1.jpg\"></figure><p>The mathematician Alfred Tarski once described the notion of 'truth' via an infinite family of truth-conditions:</p><p>The sentence 'snow is white' is true if and only if snow is white.</p><p>The sentence 'the sky is blue' is true if and only if the sky is blue.</p><p>When you write it out that way, it looks like the distinction might be trivial - indeed, why bother talking about sentences at all, if the sentence looks so much like reality when both are written out as English?</p><p>But when we go back to the Sally-Anne task, the difference looks much clearer: Sally's&nbsp;<i>belief</i>&nbsp;is embodied in a pattern of neurons and neural firings inside Sally's brain, three pounds of wet and extremely complicated tissue inside Sally's skull. The&nbsp;<i>marble itself&nbsp;</i>is a small simple plastic sphere, moving between the basket and the box. When we compare Sally's belief to the marble, we are comparing two quite different things.</p><figure class=\"image\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/0/01/B2.jpg\"></figure><p>(Then why talk about these abstract 'sentences' instead of just neurally embodied beliefs? Maybe Sally and Fred believe \"the same thing\", i.e., their brains both have internal models of the marble inside the basket - two brain-bound beliefs with the same truth condition - in which case the thing these two beliefs have in common, the shared truth condition, is abstracted into the form of a&nbsp;<i>sentence</i>&nbsp;or&nbsp;<i>proposition</i>&nbsp;that we imagine being true or false apart from any brains that believe it.)</p><figure class=\"image\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/4/4b/C.jpg\"></figure><p>Some pundits have panicked over the point that any judgment of&nbsp;<i>truth</i>&nbsp;- any comparison of belief to reality - takes place inside some particular person's mind; and indeed seems to just compare&nbsp;<i>someone else's belief</i>&nbsp;to&nbsp;<i>your belief:</i></p><figure class=\"image\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/8/84/D.jpg\"></figure><p>So is all this talk of truth just comparing other people's beliefs to our own beliefs, and trying to assert privilege? Is the word 'truth' just a weapon in a power struggle?</p><p>For that matter, you can't even&nbsp;<i>directly</i>&nbsp;compare other people's beliefs to our own beliefs. You can only internally compare your&nbsp;<i>beliefs</i>&nbsp;about someone else's belief to your own belief - compare your map of their map, to your map of the territory.</p><p>Similarly, to say of your own beliefs, that the belief is 'true', just means you're comparing&nbsp;<i>your map of your map,</i>&nbsp;to&nbsp;<i>your map of the territory.&nbsp;</i>People&nbsp;<i>usually</i>&nbsp;are not mistaken about what they themselves believe - though there are&nbsp;<a href=\"/lw/i4/belief_in_belief/\">certain</a>&nbsp;<a href=\"/lw/s/belief_in_selfdeception/\">exceptions</a>&nbsp;to this rule - yet nonetheless, the map of the map is usually accurate, i.e., people are usually right about the question of&nbsp;<i>what they believe:</i></p><figure class=\"image\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/3/30/E.jpg\"></figure><p>And so saying 'I believe the sky is blue, and that's true!' typically conveys the same information as 'I believe the sky is blue' or just saying 'The sky is blue' - namely, that your mental model of the world contains a blue sky.</p><p><i>Meditation:</i></p><p>If the above is true, aren't the postmodernists right? Isn't all this talk of 'truth' just an attempt to assert the privilege of your own beliefs over others, when there's nothing that can actually compare a belief to reality itself, outside of anyone's head?</p><p><i>(A 'meditation' is a puzzle that the reader is meant to attempt to solve before continuing. It's my somewhat awkward attempt to reflect the research which shows that you're much more likely to remember a fact or solution if you try to solve the problem yourself before reading the solution; succeed or fail, the important thing is to have tried&nbsp;<u>first</u></i>&nbsp;<i>. This also reflects a problem Michael Vassar thinks is occurring, which is that since LW posts often sound&nbsp;</i><a href=\"/lw/im/hindsight_devalues_science/\"><i>obvious in retrospect</i></a><i>, it's hard for people to visualize the diff between 'before' and 'after'; and this diff is also useful to have for learning purposes. So please try to say your own answer to the&nbsp;meditation&nbsp;- ideally whispering it to yourself, or moving your lips as you pretend to say it, so as to make sure it's fully explicit and available for memory - before continuing; and try to consciously note the difference between your reply and the post's reply, including any extra details present or missing, without trying to minimize or maximize the difference.)</i></p><p>...<br>...<br>...</p><p><i>Reply:</i></p><p>The reply I gave to Dale Carrico - who declaimed to me that he knew what it meant for a belief to be falsifiable, but not what it meant for beliefs to be true - was that my&nbsp;<i>beliefs</i>&nbsp;determine my experimental&nbsp;<i>predictions,</i>&nbsp;but only&nbsp;<i>reality</i>&nbsp;gets to determine my experimental&nbsp;<i>results.</i>&nbsp;If I believe very strongly that I can fly, then this belief may lead me to step off a cliff, expecting to be safe; but only the&nbsp;<i>truth</i>&nbsp;of this belief can possibly save me from plummeting to the ground and ending my experiences with a splat.</p><figure class=\"image\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/f/f0/F.jpg\"></figure><p>Since my expectations sometimes conflict with my subsequent experiences, I need different names for the thingies that determine my experimental predictions and the thingy that determines my experimental results. I call the former thingies 'beliefs', and the latter thingy 'reality'.</p><p>You won't get a direct collision between belief and reality - or between someone else's beliefs and reality - by sitting in your living-room with your eyes closed. But the situation is different if you open your eyes!</p><p>Consider how your brain ends up knowing that its shoelaces are untied:</p><ul><li>A photon departs from the Sun, and flies to the Earth and through Earth's atmosphere.</li><li>Your shoelace absorbs and re-emits the photon.</li><li>The reflected photon passes through your eye's pupil and toward your retina.</li><li>The photon strikes a rod cell or cone cell, or to be more precise, it strikes a photoreceptor, a form of vitamin-A known as&nbsp;<i>retinal,</i>&nbsp;which undergoes a change in its molecular shape (rotating around a double bond) powered by absorption of the photon's energy. A bound protein called an&nbsp;<i>opsin</i>&nbsp;undergoes a conformational change in response, and this further propagates to a neural cell body which pumps a proton and increases its polarization.</li><li>The gradual polarization change is propagated to a bipolar cell and then a ganglion cell. If the ganglion cell's polarization goes over a threshold, it sends out a&nbsp;<i>nerve impulse,</i>&nbsp;a propagating electrochemical phenomenon of polarization-depolarization that travels through the brain at between 1 and 100 meters per second. Now the incoming light from the outside world has been transduced to neural information, commensurate with the substrate of other thoughts.</li><li>The neural signal is preprocessed by other neurons in the retina, further preprocessed by the lateral geniculate nucleus in the middle of the brain, and then, in the visual cortex located at the back of your head, reconstructed into&nbsp;<i>an actual little tiny picture</i>&nbsp;of the surrounding world - a picture embodied in the firing frequencies of the neurons making up the visual field. (A distorted picture, since the center of the visual field is processed in much greater detail - i.e. spread across more neurons and more cortical area - than the edges.)</li><li>Information from the visual cortex is then routed to the temporal lobes, which handle object recognition.</li><li>Your brain recognizes the form of an untied shoelace.</li></ul><figure class=\"image\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/6/6e/G.jpg\"></figure><p>And so your brain updates its map of the world to include the fact that your shoelaces are untied. Even if, previously, it expected them to be tied! &nbsp;There's no reason for your brain&nbsp;<i>not</i>&nbsp;to update if politics aren't involved. Once photons heading into the eye are turned into neural firings, they're commensurate with other mind-information and can be compared to previous beliefs.</p><p>Belief and reality interact&nbsp;<i>all the time</i>. If the environment and the brain never touched in any way, we wouldn't need eyes - or hands - and the brain could afford to be a&nbsp;<i>whole</i>&nbsp;lot simpler. In fact, organisms wouldn't need brains at all.</p><p>So, fine, belief and reality are distinct entities which do intersect and interact. But to say that we need separate concepts for 'beliefs' and 'reality' doesn't get us to needing the concept of 'truth', a comparison between them. Maybe we can just separately (a) talk about an agent's belief that the sky is blue and (b) talk about the sky itself. Instead of saying, \"Jane believes the sky is blue, and she's right\", we could say, \"Jane believes 'the sky is blue'; also, the sky is blue\" and convey the same information about what (a) we believe about the sky and (b) what we believe Jane believes. We could always apply Tarski's schema - \"The sentence 'X' is&nbsp;<i>true&nbsp;</i>iff X\" - and replace every instance of alleged truth by talking directly about the truth-condition, the corresponding state of reality (i.e. the sky or whatever). Thus we could eliminate that bothersome word, 'truth', which is so controversial to philosophers, and misused by various annoying people.</p><p>Suppose you had a rational agent, or for concreteness, an Artificial Intelligence, which was carrying out its work in isolation and certainly never needed to argue politics with anyone. The AI knows that \"My model assigns 90% probability that the sky is blue\"; it is quite sure that this probability is the exact statement stored in its RAM. Separately, the AI models that \"The probability that my optical sensors will detect blue out the window is 99%,&nbsp;<i>given</i>&nbsp;that the sky is blue\"; and it doesn't confuse this proposition with the quite different proposition that the optical sensors will detect blue whenever it&nbsp;<i>believes</i>&nbsp;the sky is blue. So the AI can definitely differentiate the map and the territory; it knows that the possible states of its RAM storage do not have the same consequences and causal powers as the possible states of sky.</p><p>But does this AI ever need a concept for the notion of&nbsp;<i>truth in general</i>&nbsp;- does it ever need to invent the word 'truth'? Why would it work better if it did?</p><p><i>Meditation:</i>&nbsp;If we were dealing with an Artificial Intelligence that never had to argue politics with anyone, would it ever need a word or a concept for 'truth'?</p><p>...<br>...<br>...</p><p><i>Reply:</i>&nbsp;The abstract concept of 'truth' - the general idea of a map-territory correspondence - is required to express ideas such as:</p><p><i>Generalized across possible maps and possible cities,</i>&nbsp;if your map of a city is accurate, navigating according to that map is more likely to get you to the airport on time.</p><p>To draw a true map of a city, someone has to go out and look at the buildings; there's no way you'd end up with an accurate map by sitting in your living-room with your eyes closed trying to imagine what you wish the city would look like.</p><p>True beliefs are more likely than false beliefs to make correct experimental predictions, so if we increase our credence in hypotheses that make correct experimental predictions, our model of reality should become incrementally more true over time.</p><p>This is the main benefit of talking and thinking about 'truth' - that we can generalize rules about how to make maps match territories&nbsp;<i>in general;&nbsp;</i>we can learn lessons that transfer beyond particular skies being blue.</p><hr><p><i>Next in main sequence:</i></p><p>Complete philosophical panic has turned out not to be justified (it never is). But there is a key practical problem that results from our internal evaluation of 'truth' being a comparison of a map of a map, to a map of reality: On this schema it is&nbsp;<i>very</i>&nbsp;<i>easy</i>&nbsp;for the brain to end up believing that a&nbsp;<i>completely</i>&nbsp;<i>meaningless</i>&nbsp;statement is 'true'.</p><p>Some literature professor lectures that the famous authors Carol, Danny, and Elaine are all 'post-utopians', which you can tell because their writings exhibit signs of 'colonial alienation'. For most college students the typical result will be that their brain's version of an object-attribute list will assign the attribute 'post-utopian' to the authors Carol, Danny, and Elaine. When the subsequent test asks for \"an example of a post-utopian author\", the student will write down \"Elaine\". What if the student writes down, \"I think Elaine is&nbsp;<i>not</i>&nbsp;a post-utopian\"? Then the professor models thusly...</p><figure class=\"image\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/d/d4/H.jpg\"></figure><p>...and marks the answer&nbsp;<i>false.</i></p><p>After all...</p><p>The sentence \"Elaine is a post-utopian\" is&nbsp;<i>true</i>&nbsp;if and only if Elaine is a post-utopian.</p><p>...right?</p><p>Now of course it could be that this term <i>does</i>&nbsp;mean something (even though I made it up). &nbsp;It might even be that, although the professor can't give a good explicit answer to&nbsp;\"What <i>is </i>post-utopianism, anyway?\", you can nonetheless take many literary professors and separately show them new pieces of writing by unknown authors and they'll all independently arrive at the same answer, in which case they're clearly detecting <i>some</i>&nbsp;sensory-visible feature of the writing. &nbsp;We don't always know how our brains work, and we don't always know what we see, and the sky was seen as blue long before the word \"blue\" was invented; for a part of your brain's world-model to be meaningful doesn't require that you can explain it in words.</p><p>On the other hand, it could also be the case that the professor learned about \"colonial alienation\" by memorizing what to say to <i>his</i>&nbsp;professor. &nbsp;It could be that the only person whose brain assigned a real meaning to the word is dead. &nbsp;So that by the time the students are learning that \"post-utopian\" is the password when hit with the query \"colonial alienation?\", both phrases are <i>just</i>&nbsp;verbal responses to be rehearsed, <i>nothing but</i>&nbsp;an answer on a test.</p><p>The two phrases don't feel \"disconnected\" individually because they're connected to each other -&nbsp;post-utopianism&nbsp;has the apparent consequence of colonial alienation, and if you ask what&nbsp;colonial alienation&nbsp;implies, it means the author is probably a post-utopian. &nbsp;But if you draw a circle around both phrases, they don't connect to anything <i>else.</i>&nbsp; They're&nbsp;<i>floating beliefs&nbsp;</i>not connected with the rest of the model.&nbsp;And yet there's no internal alarm that goes off when this happens. Just as \"being wrong feels like being right\" - just as having a false belief feels the same internally as having a true belief, at least until you run an experiment - having a meaningless belief can&nbsp;<i>feel</i>&nbsp;just like having a meaningful belief.</p><p>(You can even have fights over completely meaningless beliefs. &nbsp;If someone says \"Is Elaine a post-utopian?\" and one group shouts \"Yes!\" and the other group shouts \"No!\", they can fight over having shouted different things; it's not necessary for the words to <i>mean</i>&nbsp;anything for the battle to get started. &nbsp;Heck, you could have a battle over one group shouting \"Mun!\" and the other shouting \"Fleem!\" &nbsp;More generally, it's important to distinguish the visible consequences of the professor-brain's <i>quoted</i>&nbsp;belief (students had better write down a certain thing on his test, or they'll be marked wrong) from the proposition that there's an&nbsp;<i>unquoted state of reality</i>&nbsp;(Elaine<i>&nbsp;actually</i>&nbsp;being a post-utopian in the territory) which has visible consquences.)</p><p>One classic response to this problem was&nbsp;<i>verificationism,</i>&nbsp;which held that the sentence \"Elaine is a post-utopian\" is&nbsp;<i>meaningless&nbsp;</i>if it doesn't tell us which sensory experiences we should expect to see if the sentence is true, and how those experiences differ from the case if the sentence is false.</p><p>But then suppose that I&nbsp;<a href=\"/lw/pb/belief_in_the_implied_invisible/\">transmit a photon aimed at the void between galaxies</a>&nbsp;- heading far off into space, away into the night. In an expanding universe, this photon will eventually cross the&nbsp;<i>cosmological horizon</i>&nbsp;where, even if the photon hit a mirror reflecting it squarely back toward Earth, the photon would never get here because the universe would expand too fast in the meanwhile. Thus, after the photon goes past a certain point, there are&nbsp;<i>no experimental consequences whatsoever, ever,</i>&nbsp;to the statement \"The photon continues to exist, rather than blinking out of existence.\"</p><p>And yet it seems to me - and I hope to you as well - that the statement \"The photon suddenly blinks out of existence as soon as we can't see it, violating Conservation of Energy and behaving unlike all photons we can actually see\" is&nbsp;<i>false,</i>&nbsp;while the statement \"The photon continues to exist, heading off to nowhere\" is&nbsp;<i>true.</i>&nbsp;And this sort of question can have important policy consequences: suppose we were thinking of sending off a near-light-speed colonization vessel as far away as possible, so that it would be over the cosmological horizon before it slowed down to colonize some distant supercluster. If we thought the colonization ship would just blink out of existence before it arrived, we wouldn't bother sending it.</p><p>It is both useful and wise to ask after the sensory consequences of our beliefs. But it's not quite the&nbsp;<i>fundamental</i>&nbsp;definition of meaningful statements. It's an excellent&nbsp;<i>hint</i>&nbsp;that something might be a disconnected 'floating belief',&nbsp;but it's not a hard-and-fast rule.</p><p>You might next try the answer that for a statement to be meaningful, there must be some&nbsp;<i>way reality can be</i>&nbsp;which makes the statement true or false; and that since the universe is made of atoms, there must be some way to&nbsp;<i>arrange the atoms in the universe</i>&nbsp;that would make a statement true or false. E.g. to make the statement \"I am in Paris\" true, we would have to move the atoms comprising myself to Paris. A literateur claims that Elaine has an attribute called post-utopianism, but there's no way to translate this claim into a way to&nbsp;<i>arrange the atoms in the universe</i>&nbsp;so as to make the claim true, or alternatively false; so it has no truth-condition, and must be meaningless.</p><p>Indeed there are claims where, if you pause and ask, \"How could a universe be arranged so as to make this claim true, or alternatively false?\", you'll suddenly realize that you didn't have as strong a grasp on the claim's truth-condition as you believed. \"Suffering builds character\", say, or \"All depressions result from bad monetary policy.\" These claims aren't necessarily meaningless, but they're a lot easier to&nbsp;<i>say</i>, than to visualize the universe that makes them true or false. Just like asking after sensory consequences is an important hint to meaning or meaninglessness, so is asking how to configure the universe.</p><p>But if you say there has to be some arrangement of&nbsp;<i>atoms</i>&nbsp;that makes a meaningful claim true or false...</p><p>Then the theory of quantum mechanics would be meaningless&nbsp;<i>a priori</i>, because there's no way to arrange&nbsp;<i>atoms</i>&nbsp;to make the theory of quantum mechanics true.</p><p>And when we discovered that the universe was not made of atoms, but rather quantum fields, all&nbsp;<i>meaningful</i>&nbsp;statements everywhere would have been revealed as&nbsp;<i>false</i>&nbsp;- since there'd be no atoms arranged to fulfill their truth-conditions.</p><p><i>Meditation:</i>&nbsp;What rule could restrict our beliefs to&nbsp;<i>just</i>&nbsp;propositions that can be meaningful, without excluding a priori anything that could in principle be true?</p><hr><ul><li><a href=\"https://www.lesswrong.com/posts/XqvnWFtRD2keJdwjX/the-useful-idea-of-truth#kvdGeJKRX36vy2D3A\"><strong>Meditation Answers</strong></a><strong> - </strong><i>(A central comment for readers who want to try answering the above meditation&nbsp;(before reading whatever post in the Sequence answers it) or read contributed answers.)</i></li><li><a href=\"https://www.lesswrong.com/posts/XqvnWFtRD2keJdwjX/the-useful-idea-of-truth#d3si8kHDkDapPyjLd\"><strong>Mainstream Status</strong></a><strong> - </strong><i>(A central comment where I say what I think the status of the post is relative to mainstream modern epistemology or other fields, and people can post summaries or excerpts of any papers they think are relevant.)</i></li></ul><p>&nbsp;</p><p>Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><i>Highly Advanced Epistemology 101 for Beginners</i></a></p><p>Next post: \"<a href=\"/lw/erp/skill_the_map_is_not_the_territory/\">Skill: The Map is Not the Territory</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PJKgSRkXkCqXmCk3M": 1, "cq69M9ceLNA35ShTR": 1, "aa3Qg7Qrp9LM7QMaz": 1, "LnEEs8xGooYmQ8iLA": 14, "wMPYFGmhcFg4bSb4Z": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XqvnWFtRD2keJdwjX", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 123, "baseScore": 148, "extendedScore": null, "score": 0.00032, "legacy": true, "legacyId": "19103", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4796c21df0a6970726273b4463edc825263ff28f8e3a497e.png", "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "skill-the-map-is-not-the-territory", "canonicalPrevPostSlug": "", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 148, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 541, "af": false, "version": "1.5.0", "pingbacks": {"Posts": ["CqyJzDZWvGhhFJ7dY", "wP2ymm44kZZwaFPYh", "WnheMGAka4fL99eae", "3XMwPNMSbaPm2suGz", "KJ9MFBPwXGwNpadf2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": "1.2.0", "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": 6, "afLastCommentedAt": "2018-09-30T02:08:36.064Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-02T18:50:16.067Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes October 2012", "slug": "rationality-quotes-october-2012", "viewCount": null, "lastCommentedAt": "2014-02-22T04:37:31.725Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bH5r7PR72C7H2N32k/rationality-quotes-october-2012", "pageUrlRelative": "/posts/bH5r7PR72C7H2N32k/rationality-quotes-october-2012", "linkUrl": "https://www.lesswrong.com/posts/bH5r7PR72C7H2N32k/rationality-quotes-october-2012", "postedAtFormatted": "Tuesday, October 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20October%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20October%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbH5r7PR72C7H2N32k%2Frationality-quotes-october-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20October%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbH5r7PR72C7H2N32k%2Frationality-quotes-october-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbH5r7PR72C7H2N32k%2Frationality-quotes-october-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>Here's the new thread for posting quotes, with the usual rules:</p>\n<ul>\n<li>Please post all quotes separately, so that they can be voted up/down separately. &nbsp;(If they are strongly related, reply to your own comments. &nbsp;If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself</li>\n<li>Do not quote comments/posts on LW/OB</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bH5r7PR72C7H2N32k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 16, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "19094", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 297, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-02T21:00:24.136Z", "modifiedAt": null, "url": null, "title": "When does something stop being a \u201cself-consistent idea\u201d and become scientific fact?", "slug": "when-does-something-stop-being-a-self-consistent-idea-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:01.339Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ancientcampus", "createdAt": "2011-11-09T20:20:29.779Z", "isAdmin": false, "displayName": "ancientcampus"}, "userId": "BGcNEeTWb7Qxcf5Pc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/53z6ZYJigbnCJcNdn/when-does-something-stop-being-a-self-consistent-idea-and", "pageUrlRelative": "/posts/53z6ZYJigbnCJcNdn/when-does-something-stop-being-a-self-consistent-idea-and", "linkUrl": "https://www.lesswrong.com/posts/53z6ZYJigbnCJcNdn/when-does-something-stop-being-a-self-consistent-idea-and", "postedAtFormatted": "Tuesday, October 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20When%20does%20something%20stop%20being%20a%20%E2%80%9Cself-consistent%20idea%E2%80%9D%20and%20become%20scientific%20fact%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhen%20does%20something%20stop%20being%20a%20%E2%80%9Cself-consistent%20idea%E2%80%9D%20and%20become%20scientific%20fact%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F53z6ZYJigbnCJcNdn%2Fwhen-does-something-stop-being-a-self-consistent-idea-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=When%20does%20something%20stop%20being%20a%20%E2%80%9Cself-consistent%20idea%E2%80%9D%20and%20become%20scientific%20fact%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F53z6ZYJigbnCJcNdn%2Fwhen-does-something-stop-being-a-self-consistent-idea-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F53z6ZYJigbnCJcNdn%2Fwhen-does-something-stop-being-a-self-consistent-idea-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 499, "htmlBody": "<p>\n<p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Topic: When does something stop being a &ldquo;useful theory&rdquo; and become something we can believe?&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">[Preface - Hi! This is essentially my first time posting here. If I did something wrong, let me know. I've read about 1/3-1/2 of the major sequences, but feel free to reference a specific article if you think it helps answer the question]</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">We see something in the world that appears mysterious to us, and we come up with an idea (&ldquo;idea X&rdquo;) that explains it. For X to be fact, it should:</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">1) be internally consistent with itself, as well as with all its implications. i.e. if X, then Y, and if Y, then Z. If we know Z to be obviously false, then we know X must be false.</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">2) be externally consistent with reality as we know it. We can&rsquo;t find something in reality that makes X clearly unture.</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">3) Explain things we have already observed (that&rsquo;s why we&rsquo;ve come up with idea X in the first place)</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">4) preferably, &ldquo;make beliefs pay rent&rdquo; &ndash; we should be able to use X to make predictions of the future, otherwise it doesn&rsquo;t hold much value.</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">(By the way, if you disagree with 4 let me know, I&rsquo;d love to hear your take. I&rsquo;m not being sarcastic. Truth is still truth, even if there isn't any utility in believing it.)</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">So, lets say we&rsquo;ve got a Generic Tribe of Primitive Peoples. They experience an earthquake. It&rsquo;s the first earthquake in 50 years. They ask Wise Old Jim what all the commotion was. Jim, who is 57 and the only one who&rsquo;s seen one before, thinks for a while, then says, &ldquo;I&rsquo;m not sure, but here&rsquo;s one possibility: That&rsquo;s George the Giant, who lives on the other side of the mountains, rolling over in his sleep. He does that occasionally.&rdquo;</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Now, this story about George is:</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">1) internally consistent (it makes sense that giants would sleep for a long time, and roll over occasionally.)</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">2) externally consistent (George is on the other side of the mountains, which is mysterious uncharted territory. He&rsquo;s heavy enough to rumble the earth all the way over here. No-one can think of anything they&rsquo;ve seen to make his existence unlikely.)</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">3) explains the earthquake</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">4) It lets us know that the world isn&rsquo;t ending, that nothing major has changed, and that this is a natural occurrence which will probably happen again years later. All of which are true.</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">[In this scenario, there is not in-reality a giant on the other side of the mountains, but they have no way of crossing the mountains to do the obvious empirical test to confirm it.]</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">So&hellip; what are these folk doing wrong? Is this theory supported and useful enough for them to accept it into their belief system? Should they develop some kind of falsifiable hypothesis? If so, what?</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Even if *you* can think of a solid test, what if the whole tribe tried to think of a test, but&nbsp;couldn't&nbsp;think of a good one that would confirm or reject the theory? Should they accept this theory or not?</p>\n</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "53z6ZYJigbnCJcNdn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 7, "extendedScore": null, "score": 1.0002318445577852e-06, "legacy": true, "legacyId": "19117", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-03T03:02:37.720Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Inner Goodness", "slug": "seq-rerun-inner-goodness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:01.084Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YaTzzH2gWfreMTfuS/seq-rerun-inner-goodness", "pageUrlRelative": "/posts/YaTzzH2gWfreMTfuS/seq-rerun-inner-goodness", "linkUrl": "https://www.lesswrong.com/posts/YaTzzH2gWfreMTfuS/seq-rerun-inner-goodness", "postedAtFormatted": "Wednesday, October 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Inner%20Goodness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Inner%20Goodness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYaTzzH2gWfreMTfuS%2Fseq-rerun-inner-goodness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Inner%20Goodness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYaTzzH2gWfreMTfuS%2Fseq-rerun-inner-goodness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYaTzzH2gWfreMTfuS%2Fseq-rerun-inner-goodness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<p>Today's post, <a href=\"/lw/v5/inner_goodness/\">Inner Goodness</a> was originally published on 23 October 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Goodness comes from inside you. Morality is not some sort of obligation weighing you down. It's something you want.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/eqm/seq_rerun_which_parts_are_me/\">Which Parts Are \"Me\"?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YaTzzH2gWfreMTfuS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.000422232117624e-06, "legacy": true, "legacyId": "19124", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EA39yRbhBbrccXnHi", "qkkFvMd6dZBrARPCF", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-03T08:26:09.910Z", "modifiedAt": null, "url": null, "title": "Meetup : Different location: Berkeley meetup", "slug": "meetup-different-location-berkeley-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bQ2CWfey4xxfCHFZy/meetup-different-location-berkeley-meetup", "pageUrlRelative": "/posts/bQ2CWfey4xxfCHFZy/meetup-different-location-berkeley-meetup", "linkUrl": "https://www.lesswrong.com/posts/bQ2CWfey4xxfCHFZy/meetup-different-location-berkeley-meetup", "postedAtFormatted": "Wednesday, October 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Different%20location%3A%20Berkeley%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Different%20location%3A%20Berkeley%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbQ2CWfey4xxfCHFZy%2Fmeetup-different-location-berkeley-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Different%20location%3A%20Berkeley%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbQ2CWfey4xxfCHFZy%2Fmeetup-different-location-berkeley-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbQ2CWfey4xxfCHFZy%2Fmeetup-different-location-berkeley-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ej'>Different location: Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 October 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Moffitt Library, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week Zendo is unavailable for a meetup. Instead we will meet at the Free Speech Cafe in Moffitt Library on campus. Wednesday, October 3, 7pm. They have sandwiches and outdoor seating.\nThere will not be a Berkeley meetup on Wednesday of next week. Instead there will be a meetup party on Thursday of next week.\nMap: <a href=\"http://goo.gl/maps/5nCVW\" rel=\"nofollow\">http://goo.gl/maps/5nCVW</a></p>\n\n<p>If you need help finding the place, call me at <a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ej'>Different location: Berkeley meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bQ2CWfey4xxfCHFZy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0005923382974538e-06, "legacy": true, "legacyId": "19133", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Different_location__Berkeley_meetup\">Discussion article for the meetup : <a href=\"/meetups/ej\">Different location: Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 October 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Moffitt Library, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week Zendo is unavailable for a meetup. Instead we will meet at the Free Speech Cafe in Moffitt Library on campus. Wednesday, October 3, 7pm. They have sandwiches and outdoor seating.\nThere will not be a Berkeley meetup on Wednesday of next week. Instead there will be a meetup party on Thursday of next week.\nMap: <a href=\"http://goo.gl/maps/5nCVW\" rel=\"nofollow\">http://goo.gl/maps/5nCVW</a></p>\n\n<p>If you need help finding the place, call me at <a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Different_location__Berkeley_meetup1\">Discussion article for the meetup : <a href=\"/meetups/ej\">Different location: Berkeley meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Different location: Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Different_location__Berkeley_meetup", "level": 1}, {"title": "Discussion article for the meetup : Different location: Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Different_location__Berkeley_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-03T12:18:10.154Z", "modifiedAt": null, "url": null, "title": "Taking \"correlation does not imply causation\" back from the internet", "slug": "taking-correlation-does-not-imply-causation-back-from-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:37.944Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sixes_and_sevens", "createdAt": "2009-11-11T14:42:23.502Z", "isAdmin": false, "displayName": "sixes_and_sevens"}, "userId": "n83meJ5yG2WQzygvw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5h4wMsndEcfxknu7K/taking-correlation-does-not-imply-causation-back-from-the", "pageUrlRelative": "/posts/5h4wMsndEcfxknu7K/taking-correlation-does-not-imply-causation-back-from-the", "linkUrl": "https://www.lesswrong.com/posts/5h4wMsndEcfxknu7K/taking-correlation-does-not-imply-causation-back-from-the", "postedAtFormatted": "Wednesday, October 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Taking%20%22correlation%20does%20not%20imply%20causation%22%20back%20from%20the%20internet&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATaking%20%22correlation%20does%20not%20imply%20causation%22%20back%20from%20the%20internet%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5h4wMsndEcfxknu7K%2Ftaking-correlation-does-not-imply-causation-back-from-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Taking%20%22correlation%20does%20not%20imply%20causation%22%20back%20from%20the%20internet%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5h4wMsndEcfxknu7K%2Ftaking-correlation-does-not-imply-causation-back-from-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5h4wMsndEcfxknu7K%2Ftaking-correlation-does-not-imply-causation-back-from-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 280, "htmlBody": "<p>(An idea I had while responding to <a href=\"/lw/eqe/rationality_quotes_october_2012/7jzf\">this quotes thread</a>)</p>\n<p>\"Correlation does not imply causation\" is bandied around inexpertly and inappropriately all over the internet. &nbsp;Lots of us hate this.</p>\n<p>But get this: the phrase, and the most obvious follow-up phrases like \"what does imply causation?\" are <a href=\"http://www.keywordspy.com/research/search.aspx?q=correlation%20does%20not%20imply%20causation&amp;type=keywords\">not high-competition search terms</a>. &nbsp;Up until about an hour ago, the domain name correlationdoesnotimplycausation.com was not taken. &nbsp;I have just bought it.</p>\n<p>There is a correlation-does-not-imply-causation shaped space on the internet, and it's ours for the taking. &nbsp;I would like to fill this space with a small collection of relevant educational resources explaining what is meant by the term, why it's important, why it's often used inappropriately, and the circumstances under which one may legitimately infer causation.</p>\n<p>At the moment the <a href=\"http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation\">Wikipedia page</a>&nbsp;is trying to do this, but it's not really optimised for the task. &nbsp;It also doesn't carry the undercurrent of \"no, seriously, <em>lots</em> of smart people get this wrong; let's make sure you're not one of them\", and I think it should.</p>\n<p>The purpose of this post is two-fold:</p>\n<p>Firstly, it lets me say \"hey dudes, I've just had this idea. &nbsp;Does anyone have any suggestions (pragmatic/technical, content-related, pointing out why it's a terrible idea, etc.), or alternatively, would anyone like to help?\"</p>\n<p>Secondly, it raises the question of what other corners of the internet are ripe for the planting of sanity waterline-raising resources. &nbsp;Are there any other similar concepts that people commonly get wrong, but don't have much of a guiding explanatory web presence to them? &nbsp;Could we put together a simple web platform for carrying out this task in lots of different places? &nbsp;The LW readership seems ideally placed to collectively do this sort of work.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MXcpQvaPGtXpB6vkM": 1, "cq69M9ceLNA35ShTR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5h4wMsndEcfxknu7K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 62, "extendedScore": null, "score": 0.000204, "legacy": true, "legacyId": "19134", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 70, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-03T13:43:58.406Z", "modifiedAt": null, "url": null, "title": "Parenting and Happiness", "slug": "parenting-and-happiness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:39.912Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/siB3rGkGmn46GFKQt/parenting-and-happiness", "pageUrlRelative": "/posts/siB3rGkGmn46GFKQt/parenting-and-happiness", "linkUrl": "https://www.lesswrong.com/posts/siB3rGkGmn46GFKQt/parenting-and-happiness", "postedAtFormatted": "Wednesday, October 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Parenting%20and%20Happiness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AParenting%20and%20Happiness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsiB3rGkGmn46GFKQt%2Fparenting-and-happiness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Parenting%20and%20Happiness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsiB3rGkGmn46GFKQt%2Fparenting-and-happiness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsiB3rGkGmn46GFKQt%2Fparenting-and-happiness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1490, "htmlBody": "<p>If you're trying to decide whether to have kids it would be helpful to know whether you should expect yourself to be happier childfree or as a parent.  The press reports it <a href=\"http://parenting.blogs.nytimes.com/2012/05/17/having-children-makes-you-relatively-happier/\">both</a> <a href=\"http://nymag.com/news/features/67024/\">ways</a>, passing on the results of conflicting studies.  I would like to find a good summary by someone who has read all the relevant research and understands which are the strong studies, but I've not found one. [1] Instead I've tried to read up on this enough that I can have an informed opinion, but there's enough research out there that I could be off.  So here's what I've learned, after reading lots of abstracts, the \"related work sections\" of several papers, and finding two papers that I think do about as well as you can.</p>\n<p>Summary: the research isn't that good, is all correlational, and how parenting affects your happiness varies widely by demographics (age, gender, income).  Neither a simple \"parenting makes people happy\" nor a \"parenting makes people miserable\" are justified.</p>\n<p>The most basic question in any research about happiness is \"how do you know how happy people are?\".  While it's unsatisfying, the best we can do really is just to ask.  How you ask matters, however: when parents are primed to think of their kids when asked about their happiness (\"How happy are you as a parent?\", \"Has having kids made you happier?\") they tend to say that their kids make them very happy.  Some of this is probably a warm glow that comes of thinking of your kids, however, and so by first bringing up this glow and then asking about happiness you get answers that are atypically positive.  Plus the social pressure to say you're happy as a parent. Studies that simply ask about happiness and parental status separately, as part of a broad survey, seem much more trustworthy to me because they should avoid this bias.</p>\n<p>So what do they ask?  I looked at two studies in depth that analyzed existing surveys. Those surveys asked:</p>\n<dl> <dt>GSS: General Social Survey (USA)</dt> <dd>\"Taken all together, how would you say things are these days?\"</dd> <dt>LSS: DDB Lifestyle Survey (USA)</dt> <dd>\"I am very satisfied with the way things are going in my life these days\"         (\"definitely agree, generally agree, moderately agree, moderately, disagree,         generally disagree, or definitely disagree\"). </dd><dt>SOEP: Socio-Economic Panel (Germany)</dt> <dd>\"How satisfied are you with your life, all things considered?\" [2]</dd> <dt>BHPS: British Household Panel Survey (UK)</dt> <dd>\"Have you recently been feeling reasonably happy, all things considered?\"</dd> </dl>\n<p>These questions aren't ideal; I'd rather we had some way to directly measure happiness. But given that we don't have that, knowing how likely each choice is to lead to me answering positively to the questions above is still useful.</p>\n<p>The second big question is how to deal with confounding factors.  Many studies simply gather a sample of parents and non-parents and ask them how happy they are.  Parents and non-parents differ in consistent ways, however. Two of the big ones are that parents are more likely to be married and have a lower household income (Herbst and Ifcher 2011). Marriage and money tend to both make people happier, so if you just compare parents to non-parents you're going to be misled in one direction by marriage and in the other by income.</p>\n<p>The most robust way to deal with this problem is a randomized controlled trial.  Then we can compare the happiness of the people assigned to have children with those assigned not to.  I've not found any studies doing this, probably because I can't see anyone agreeing to participate in one. [3] Some sort of natural experiment, like a factory that had a major accident sterilizing a shift's worth of workers, might provide a setting for good data, but even then you'd have to worry about the long-term effects on happiness of involuntary sterilization via industrial disaster.</p>\n<p>Which means we're left with studies that look at variation, trying to tease apart causation.  The standard way to do that is to look at parents and non-parents self-reported happiness and control for anything you suspect might be interfering.  In <a href=\"http://www.chrisherbst.net/files/Download/C._Herbst_Parents_SWB.pdf\">A Bundle of                                                                                             Joy: Does Parenting Really Make Us Miserable?</a> (2011), Chris Herbst and John Ifcher ran this on two very large US surveys, the General Social Survey (GSS) and DDB Lifestyle Survey (LSS).  They compared people living with children under 18 (parents) to everyone else (non-parents), which means they're more measuring the effect on happiness of active parenting, not of having had kids.  For example, grandparents would generally be in the 'non-parent' category, even if they're getting substantial happiness from their grandchildren which they wouldn't have if they'd not had kids.  Still, the effect just over the 18 years when a child is typically at home is still of interest.</p>\n<p>They found that as a whole parents were less happy, but also that they were more likely to be younger, married, lower-income, all of which have substantial effects on happiness. Adding controls, they found that the negative effect of parental status on happiness goes away if you control for age, gender, and race, and it becomes positive if you add income, employment, marital status, and education.  Whether it's reasonable to use income as a control, however, is not clear.  They found income was substantially lower among parents (GSS: $30,767 vs $38,959; LSS: $27,973 vs $39,445) but it's hard to say whether this is because richer people are less likely to have kids, stay-at-home parents give lower household income numbers, or something else.  There's also the question of whether there's anything non-demographic that might lead affect both happiness and the desire to have children.</p>\n<p>Mikko Myrskyla and Rachel Margolis try to get around this in <a href=\"http://www.demogr.mpg.de/papers/working/wp-2012-013.pdf\">Happiness: Before and After                                                                                         the Kids</a> (2012) by looking at individuals over a time window around having children. They used two surveys, one German (SOEP) and one British (BHPS) that tracked people for an average of ~11 years, looking at self-reported happiness among other things.  They limited their interpretation of the surveys to people who had no children at the beginning and then had their first child during the study period, which was 4,513 people for the SOEP and 2,689 people for the BHPS.  They found that people tended to see a temporary increase in happiness in the years before the birth of a child, but that afterwards they tend to drop back to their long-term average.  They made charts of this, broken down by several factors:</p>\n<p>&nbsp;</p>\n<p><strong>By age at birth of child</strong><br /> <img src=\"http://www.jefftk.com/parenting_happiness/age.png\" alt=\"\" /></p>\n<p>&nbsp;</p>\n<p><strong>By age at birth of child and gender, German (SOEP) only</strong><br /> <img src=\"http://www.jefftk.com/parenting_happiness/german_age_gender.png\" alt=\"\" /></p>\n<p>&nbsp;</p>\n<p><strong>By gender, with controls, German (SOEP) only</strong><br /> <img src=\"http://www.jefftk.com/parenting_happiness/german_gender_controls.png\" alt=\"\" /></p>\n<p>&nbsp;</p>\n<p><strong>By birth order, German (SOEP) only</strong><br /> <img src=\"http://www.jefftk.com/parenting_happiness/german_birth_order.png\" alt=\"\" /></p>\n<p>&nbsp;</p>\n<p><strong>By marital status and gender</strong><br /> <img src=\"http://www.jefftk.com/parenting_happiness/marital_status_gender.png\" alt=\"\" /></p>\n<p>I think the most straightforward interpretation of these charts is that having children doesn't have much of an effect on your happiness overall, but being about to have children does make you happier.  Another intepretation, however, would be that people often decide to have kids when they're at a point where their life is together and they're quite happy, and then when the child is born they're less happy.  Either way this and the finding that older parents are happier suggests a strategy: keep planning to have kids 'very soon', putting it off month after month until you're in your early 30s.</p>\n<p>With the finding that younger parents tend to be less happy, in addition to the explanation that older parents tend to have more resources I'm also curious whether younger parents are more likely to have children when they don't intend to.  Given that only people deciding to have kids face this question, I'd rather exclude from the sample all people who didn't make the choice.  Unfortunately I don't think \"was your first child an accident?\" is a standard survey question.</p>\n<p>Another issue is that because older people tend to report that they are happier, independent of children, I'd really like to see what these charts look like if you include non-parents.  You'd show on each chart a line representing the happiness of an age-matched sample of non-parents from the same study.  Without that I'm somewhat skeptical of the age-happiness pattern of the first chart.</p>\n<p>There's also the question of how applicable results from Germany and Britain are to people making this choice in the US.  In general the results from the German and British studies are close enough to each other, however, that I think they probably do generalize well to the US.</p>\n<p>(Prompted by a <a href=\"https://www.facebook.com/groups/utilitarianism/permalink/284308308339687\">discussion</a> in which people were asserting that \"there is a negative correlation between having kids and being happy\" and \"people usually think that having kids will make them happy, but studies show that this is a misconception\".)</p>\n<p><br /> [1] Know of one? Let me know!</p>\n<p>[2] This is a translation from the German.  The wording of happiness-related questions can be important when comparing between studies, so don't read too much into these words.</p>\n<p>[3] And if you did get some people to participate, I'd seriously worry about how representative they were of the general population.</p>\n<p><em><small>(I also posted this <a href=\"http://www.jefftk.com/news/2012-10-03\">on my blog</a>)</small></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 2, "Jzm2mYuuDBCNWq8hi": 1, "b7ZSAGimsbzrLR5CR": 8}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "siB3rGkGmn46GFKQt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 39, "extendedScore": null, "score": 8.4e-05, "legacy": true, "legacyId": "19135", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-03T16:09:15.196Z", "modifiedAt": null, "url": null, "title": "[Link] The real end of science", "slug": "link-the-real-end-of-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:31.725Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WBzd6thZHLvEdbzgt/link-the-real-end-of-science", "pageUrlRelative": "/posts/WBzd6thZHLvEdbzgt/link-the-real-end-of-science", "linkUrl": "https://www.lesswrong.com/posts/WBzd6thZHLvEdbzgt/link-the-real-end-of-science", "postedAtFormatted": "Wednesday, October 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20The%20real%20end%20of%20science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20The%20real%20end%20of%20science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWBzd6thZHLvEdbzgt%2Flink-the-real-end-of-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20The%20real%20end%20of%20science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWBzd6thZHLvEdbzgt%2Flink-the-real-end-of-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWBzd6thZHLvEdbzgt%2Flink-the-real-end-of-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 635, "htmlBody": "<p>From <a href=\"http://blogs.discovermagazine.com/gnxp/\">Gene Expression</a> by Razib Khan who some of you may also know from the <a href=\"http://www.gnxp.com/\">old gnxp site</a> or perhaps from his <a href=\"/lw/1qu/bhtv_eliezer_yudkowsky_razib_khan/\">BHTV debate with Eliezer</a>.</p>\n<blockquote>\n<h2><a title=\"Permanent Link: The real end of science\" rel=\"bookmark\" href=\"http://blogs.discovermagazine.com/gnxp/2012/10/the-real-end-of-science/\"></a></h2>\n<p><a href=\"http://blogs.discovermagazine.com/gnxp/files/2012/10/fraud.jpg\"><img class=\"alignleft size-full wp-image-18570\" title=\"fraud\" src=\"http://blogs.discovermagazine.com/gnxp/files/2012/10/fraud.jpg\" alt=\"\" width=\"368\" height=\"244\" /></a>Fifteen years ago John Horgan wrote <a href=\"http://www.amazon.com/exec/obidos/ASIN/0553061747/geneexpressio-20\">The End Of Science: Facing The Limits Of Knowledge In The Twilight Of The Scientific Age</a>. I remain skeptical as to the specific details of this book, but Carl&rsquo;s write-up in <a href=\"http://www.nytimes.com/2012/10/02/science/study-finds-fraud-is-widespread-in-retracted-scientific-papers.html\">The New York Times</a> of a new paper in <a href=\"http://www.pnas.org/content/early/2012/09/27/1212247109.abstract\">PNAS</a> on the relative commonness of scientific misconduct in cases of retraction <strong>makes me mull over the genuine possibility of the end of science as we know it</strong>. This sounds ridiculous on the face of it, but you have to understand my model of and framework for what science is. In short: <strong>science is people</strong>. I accept the reality that science existed in some form among strands of pre-Socratic thought, or among late antique and medieval Muslims and Christians (not to mention among some Chinese as well). Additionally, I can accept the cognitive model whereby science and scientific curiosity is rooted in our psychology in a very deep sense, so that even small children engage in theory-building.</p>\n<p>That is all well and good. The basic building blocks for many inventions and institutions existed long before their instantiation. But nevertheless the creation of institutions and inventions<strong> at a given moment is deeply contingent</strong>. Between 1600 and 1800 the culture of science as we know it emerged in the West. In the 19th and 20th centuries this culture became professionalized, but despite the explicit institutions and formal titles it is bound together by a common set of norms, an&nbsp;<em>ethos</em> if you will. Scientists work long hours for modest remuneration for the vain hope that they will grasp onto one fragment of reality, and pull it out of the darkness and declare to all, &ldquo;behold!&rdquo; That&rsquo;s a rather flowery way of putting the reality that the game is about fun &amp; fame. Most will not gain fame, but hopefully the fun will continue. Even if others may find one&rsquo;s interests abstruse or esoteric, it is a special thing to be paid to reflect upon and explore what one is interested in.</p>\n<p>Obviously this is an idealization. Science is a highly social and political enterprise, and injustice does occur. Merit and effort are not always rewarded, and on occasion machination truly pays. But overall the culture and enterprise muddle along, and are better in terms of yielding a better sense of reality as it is than its competitors. And yet all great things can end, and free-riders can destroy a system. If your rivals and competitors and cheat and getting ahead, what&rsquo;s to stop you but your own conscience? People will flinch from violating norms initially, even if those actions are in their own self-interest, but eventually they will break. And once they break the norms have shifted, and once a few break, the rest will follow. This is the logic which drives a vicious positive feedback loop, and individuals in their rational self-interest begin to cannibalize the components of the institutions which ideally would allow all to flourish. No one wants to be the last one in a collapsing building, the sucker who asserts that the structure will hold despite all evidence to the contrary.</p>\n<p>Deluded as most graduate students are, they by and large are driven by an ideal. Once the ideal, the illusion, is ripped apart, and eaten away from within, one can&rsquo;t rebuild it in a day. Trust evolves and accumulates it organically. One can not will it into existence. Centuries of capital are at stake, and it would be best to learn the lessons of history. We may declare that history has ended, but we can&rsquo;t unilaterally abolish eternal laws.</p>\n<p><strong>Update:</strong></p>\n<p><a href=\"http://blogs.discovermagazine.com/gnxp/files/2012/10/retrac2.jpg\"><img class=\"alignnone size-full wp-image-18595\" title=\"retrac2\" src=\"http://blogs.discovermagazine.com/gnxp/files/2012/10/retrac2.jpg\" alt=\"\" width=\"547\" height=\"245\" /></a></p>\n</blockquote>\n<p><a href=\"http://blogs.discovermagazine.com/gnxp/2012/10/the-real-end-of-science/\">Link</a> to original post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WBzd6thZHLvEdbzgt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 21, "extendedScore": null, "score": 5.4e-05, "legacy": true, "legacyId": "19136", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4gP3692vXuauKGjMR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-03T17:22:13.691Z", "modifiedAt": null, "url": null, "title": "[Link] Nobel laureate challenges psychologists to clean up their act", "slug": "link-nobel-laureate-challenges-psychologists-to-clean-up", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:03.870Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mstevens", "createdAt": "2009-05-07T14:46:38.489Z", "isAdmin": false, "displayName": "mstevens"}, "userId": "wgKFztEMLyjFRm4ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EnySijuuhT673KXWy/link-nobel-laureate-challenges-psychologists-to-clean-up", "pageUrlRelative": "/posts/EnySijuuhT673KXWy/link-nobel-laureate-challenges-psychologists-to-clean-up", "linkUrl": "https://www.lesswrong.com/posts/EnySijuuhT673KXWy/link-nobel-laureate-challenges-psychologists-to-clean-up", "postedAtFormatted": "Wednesday, October 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Nobel%20laureate%20challenges%20psychologists%20to%20clean%20up%20their%20act&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Nobel%20laureate%20challenges%20psychologists%20to%20clean%20up%20their%20act%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEnySijuuhT673KXWy%2Flink-nobel-laureate-challenges-psychologists-to-clean-up%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Nobel%20laureate%20challenges%20psychologists%20to%20clean%20up%20their%20act%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEnySijuuhT673KXWy%2Flink-nobel-laureate-challenges-psychologists-to-clean-up", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEnySijuuhT673KXWy%2Flink-nobel-laureate-challenges-psychologists-to-clean-up", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 136, "htmlBody": "<blockquote>\n<p><a href=\"http://www.nature.com/news/nobel-laureate-challenges-psychologists-to-clean-up-their-act-1.11535\">Nobel laureate challenges psychologists to clean up their act</a></p>\n<p>Nobel prize-winner Daniel Kahneman has issued a strongly worded call to one group of psychologists to restore the credibility of their field by creating a replication ring to check each others&rsquo; results.</p>\n<p>Kahneman, a psychologist at Princeton University in New Jersey, addressed his&nbsp;<a href=\"http://www.nature.com/polopoly_fs/7.6716.1349271308!/suppinfoFile/Kahneman%20Letter.pdf\">open e-mail</a> to researchers who work on social priming, the study of how subtle cues can unconsciously influence our thoughts or behaviour. For example, volunteers might walk more slowly down a corridor after seeing words related to old age<sup><a title=\"Bargh, J. A., Chen, M. &amp; Burrows, L. J. Pers. Soc. Psych. 71, 230&ndash;244 (1996).\" href=\"http://www.nature.com/news/nobel-laureate-challenges-psychologists-to-clean-up-their-act-1.11535#b1\">1</a></sup>, or fare better in general-knowledge tests after writing down the attributes of a typical professor<sup><a title=\"Dijksterhuis, A. &amp; van Knippenberg, A. J. Pers. Soc. Psych. 74, 865-877 (1998).\" href=\"http://www.nature.com/news/nobel-laureate-challenges-psychologists-to-clean-up-their-act-1.11535#b2\">2</a></sup>.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EnySijuuhT673KXWy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 22, "extendedScore": null, "score": 1.0008742976177194e-06, "legacy": true, "legacyId": "19137", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-03T19:17:53.201Z", "modifiedAt": null, "url": null, "title": "Cognitive bias and conservatism", "slug": "cognitive-bias-and-conservatism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:03.316Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Konradical", "createdAt": "2012-01-19T21:02:15.634Z", "isAdmin": false, "displayName": "Konradical"}, "userId": "26KFxvSgJ9WGfy4kW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EQkxiajFQjDijg5rg/cognitive-bias-and-conservatism", "pageUrlRelative": "/posts/EQkxiajFQjDijg5rg/cognitive-bias-and-conservatism", "linkUrl": "https://www.lesswrong.com/posts/EQkxiajFQjDijg5rg/cognitive-bias-and-conservatism", "postedAtFormatted": "Wednesday, October 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cognitive%20bias%20and%20conservatism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACognitive%20bias%20and%20conservatism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEQkxiajFQjDijg5rg%2Fcognitive-bias-and-conservatism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cognitive%20bias%20and%20conservatism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEQkxiajFQjDijg5rg%2Fcognitive-bias-and-conservatism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEQkxiajFQjDijg5rg%2Fcognitive-bias-and-conservatism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<p>Just a thought I had the other day; what do you think that the political ideas of conservatism have to do with cognitive bias? I mean, how much are people willing to change naturally, without arguing any points?<br />I know very little about all of these things, so forgive me if this is a silly thought.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EQkxiajFQjDijg5rg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": -31, "extendedScore": null, "score": -7.3e-05, "legacy": true, "legacyId": "19138", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-03T21:13:03.943Z", "modifiedAt": null, "url": null, "title": "[Link] Knowledge, not opinion, information extraction, not persuasion", "slug": "link-knowledge-not-opinion-information-extraction-not", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:02.079Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fNJsLACK4NEHyzvba/link-knowledge-not-opinion-information-extraction-not", "pageUrlRelative": "/posts/fNJsLACK4NEHyzvba/link-knowledge-not-opinion-information-extraction-not", "linkUrl": "https://www.lesswrong.com/posts/fNJsLACK4NEHyzvba/link-knowledge-not-opinion-information-extraction-not", "postedAtFormatted": "Wednesday, October 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Knowledge%2C%20not%20opinion%2C%20information%20extraction%2C%20not%20persuasion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Knowledge%2C%20not%20opinion%2C%20information%20extraction%2C%20not%20persuasion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfNJsLACK4NEHyzvba%2Flink-knowledge-not-opinion-information-extraction-not%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Knowledge%2C%20not%20opinion%2C%20information%20extraction%2C%20not%20persuasion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfNJsLACK4NEHyzvba%2Flink-knowledge-not-opinion-information-extraction-not", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfNJsLACK4NEHyzvba%2Flink-knowledge-not-opinion-information-extraction-not", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1165, "htmlBody": "<p>A <a href=\"http://blogs.discovermagazine.com/gnxp/2012/10/knowledge-not-opinion-information-extraction-not-persuasion/\">post</a> from <a href=\"http://blogs.discovermagazine.com/gnxp/\">Gene Expression</a> by Razib Khan who some of you may also know from the <a href=\"http://www.gnxp.com/\">old gnxp site</a> or perhaps from his <a href=\"/lw/1qu/bhtv_eliezer_yudkowsky_razib_khan/\">BHTV debate with Eliezer</a>. Some thoughts on the problem of trying to optimize your interactions to help you be less wrong. Your time is quite limited. Expect trade-offs.</p>\n<blockquote>\n<p><a href=\"http://blogs.discovermagazine.com/gnxp/files/2012/10/deathsignal.jpg\"><img class=\"alignleft size-medium wp-image-18545\" title=\"deathsignal\" src=\"http://blogs.discovermagazine.com/gnxp/files/2012/10/deathsignal-300x267.jpg\" alt=\"\" width=\"300\" height=\"267\" /></a></p>\n<p>A few days ago I was having drinks with some friends, and it came up that some of them had only recently become conscious of the fact that I leaned more toward the Republican party than the Democratic (I had remarked that my wife preferred that I keep my sideburns, as otherwise I would look too much like a Republican&hellip;though I sort of was one!). More shockingly for them was that I did not consider myself a liberal. I was somewhat bemused by the whole situation because it isn&rsquo;t as if I&rsquo;m particularly shy about expressing my various politically-incorrect opinions on any specific topic at work or play (these are people who I have met within the past ~2 years).</p>\n<p>I assume that the problem here is that I violated a cognitive <a href=\"http://en.wikipedia.org/wiki/Schema_%28psychology%29\">schema</a>: <strong>liberal people are smarter than conservative people.</strong> Since I was conservative, they were, logically, smarter than me. The reality is probably not so convenient for the theory in this case, generating some dissonance. In the course of conversation I expressed frankly what I actually do hold to be a rough &amp; ready approximation of my attitude toward discussion: <strong>I have almost no interest in <em>persuading</em> anyone of the truth of my particular views on any issue</strong>. This was relevant in that context because on occasion people try and draw me out as to the details of my disagreement with the consensus on an array of topics, when I often have no interest in expending the mental energy to do any such thing. It isn&rsquo;t that I&rsquo;m worried about getting into any argument with everyone else in the room. My friends are mostly natural scientists so I am very confident that I can alone hold my ground on any topic having to do with history and quantitative social science. Rather, the problem is my worry as to the point of it all. Who exactly is being edified by such exchanges? I never learn anything, as I am well acquainted with the standard arsenal of conventional Left-liberal talking points, while my interlocutors&nbsp;are often too amazed as my incomprehensible existence (i.e., not stupid, but not right-thinking) to really take in anything I&rsquo;m saying.</p>\n<p>Yet on a one-on-one basis I am much more likely to be open to a deep and thorough exchange. Why? The dynamic of&nbsp;<a href=\"http://en.wikipedia.org/wiki/Signalling_theory\">signalling</a>&nbsp;and group conformity is strongly dampened by removing third party observers from the interaction. With that tension removed I myself often feel less irritated if I have to invest a great deal of background information to make my own position clearer. Similarly, I often feel that my interlocutors&nbsp;are much less likely to trot out hackneyed and unpersuasive, but group approved, arguments.* There is quite often idiocy in crowds.</p>\n<p>Ultimately we have to take a step back and reflect on what the point of it all is. <strong>For me the answer is rather easy: the point of it all is to understand the shape of reality as best as I can</strong>. It is impossible to do such a thing sitting back in an armchair and reflecting as an individual. Learning is a social process. You need feedback from others, and you need to mine and cull appropriate data and analyses from those who are more well versed in a given topic than you are. This is not easy, and time is finite. Avoiding stupid people is easy. The more difficult trick, at least for me, is avoiding smart people who offer stupid opinions on topics with which they are absolutely unfamiliar.**&nbsp;<a href=\"http://rationalwiki.org/wiki/Engineers_and_woo#Religious_conservatism\">Creationist engineers</a> are classic cases of the power of ignorance in the hands of the intelligent.</p>\n<p>This brings me to learning more generally. Obviously I have no problem with people being autodidacts. Today the ability for one to be an autodidact has greatly expanded, but with power comes responsibility, and the necessity of prudence. I&rsquo;m speaking obviously about the internet. But now we have the rise of online education. Recently <a href=\"http://mruniversity.com/\">MRUniversity</a> opened, and <a href=\"http://www.khanacademy.org/\">Khan Academ</a>y is already rather famous. Tyler Cowen and Alex Taborrak&rsquo;s endeavor has already received <a href=\"http://www.tnr.com/blog/plank/107877/blog-hopes-its-new-online-course-will-be-more-marginal\">some praise</a>:</p>\n<address>MRU<strong> is ultimately aiming for a better&nbsp;<em>actual</em>&nbsp;education, not a better means of signaling.</strong> Cowen and Tabarrok are betting that there is an extraordinary amount of dead weight in current university classes (for example, on MRU the professor need not repeat himself as he inevitably must during live lectures, because if a student requires repetition, she can just watch the video again). &ldquo;You can think of this,&rdquo; Cowen says, laughing for the only time during our phone conversation and only lightly, &ldquo;as a marginal attempt&mdash;a marginal revolution, so to speak&mdash;to get education to be more about learning.&rdquo;</address>\n<p>I am moderately skeptical, but I also think such experiments are necessary. Over the long term it seems likely that new forms of educational delivery and assessment with replace the middle and lower tiers of American higher education, and modify even the elite levels. But I don&rsquo;t think we know yet what the exact nature of the information ecology is going to be.</p>\n<p>Here is what I&rsquo;d really like in the future:<strong> an app which analyzes someone&rsquo;s stream of assertions and immediately assesses whether they are full of crap or not</strong>.*** There are many domains where I can do this analysis myself, and know to tune someone out because I know they&rsquo;re signalling to ignorant people. But, there are many, many, more domains where I am ignorant and lost, and may fall prey to the bluffs and assertions of high caliber signalers, who have fashioned the&nbsp;<em>simulacrum&nbsp;</em>of intelligence. More concretely, people who are trying to impress without deep knowledge often fumble on many facts, something which could be run through an application such as <a href=\"http://www.wolframalpha.com/\">WolframAlpha</a>.</p>\n<p>Of course things have changed a great deal. Over the past few years smartphones have cast a pall over the skills of the professional bullshitter. I think that there has been a qualitative change for the better. &nbsp;Bullshitters known that they need to be cautious, so there is a preemptive effect.</p>\n<p>* I am never in social circumstances where the political context is conservative.</p>\n<p>** You also need to avoid socializing only with your own ideological set. This is easy for me since I don&rsquo;t socialize with anyone who shares my politics or metaphysical opinions.</p>\n<p>*** Looking things up manually is time consuming.</p>\n</blockquote>\n<p>Debating group consensus with the group is less productive than debating it with individuals making up that group. Avoiding smart people who offer stupid opinions on topics with which they are absolutely unfamiliar is expensive<em></em>. The internet has made this somewhat harder. We should like make an app to fix this or something.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fNJsLACK4NEHyzvba", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 5, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "19139", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4gP3692vXuauKGjMR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-04T01:57:40.791Z", "modifiedAt": null, "url": null, "title": "Fun fact: in medical school, we had a mini-lesson on common cognitive errors in medicine", "slug": "fun-fact-in-medical-school-we-had-a-mini-lesson-on-common", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:04.151Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ancientcampus", "createdAt": "2011-11-09T20:20:29.779Z", "isAdmin": false, "displayName": "ancientcampus"}, "userId": "BGcNEeTWb7Qxcf5Pc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bKMfJArJPGDG5bBrp/fun-fact-in-medical-school-we-had-a-mini-lesson-on-common", "pageUrlRelative": "/posts/bKMfJArJPGDG5bBrp/fun-fact-in-medical-school-we-had-a-mini-lesson-on-common", "linkUrl": "https://www.lesswrong.com/posts/bKMfJArJPGDG5bBrp/fun-fact-in-medical-school-we-had-a-mini-lesson-on-common", "postedAtFormatted": "Thursday, October 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fun%20fact%3A%20in%20medical%20school%2C%20we%20had%20a%20mini-lesson%20on%20common%20cognitive%20errors%20in%20medicine&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFun%20fact%3A%20in%20medical%20school%2C%20we%20had%20a%20mini-lesson%20on%20common%20cognitive%20errors%20in%20medicine%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbKMfJArJPGDG5bBrp%2Ffun-fact-in-medical-school-we-had-a-mini-lesson-on-common%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fun%20fact%3A%20in%20medical%20school%2C%20we%20had%20a%20mini-lesson%20on%20common%20cognitive%20errors%20in%20medicine%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbKMfJArJPGDG5bBrp%2Ffun-fact-in-medical-school-we-had-a-mini-lesson-on-common", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbKMfJArJPGDG5bBrp%2Ffun-fact-in-medical-school-we-had-a-mini-lesson-on-common", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p>Yesterday in medical school, we had a lecture on common mistakes doctors make. I saw this slide:</p>\n<p>Attribution Errors</p>\n<p>Confirmation Bias</p>\n<p>Commission Bias</p>\n<p>Omission Bias</p>\n<p>Anchoring</p>\n<div><br /></div>\n<div>-which made me smile, and thought I'd share.&nbsp;I'm interested in Emergency Medicine, and I realize that some of these are a real problem in that field. I.e. I know one doctor who had a typical&nbsp;Friday&nbsp;night - most of the patients are super drunk. One 30-year-old girl comes in disoriented, off-balance, with slurred speech. They suggest she sleep it off.&nbsp;Unfortunately, this particular patient was having a stroke, and died. So yeah. Avoiding biases.</div>\n<div><br /></div>\n<div>(If anyone cares - Anchoring is basically the bias towards your first guess, or in this case, diagnosis, aka \"we change our minds less likely than we think\". Commission and Omission bias are, in this case, the tendency to act (in order to help) when you perhaps shouldn't, and the the tendency to inaction (to avoid harm) when you should act, respectively.)</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bKMfJArJPGDG5bBrp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 21, "extendedScore": null, "score": 1.0011455483654696e-06, "legacy": true, "legacyId": "19143", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-04T03:19:25.460Z", "modifiedAt": null, "url": null, "title": "Deliberate Practice for Decision Making", "slug": "deliberate-practice-for-decision-making", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:04.822Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wattsd", "createdAt": "2011-06-03T21:18:16.454Z", "isAdmin": false, "displayName": "wattsd"}, "userId": "5orQJ57owKabeCLJd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ERYTFGfPfcjRxhkba/deliberate-practice-for-decision-making", "pageUrlRelative": "/posts/ERYTFGfPfcjRxhkba/deliberate-practice-for-decision-making", "linkUrl": "https://www.lesswrong.com/posts/ERYTFGfPfcjRxhkba/deliberate-practice-for-decision-making", "postedAtFormatted": "Thursday, October 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Deliberate%20Practice%20for%20Decision%20Making&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADeliberate%20Practice%20for%20Decision%20Making%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FERYTFGfPfcjRxhkba%2Fdeliberate-practice-for-decision-making%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Deliberate%20Practice%20for%20Decision%20Making%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FERYTFGfPfcjRxhkba%2Fdeliberate-practice-for-decision-making", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FERYTFGfPfcjRxhkba%2Fdeliberate-practice-for-decision-making", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1463, "htmlBody": "<p><em>Much of this material is sourced/summarized from <a href=\"http://peterfadde.com/Research/Deliberate_Performance-PI-1011.pdf\"><span class=\"st\">Deliberate Performance: Accelerating Expertise in Natural Settings</span></a> and &ldquo;<a href=\"http://www.amazon.com/Power-Intuition-Feelings-Better-Decisions/dp/0385502893\">The Power of Intuition</a>&rdquo;. Both contain much more than what is written here. An earlier draft/version of this was posted as \"<a href=\"/lw/e93/draft_productive_use_of_heuristics_and_biases/\">Productive Use of Heuristics and Biases</a>\". Based on feedback in the comments, much has changed. Feedback is appreciated, as are any other ideas on how deliberate practice can be applied. <br /></em></p>\n<p>How does someone incorporate something like deliberate practice into a typical job and everyday life? Deliberate practice is meant to be challenging, and because of this, it is draining. Much of the expertise literature describes a limit of a few hours of deliberate practice per day. If you can sustain more than that, you probably aren&rsquo;t doing it right.<br /><br />While the amount you can put in per day is limited, it still takes many hours of this intense practice, so reaching expert performance in a domain takes anywhere from years to decades. Deliberate performance is a concept related to deliberate practice, but perhaps more effective and efficient. Rather than separating practice and performance, the idea is to overlap the two as much as possible. The primary aim is to accelerate the development of expertise, while also improving the productivity of practice. Since you are practicing using normal tasks, you don&rsquo;t have to set as much time aside where you aren&rsquo;t working.<br /><br />Deliberate performance is readily applicable to decision making. You make decisions as you normally would but also record your expectations and thinking behind the decision. What do you think will happen and why? How do you feel about the decision? When you have feedback on how the decision turned out, you can go back to see what you were thinking and how well your expectations matched reality. The idea of recording this information has been recommended by <a href=\"http://hbr.org/2005/01/managing-oneself/ar/1\">Peter Drucker</a>, and more recently by <a title=\"Daniel Kahneman\" href=\"http://www.dailyfinance.com/2012/09/19/a-foolish-interview-with-michael-mauboussin/\">Daniel Kahneman</a> (Kahneman sees it as a way to reduce hindsight bias).<br /><br />The term deliberate performance comes from the paper <a href=\"http://peterfadde.com/Research/Deliberate_Performance-PI-1011.pdf\"><span class=\"st\">Deliberate Performance: Accelerating Expertise in Natural Settings</span></a>, where the concept is elaborated into four exercises:<br /><br /><strong>Estimate</strong><br />One of the simplest ways to do this is to estimate how long a task you have will take. Accurately predicting how long things will take is a valuable skill on its own, but in estimating it, you form a mental model of the task and where the obstacles lie. When your estimate differs from reality, you can go back and see where you went wrong.<br /><br /><strong>Experiment</strong><br />The simplest way to experiment is just to try something and see what happens. This can be helpful in early stages when you are exploring, but typically, a better way is explicitly state what you expect to occur. As in estimation, if the actual results aren&rsquo;t what you expect, you can update your mental models. It is usually a good idea to make your experiments cheap, in the sense that you expect failure and change to occur. Steven Spear, in his studies of Toyota, described cheap experiments as preferring bolting to welding, clamping to bolting, taping to clamping, and holding to taping. The more rapidly you can experiment, the more rapidly you can get feedback and update your models.<br /><br />Experimentation is sort of where the rubber meets the road in deliberate performance. Where the other exercises are more focused on thought, experimentation is more about action. You see how much your mental models match the real world.<br /><br /><strong>Explain</strong><br />Why did things happen as they did? Try to explain from the available evidence. Explanation gives you an opportunity to make sense of why things turned out the way they did. Whether or not things went as you expected, try to explain why and how it happened. You can also seek out explanations from others, but you should probably form your own first so you can compare.<br /><br /><strong>Extrapolate</strong><br />What do you know now that you can use to reason about something you don&rsquo;t know or something that doesn&rsquo;t yet exist? What do you already know that you can use to solve a new problem? Using induction in proofs is an example of this, as is analogical reasoning. Klein describes an example of the latter in &ldquo;Sources of Power&rdquo;. Engineers are trying to figure out how much parts of new airplane designs will cost, but the parts are still being developed. The engineers end up looking at existing parts for analogs. None of the analogs are perfect matches, so they extrapolate the characteristics of multiple parts to create a new &ldquo;Frankenstein&rdquo; part.<br /><strong><br />Feedback and Coaching</strong><br />Unlike deliberate practice, where the role of a coach is more formal and explicit, deliberate performance has the more realistic expectation that you will not have a coach. Instead, you try to make the most of the feedback you can get. Rather than a coach pointing out mistakes and what you can improve, you find errors in your mental models and ask effective questions. The 4Es will likely prompt some questions, while also giving you some knowledge and experience to make sense of the answers. While the exercises probably sound simplistic, something important happens as you do them: you are forced to form mental models, compare them to reality, and adjust them as needed.<br /><br />Another useful method for getting feedback is to predict or emulate what an expert would do. When things don&rsquo;t go as expected, you can then explain what you did and your reasoning to the expert and get feedback. As you find and correct mistakes in your mental models, your &ldquo;expert emulation&rdquo; should improve. Seeking explanations to compare to your own is also an example of this.<br /><br /><strong>Using the Exercises</strong><br />The first example, keeping a decision journal and reflecting on how things turns out, demonstrates that the 4Es can be combined and work well together. In fact, that is probably the best way to use them, looping through them multiple times. Similar to the scientific method, start with an estimate or hypothesis. Then, experiment to see if your hypothesis is correct. Next, attempt to explain what you saw in the results of the experiment in relation to your hypothesis. You&rsquo;d then finish the first loop by extrapolating from the explanation, which can provide a new hypothesis to test.<br /><br />One other method that combines multiple exercises is to do what Klein calls a premortem. If you have some large project, start by imagining that what you are trying to do has been a complete failure, and you are now doing a post mortem to understand what went wrong. Assuming things have gone wrong, why did they go wrong? The reasoning behind this technique is that people don&rsquo;t want something to fail (or look like they want it to fail), but assuming it already has failed reduces that bias. In a sense, this is similar to <a href=\"http://en.wikipedia.org/wiki/Red_team\">red teams</a> but easier to implement and less resource intensive. This gives you a chance to extrapolate and estimate, then form an explanation for why you think things might go wrong.<br /><br /><strong>A few more exercises specific to decision making</strong><br />In &ldquo;The Power of Intuition&rdquo;, Klein advocates identifying decisions where problems have occurred, forming exercises based on those decisions, playing through the exercises, and then critiquing your decision making process. The idea is that the most difficult scenarios occur rarely enough that developing expertise in them is impossible without some intervention, they occur too rarely to gain significant experience.<br /><br />First, identify a decision. When reviewing the decisions, note what makes it difficult, what kinds of errors are often made, how an expert might approach it differently than a novice, and how the decision can be practiced and how you can get feedback.<br /><br />Identified decisions can then be turned into scenarios which can be repeatedly practiced (typically in groups). Start with describing the events that led to the decision. The players are then told what they are trying to achieve, the context, and the constraints. Try to include a visual representation whenever possible. Even if you don&rsquo;t work through the exercise, forming it can be useful in and of itself.<br /><br />After the exercise, critique the decision and the process used to make it. Start with a timeline and identify key judgments. For each of the key judgments, note why it was difficult, how you were interpreting the situation, what cues/patterns you should have been picking up, why you chose to do what you did, and what you would&rsquo;ve done differently with the benefit of hindsight.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ux2x9RrJsuykQxT79": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ERYTFGfPfcjRxhkba", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 29, "extendedScore": null, "score": 6.4e-05, "legacy": true, "legacyId": "19125", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4GsrkRKCjn5ufhxkF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-04T04:28:38.780Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Expected Creative Surprises", "slug": "seq-rerun-expected-creative-surprises", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/orBNBQg5iEoACtpvo/seq-rerun-expected-creative-surprises", "pageUrlRelative": "/posts/orBNBQg5iEoACtpvo/seq-rerun-expected-creative-surprises", "linkUrl": "https://www.lesswrong.com/posts/orBNBQg5iEoACtpvo/seq-rerun-expected-creative-surprises", "postedAtFormatted": "Thursday, October 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Expected%20Creative%20Surprises&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Expected%20Creative%20Surprises%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2ForBNBQg5iEoACtpvo%2Fseq-rerun-expected-creative-surprises%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Expected%20Creative%20Surprises%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2ForBNBQg5iEoACtpvo%2Fseq-rerun-expected-creative-surprises", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2ForBNBQg5iEoACtpvo%2Fseq-rerun-expected-creative-surprises", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p>Today's post, <a href=\"/lw/v7/expected_creative_surprises/\">Expected Creative Surprises</a> was originally published on 24 October 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Expected_Creative_Surprises\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The unpredictability of intelligence is a very special and unusual kind of surprise, which is not at all like noise or randomness. There is a weird balance between the unpredictability of actions and the predictability of outcomes.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/er8/seq_rerun_inner_goodness/\">Inner Goodness</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "orBNBQg5iEoACtpvo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0012250174338495e-06, "legacy": true, "legacyId": "19144", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rEDpaTTEzhPLz4fHh", "YaTzzH2gWfreMTfuS", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-04T06:34:33.842Z", "modifiedAt": null, "url": null, "title": "[LINK] Different heuristics at different decision speeds", "slug": "link-different-heuristics-at-different-decision-speeds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:01.335Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kawoomba", "createdAt": "2012-05-01T11:54:25.423Z", "isAdmin": false, "displayName": "Kawoomba"}, "userId": "FScr44PGNPbBCodRv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WFnTGaMn4uMae8FwL/link-different-heuristics-at-different-decision-speeds", "pageUrlRelative": "/posts/WFnTGaMn4uMae8FwL/link-different-heuristics-at-different-decision-speeds", "linkUrl": "https://www.lesswrong.com/posts/WFnTGaMn4uMae8FwL/link-different-heuristics-at-different-decision-speeds", "postedAtFormatted": "Thursday, October 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Different%20heuristics%20at%20different%20decision%20speeds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Different%20heuristics%20at%20different%20decision%20speeds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWFnTGaMn4uMae8FwL%2Flink-different-heuristics-at-different-decision-speeds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Different%20heuristics%20at%20different%20decision%20speeds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWFnTGaMn4uMae8FwL%2Flink-different-heuristics-at-different-decision-speeds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWFnTGaMn4uMae8FwL%2Flink-different-heuristics-at-different-decision-speeds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<div>The \"intuitive\" (fast) interaction heuristic is predisposed towards selflessness, while the \"calculated\" (slow) decision making process favors greedier behavior: review of game studies.</div>\n<div><br /></div>\n<div><a title=\"[Abstract of nature paper].\" href=\"http://www.ncbi.nlm.nih.gov/pubmed/22996558\" target=\"_blank\">http://www.ncbi.nlm.nih.gov/pubmed/22996558</a></div>\n<div><br /></div>\n<div>Sidenote: Strange to have a <em>Nature</em> paper conflate a faster-payoff / 'egoistic' approach with being 'rational', and to contrast intuition with calculation, of all things. As if not being consciously aware of the calculation implicit in fast \"intuitive\" decision algorithms implied a lack of cognitive processing.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WFnTGaMn4uMae8FwL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 1.0012913091294541e-06, "legacy": true, "legacyId": "19155", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-05T02:01:48.605Z", "modifiedAt": null, "url": null, "title": "Walking and Chewing Gum", "slug": "walking-and-chewing-gum", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:02.966Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MaoShan", "createdAt": "2010-08-08T15:16:25.180Z", "isAdmin": false, "displayName": "MaoShan"}, "userId": "HW6b3PceS2S9vLCvP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q9sBEE9JCc5TFoGvX/walking-and-chewing-gum", "pageUrlRelative": "/posts/q9sBEE9JCc5TFoGvX/walking-and-chewing-gum", "linkUrl": "https://www.lesswrong.com/posts/q9sBEE9JCc5TFoGvX/walking-and-chewing-gum", "postedAtFormatted": "Friday, October 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Walking%20and%20Chewing%20Gum&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWalking%20and%20Chewing%20Gum%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq9sBEE9JCc5TFoGvX%2Fwalking-and-chewing-gum%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Walking%20and%20Chewing%20Gum%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq9sBEE9JCc5TFoGvX%2Fwalking-and-chewing-gum", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq9sBEE9JCc5TFoGvX%2Fwalking-and-chewing-gum", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<p>I personally have a tough time filtering out inputs, and find my attention forcibly split between paying attention to nearly everything whether I want to or not. This makes me act and plan more linearly than is optimal, and is horrible for crowded rooms, but otherwise doesn't disable me. Also, I have a strong dislike of the action of chewing gum. I was recently joking about not being able to walk and chew gum at the same time, and I wonder if it might actually have some truth to it? I would like to know everyone's (but especially people with similar filtering issues) habits regarding gum.</p>\n<p>How often do you chew gum?</p>\n<p>(The poll is in the first comment.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q9sBEE9JCc5TFoGvX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -14, "extendedScore": null, "score": 1.0019061988613566e-06, "legacy": true, "legacyId": "19160", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-05T04:55:13.599Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Belief in Intelligence", "slug": "seq-rerun-belief-in-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:02.238Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ktd3qtF7Nqo2Nhtbn/seq-rerun-belief-in-intelligence", "pageUrlRelative": "/posts/Ktd3qtF7Nqo2Nhtbn/seq-rerun-belief-in-intelligence", "linkUrl": "https://www.lesswrong.com/posts/Ktd3qtF7Nqo2Nhtbn/seq-rerun-belief-in-intelligence", "postedAtFormatted": "Friday, October 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Belief%20in%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Belief%20in%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKtd3qtF7Nqo2Nhtbn%2Fseq-rerun-belief-in-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Belief%20in%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKtd3qtF7Nqo2Nhtbn%2Fseq-rerun-belief-in-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKtd3qtF7Nqo2Nhtbn%2Fseq-rerun-belief-in-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>Today's post, <a href=\"/lw/v8/belief_in_intelligence/\">Belief in Intelligence</a> was originally published on 25 October 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Belief_in_Intelligence\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>What does a belief that an agent is intelligent look like? What predictions does it make?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ers/seq_rerun_expected_creative_surprises/\">Expected Creative Surprises</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ktd3qtF7Nqo2Nhtbn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0019976094074125e-06, "legacy": true, "legacyId": "19161", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HktFCy6dgsqJ9WPpX", "orBNBQg5iEoACtpvo", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-05T07:23:56.318Z", "modifiedAt": null, "url": null, "title": "\"Hide comments in downvoted threads\" is now active", "slug": "hide-comments-in-downvoted-threads-is-now-active", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:06.947Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XrLvxW7Wjo6b9rRCT/hide-comments-in-downvoted-threads-is-now-active", "pageUrlRelative": "/posts/XrLvxW7Wjo6b9rRCT/hide-comments-in-downvoted-threads-is-now-active", "linkUrl": "https://www.lesswrong.com/posts/XrLvxW7Wjo6b9rRCT/hide-comments-in-downvoted-threads-is-now-active", "postedAtFormatted": "Friday, October 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Hide%20comments%20in%20downvoted%20threads%22%20is%20now%20active&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Hide%20comments%20in%20downvoted%20threads%22%20is%20now%20active%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXrLvxW7Wjo6b9rRCT%2Fhide-comments-in-downvoted-threads-is-now-active%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Hide%20comments%20in%20downvoted%20threads%22%20is%20now%20active%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXrLvxW7Wjo6b9rRCT%2Fhide-comments-in-downvoted-threads-is-now-active", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXrLvxW7Wjo6b9rRCT%2Fhide-comments-in-downvoted-threads-is-now-active", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 91, "htmlBody": "<p>I just found out that <a href=\"http://code.google.com/p/lesswrong/issues/detail?id=345\">a new website feature</a>&nbsp;was implemented 2 days ago. If a comment is voted to -4 or below, it and all replies and downstream comments from it will be hidden from Recent Comments, and further replies in that subthread will incur 5 karma points penalty. The hiding, but not karma penalty, applies retroactively to comments in that subthread posted before the -4 vote.</p>\n<p>This seems to be worth a discussion post since most people are probably still voting things to below -3 without knowing the new&nbsp;consequences&nbsp;of doing so.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XrLvxW7Wjo6b9rRCT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 27, "extendedScore": null, "score": 6.9e-05, "legacy": true, "legacyId": "19173", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-05T12:02:26.775Z", "modifiedAt": null, "url": null, "title": "Hangman as analogy for Natural Selection", "slug": "hangman-as-analogy-for-natural-selection-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:36.377Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Delta", "createdAt": "2012-08-01T11:37:41.645Z", "isAdmin": false, "displayName": "Delta"}, "userId": "DT22husLNHbuy8TaZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ya2XmTJ2aQfPXtRJS/hangman-as-analogy-for-natural-selection-0", "pageUrlRelative": "/posts/ya2XmTJ2aQfPXtRJS/hangman-as-analogy-for-natural-selection-0", "linkUrl": "https://www.lesswrong.com/posts/ya2XmTJ2aQfPXtRJS/hangman-as-analogy-for-natural-selection-0", "postedAtFormatted": "Friday, October 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hangman%20as%20analogy%20for%20Natural%20Selection&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHangman%20as%20analogy%20for%20Natural%20Selection%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fya2XmTJ2aQfPXtRJS%2Fhangman-as-analogy-for-natural-selection-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hangman%20as%20analogy%20for%20Natural%20Selection%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fya2XmTJ2aQfPXtRJS%2Fhangman-as-analogy-for-natural-selection-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fya2XmTJ2aQfPXtRJS%2Fhangman-as-analogy-for-natural-selection-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 489, "htmlBody": "<p>Hi guys,</p>\n<p>I was trying to come up with a helpful analogy to help explain natural selection in simple terms and it occurred to me that the game Hangman might make a useful analogy, albeit an imperfect and simplified one. I'd be interested to hear your thoughts on this and any other useful analogies or strategies for explaining in simple terms how natural selection allows complexity to arise from simplicity and how it is distinct from random chance.</p>\n<p>The Hangman analogy I propose would read as follows:</p>\n<blockquote>\n<p>A long word is chosen, say with a dozen letters, and a dozen blanks are drawn on the paper. Person A then guesses a letter. If the letter is present in the word a blank is filled in and the player can try another letter and so on. Their further guesses will be informed by the letters they have already discovered rather than being completely random. If the letter is not present the player loses a life (represented by the drawing of part of the gallows). If they run out of lives the game is over and a new player, Person B takes their place. Person B must start from the beginning.</p>\n<p>In this analogy the long word is a complex adaption, requiring many seperate chance mutations to build it. Each guessed letter is a chance mutation that can be beneficial (correct answers bring you closer) or detrimental (wrong ones cost you lives). The loss of all lives represents the extinction of the species, meaning no further mutations can occur. Person B is an entirely different species that can't \"compare notes\" with Person A and hence must start from the beginning (though they may take a different route).</p>\n</blockquote>\n<p>The benefit of this analogy is it's an example of random guesses still having a sense of forward progression (discovered letters are not removed, and gradually build up), and that it refers to a simple game I think most people will be familiar with. You could then go on to explain how a complex adaption takes many more than a dozen steps, that there are many more than 24 possible mutations, and that each guess takes many generations, to give a sense of the timescales involved.</p>\n<p>The weaknesses are considerable and include the inability to go backwards (beneficial changes can be lost as well as gained) and the existence of a single specific end goal (the unknown word), rather than this being a continual process without set targets. It also ignores the possibility that a beneficial mutation does not spread throughout the species.</p>\n<p>I very much doubt this is an original suggestion, but it seemed a handy simplification of the \"password-guessing\" analogy I was just reading about in Dawkins' \"The Blind Watchmaker\". Any comments or alternative methods would be welcome (I'm still not very widely read on the subject of evolution so I'm sure others have put it more clearly than I could).</p>\n<p>Thanks for your time.</p>\n<p><br />David</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ya2XmTJ2aQfPXtRJS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 2, "extendedScore": null, "score": 1.0022228661435613e-06, "legacy": true, "legacyId": "19176", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-05T14:46:39.666Z", "modifiedAt": null, "url": null, "title": "We won't be able to recognise the human G\u00f6del sentence", "slug": "we-won-t-be-able-to-recognise-the-human-goedel-sentence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:35.933Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hjSSSJdLEt8enediP/we-won-t-be-able-to-recognise-the-human-goedel-sentence", "pageUrlRelative": "/posts/hjSSSJdLEt8enediP/we-won-t-be-able-to-recognise-the-human-goedel-sentence", "linkUrl": "https://www.lesswrong.com/posts/hjSSSJdLEt8enediP/we-won-t-be-able-to-recognise-the-human-goedel-sentence", "postedAtFormatted": "Friday, October 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20We%20won't%20be%20able%20to%20recognise%20the%20human%20G%C3%B6del%20sentence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWe%20won't%20be%20able%20to%20recognise%20the%20human%20G%C3%B6del%20sentence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhjSSSJdLEt8enediP%2Fwe-won-t-be-able-to-recognise-the-human-goedel-sentence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=We%20won't%20be%20able%20to%20recognise%20the%20human%20G%C3%B6del%20sentence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhjSSSJdLEt8enediP%2Fwe-won-t-be-able-to-recognise-the-human-goedel-sentence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhjSSSJdLEt8enediP%2Fwe-won-t-be-able-to-recognise-the-human-goedel-sentence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<p>Building on the very bad G&ouml;del <a href=\"/lw/e3m/the_weakest_arguments_for_and_against_human_level/\">anti-AI argument</a> (computers's are formal and can't prove their own <a href=\"/lw/93q/completeness_incompleteness_and_what_it_all_means/\">G&ouml;del sentence</a>, hence no AI), it occurred to me that you could make a strong case that humans could never recognise a human&nbsp;G&ouml;del sentence. The argument goes like this:</p>\n<ol>\n<li>Humans have a meta-proof that all&nbsp;G&ouml;del sentences are true.</li>\n<li>If humans could recognise a human G&ouml;del sentence G as being a&nbsp;G&ouml;del sentence, we would therefore prove it was true.</li>\n<li>This contradicts the definition of G, which humans should never be able to prove.</li>\n<li>Hence humans could never recognise that G was a human&nbsp;G&ouml;del sentence.</li>\n</ol>\n<p>Now, the more usual way of dealing with human&nbsp;G&ouml;del sentences&nbsp;is to say that humans are inconsistent, but that the inconsistency doesn't blow up our reasoning system because we use something akin to <a href=\"/lw/bt8/if_you_cant_be_right_at_least_be_relevant/\">relevance logic</a>.</p>\n<p>But, if we do assume humans are consistent (or can become consistent), then it does seem we will never&nbsp;knowingly&nbsp;encounter our own&nbsp;G&ouml;del sentences. As to where this G could hide and we could never find it? My guess would be somewhere in the larger&nbsp;<a href=\"http://en.wikipedia.org/wiki/Ordinal_number\">ordinals</a>, up where our understanding starts to get <a href=\"http://en.wikipedia.org/wiki/Large_countable_ordinal\">flaky</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AJDHQ4mFnsNbBzPhT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hjSSSJdLEt8enediP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 1.0023094743534791e-06, "legacy": true, "legacyId": "19177", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QSnZM5JZxCboMh8Qf", "MLqhJ8eDy5smbtGrf", "oLQamyNmzsgPs8nE6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-05T15:07:29.178Z", "modifiedAt": null, "url": null, "title": "[Link] Technology will destroy human nature", "slug": "link-technology-will-destroy-human-nature", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:05.418Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F3cQ6zJYiGFy2FpJP/link-technology-will-destroy-human-nature", "pageUrlRelative": "/posts/F3cQ6zJYiGFy2FpJP/link-technology-will-destroy-human-nature", "linkUrl": "https://www.lesswrong.com/posts/F3cQ6zJYiGFy2FpJP/link-technology-will-destroy-human-nature", "postedAtFormatted": "Friday, October 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Technology%20will%20destroy%20human%20nature&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Technology%20will%20destroy%20human%20nature%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF3cQ6zJYiGFy2FpJP%2Flink-technology-will-destroy-human-nature%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Technology%20will%20destroy%20human%20nature%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF3cQ6zJYiGFy2FpJP%2Flink-technology-will-destroy-human-nature", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF3cQ6zJYiGFy2FpJP%2Flink-technology-will-destroy-human-nature", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1999, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/egj/kaj_sotalas_posts/\">Kaj Sotala's Posts</a>, <a href=\"/lw/d8t/blogs_by_lwers/\">Blogs by LWers</a></p>\n<p>By fellow LessWronger <a href=\"/user/Kaj_Sotala/\">Kaj_Sotala</a> on his <a href=\"http://kajsotala.fi/2012/10/technology-will-destroy-human-nature/\">blog</a>.</p>\n<blockquote>\n<p><a href=\"http://squid314.livejournal.com/\">Scott</a> recently made two posts [<a href=\"http://squid314.livejournal.com/332946.html\">1</a> <a href=\"http://squid314.livejournal.com/333168.html\">2</a>] about some of the dangers of technology, and of becoming too powerful for yourself. Now, I&rsquo;ll admit that I didn&rsquo;t entirely understand his concern. As far as I could tell, he was worried that at some point, we might perfectly know the best possible strategy for pursuing all of our desires, and have the willpower to do so. Then, in a sense, one could say that we&rsquo;d no longer experience having a free will. There would always be only one reasonable action in any situation, and we would always pick that one.</p>\n<p>Well, I&rsquo;m not too concerned about that. But the post hilighted one possible way that technology could damage something that we consider dear and essential, by removing essential constraints. That&rsquo;s actually a rather major worry, and a far broader one than just one example suggests. (This essay was also influenced by a recent comment by <a href=\"http://rak.minduploading.org/\">Randal Koene</a>.)</p>\n<p>First, though, let&rsquo;s review a bit of history.</p>\n<p>In 1967, the biologist Sol Spiegelman took a strand of viral RNA, and placed it on a dish containing various raw materials that the RNA could use to build new copies of itself. After the RNA strands had replicated on the dish, Spiegelman extracted some of them and put them on another dish, again with raw materials that the strands could use to replicate themselves. He then kept repeating this process.</p>\n<p>No longer burdened with the constraints of needing to work for a living, produce protein coats, or to do anything but reproduce, the RNA evolved to match its new environment. The RNA mutated, and the strands which could copy themselves the fastest won out. Everything in those strands that wasn&rsquo;t needed for reproduction had just become an unnecessary liability. After just 74 generations, the original 4,500 nucleotide bases had been reduced to a mere 220. Useless parts of the genome had been discarded; the viral RNA had now become a pure replicator, dubbed &ldquo;Spiegelman&rsquo;s monster&rdquo;. (<a href=\"https://plus.google.com/u/0/117663015413546257905/posts/QKNo5cLZNfE\">Source.</a>)</p>\n<p>What happens in evolution is that organisms adapt themselves to exploit, and protect themselves from, the various regularities of the environment. Light reflects off distant objects in a predictable manner, so creatures have evolved eyes that they can use to see. If the environment ceases to possess some regularities, it will necessarily change the organisms. Put a fish with eyes in a cave with no light, and it will lose its sight over a few thousand years at most. <a href=\"/lw/28k/the_psychological_diversity_of_mankind/\">Even humans have kept evolving</a> as our environment has changed. <a href=\"https://en.wikipedia.org/wiki/Sickle-cell_disease\">Sickle-cell disease</a> is more common in people whose ancestors are from regions with malaria. A single sickle-cell gene makes you more resistant to malaria, but two give you the disease. That&rsquo;s an acceptable tradeoff in an environment with a lot of malaria, but a burden outside that environment.</p>\n<p>You could say that the environment <em>constrains</em> the kind of organisms that can exist there. Now, those constraints aren&rsquo;t immediate: that cave fish won&rsquo;t lose its eyes right away. But over enough time, as different kinds of fish compete for survival, the ones which don&rsquo;t waste their energy on growing useless eyes will win out.</p>\n<p>Humans, as I was suggesting before, have also evolved to meet some very specific environmental constraints. As our environment has changed &ndash; either by our own doing, or due to reasons that have nothing to do with us &ndash; those constraints have changed somewhat, and <a href=\"/lw/28k/the_psychological_diversity_of_mankind/\">we have changed with them</a>. But many things about our nature, things that we might consider fundamental, have not changed. We still tell stories, enjoy the company of others, and are distinct individuals. Sure, the exact forms that those things take have changed over time. Today we are more likely to watch a story on TV than to hear one over a campfire &ndash; but both are still recognizable forms of story-telling. Countless of <a href=\"http://condor.depaul.edu/mfiddler/hyphen/humunivers.htm\">human universals</a> are found in cultures all over the planet:</p>\n<p>aesthetics; affection expressed and felt; age grades; body adornments; childhood fears; classification of kin; cooking; cooperation; customary greetings; daily routines; dance; distinguishing right and wrong; dreams; emotions; empathy; envy; family (or household); folklore; generosity admired; gossip; hope; hospitality; imagery; jokes; judging others; leaders; likes and dislikes; manipulating social relations; marriage; meal times; mourning; music &hellip;</p>\n<p>Individuals may disagree about which of those things really are fundamental &ndash; whether losing some specific universal would really be a loss &ndash; but most people are likely to say that at least <em>some</em> of those things are important and worth keeping.</p>\n<p>But as technology keeps evolving, it will make it easier and easier to overcome various constraints in our environment, our bodies, and in our minds. And then it will become increasing tempting to become a Spiegelman&rsquo;s monster: to rid yourself of the things that the loosened constraints have made unnecessary, to become something that is no longer even remotely human. If you don&rsquo;t do it, then someone else will. With enough time, they may end up ruling the world, outcompeting you like Spiegelman&rsquo;s monster outcompeted the original, umutated RNA strands.</p>\n<p>Exactly what kinds of constraints am I talking about, here? Well, there are several, in a roughly increasing order of severity:</p>\n<ul>\n<li><a href=\"http://squid314.livejournal.com/333168.html\">Not being too powerful for yourself</a>. Scott&rsquo;s concern: that at some point, we might perfectly know the best possible strategy for pursuing all of our desires, and have the willpower to do so. Then, in a sense, one could say that we no longer experienced having a free will &ndash; there would always only be one reasonable action in any situation, and we would always pick that one.</li>\n<li>Having distinct minds. We might not be too far away from having the ability to <a href=\"http://kajsotala.fi/Papers/CoalescingMinds.pdf\">directly connect brains with each other</a>. I think about something, and the thought crosses over to your brain, merging with your stream of consciousness. With time, this technology could be perfected so that large groups of people could join together into a single entity, coordinating and doing everything much better than any &ldquo;traditional&rdquo; human. Combined with an ability to copy memories, the concept of &ldquo;personal identity&rdquo; might cease to have any meaning at all &ndash; there would be no persons, just an amorphous mass of consciousnesses all sharing most of the same memories.</li>\n<li>Unmodifiable desires: as <a href=\"http://noahpinionblog.blogspot.de/2012/08/desire-modification-ultimate-technology.html\">desire modification</a> becomes possible, anyone could reprogram their brains to be constantly perfectly satisfied and never do anything else (except possibly the bare minimum needed for survival). Sure, the possibility feels unappealing now&hellip; but maybe you&rsquo;re having a bad day, and you choose to modify your brain to feel just a little better, all the time. And then the thought of being permanently blissed out doesn&rsquo;t feel so bad after all, and you modify your brain just a little more&hellip; how could you not envy the folks who are never unhappy, especially since <a href=\"/lw/x3/devils_offers/\">the option to self-modify is always there</a>?</li>\n<li>Inability to design superintelligent AGIs. We are constantly investing in ever-improving AI, for the obvious economic reasons: it allows for ever-more work to be automated. It may indeed <a href=\"http://jetpress.org/v22/goertzel-pitt.htm\">prove impossible to regulate</a> AI development in order to stop super-intelligent AGIs (artificial <em>general</em> intelligences) from arising. If so, then it might also <a href=\"https://singularity.org/summary/\">prove impossible</a> to ensure that safe and human-friendly AGIs prevail: like with Spiegelman&rsquo;s monsters, the AGIs not burdened with the constraints of respecting human life and property may end up winning the AGIs that wish to protect humanity, after which they&rsquo;ll recycle human settlements into their raw materials.</li>\n<li>An inability to become <a href=\"http://www.nickbostrom.com/fut/evolution.html\">mindless outsourcers</a>. Nick Bostrom suggests a scenario where we learn to offload all of our thought to non-conscious external programs. To quote: &ldquo;<em>Why do I need to know arithmetic when I can buy time on Arithmetic-Modules Inc. whenever I need to do my accounts? Why do I need to be good with language when I can hire a professional language module to articulate my thoughts? Why do I need to bother with making decisions about my personal life when there are certified executive-modules that can scan my goal structure and manage my assets so as best to fulfill my goals?</em>&rdquo; And so, we give in to the temptation to cut away more and more parts of our brains, letting computer programs run those tasks&hellip; until there is no conscious experience left.</li>\n<li>An inability to copy the best workers, choosing only the ones best fit for their tasks. If we could upload brains to computers, it could also become possible to copy minds. This could be far quicker than ordinary reproduction, making copying the primary method by which humans multiplied &ndash; and one&rsquo;s ability to acquire and retain more hardware to run one&rsquo;s copies on, would become the <a href=\"http://hanson.gmu.edu/uploads.html\">main criteria</a> that evolution selected for. <a href=\"http://www.nickbostrom.com/fut/evolution.html\">As Nick Bostrom writes</a>: <em>Much of human life&rsquo;s meaning arguably depends on the enjoyment, for its own sake, of humor, love, game-playing, art, sex, dancing, social conversation, philosophy, literature, scientific discovery, food and drink, friendship, parenting, and sport. We have preferences and capabilities that make us engage in such activities, and these predispositions were adaptive in our species&rsquo; evolutionary past; but what ground do we have for being confident that these or similar activities will continue to be adaptive in the future? Perhaps what will maximize fitness in the future will be nothing but non-stop high-intensity drudgery, work of a drab and repetitive nature, aimed at improving the eighth decimal of some economic output measure. Even if the workers selected for in this scenario were conscious, the resulting world would still be radically impoverished in terms of the qualities that give value to life. </em></li>\n</ul>\n<p>To rephrase what I have been saying:</p>\n<p>&ldquo;Humans&rdquo; inhabit a narrow region in a multidimensional space of possibilities, and various constraints currently keep everyone stuck in that tiny space. If any of those constraints were to be relaxed &ndash; the <a href=\"/lw/rm/the_design_space_of_mindsingeneral/\">space of possible minds</a> stretched in any direction &ndash; then the new kinds of minds, no longer burdened with the constraints that make our fundamental values so adaptive, would be free to expand in entirely new directions. And it seems inevitable that, given a broader space of possible adaptations, evolutionary pressures would eventually lead to the dominance of minds &ndash; or at least replicators &ndash; which were very different from what most of us would value as &ldquo;human&rdquo;.</p>\n<p>Get enough of one constraint, and you might still recognize the outcome as having once been human. Get rid of enough constraints, and you&rsquo;ll get the equivalent of a Spiegelman&rsquo;s monster, no longer even remotely human.</p>\n<p>There have been some suggestions of how to avoid this. Nick Bostrom has suggested [<a href=\"http://www.nickbostrom.com/fut/evolution.html\">1</a> <a href=\"http://www.nickbostrom.com/fut/singleton.html\">2</a>] that we create a &ldquo;singleton&rdquo;, a world-order with a single decision-making agency at the highest level, capable of controlling evolution. The singleton could be an appropriately-programmed AGI, the right group of uploads, or something else. Maybe this will work: but I doubt it. I expect all such efforts to fail, and humanity to eventually vanish. Possibly within my lifetime, if we&rsquo;re unlucky.</p>\n<p>I&rsquo;ll conclude this essay with the <a href=\"https://en.wikisource.org/wiki/The_Call_of_Cthulhu/Chapter_I\">immortal words of H.P. Lovecraft</a>:</p>\n<p><em>The most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents. We live on a placid island of ignorance in the midst of black seas of infinity, and it was not meant that we should voyage far. The sciences, each straining in its own direction, have hitherto harmed us little; but some day the piecing together of dissociated knowledge will open up such terrifying vistas of reality, and of our frightful position therein, that we shall either go mad from the revelation or flee from the deadly light into the peace and safety of a new dark age.</em></p>\n<p>Needless to say, Lovecraft was being too optimistic.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F3cQ6zJYiGFy2FpJP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 21, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "19178", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bJ7vvyzBoiKSkFmTt", "h83ZzxEpKiPPjsRKs", "2oybbEw697CQgcRE5", "MTjej6HKvPByx3dEA", "tnWRXkcDi5Tw9rzXw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-05T16:13:13.326Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Berlin, Melbourne, Moscow, Purdue, Vienna, Washington DC", "slug": "weekly-lw-meetups-austin-berlin-melbourne-moscow-purdue", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dTMhmeyyLwrrpQdTj/weekly-lw-meetups-austin-berlin-melbourne-moscow-purdue", "pageUrlRelative": "/posts/dTMhmeyyLwrrpQdTj/weekly-lw-meetups-austin-berlin-melbourne-moscow-purdue", "linkUrl": "https://www.lesswrong.com/posts/dTMhmeyyLwrrpQdTj/weekly-lw-meetups-austin-berlin-melbourne-moscow-purdue", "postedAtFormatted": "Friday, October 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Melbourne%2C%20Moscow%2C%20Purdue%2C%20Vienna%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Melbourne%2C%20Moscow%2C%20Purdue%2C%20Vienna%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdTMhmeyyLwrrpQdTj%2Fweekly-lw-meetups-austin-berlin-melbourne-moscow-purdue%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Melbourne%2C%20Moscow%2C%20Purdue%2C%20Vienna%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdTMhmeyyLwrrpQdTj%2Fweekly-lw-meetups-austin-berlin-melbourne-moscow-purdue", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdTMhmeyyLwrrpQdTj%2Fweekly-lw-meetups-austin-berlin-melbourne-moscow-purdue", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 488, "htmlBody": "<p><strong>This summary was posted to LW main on September 28th, and has been moved to discussion. The more recent summary is <a href=\"/lw/esr/weekly_lw_meetups_austin_berkeley_bielefield/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/dv\">Berlin Meetup:&nbsp;<span class=\"date\">28 September 2012 07:30PM</span></a></li>\n<li><a href=\"/meetups/e0\">Vienna meetup:&nbsp;<span class=\"date\">28 September 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/e7\">Purdue Meetup:&nbsp;<span class=\"date\">28 September 2012 09:45PM</span></a></li>\n<li><a href=\"/meetups/dm\">Moscow: applied rationality and web resources:&nbsp;<span class=\"date\">29 September 2012 04:00PM</span></a></li>\n<li><a href=\"/meetups/ec\">Washington DC: Choice Blindness:&nbsp;<span class=\"date\">30 September 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/e9\">Munich Meetup, October 6th :&nbsp;<span class=\"date\">06 October 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/eb\">(Durham NC) HPMoR Discussion, chapters 4-7:&nbsp;<span class=\"date\">06 October 2012 11:00AM</span></a></li>\n<li><a href=\"/meetups/e1\">(Durham NC) Research Triangle Area Less Wrong:&nbsp;<span class=\"date\">11 October 2012 06:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">29 September 2018 01:30PM</span></a></li>\n<li><a href=\"/meetups/e4\">Melbourne, practical rationality:&nbsp;<span class=\"date\">05 October 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/df\">Cambridge (MA) first-Sundays meetup:&nbsp;<span class=\"date\">07 October 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/e5\">(Berkeley CA) Pre-Singularity Summit Overcoming Bias / Less Wrong Meetup Party:&nbsp;<span class=\"date\">11 October 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/dg\">Cambridge (MA) third-Sundays Meetup:&nbsp;<span class=\"date\">21 October 2012 02:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dTMhmeyyLwrrpQdTj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 1.0023551326175637e-06, "legacy": true, "legacyId": "19057", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["E5EuJAe67GRPbpXBZ", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-05T18:23:46.223Z", "modifiedAt": null, "url": null, "title": "[Link] Inside the Cold, Calculating Mind of LessWrong?", "slug": "link-inside-the-cold-calculating-mind-of-lesswrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:33.794Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vAxxBCFDM2oCZ8DxT/link-inside-the-cold-calculating-mind-of-lesswrong", "pageUrlRelative": "/posts/vAxxBCFDM2oCZ8DxT/link-inside-the-cold-calculating-mind-of-lesswrong", "linkUrl": "https://www.lesswrong.com/posts/vAxxBCFDM2oCZ8DxT/link-inside-the-cold-calculating-mind-of-lesswrong", "postedAtFormatted": "Friday, October 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Inside%20the%20Cold%2C%20Calculating%20Mind%20of%20LessWrong%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Inside%20the%20Cold%2C%20Calculating%20Mind%20of%20LessWrong%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvAxxBCFDM2oCZ8DxT%2Flink-inside-the-cold-calculating-mind-of-lesswrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Inside%20the%20Cold%2C%20Calculating%20Mind%20of%20LessWrong%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvAxxBCFDM2oCZ8DxT%2Flink-inside-the-cold-calculating-mind-of-lesswrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvAxxBCFDM2oCZ8DxT%2Flink-inside-the-cold-calculating-mind-of-lesswrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 871, "htmlBody": "<p>An <a href=\"http://online.wsj.com/article/SB10000872396390444358804578016291138331904.html\">article</a> from the Wall Street Journal. The original title might be slightly mind-killing for some people, but I found it moderately interesting especially considering that <a href=\"/lw/8lk/poll_lesswrong_group_on_yourmoralsorg/\">many LessWrongers formed part of the data set</a> for the study the article talks about and a large fraction of us identified as libertarian on the last survey. &nbsp;</p>\n<h2>Inside the Cold, Calculating Libertarian Mind</h2>\n<blockquote>\n<p>An individual's personality shapes his or her political ideology at least as much as circumstances, background and influences. That is the gist of a recent strand of psychological research identified especially with the work of Jonathan Haidt. The baffling (to liberals) fact that a large minority of working-class white people vote for conservative candidates is explained by psychological dispositions that override their narrow economic interests.</p>\n<div class=\"insetContent insetCol3wide embedType-image imageFormat-D\">\n<div class=\"insetTree\">\n<div id=\"articleThumbnail_1\" class=\"insettipUnit insetZoomTarget\"><cite></cite>\n<p class=\"targetCaption\">In tests, libertarians displayed less emotion, empathy and disgust than conservatives or liberals.</p>\n</div>\n</div>\n</div>\n<p><a name=\"U71382601385P0D\"></a></p>\n<p>In his recent book \"The Righteous Mind,\" Dr. Haidt confronted liberal bafflement and made the case that conservatives are motivated by morality just as liberals are, but also by a larger set of moral \"tastes\"&mdash;loyalty, authority and sanctity, in addition to the liberal tastes for compassion and fairness. Studies show that conservatives are more conscientious and sensitive to disgust but less tolerant of change; liberals are more empathic and open to new experiences.</p>\n<p><a name=\"U71382601385L1\"></a></p>\n<p>&nbsp;</p>\n<p>But ideology does not have to be bipolar. It need not fall on a line from conservative to liberal. <strong>In a recently published paper, Ravi Iyer from the University of Southern California, together with Dr. Haidt and other researchers at the data-collection platform YourMorals.org, dissect the personalities of those who describe themselves as libertarian.</strong></p>\n<p>These are people who often call themselves economically conservative but socially liberal. They like free societies as well as free markets, and they want the government to get out of the bedroom as well as the boardroom. They don't see why, in order to get a small-government president, they have to vote for somebody who is keen on military spending and religion; or to get a tolerant and compassionate society they have to vote for a large and intrusive state.</p>\n<p><a name=\"U71382601385SUB\"></a></p>\n<p><strong>The study collated the results of 16 personality surveys and experiments completed by nearly 12,000 self-identified libertarians who visited YourMorals.org.</strong> The researchers compared the libertarians to tens of thousands of self-identified liberals and conservatives. It was hardly surprising that the team found that libertarians strongly value liberty, especially the \"negative liberty\" of freedom from interference by others. Given the philosophy of their heroes, from John Locke and John Stuart Mill to Ayn Rand and Ron Paul, it also comes as no surprise that libertarians are also individualistic, stressing the right and the need for people to stand on their own two feet, rather than the duty of others, or government, to care for people.</p>\n<p><a name=\"U71382601385POC\"></a></p>\n<p><strong>Perhaps more intriguingly, when libertarians reacted to moral dilemmas and in other tests, they displayed less emotion, less empathy and less disgust than either conservatives or liberals. They appeared to use \"cold\" calculation to reach utilitarian conclusions about whether (for instance) to save lives by sacrificing fewer lives. They reached correct, rather than intuitive, answers to math and logic problems, and they enjoyed \"effortful and thoughtful cognitive tasks\" more than others do.</strong></p>\n<p><a name=\"U71382601385B0B\"></a></p>\n<p><strong>The researchers found that libertarians had the most \"masculine\" psychological profile, while liberals had the most feminine, and these results held up even when they examined each gender separately, which \"may explain why libertarianism appeals to men more than women.\" </strong></p>\n<p><a name=\"U71382601385C0F\"></a></p>\n<p>All Americans value liberty, but libertarians seem to value it more. For social conservatives, liberty is often a means to the end of rolling back the welfare state, with its lax morals and redistributive taxation, so liberty can be infringed in the bedroom. For liberals, liberty is a way to extend rights to groups perceived to be oppressed, so liberty can be infringed in the boardroom. But for libertarians, liberty is an end in itself, trumping all other moral values.</p>\n<p><a name=\"U71382601385RNB\"></a></p>\n<p>Dr. Iyer's conclusion is that libertarians are a distinct species&mdash;psychologically as well as politically.</p>\n<p class=\"articleVersion\">A version of this article appeared September 29, 2012, on page C4 in the U.S. edition of The Wall Street Journal, with the headline: Inside the Cold, Calculating Libertarian Mind.</p>\n</blockquote>\n<p class=\"articleVersion\">The <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1665934\">original paper</a>.</p>\n<h2 class=\"articleVersion\">Understanding Libertarian Morality: The Psychological Roots of an Individualist Ideology</h2>\n<blockquote>\n<p class=\"articleVersion\"><strong>Abstract:</strong> Libertarians are an increasingly vocal ideological group in U.S. politics, yet they are understudied compared to liberals and conservatives. Much of what is known about libertarians is based on the writing of libertarian intellectuals and political leaders, rather than surveying libertarians in the general population. Across three studies, 15 measures, and a large web-based sample (N = 152,239), we sought to understand the morality of selfdescribed libertarians. Based on an intuitionist view of moral judgment, we focused on the underlying affective and cognitive dispositions that accompany this unique worldview. We found that, compared to liberals and conservatives, libertarians show 1) stronger endorsement of individual liberty as their foremost guiding principle and correspondingly weaker endorsement of other moral principles, 2) a relatively cerebral as opposed to emotional intellectual style, and 3) lower interdependence and social relatedness. Our findings add to a growing recognition of the role of psychological predispositions in the organization of political attitudes.</p>\n</blockquote>\n<p><strong></strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vAxxBCFDM2oCZ8DxT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 17, "extendedScore": null, "score": 1.002423999806992e-06, "legacy": true, "legacyId": "19180", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>An <a href=\"http://online.wsj.com/article/SB10000872396390444358804578016291138331904.html\">article</a> from the Wall Street Journal. The original title might be slightly mind-killing for some people, but I found it moderately interesting especially considering that <a href=\"/lw/8lk/poll_lesswrong_group_on_yourmoralsorg/\">many LessWrongers formed part of the data set</a> for the study the article talks about and a large fraction of us identified as libertarian on the last survey. &nbsp;</p>\n<h2 id=\"Inside_the_Cold__Calculating_Libertarian_Mind\">Inside the Cold, Calculating Libertarian Mind</h2>\n<blockquote>\n<p>An individual's personality shapes his or her political ideology at least as much as circumstances, background and influences. That is the gist of a recent strand of psychological research identified especially with the work of Jonathan Haidt. The baffling (to liberals) fact that a large minority of working-class white people vote for conservative candidates is explained by psychological dispositions that override their narrow economic interests.</p>\n<div class=\"insetContent insetCol3wide embedType-image imageFormat-D\">\n<div class=\"insetTree\">\n<div id=\"articleThumbnail_1\" class=\"insettipUnit insetZoomTarget\"><cite></cite>\n<p class=\"targetCaption\">In tests, libertarians displayed less emotion, empathy and disgust than conservatives or liberals.</p>\n</div>\n</div>\n</div>\n<p><a name=\"U71382601385P0D\"></a></p>\n<p>In his recent book \"The Righteous Mind,\" Dr. Haidt confronted liberal bafflement and made the case that conservatives are motivated by morality just as liberals are, but also by a larger set of moral \"tastes\"\u2014loyalty, authority and sanctity, in addition to the liberal tastes for compassion and fairness. Studies show that conservatives are more conscientious and sensitive to disgust but less tolerant of change; liberals are more empathic and open to new experiences.</p>\n<p><a name=\"U71382601385L1\"></a></p>\n<p>&nbsp;</p>\n<p>But ideology does not have to be bipolar. It need not fall on a line from conservative to liberal. <strong>In a recently published paper, Ravi Iyer from the University of Southern California, together with Dr. Haidt and other researchers at the data-collection platform YourMorals.org, dissect the personalities of those who describe themselves as libertarian.</strong></p>\n<p>These are people who often call themselves economically conservative but socially liberal. They like free societies as well as free markets, and they want the government to get out of the bedroom as well as the boardroom. They don't see why, in order to get a small-government president, they have to vote for somebody who is keen on military spending and religion; or to get a tolerant and compassionate society they have to vote for a large and intrusive state.</p>\n<p><a name=\"U71382601385SUB\"></a></p>\n<p><strong>The study collated the results of 16 personality surveys and experiments completed by nearly 12,000 self-identified libertarians who visited YourMorals.org.</strong> The researchers compared the libertarians to tens of thousands of self-identified liberals and conservatives. It was hardly surprising that the team found that libertarians strongly value liberty, especially the \"negative liberty\" of freedom from interference by others. Given the philosophy of their heroes, from John Locke and John Stuart Mill to Ayn Rand and Ron Paul, it also comes as no surprise that libertarians are also individualistic, stressing the right and the need for people to stand on their own two feet, rather than the duty of others, or government, to care for people.</p>\n<p><a name=\"U71382601385POC\"></a></p>\n<p><strong id=\"Perhaps_more_intriguingly__when_libertarians_reacted_to_moral_dilemmas_and_in_other_tests__they_displayed_less_emotion__less_empathy_and_less_disgust_than_either_conservatives_or_liberals__They_appeared_to_use__cold__calculation_to_reach_utilitarian_conclusions_about_whether__for_instance__to_save_lives_by_sacrificing_fewer_lives__They_reached_correct__rather_than_intuitive__answers_to_math_and_logic_problems__and_they_enjoyed__effortful_and_thoughtful_cognitive_tasks__more_than_others_do_\">Perhaps more intriguingly, when libertarians reacted to moral dilemmas and in other tests, they displayed less emotion, less empathy and less disgust than either conservatives or liberals. They appeared to use \"cold\" calculation to reach utilitarian conclusions about whether (for instance) to save lives by sacrificing fewer lives. They reached correct, rather than intuitive, answers to math and logic problems, and they enjoyed \"effortful and thoughtful cognitive tasks\" more than others do.</strong></p>\n<p><a name=\"U71382601385B0B\"></a></p>\n<p><strong id=\"The_researchers_found_that_libertarians_had_the_most__masculine__psychological_profile__while_liberals_had_the_most_feminine__and_these_results_held_up_even_when_they_examined_each_gender_separately__which__may_explain_why_libertarianism_appeals_to_men_more_than_women___\">The researchers found that libertarians had the most \"masculine\" psychological profile, while liberals had the most feminine, and these results held up even when they examined each gender separately, which \"may explain why libertarianism appeals to men more than women.\" </strong></p>\n<p><a name=\"U71382601385C0F\"></a></p>\n<p>All Americans value liberty, but libertarians seem to value it more. For social conservatives, liberty is often a means to the end of rolling back the welfare state, with its lax morals and redistributive taxation, so liberty can be infringed in the bedroom. For liberals, liberty is a way to extend rights to groups perceived to be oppressed, so liberty can be infringed in the boardroom. But for libertarians, liberty is an end in itself, trumping all other moral values.</p>\n<p><a name=\"U71382601385RNB\"></a></p>\n<p>Dr. Iyer's conclusion is that libertarians are a distinct species\u2014psychologically as well as politically.</p>\n<p class=\"articleVersion\">A version of this article appeared September 29, 2012, on page C4 in the U.S. edition of The Wall Street Journal, with the headline: Inside the Cold, Calculating Libertarian Mind.</p>\n</blockquote>\n<p class=\"articleVersion\">The <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1665934\">original paper</a>.</p>\n<h2 class=\"articleVersion\" id=\"Understanding_Libertarian_Morality__The_Psychological_Roots_of_an_Individualist_Ideology\">Understanding Libertarian Morality: The Psychological Roots of an Individualist Ideology</h2>\n<blockquote>\n<p class=\"articleVersion\"><strong>Abstract:</strong> Libertarians are an increasingly vocal ideological group in U.S. politics, yet they are understudied compared to liberals and conservatives. Much of what is known about libertarians is based on the writing of libertarian intellectuals and political leaders, rather than surveying libertarians in the general population. Across three studies, 15 measures, and a large web-based sample (N = 152,239), we sought to understand the morality of selfdescribed libertarians. Based on an intuitionist view of moral judgment, we focused on the underlying affective and cognitive dispositions that accompany this unique worldview. We found that, compared to liberals and conservatives, libertarians show 1) stronger endorsement of individual liberty as their foremost guiding principle and correspondingly weaker endorsement of other moral principles, 2) a relatively cerebral as opposed to emotional intellectual style, and 3) lower interdependence and social relatedness. Our findings add to a growing recognition of the role of psychological predispositions in the organization of political attitudes.</p>\n</blockquote>\n<p><strong></strong></p>", "sections": [{"title": "Inside the Cold, Calculating Libertarian Mind", "anchor": "Inside_the_Cold__Calculating_Libertarian_Mind", "level": 1}, {"title": "Perhaps more intriguingly, when libertarians reacted to moral dilemmas and in other tests, they displayed less emotion, less empathy and less disgust than either conservatives or liberals. They appeared to use \"cold\" calculation to reach utilitarian conclusions about whether (for instance) to save lives by sacrificing fewer lives. They reached correct, rather than intuitive, answers to math and logic problems, and they enjoyed \"effortful and thoughtful cognitive tasks\" more than others do.", "anchor": "Perhaps_more_intriguingly__when_libertarians_reacted_to_moral_dilemmas_and_in_other_tests__they_displayed_less_emotion__less_empathy_and_less_disgust_than_either_conservatives_or_liberals__They_appeared_to_use__cold__calculation_to_reach_utilitarian_conclusions_about_whether__for_instance__to_save_lives_by_sacrificing_fewer_lives__They_reached_correct__rather_than_intuitive__answers_to_math_and_logic_problems__and_they_enjoyed__effortful_and_thoughtful_cognitive_tasks__more_than_others_do_", "level": 2}, {"title": "The researchers found that libertarians had the most \"masculine\" psychological profile, while liberals had the most feminine, and these results held up even when they examined each gender separately, which \"may explain why libertarianism appeals to men more than women.\" ", "anchor": "The_researchers_found_that_libertarians_had_the_most__masculine__psychological_profile__while_liberals_had_the_most_feminine__and_these_results_held_up_even_when_they_examined_each_gender_separately__which__may_explain_why_libertarianism_appeals_to_men_more_than_women___", "level": 2}, {"title": "Understanding Libertarian Morality: The Psychological Roots of an Individualist Ideology", "anchor": "Understanding_Libertarian_Morality__The_Psychological_Roots_of_an_Individualist_Ideology", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "68 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["voZGmjLfzMMQqH8k8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-05T20:47:12.320Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Show and tell meetup: Economics", "slug": "meetup-washington-dc-show-and-tell-meetup-economics", "viewCount": null, "lastCommentedAt": "2012-10-05T20:47:12.320Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p5u9dvWXLEratwn7K/meetup-washington-dc-show-and-tell-meetup-economics", "pageUrlRelative": "/posts/p5u9dvWXLEratwn7K/meetup-washington-dc-show-and-tell-meetup-economics", "linkUrl": "https://www.lesswrong.com/posts/p5u9dvWXLEratwn7K/meetup-washington-dc-show-and-tell-meetup-economics", "postedAtFormatted": "Friday, October 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Show%20and%20tell%20meetup%3A%20Economics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Show%20and%20tell%20meetup%3A%20Economics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp5u9dvWXLEratwn7K%2Fmeetup-washington-dc-show-and-tell-meetup-economics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Show%20and%20tell%20meetup%3A%20Economics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp5u9dvWXLEratwn7K%2Fmeetup-washington-dc-show-and-tell-meetup-economics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp5u9dvWXLEratwn7K%2Fmeetup-washington-dc-show-and-tell-meetup-economics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ek'>Washington DC Show and tell meetup: Economics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 October 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery Plaza, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meetup is going to be the first in what I hope will be a series of \"show and tell\" meetups. One of our members has agreed to talk about economics, one of his areas of expertise. This should be much better if people come equipped with questions or subtopics of particular interest to them, so please do so!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ek'>Washington DC Show and tell meetup: Economics</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p5u9dvWXLEratwn7K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0024996746609148e-06, "legacy": true, "legacyId": "19181", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Show_and_tell_meetup__Economics\">Discussion article for the meetup : <a href=\"/meetups/ek\">Washington DC Show and tell meetup: Economics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 October 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery Plaza, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meetup is going to be the first in what I hope will be a series of \"show and tell\" meetups. One of our members has agreed to talk about economics, one of his areas of expertise. This should be much better if people come equipped with questions or subtopics of particular interest to them, so please do so!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Show_and_tell_meetup__Economics1\">Discussion article for the meetup : <a href=\"/meetups/ek\">Washington DC Show and tell meetup: Economics</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Show and tell meetup: Economics", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Show_and_tell_meetup__Economics", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Show and tell meetup: Economics", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Show_and_tell_meetup__Economics1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 0, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-06T00:09:03.392Z", "modifiedAt": null, "url": null, "title": "Meetup : Sofia, Bulgaria Meetup", "slug": "meetup-sofia-bulgaria-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blueowl", "createdAt": "2012-10-05T21:04:29.727Z", "isAdmin": false, "displayName": "blueowl"}, "userId": "DaRXCqHTHuL7Jc79Z", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/g9SMYmbS92ezzABJa/meetup-sofia-bulgaria-meetup", "pageUrlRelative": "/posts/g9SMYmbS92ezzABJa/meetup-sofia-bulgaria-meetup", "linkUrl": "https://www.lesswrong.com/posts/g9SMYmbS92ezzABJa/meetup-sofia-bulgaria-meetup", "postedAtFormatted": "Saturday, October 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sofia%2C%20Bulgaria%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sofia%2C%20Bulgaria%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg9SMYmbS92ezzABJa%2Fmeetup-sofia-bulgaria-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sofia%2C%20Bulgaria%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg9SMYmbS92ezzABJa%2Fmeetup-sofia-bulgaria-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg9SMYmbS92ezzABJa%2Fmeetup-sofia-bulgaria-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/el'>Sofia, Bulgaria Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 December 2012 05:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Sofia, Bulgaria</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>If interested, join our group: <a href=\"http://groups.google.com/group/sofia-lesswrong\" rel=\"nofollow\">http://groups.google.com/group/sofia-lesswrong</a>\nIgnore the date in the description. Time and place will be discussed in the group at some point in the future.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/el'>Sofia, Bulgaria Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "g9SMYmbS92ezzABJa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.002606186488798e-06, "legacy": true, "legacyId": "19182", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sofia__Bulgaria_Meetup\">Discussion article for the meetup : <a href=\"/meetups/el\">Sofia, Bulgaria Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 December 2012 05:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Sofia, Bulgaria</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>If interested, join our group: <a href=\"http://groups.google.com/group/sofia-lesswrong\" rel=\"nofollow\">http://groups.google.com/group/sofia-lesswrong</a>\nIgnore the date in the description. Time and place will be discussed in the group at some point in the future.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sofia__Bulgaria_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/el\">Sofia, Bulgaria Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sofia, Bulgaria Meetup", "anchor": "Discussion_article_for_the_meetup___Sofia__Bulgaria_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Sofia, Bulgaria Meetup", "anchor": "Discussion_article_for_the_meetup___Sofia__Bulgaria_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-06T02:11:33.892Z", "modifiedAt": null, "url": null, "title": "What I Learned About Meetup Organization", "slug": "what-i-learned-about-meetup-organization", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:06.407Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gcm4pvwEKpydZiKmd/what-i-learned-about-meetup-organization", "pageUrlRelative": "/posts/gcm4pvwEKpydZiKmd/what-i-learned-about-meetup-organization", "linkUrl": "https://www.lesswrong.com/posts/gcm4pvwEKpydZiKmd/what-i-learned-about-meetup-organization", "postedAtFormatted": "Saturday, October 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20I%20Learned%20About%20Meetup%20Organization&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20I%20Learned%20About%20Meetup%20Organization%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgcm4pvwEKpydZiKmd%2Fwhat-i-learned-about-meetup-organization%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20I%20Learned%20About%20Meetup%20Organization%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgcm4pvwEKpydZiKmd%2Fwhat-i-learned-about-meetup-organization", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgcm4pvwEKpydZiKmd%2Fwhat-i-learned-about-meetup-organization", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1350, "htmlBody": "<p>Related: <a href=\"/lw/crs/how_to_run_a_successful_less_wrong_meetup/\">How to Run a Successful LW Meetup</a></p>\n<p>(posted to <a href=\"http://groups.google.com/group/lw-organizers\">lw-organizers</a> and Lesswrong discussion)</p>\n<p>I want more discussion of meetup organization tactics, failures, successes, and ideas. We are not going to build an awesome network of rationality dojos without sharing momentum with each other.</p>\n<p>Towards that end, here's some useful stuff that I learned from trying to make the <a href=\"http://groups.google.com/group/vancouver-rationalists\">vancouver meetup</a> more awesome.</p>\n<p>First of all, I'll second everything in the <a href=\"/lw/4ul/less_wrong_nyc_case_study_of_a_successful/\">NYC case study</a>. A lot of the stuff in this post is going to be rehashed stuff from that post, but I still had to learn it the hard way. Maybe if we repeat it enough, it will stick and someone won't have to learn it the hard way.</p>\n<p>When I started going to the Vancouver meetup, it wasn't really happening. I got inspired by the \"communities need heroes\" thing and started to just make it happen. It worked. You really do need someone holding the bottom line. When I fail to make things happen, other people don't reliably step in, things just stop happening. This one keeps getting drilled into me. I still seem to have an aversion to seizing power and stepping on toes, but I'm starting to see that it's really ok. Go ahead and seize power. Be the benevolent dictator. People like it.</p>\n<p>For a while, we tried to do group readings of The Sequences and some books like \"How to Win Friends and Influence People\". It didn't really work. To quote some Web 2.0 troll: \"Users don't read\". Not only do people not read, they feel like they shouldn't come if they haven't read the stuff, even if you insist it doesn't matter. What does work is one person reading something, summarizing it, and preparing a presentation of it. It's more work, but you get better discussion and more value.</p>\n<p>Another thing that works well is presentations and led activities on any interesting topic. This is even better than \"book review\" type stuff. We've had wonderful talks on how to make a lot of money, basics of decision and probability theory, some material from the CFAR minicamps. It's all gone over really well, and we get interesting discussions and lots of learning. Talks and lectures work really well. People can just come in and receive value with no effort on their part. This is good.</p>\n<p>The other day, no one prepared anything to present, and we ended up just having unfocused discussion. It sucked. We talked about inconsequential stuff and didn't learn anything valuable and didn't have fun.</p>\n<p>What especially sucks is metaphysics/definitions/philosophy-landmines discussion. Stuff like realism and so on. I didn't used to like shminux's obnoxious instrumentalism and taboo requests, but then I realized that zealously applying instrumentalism and <a href=\"/lw/nu/taboo_your_words/\">taboo</a> and <a href=\"/lw/48/the_power_of_positivist_thinking/\">positivism</a> prevents a lot of crap discussions.</p>\n<p>I'm still really uncomfortable with being a hard-ass moderator and killing tangents, keeping things on topic, and so on, but it has to be done. Some people will go on and on without adding much value if you don't cut them off. Others won't get a chance to talk at all, even if they have something to say, unless you notice this and prompt them. Sometimes the group will get sniped by less-valuable tangents. Some people are a lot more dominant, and others are really shy; someone has to step in an adjust for this. Again, I haven't quite come to terms with this stuff, but it seems really important.</p>\n<p>Sometimes informal discussion can turn out well. Yesterday we had a really good discussion about the properties of utility functions, dealing with moral uncertainty, aggregating preferences, and so on. I think it turned out well because there were at least two of us with substantial knowledge on the subject, and everyone else was at least interested. Maybe that's not it though. I don't know how to reliably make informal discussions high-signal like that, so I like to steer clear of them.</p>\n<p>Scheduling meetups reliably is good. We do ours every Thursday at 18:00, usually downtown in a public lobby or private office sometimes at a house I have access to. It reduces a lot of thinking to have it be reliable like that.</p>\n<p>Be specific about meta. One time, we all wrote out and decided what we wanted for the meetup. Great, now we had a bunch of goals to optimize. Except we never saw those goals again. More recently, we had a meta discussion about specific proposals for what to do with meetup time. This was more valuable, and now we have some really good ideas to try.</p>\n<p>Up until a while ago, we were sharing our personal goals and commitments, and getting feedback and social pressure to help us. That was mildly effective, but it got tedious and took up too much time to justify it.</p>\n<p>So that's a quick tour of what I've learned the hard way, and the sorts of things we've been doing. I noticed a definite theme in the above (authoritarianism is good) which might just be a product of my mood and thoughts right now. This is all anecdotal and half of it is probably wrong, but stories from the battle front are always valuable.</p>\n<hr />\n<p>We recently had a discussion of the direction we want to take the meetup. Here's some of the big cool stuff we decided we should be doing more of. If anyone has tried these or has good ideas about them, speak up.</p>\n<p>More rationality testing and exercise. We've been focusing a lot on theory and social stuff, but we'd like to move more towards practice and such.</p>\n<p>We want to find ways to do rationality-sparring, where we can figure out how good we are at rationality and see ourselves getting better. One of the most promising ideas for this is the Case Study Game, like they do in business school. The idea is that the Dungeon Master presents some case materials outlining some problem or situation, and the players break into groups and spend a while coming up with their best solution. At the end, the DM judges solutions. Possibly by testing, by comparing against their superior knowledge, etc. Everyone learns something, and case study problems could call on a lot of rationality skills. We haven't tested this at all, but we're itching to, and we're pretty sure there's something valuable in this space.</p>\n<p>Some of us were really excited about doing projects with other members. Another member and I have had some success making each other write stuff. Some of the other projects discussed have been stuff like amateur science (quantified self style), profit motivated mini-businesses (like personal decision coach stuff, or tutoring on craigslist). There's potentially a lot of value here, as LW meetups tend to have lots of awesome smart people.</p>\n<p>If any other meetups have done awesome stuff that worked well for the practical self-improvement/rationality-dojo vibe, please do describe. Anyways, these are some idea's we're toying with, which may or may not go anywhere.</p>\n<hr />\n<p>There's a few things we haven't been able to do, or that I want to improve.</p>\n<p>Member retention. We get a reasonable stream of first-timers who never come back. I worry that we're not awesome enough or something. Also, sometime members stop coming. Sending intermittent people direct emails to get them to come out seems to work really well.</p>\n<p>More members. We'd like even more first-timers. OKCupid, Meetup.com, LW posts, and friend referrals have been good, but we have a lot to learn.</p>\n<p>Gender ratio. We have two regular women, and neither has been coming reliably in the last while. Without them, we are reliably white, male, middle class, young, and somewhat geeky. Getting a more diverse crowd would really improve things, but I only know what people like me like, so I don't know what to do to get other types out.</p>\n<hr />\n<p>There's more, but that's all for now. Any comments, advice, similar stories, copycat posts, dangerous heroics, or other helpful things is greatly appreciated.</p>\n<p>I think I will make a habit of posting reports on what we've been doing, what has worked, and what we've learned more often so that it doesn't build up into such a huge essay. Sorry about the length.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"T57Qd9J3AfxmwhQtY": 3, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gcm4pvwEKpydZiKmd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 42, "extendedScore": null, "score": 1.0026708408103355e-06, "legacy": true, "legacyId": "19183", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qMuAazqwJvkvo8teR", "CsKboswS3z5iaiutC", "WBdvyyHLdxZSAMmoz", "azoP7WeKYYfgCozoh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-06T09:25:50.533Z", "modifiedAt": null, "url": null, "title": "[link] Pei Wang: Motivation Management in AGI Systems", "slug": "link-pei-wang-motivation-management-in-agi-systems", "viewCount": null, "lastCommentedAt": "2012-10-07T01:08:34.514Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y8b7fNd9o3ev3TD5p/link-pei-wang-motivation-management-in-agi-systems", "pageUrlRelative": "/posts/y8b7fNd9o3ev3TD5p/link-pei-wang-motivation-management-in-agi-systems", "linkUrl": "https://www.lesswrong.com/posts/y8b7fNd9o3ev3TD5p/link-pei-wang-motivation-management-in-agi-systems", "postedAtFormatted": "Saturday, October 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Pei%20Wang%3A%20Motivation%20Management%20in%20AGI%20Systems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Pei%20Wang%3A%20Motivation%20Management%20in%20AGI%20Systems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy8b7fNd9o3ev3TD5p%2Flink-pei-wang-motivation-management-in-agi-systems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Pei%20Wang%3A%20Motivation%20Management%20in%20AGI%20Systems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy8b7fNd9o3ev3TD5p%2Flink-pei-wang-motivation-management-in-agi-systems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy8b7fNd9o3ev3TD5p%2Flink-pei-wang-motivation-management-in-agi-systems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 647, "htmlBody": "<p><strong>Related post:</strong> <a href=\"/lw/bxr/muehlhauserwang_dialogue/\">Muehlhauser-Wang Dialogue</a>.</p>\n<p><a href=\"http://www.cis.temple.edu/~pwang/Publication/motivation.pdf\">Motivation Management in AGI Systems</a>, a paper to be published at <a href=\"http://agi-conf.org/2012/\">AGI-12</a>.</p>\n<blockquote>\n<p><strong>Abstract.</strong> AGI systems should be able to manage its motivations or goals that are persistent, spontaneous, mutually restricting, and changing over time. A mechanism for handles this kind of goals is introduced and discussed.</p>\n</blockquote>\n<p>From the discussion section:</p>\n<blockquote>\n<p>The major conclusion argued in this paper is that an AGI system should always maintain a goal structure (or whatever it is called) which contains multiple goals that are separately specifi\fed, with the properties that</p>\n<ul>\n<li>Some of the goals are accurately speci\ffied, and can be fully achieved, while some others are vaguely specifi\fed and only partially achievable, but nevertheless have impact on the system's decisions.</li>\n<li>The goals may conflict with each other on what the system should do at a moment, and cannot be achieved all together. Very often the system has to make compromises among the goals.</li>\n<li>Due to the restriction in computational resources, the system cannot take all existing goals into account when making each decision, and nor can it keep a complete record of the goal derivation history.</li>\n<li>The designers and users are responsible for the input goals of an AGI system, from which all the other goals are derived, according to the system's experience. There is no guarantee that the derived goals will be logically consistent with the input goals, except in highly simplifi\fed situations.</li>\n</ul>\n<p>One area that is closely related to goal management is AI ethics. The previous discussions focused on the goal the designers assign to an AGI system (\"super goal\" or \"final goal\"), with the implicit assumption that such a goal will decide the consequences caused by the A(G)I systems. However, the above analysis shows that though the input goals are indeed important, they are not the dominating factor that decides the broad impact of AI to human society. Since no AGI system can be omniscient and omnipotent, to be \"general-purpose\" means such a system has to handle problems for which its knowledge and resources are insuffi\u000ecient [16, 18], and one direct consequence is that its actions may produce unanticipated results. This consequence, plus the previous conclusion that the eff\u000bective goal for an action may be inconsistent with the input goals, will render many of the previous suggestions mostly irrelevant to AI ethics.</p>\n<p>For example, Yudkowsky's \"Friendly AI\" agenda is based on the assumption that \"a true AI might remain knowably stable in its goals, even after carrying out a large number of self-modi\fcations\" [22]. The problem about this assumption is that unless we are talking about an axiomatic system with unlimited resources, we cannot assume the system can accurately know the consequence of its actions. Furthermore, as argued previously, the goals in an intelligent system inevitable change as its experience grows, which is not necessarily a bad thing - after all, our \"human nature\" gradually grows out of, and deviates from, our \"animal nature\", at both the species level and the individual level.</p>\n<p>Omohundro argued that no matter what input goals are given to an AGI system, it usually will derive some common \"basic drives\", including \"be self-protective\" and \"to acquire resources\" [1], which leads some people to worry that such a system will become unethical. According to our previous analysis, the producing of these goals are indeed very likely, but it is only half of the story. A system with a resource-acquisition goal does not necessarily attempts to achieve it at all cost, without considering its other goals. Again, consider the human beings - everyone has some goals that can become dangerous (either to oneself or to the others) if pursued at all costs. The proper solution, both to human ethics and to AGI ethics, is to prevent this kind of goal from becoming dominant, rather than from being formed.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y8b7fNd9o3ev3TD5p", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 4, "extendedScore": null, "score": 1.002900091867219e-06, "legacy": true, "legacyId": "19197", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gJGjyWahWRyu9TEMC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-06T09:59:25.358Z", "modifiedAt": null, "url": null, "title": "Skill: The Map is Not the Territory", "slug": "skill-the-map-is-not-the-territory", "viewCount": null, "lastCommentedAt": "2022-01-01T00:07:07.118Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KJ9MFBPwXGwNpadf2/skill-the-map-is-not-the-territory", "pageUrlRelative": "/posts/KJ9MFBPwXGwNpadf2/skill-the-map-is-not-the-territory", "linkUrl": "https://www.lesswrong.com/posts/KJ9MFBPwXGwNpadf2/skill-the-map-is-not-the-territory", "postedAtFormatted": "Saturday, October 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Skill%3A%20The%20Map%20is%20Not%20the%20Territory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASkill%3A%20The%20Map%20is%20Not%20the%20Territory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKJ9MFBPwXGwNpadf2%2Fskill-the-map-is-not-the-territory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Skill%3A%20The%20Map%20is%20Not%20the%20Territory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKJ9MFBPwXGwNpadf2%2Fskill-the-map-is-not-the-territory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKJ9MFBPwXGwNpadf2%2Fskill-the-map-is-not-the-territory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1579, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/eqn/the_useful_idea_of_truth/\">The Useful Idea of Truth</a>&nbsp;<em>(minor post)</em></p>\n<p>So far as I know, the first piece of rationalist fiction - one of only two&nbsp;explicitly rationalist fictions I know of that didn't descend from HPMOR, the other being \"David's Sling\" by Marc Stiegler - is the Null-A series by A. E. van Vogt. In Vogt's story, the protagonist, Gilbert Gosseyn, has mostly non-duplicable abilities that you can't pick up and use even if they're supposedly mental - e.g. the ability to use all of his muscular strength in emergencies, thanks to his alleged training. The main explicit-rationalist skill someone could&nbsp;<em>actually</em>&nbsp;pick up from Gosseyn's adventure is embodied in his slogan:</p>\n<p>\"The map is not the territory.\"</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://wiki.lesswrong.com/mediawiki/images/3/3f/I.jpg\" alt=\"\" width=\"375\" height=\"323\" /></p>\n<p>Sometimes it still amazes me to contemplate that this proverb was&nbsp;<em>invented</em>&nbsp;at some point, and some fellow named Korzybski invented it, and this happened as late as the 20th century. I read Vogt's story and absorbed that lesson when I was rather young, so to me this phrase sounds like a sheer background axiom of existence.</p>\n<p>But as the Bayesian Conspiracy enters into its second stage of development, we must all accustom ourselves to translating mere insights into applied techniques. So:</p>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Koan\"><strong>Meditation:</strong></a>&nbsp;Under what circumstances is it helpful to consciously think of the distinction between the map and the territory - to visualize your thought bubble containing a belief, and a reality outside it, rather than just using your map to think about reality directly? &nbsp;How exactly does it help, on what sort of problem?<a id=\"more\"></a></p>\n<p>...</p>\n<p>...</p>\n<p>...</p>\n<p><em>Skill 1: The conceivability of being wrong.</em></p>\n<p>In the story, Gilbert Gosseyn is most liable to be reminded of this proverb when some belief is uncertain; \"Your belief in that does not make it so.\" It might sound basic, but this&nbsp;<em>is</em>&nbsp;where some of the earliest rationalist training starts - making the jump from living in a world where the sky just&nbsp;<em>is</em>&nbsp;blue, the grass just&nbsp;<em>is</em>&nbsp;green, and people from the Other Political Party just&nbsp;<em>are&nbsp;</em>possessed by demonic spirits of pure evil, to a world where it's possible that&nbsp;<em>reality</em>&nbsp;is going to be different from these beliefs and come back and surprise you. You might assign low probability to that in the grass-is-green case, but in a world where there's a territory separate from the map it is at least&nbsp;<em>conceivable</em>&nbsp;that reality turns out to disagree with you. There are people who could stand to rehearse this, maybe by visualizing themselves with a thought bubble, first in a world like X, then in a world like not-X, in cases where they are tempted to entirely neglect the possibility that they might be wrong. \"He hates me!\" and other beliefs about other people's motives seems to be a domain in which \"I believe that he hates me\" or \"I hypothesize that he hates me\" might work a lot better.</p>\n<p>Probabilistic reasoning is also a remedy for similar reasons: Implicit in a 75% probability of X is a 25% probability of not-X, so you're hopefully automatically considering more than one world. Assigning a probability also inherently reminds you that you're occupying an epistemic state, since only beliefs can be probabilistic, while reality itself is either one way or another.</p>\n<p><em>Skill 2: Perspective-taking on beliefs.</em></p>\n<p>What we really believe feels like the way the world&nbsp;<em>is;&nbsp;</em>from the inside, other people&nbsp;<em>feel like</em>&nbsp;they are inhabiting different worlds from you. They aren't disagreeing with you because they're obstinate, they're disagreeing because the world feels different to them - even if the two of you are in fact embedded in the same reality.</p>\n<p>This is one of the secret writing rules behind Harry Potter and the Methods of Rationality. When I write a character, e.g. Draco Malfoy, I don't just extrapolate their mind, I extrapolate the surrounding subjective world they live in, which has that character at the center; all other things seem important, or are considered at all, in relation to how important they are to that character. Most other books are never told from more than one character's viewpoint, but if they are, it's strange how often the other characters seem to be living inside the protagonist's universe and to think mostly about things that are important to the main protagonist. In HPMOR, when you enter Draco Malfoy's viewpoint, you are plunged into Draco Malfoy's subjective universe, in which Death Eaters have reasons for everything they do and Dumbledore is an exogenous reasonless evil. Since I'm not trying to show off postmodernism, everyone is still recognizably living in the same underlying reality, and the justifications of the Death Eaters only sound reasonable&nbsp;<em>to Draco,</em>&nbsp;rather than having been optimized to persuade the reader.&nbsp;It's not like the characters&nbsp;<em>literally</em>&nbsp;have their own universes, nor is morality handed out in equal portions to all parties regardless of what they do. But different elements of reality have different meanings and different importances to different characters.</p>\n<p>Joshua Greene has observed - I think this is in his&nbsp;<a href=\"http://www.wjh.harvard.edu/~jgreene/GreeneWJH/Greene-Dissertation.pdf\">Terrible, Horrible, No Good, Very Bad paper</a> - that most political discourse rarely gets beyond the point of lecturing naughty children who are just refusing to acknowledge the evident truth. As a special case, one may also appreciate internally that being wrong feels just like being right, unless you can actually perform some sort of experimental check.</p>\n<p><em>Skill 3: You are less bamboozleable by anti-epistemology or motivated neutrality which explicitly claims that there's no truth.</em></p>\n<p>This is a&nbsp;<em>negative</em>&nbsp;skill - avoiding one more wrong way to do it - and mostly about quoted arguments rather than positive reasoning you'd want to conduct yourself. Hence the sort of thing we want to put less emphasis on in training. Nonetheless, it's easier not to fall for somebody's line about the absence of objective truth, if you've previously spent a bit of time visualizing Sally and Anne with different beliefs, and separately, a marble for those beliefs to be compared-to. Sally and Anne have different <em>beliefs</em>, but there's only one way-things-are, the actual state of the marble, to which the beliefs can be compared; so no, they don't have 'different truths'. &nbsp;A real belief (as opposed to a belief-in-belief) will <em>feel</em>&nbsp;true, yes, so the two have different feelings-of-truth, but the feeling-of-truth is not the territory.</p>\n<p>To rehearse this, I suppose, you'd try to notice this kind of anti-epistemology when you ran across it, and maybe respond internally by actually visualizing two figures with thought bubbles and their single environment. Though I don't&nbsp;<em>think</em>&nbsp;most people who understood the core insight would require any further persuasion or rehearsal to avoid contamination by the fallacy.</p>\n<p><em>Skill 4: World-first reasoning about decisions a.k.a. the&nbsp;</em><em>Tarski Method&nbsp;</em><em>aka Litany of Tarski.</em></p>\n<p>Suppose you're considering whether to wash your white athletic socks with a dark load of laundry, and you're worried the colors might bleed into the socks, but on the other hand you really don't want to have to do another load just for the white socks. You might find your brain selectively rationalizing reasons why it's not all&nbsp;<em>that</em>&nbsp;likely for the colors to bleed - there's no really new dark clothes in there, say - trying to persuade itself that the socks won't be ruined. At which point it may help to say:</p>\n<p>\"If my socks will stain, I want to believe my socks will stain;<br />If my socks won't stain, I don't want to believe my socks will stain;<br />Let me not become attached to beliefs I may not want.\"</p>\n<p>To stop your brain trying to persuade itself, visualize that you are either&nbsp;<em>already in</em>&nbsp;the world where your socks will end up discolored, or already in the world where your socks will be fine, and in either case it is better for you to believe you're in the world you're actually in. Related mantras include \"That which can be destroyed by the truth should be\" and \"Reality is that which, when we stop believing in it, doesn't go away\". Appreciating that belief is not reality can help us to appreciate the primacy of reality, and either stop arguing with it and accept it, or actually become curious about it.</p>\n<p>Anna Salamon and I usually apply the Tarski Method by visualizing a world that is not-how-we'd-like or not-how-we-previously-believed, and ourselves as believing the contrary, and the disaster that would then follow. &nbsp;For example, let's say that you've been driving for a while, haven't reached your hotel, and are starting to wonder if you took a wrong turn... in which case you'd have to go back and drive another 40 miles in the opposite direction, which is an unpleasant thing to think about, so your brain tries to persuade itself that it's not lost. &nbsp;Anna and I use the form of the skill where we visualize the world where we <em>are</em>&nbsp;lost and <em>keep driving.</em></p>\n<p><em></em>Note that in principle, this is only one quadrant of a 2 x 2 matrix:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px;\">&nbsp;</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">In <strong>reality</strong>, you're<strong> </strong>heading in the right direction</span></td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">In <strong>reality</strong>, you're totally lost</span></td>\n</tr>\n<tr>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">You <strong>believe </strong>you're heading in the right direction</span></td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">No need to change anything - just keep doing what you're doing, and you'll get to the conference hotel</span></td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Just keep doing what you're doing, and you'll eventually drive your rental car directly into the sea</span></td>\n</tr>\n<tr>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">You <strong>believe </strong>you're lost</span></td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Alas! &nbsp;You spend 5 whole minutes of your life pulling over and asking for directions you didn't need</span></td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">After spending 5 minutes getting directions, you've got to turn around and drive 40 minutes the other way. </span></td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p>Michael \"Valentine\" Smith says that he practiced this skill by actually visualizing all four quadrants in turn, and that with a bit of practice he could do it very quickly, and that he thinks visualizing all four quadrants helped.</p>\n<p>(<a href=\"/lw/erp/skill_the_map_is_not_the_territory/#7k96\">Mainstream status</a> here.)</p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/eta/rationality_appreciating_cognitive_algorithms/\">Rationality: Appreciating Cognitive Algorithms</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/eqn/the_useful_idea_of_truth/\">The Useful Idea of Truth</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wMPYFGmhcFg4bSb4Z": 7, "Ng8Gice9KNkncxqcj": 2, "AHK82ypfxF45rqh9D": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KJ9MFBPwXGwNpadf2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 80, "baseScore": 94, "extendedScore": null, "score": 0.000208, "legacy": true, "legacyId": "19141", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "rationality-appreciating-cognitive-algorithms", "canonicalPrevPostSlug": "the-useful-idea-of-truth", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 94, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 181, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqvnWFtRD2keJdwjX", "HcCpvYLoSFP4iAqSz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-06T09:59:38.292Z", "modifiedAt": null, "url": null, "title": "Rationality: Appreciating Cognitive Algorithms", "slug": "rationality-appreciating-cognitive-algorithms", "viewCount": null, "lastCommentedAt": "2021-04-22T22:32:36.818Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HcCpvYLoSFP4iAqSz/rationality-appreciating-cognitive-algorithms", "pageUrlRelative": "/posts/HcCpvYLoSFP4iAqSz/rationality-appreciating-cognitive-algorithms", "linkUrl": "https://www.lesswrong.com/posts/HcCpvYLoSFP4iAqSz/rationality-appreciating-cognitive-algorithms", "postedAtFormatted": "Saturday, October 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%3A%20Appreciating%20Cognitive%20Algorithms&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%3A%20Appreciating%20Cognitive%20Algorithms%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHcCpvYLoSFP4iAqSz%2Frationality-appreciating-cognitive-algorithms%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%3A%20Appreciating%20Cognitive%20Algorithms%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHcCpvYLoSFP4iAqSz%2Frationality-appreciating-cognitive-algorithms", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHcCpvYLoSFP4iAqSz%2Frationality-appreciating-cognitive-algorithms", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1602, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/eqn/the_useful_idea_of_truth/\">The Useful Idea of Truth</a></p>\n<p>It is an error mode, and indeed an annoyance mode, to go about preaching the importance of the \"Truth\", especially if the Truth is supposed to be something incredibly lofty instead of some&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Joy_in_the_Merely_Real\">boring</a>,&nbsp;<a href=\"http://hpmor.com/chapter/73\">mundane</a>&nbsp;truth about gravity or rainbows or what your coworker said about your manager.</p>\n<p>Thus it is a worthwhile exercise to practice deflating the word 'true' out of any sentence in which it appears. (Note that this is a special case of&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Rationalist_taboo\">rationalist taboo</a>.) For example, instead of saying, \"I believe that the sky is blue, and that's&nbsp;<em>true!</em>\" you can just say, \"The sky is blue\", which conveys essentially the same information about what color you think the sky is. Or if it feels&nbsp;<em>different</em>&nbsp;to say \"I believe the Democrats will win the election!\" than to say, \"The Democrats will win the election\", this is an important warning of <a href=\"/lw/i4/belief_in_belief/\">belief-alief divergence</a>.</p>\n<p>Try it with these:</p>\n<ul>\n<li>I believe Jess just wants to win arguments. </li>\n<li>It&rsquo;s true that you weren&rsquo;t paying attention. </li>\n<li>I believe I will get better. </li>\n<li>In reality, teachers care a lot about students. </li>\n</ul>\n<p>If 'truth' is defined by an infinite family of sentences like 'The sentence \"the sky is blue\" is true if and only if the sky is blue', then why would we ever need to talk about 'truth' at all?</p>\n<p>Well, you can't deflate 'truth' out of the sentence \"True beliefs are more likely to make successful experimental predictions\" because it states a property of map-territory correspondences&nbsp;<em>in general</em>. You could say 'accurate maps' instead of 'true beliefs', but you would still be invoking the same&nbsp;<em>concept.</em></p>\n<p>It's only because most sentences containing the word 'true' are&nbsp;<em>not</em>&nbsp;talking about map-territory correspondences in general, that most such sentences can be deflated.</p>\n<p>Now consider - when are you&nbsp;<em>forced</em>&nbsp;to use the word 'rational'?<a id=\"more\"></a></p>\n<p>As with the word 'true', there are very few sentences that truly&nbsp;<em>need</em>&nbsp;to contain the word 'rational' in them. Consider the following deflations, all of which convey essentially the same information about your own opinions:</p>\n<ul>\n<li>\n<p>\"It's rational to believe the sky is blue\"&nbsp;<br />-&gt; \"I think the sky is blue\"&nbsp;<br />-&gt; \"The sky is blue\"</p>\n</li>\n<li>\n<p>\"Rational Dieting: Why To Choose Paleo\"&nbsp;<br />-&gt; \"Why you should think the paleo diet has the best consequences for health\"&nbsp;<br />-&gt; \"I like the paleo diet\"</p>\n</li>\n</ul>\n<p>Generally, when people bless something as 'rational', you could directly substitute the word 'optimal' with no loss of content - or in some cases the phrases 'true' or 'believed-by-me', if we're talking about a belief rather than a strategy.</p>\n<p>Try it with these:</p>\n<ul>\n<li>\"It&rsquo;s rational to teach your children calculus.\" </li>\n<li>\"I think this is the most rational book ever.\" </li>\n<li>\"It's rational to believe in gravity.\"</li>\n</ul>\n<p><em>M</em><em>editation</em><em>:</em>&nbsp;Under what rare circumstances can you&nbsp;<em>not</em>&nbsp;deflate the word 'rational' out of a sentence?</p>\n<p>...<br />...<br />...</p>\n<p>Reply: We need the word 'rational' in order to talk about&nbsp;<em>cognitive algorithms</em>&nbsp;or&nbsp;<em>mental processes</em>&nbsp;with the property \"systematically increases map-territory correspondence\" (epistemic rationality) or \"systematically finds a better path to goals\" (instrumental rationality).</p>\n<p>E.g.:</p>\n<p>\"It's (epistemically) rational to believe more in hypotheses that make successful experimental predictions.\"</p>\n<p>or</p>\n<p>\"Chasing <a href=\"http://en.wikipedia.org/wiki/Sunk_costs\">sunk costs</a> is (instrumentally) irrational.\"</p>\n<p>You can't deflate the&nbsp;<em>concept</em>&nbsp;of rationality out of the intended meaning of those sentences. You could find some way to rephrase it without the&nbsp;<em>word</em>&nbsp;'rational'; but then you'd have to use other words describing the same concept, e.g:</p>\n<p>\"If you believe more in hypotheses that make successful predictions, your map will better correspond to reality over time.\"</p>\n<p>or</p>\n<p>\"If you chase sunk costs, you won't achieve your goals as well as you could otherwise.\"</p>\n<p>The word 'rational' is properly used to talk about&nbsp;<em>cognitive algorithms</em>&nbsp;which&nbsp;<em>systematically</em>&nbsp;promote map-territory correspondences or goal achievement.</p>\n<p>Similarly, a rationalist isn't just somebody who respects the Truth.</p>\n<p>All too many people respect the Truth.</p>\n<p>They respect the Truth that the U.S. government planted explosives in the World Trade Center, the Truth that the stars control human destiny (ironically, the exact reverse will be true if everything goes right), the Truth that global warming is a lie... and so it goes.</p>\n<p>A rationalist is somebody who respects the&nbsp;<em>processes of finding truth.</em>&nbsp;They respect somebody who seems to be showing genuine curiosity, even if that curiosity is about a should-already-be-settled issue like whether the World Trade Center was brought down by explosives, because genuine curiosity is part of a lovable algorithm and respectable process. They respect Stuart Hameroff for trying to test whether neurons have properties conducive to quantum computing, even if this idea seems exceedingly unlikely a priori and was suggested by awful G&ouml;delian arguments about why brains can't be mechanisms, because Hameroff was&nbsp;<em>trying to test his wacky beliefs experimentally,</em>&nbsp;and humanity would still be living on the savanna if 'wacky' beliefs never got tested experimentally.</p>\n<p>Or consider the controversy over the way CSICOP (Committee for Skeptical Investigation of Claims of the Paranormal) handled the so-called&nbsp;<a href=\"http://en.wikipedia.org/wiki/Mars_effect\">Mars effect</a>, the controversy which led founder Dennis Rawlins to leave CSICOP. Does the position of the planet Mars in the sky during your hour of birth,&nbsp;<em>actually</em>&nbsp;have an effect on whether you'll become a famous athlete? I'll go out on a limb and say no. And if you <em>only </em>respect the Truth, then it doesn't matter very much whether CSICOP raised the goalposts on the astrologer Gauquelin - i.e., stated a test and then made up new reasons to reject the results after Gauquelin's result came out positive. The astrological conclusion is almost certainly un-true... and that conclusion was indeed derogated, the Truth upheld.</p>\n<p>But a&nbsp;<em>rationalist</em>&nbsp;is disturbed by the claim that there were&nbsp;<em>rational process violations</em>. As a Bayesian, in a case like this you do update to a very small degree in favor of astrology, just not enough to overcome the prior odds; and you update to a larger degree that Gauquelin has inadvertantly uncovered some other phenomenon that might be worth tracking down. One definitely shouldn't state a test and then ignore the results, or find new reasons the test is invalid, when the results don't come out your way. That process has bad&nbsp;<em>systematic</em>&nbsp;properties for finding truth - and a rationalist doesn't just appreciate the beauty of the Truth, but the beauty of the processes and cognitive algorithms that get us there.[1]</p>\n<p>The reason why rationalists can have unusually productive and friendly conversations<em>&nbsp;at least when everything goes right,</em>&nbsp;is not that everyone involved has a great and abiding respect for whatever they think is the True or the Optimal in any given moment. Under most everyday conditions, people who argue heatedly aren't doing so because they know the truth but disrespect it. Rationalist conversations are (potentially) more productive to the degree that everyone respects the&nbsp;<em>process,</em>&nbsp;and is on mostly the same page about what the process should be, thanks to all that explicit study of things like cognitive psychology and probability theory. When Anna tells me, \"I'm worried that you don't seem very curious about this,\" there's this state of mind called 'curiosity' that we both agree is important - as a matter of&nbsp;<em>rational process,</em>&nbsp;on a meta-level above the particular issue at hand - and I know as a matter of process that when a respected fellow rationalist tells me that I need to become curious, I should pause and check my curiosity levels and try to increase them.</p>\n<p>Is rationality-use necessarily tied to rationality-appreciation? &nbsp;I can imagine a world filled with hordes of rationality-users who were taught in school to use the Art competently, even though only very few people love the Art enough to try to advance it further; and everyone else has no particular love or interest in the Art apart from the practical results it brings. Similarly, I can imagine a competent applied mathematician who only worked at a hedge fund for the money, and had never loved math or programming or optimization in the first place - who'd been in it for the money from day one. I can imagine a competent musician who had no particular love in composition or joy in music, and who only cared for the album sales and groupies. Just because something is imaginable doesn't make it probable in real life... but then there are many children who learn to play the piano despite having no love for it; \"musicians\" are those who are <em>unusually</em>&nbsp;good at it, not the adequately-competent.</p>\n<p>But for now, in this world where the Art is&nbsp;<em>not</em>&nbsp;yet forcibly impressed on schoolchildren nor yet explicitly rewarded in a standard way on standard career tracks, almost everyone who has any skill at rationality is the sort of person who finds the Art intriguing for its own sake. Which - perhaps unfortunately - explains quite a bit, both about rationalist communities and about the world.</p>\n<hr />\n<p>[1] RationalWiki really needs to rename itself to SkepticWiki. They're very interested in kicking hell out of homeopathy, but not as a group interested in the abstract beauty of questions like \"What trials should a strange new hypothesis undergo, which it will&nbsp;<em>not</em>fail if the hypothesis is true?\" You can go to them and be like, \"You're criticizing theory X because some people who believe in it are stupid; but many true theories have stupid believers, like how Deepak Chopra claims to be talking about quantum physics; so this is not a useful method in general for discriminating true and false theories\" and they'll be like, \"Ha! So what? Who cares? X is crazy!\" I think it was actually RationalWiki which first observed that it and Less Wrong ought to swap names.</p>\n<hr />\n<p>(<a href=\"#7kyw\">Mainstream status here</a>.)</p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/etf/firewalling_the_optimal_from_the_rational/\">Firewalling the Optimal from the Rational</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/erp/skill_the_map_is_not_the_territory/\">Skill: The Map is Not the Territory</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 18, "HXA9WxPpzZCCEwXHT": 2, "LnEEs8xGooYmQ8iLA": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HcCpvYLoSFP4iAqSz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 63, "baseScore": 74, "extendedScore": null, "score": 0.000161, "legacy": true, "legacyId": "19198", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "firewalling-the-optimal-from-the-rational", "canonicalPrevPostSlug": "skill-the-map-is-not-the-territory", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 74, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 136, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqvnWFtRD2keJdwjX", "CqyJzDZWvGhhFJ7dY", "WbLAA8qZQNdbRgKte", "KJ9MFBPwXGwNpadf2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-06T11:25:53.904Z", "modifiedAt": null, "url": null, "title": "[LINK] The half-life of a fact", "slug": "link-the-half-life-of-a-fact", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:05.630Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G86ZKPgKGdWeHTQAW/link-the-half-life-of-a-fact", "pageUrlRelative": "/posts/G86ZKPgKGdWeHTQAW/link-the-half-life-of-a-fact", "linkUrl": "https://www.lesswrong.com/posts/G86ZKPgKGdWeHTQAW/link-the-half-life-of-a-fact", "postedAtFormatted": "Saturday, October 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20The%20half-life%20of%20a%20fact&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20The%20half-life%20of%20a%20fact%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG86ZKPgKGdWeHTQAW%2Flink-the-half-life-of-a-fact%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20The%20half-life%20of%20a%20fact%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG86ZKPgKGdWeHTQAW%2Flink-the-half-life-of-a-fact", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG86ZKPgKGdWeHTQAW%2Flink-the-half-life-of-a-fact", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 198, "htmlBody": "<p>Everything you know will, in due course, be wrong. A <a href=\"http://www.slate.com/articles/health_and_science/books/2012/10/samuel_arbesman_s_the_half_life_of_facts_reviewed_.html\">review</a> of Samuel Arbesman's <em><a href=\"http://www.amazon.com/gp/product/159184472X\">The Half-Life of Facts: Why Everything We Know Has an Expiration Date</a></em> in Slate:</p>\n<p style=\"padding-left: 30px;\">Arbesman's book expands on a piece he wrote in 2010 for the Ideas section of the <em>Boston Globe</em>. That short essay, called \"<a href=\"http://www.boston.com/bostonglobe/ideas/articles/2010/02/28/warning_your_reality_is_out_of_date/\">Warning: Your reality is out of date</a>,\" laid out a theory of what Arbesman named the <em>mesofact</em>. \"When people think of knowledge,\" he wrote, \"they generally think of two sorts of facts.\" One includes the data that should never change, like the atomic weight of hydrogen, while the other comprises all the tidbits that shift from day to day, like the closing price of the Dow Jones Industrial Average. Even in the stable camp, facts can mutate: An atom's weight, for example, varies depending on the isotope. But Arbesman is more interested in a third category of knowledge, one that's nestled between the other two in terms of how amenable it is to change. These are the facts that shift too slowly for us to notice, but not so slowly that they'll only matter to our children. \"Mesofacts,\" he says, evolve within our lifetimes but often out of view.</p>\n<p><a href=\"http://www.slate.com/articles/health_and_science/books/2012/10/samuel_arbesman_s_the_half_life_of_facts_reviewed_.html\">http://www.slate.com/articles/health_and_science/books/2012/10/samuel_arbesman_s_the_half_life_of_facts_reviewed_.html</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G86ZKPgKGdWeHTQAW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 3, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "19199", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-06T13:35:42.346Z", "modifiedAt": null, "url": null, "title": "Online Optimal Philanthropy Meetup: Tue 10/9, 8pm ET", "slug": "online-optimal-philanthropy-meetup-tue-10-9-8pm-et", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:06.422Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8wfAnjrxRG4oBG3An/online-optimal-philanthropy-meetup-tue-10-9-8pm-et", "pageUrlRelative": "/posts/8wfAnjrxRG4oBG3An/online-optimal-philanthropy-meetup-tue-10-9-8pm-et", "linkUrl": "https://www.lesswrong.com/posts/8wfAnjrxRG4oBG3An/online-optimal-philanthropy-meetup-tue-10-9-8pm-et", "postedAtFormatted": "Saturday, October 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Online%20Optimal%20Philanthropy%20Meetup%3A%20Tue%2010%2F9%2C%208pm%20ET&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOnline%20Optimal%20Philanthropy%20Meetup%3A%20Tue%2010%2F9%2C%208pm%20ET%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8wfAnjrxRG4oBG3An%2Fonline-optimal-philanthropy-meetup-tue-10-9-8pm-et%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Online%20Optimal%20Philanthropy%20Meetup%3A%20Tue%2010%2F9%2C%208pm%20ET%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8wfAnjrxRG4oBG3An%2Fonline-optimal-philanthropy-meetup-tue-10-9-8pm-et", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8wfAnjrxRG4oBG3An%2Fonline-optimal-philanthropy-meetup-tue-10-9-8pm-et", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<p>How can we do the most good in the world? What impact can we have, and what's the impact on us?? &nbsp;What are the best charities? &nbsp;How can we evaluate them? &nbsp;Online discussion via g+ hangout, Tuesday October 9th, 8pm ET (Boston time).</p>\n<p>https://plus.google.com/u/0/events/cj57chi6jgse1avv8f80hjh9g3c?a=b</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8wfAnjrxRG4oBG3An", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.0030320346055073e-06, "legacy": true, "legacyId": "19200", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-07T02:59:55.128Z", "modifiedAt": null, "url": null, "title": "Giulio Tononi's \"Integrated Information Theory\" of Consciousness", "slug": "giulio-tononi-s-integrated-information-theory-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:33.236Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pjQgJZYTRw3LYjSGa/giulio-tononi-s-integrated-information-theory-of", "pageUrlRelative": "/posts/pjQgJZYTRw3LYjSGa/giulio-tononi-s-integrated-information-theory-of", "linkUrl": "https://www.lesswrong.com/posts/pjQgJZYTRw3LYjSGa/giulio-tononi-s-integrated-information-theory-of", "postedAtFormatted": "Sunday, October 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Giulio%20Tononi's%20%22Integrated%20Information%20Theory%22%20of%20Consciousness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGiulio%20Tononi's%20%22Integrated%20Information%20Theory%22%20of%20Consciousness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpjQgJZYTRw3LYjSGa%2Fgiulio-tononi-s-integrated-information-theory-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Giulio%20Tononi's%20%22Integrated%20Information%20Theory%22%20of%20Consciousness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpjQgJZYTRw3LYjSGa%2Fgiulio-tononi-s-integrated-information-theory-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpjQgJZYTRw3LYjSGa%2Fgiulio-tononi-s-integrated-information-theory-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p>My daily browsing has come across an idea I haven't seen before, though it has been <a href=\"https://encrypted.google.com/search?q=site%3Alesswrong.com+tononi\">mentioned on LW</a> occasionally. Created by <a href=\"http://tononi.psychiatry.wisc.edu/People/GiulioTononi.php\">Giulio Tononi</a>, the basic premise seems to be that consciousness can be quantified, by measuring how much information is contained within the overall patterns of a system in excess of the information contained within its subsystems - that the whole is greater than the sum of its parts.</p>\n<p>Some further browsing has let me find a somewhat uninformative <a href=\"https://en.wikipedia.org/wiki/Integrated_Information_Theory_%28IIT%29\">Wikipedia article</a>, a <a href=\"http://www.amazon.com/Phi-A-Voyage-Brain-Soul/dp/030790721X/ref=sr_1_1\">printed book</a> using a fictional narrative, a <a href=\"http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002236\">scientific paper</a>, and a '<a href=\"http://www.biolbull.org/content/215/3/216.long\">Provisional Manifesto</a>'.</p>\n<p>The general idea doesn't seem to fall prey to most of the more obvious flaws that theories about consciousness tend to end up suffering. But I'm far from an expert in the field, or potentially related fields such as considering &Phi;'s possible use as a basis for developing AGI/FAI. So: what does the wisdom of the crowd of LW have to say about this concept?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pjQgJZYTRw3LYjSGa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 4, "extendedScore": null, "score": 1.0034569157111088e-06, "legacy": true, "legacyId": "19204", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-07T03:15:09.561Z", "modifiedAt": null, "url": null, "title": "[LINK] Learning without practice, through fMRI induction", "slug": "link-learning-without-practice-through-fmri-induction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:03.256Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "maia", "createdAt": "2012-02-08T20:28:42.643Z", "isAdmin": false, "displayName": "maia"}, "userId": "QW72Lk68mKnTfmdAB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zEQZjDea7SC4edob3/link-learning-without-practice-through-fmri-induction", "pageUrlRelative": "/posts/zEQZjDea7SC4edob3/link-learning-without-practice-through-fmri-induction", "linkUrl": "https://www.lesswrong.com/posts/zEQZjDea7SC4edob3/link-learning-without-practice-through-fmri-induction", "postedAtFormatted": "Sunday, October 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Learning%20without%20practice%2C%20through%20fMRI%20induction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Learning%20without%20practice%2C%20through%20fMRI%20induction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzEQZjDea7SC4edob3%2Flink-learning-without-practice-through-fmri-induction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Learning%20without%20practice%2C%20through%20fMRI%20induction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzEQZjDea7SC4edob3%2Flink-learning-without-practice-through-fmri-induction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzEQZjDea7SC4edob3%2Flink-learning-without-practice-through-fmri-induction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p><a href=\"http://www.nsf.gov/news/news_summ.jsp?cntn_id=122523&amp;org=NSF&amp;from=news\">http://www.nsf.gov/news/news_summ.jsp?cntn_id=122523&amp;org=NSF&amp;from=news<br /></a>From the article:</p>\n<blockquote>\n<p>New research published today in the journal&nbsp;<em>Science</em>&nbsp;suggests it may be possible to use brain technology to learn to play a piano, reduce mental stress or hit a curve ball with little or no conscious effort.&nbsp;It's the kind of thing seen in Hollywood's \"Matrix\" franchise.</p>\n<p>Think of a person watching a computer screen and having his or her brain patterns modified to match those of a high-performing athlete or modified to recuperate from an accident or disease. Though preliminary, researchers say such possibilities may exist in the future.</p>\n<p>Experiments conducted at Boston University (BU) and ATR Computational Neuroscience Laboratories in Kyoto, Japan, recently demonstrated that through a person's visual cortex, researchers could use decoded functional magnetic resonance imaging (fMRI) to induce brain activity patterns to match a previously known target state and thereby improve performance on visual tasks.</p>\n</blockquote>\n<p>EDIT: To clarify, this is almost certainly over-hyped. However, it appears to at least be an instance of very interesting biofeedback.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zEQZjDea7SC4edob3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 1.0034649706515869e-06, "legacy": true, "legacyId": "19205", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-07T03:44:03.557Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Aiming at the Target", "slug": "seq-rerun-aiming-at-the-target", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:23.507Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7pW2FdS26yRdbC4ga/seq-rerun-aiming-at-the-target", "pageUrlRelative": "/posts/7pW2FdS26yRdbC4ga/seq-rerun-aiming-at-the-target", "linkUrl": "https://www.lesswrong.com/posts/7pW2FdS26yRdbC4ga/seq-rerun-aiming-at-the-target", "postedAtFormatted": "Sunday, October 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Aiming%20at%20the%20Target&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Aiming%20at%20the%20Target%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7pW2FdS26yRdbC4ga%2Fseq-rerun-aiming-at-the-target%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Aiming%20at%20the%20Target%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7pW2FdS26yRdbC4ga%2Fseq-rerun-aiming-at-the-target", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7pW2FdS26yRdbC4ga%2Fseq-rerun-aiming-at-the-target", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<p>Today's post, <a href=\"/lw/v9/aiming_at_the_target/\">Aiming at the Target</a> was originally published on 26 October 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Aiming_at_the_Target\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When you make plans, you are trying to steer the future into regions higher in your preference ordering.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/es9/seq_rerun_belief_in_intelligence/\">Belief in Intelligence</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7pW2FdS26yRdbC4ga", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.003480245167041e-06, "legacy": true, "legacyId": "19206", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CW6HDvodPpNe38Cry", "Ktd3qtF7Nqo2Nhtbn", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-07T07:29:43.869Z", "modifiedAt": null, "url": null, "title": "[Book Review] \"The Signal and the Noise: Why So Many Predictions Fail\u2014But Some Don\u2019t.\", by Nate Silver", "slug": "book-review-the-signal-and-the-noise-why-so-many-predictions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:07.078Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CSs3x3siBHqQeEWGE/book-review-the-signal-and-the-noise-why-so-many-predictions", "pageUrlRelative": "/posts/CSs3x3siBHqQeEWGE/book-review-the-signal-and-the-noise-why-so-many-predictions", "linkUrl": "https://www.lesswrong.com/posts/CSs3x3siBHqQeEWGE/book-review-the-signal-and-the-noise-why-so-many-predictions", "postedAtFormatted": "Sunday, October 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BBook%20Review%5D%20%22The%20Signal%20and%20the%20Noise%3A%20Why%20So%20Many%20Predictions%20Fail%E2%80%94But%20Some%20Don%E2%80%99t.%22%2C%20by%20Nate%20Silver&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BBook%20Review%5D%20%22The%20Signal%20and%20the%20Noise%3A%20Why%20So%20Many%20Predictions%20Fail%E2%80%94But%20Some%20Don%E2%80%99t.%22%2C%20by%20Nate%20Silver%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCSs3x3siBHqQeEWGE%2Fbook-review-the-signal-and-the-noise-why-so-many-predictions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BBook%20Review%5D%20%22The%20Signal%20and%20the%20Noise%3A%20Why%20So%20Many%20Predictions%20Fail%E2%80%94But%20Some%20Don%E2%80%99t.%22%2C%20by%20Nate%20Silver%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCSs3x3siBHqQeEWGE%2Fbook-review-the-signal-and-the-noise-why-so-many-predictions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCSs3x3siBHqQeEWGE%2Fbook-review-the-signal-and-the-noise-why-so-many-predictions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<p>Here's a link to a review, by The Economist, of a book about prediction, some of the common ways in which people make mistakes and some of the methods by which they could improve:</p>\n<p><a href=\"https://www.economist.com/node/21564181\">Looking ahead : How to look ahead&mdash;and get it right</a></p>\n<p>One paragraph from that review:</p>\n<blockquote>\n<p>A guiding light for Mr Silver is Thomas Bayes, an 18th-century English  churchman and pioneer of probability theory. Uncertainty and  subjectivity are inevitable, says Mr Silver. People should not get hung  up on this, and instead think about the future the way gamblers do: &ldquo;as  speckles of probability&rdquo;. In one surprising chapter, poker, a game from  which Mr Silver once earned a living, emerges as a powerful teacher of  the virtues of humility and patience.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 2, "8daMDi9NEShyLqxth": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CSs3x3siBHqQeEWGE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 16, "extendedScore": null, "score": 1.0035995340243065e-06, "legacy": true, "legacyId": "19208", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-07T16:23:05.247Z", "modifiedAt": null, "url": null, "title": "Raising the waterline", "slug": "raising-the-waterline", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:33.374Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RiCkRACFwQPsH349u/raising-the-waterline", "pageUrlRelative": "/posts/RiCkRACFwQPsH349u/raising-the-waterline", "linkUrl": "https://www.lesswrong.com/posts/RiCkRACFwQPsH349u/raising-the-waterline", "postedAtFormatted": "Sunday, October 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Raising%20the%20waterline&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARaising%20the%20waterline%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRiCkRACFwQPsH349u%2Fraising-the-waterline%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Raising%20the%20waterline%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRiCkRACFwQPsH349u%2Fraising-the-waterline", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRiCkRACFwQPsH349u%2Fraising-the-waterline", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1850, "htmlBody": "<p>Among the goals of Less Wrong is to \"<a href=\"/lw/1e/raising_the_sanity_waterline/\">raise the sanity waterline</a>\" of humanity. We've also talked about \"raising the <a href=\"/lw/8j2/infographic_a_reminder_as_to_how_far_the/\">rationality waterline</a>\": the phrase is somewhat <a href=\"/search/results?cx=015839050583929870010%3A-802ptn4igi&amp;cof=FORID%3A11&amp;ie=UTF-8&amp;q=rationality+waterline&amp;sa=Search\">popular</a> around these parts, which suggests that the metaphor is catchy. But is that all there is to it, a catchy metaphor? Or can the phrase be more usefully cashed out?</p>\n<p>While reading Nate Silver's <a href=\"http://www.amazon.com/dp/159420411X\"><em>The Signal and the Noise</em></a>, I came across a discussion of \"raising the waterline\" which fleshes out the metaphor with a more substantial model. This model preserves some of the salient aspects of the metaphor as discussed on LW, for instance the perception that the current waterline (as regards sanity and rationality) is \"ridiculously low\". More interestingly, it fleshes out some of the specific ways that a \"waterline\" belief should constrain our future sensory experiences, maybe even to the point of <em>quantifying</em> what should result from low (or rising) waterlines.</p>\n<p>This is intended as a short series:</p>\n<ul>\n<li>\"<strong>Raising the waterline</strong>\", this introductory post, will summarize Nate Silver's \"waterline\" model, within its original context of playing Poker, which Silver frames as a game of prediction under <a href=\"/lw/vo/lawful_uncertainty/\">uncertainty</a>. Poker therefore serves as a \"toy model\" for a much more general class of problems.</li>\n<li>\"<strong>Raising the forecasting waterline</strong>\" will extend the discussion to the kind of forecasts studied by Philip Tetlock's <a href=\"/lw/6ya/link_get_paid_to_train_your_rationality/\">Good Judgement Project</a>, a prediction game somewhat similar to PredictionBook and related to prediction markets; I will leverage the waterline model to extract useful insights from my participation in GJP.</li>\n<li>\"<strong>Raising the discussion waterline</strong>\", a shamelessly speculative coda, will relate the previous two posts to the question of \"how do Internet discussions reliably lead to correct inferences from true beliefs, or fail to do so\"; I will argue that the waterline model brings some hope that a few basic tactics could nevertheless provide large wins, and raise the more general question of what other low waterlines we could aim to exploit.</li>\n</ul>\n<h2><a id=\"more\"></a>The Model</h2>\n<p>The waterline model is introduced in Chaper 10, \"The Poker Bubble\", to explain how for a period of time in the 2000's Silver found it fairly easy to make a living from playing online poker, but this source of revenue later dried up altogether.</p>\n<blockquote>\n<p>I was one of those people. I lived the poker dream for a while, and then it died. I learned that poker sits at the muddy confluence of the signal and the noise. My years in the game taught me a great deal about the role that chance plays in our lives and the delusions it can produce when we seek to understand the world and predict its course.</p>\n</blockquote>\n<p>One graph neatly summarizes all features of the model, and strikes me as a good candidate for illlustrating the \"one picture worth a thousand words\" dictum:</p>\n<p><img src=\"http://i.imgur.com/ieQdX.png\" alt=\"\" width=\"600\" height=\"428\" /></p>\n<p>The horizontal dimension is \"effort\" or \"experience\". One possible unit of measurement there could be \"hours\" - with the caveat that they should be hours of <em>deliberate</em> practice. (I'm not entirely sure why this is expressed as a percentage - I'll come back to taking the graph with a grain of salt.)</p>\n<p>The vertical dimension is labeled \"accuracy\", but we could more simply call it \"gain\". One possible unit of measurement could be \"money earned over some period of time playing the game\".</p>\n<h2>Chance, skill and practice</h2>\n<p>I want to briefly come back to the distinction between experience and practice, as this is a very important but often missed point.</p>\n<p>Poker, like all games of chance, driven by random reinforcement, is a highly addictive activity. (In the days following having my interest piqued by Silver's description and wanting to give it a try, I found myself losing more hours to the game than I care to think about - and I was only playing computer opponents for virtual chips, poker's methadone compared to the crack-like properties of online play for real money&nbsp;<a style=\"background-color: #ffffff;\" name=\"f1c\"></a><sup><a href=\"#f1\">1</a></sup>.) It is entirely possible to spend a lot of time in actual play without ever having much to show for it in terms of improvement.</p>\n<p>Thus, a less easily measurable but more appropriate construct for the horizontal axis would be something like \"number of basic insights absorbed, in the appropriate order\". One way to operationalize this would be to devise a number of tests, for instance, and apply these tests externally to the behaviour of a player: do they fold under circumstance X, raise under circumstance Y, are they able to quantify this or that aspect of the game?</p>\n<p>This distinction is well-known in other domains, such as software engineering, where \"10 years of experience is not the same as one year of experience repeated 10 times\" turns out to be a useful mantra in hiring situations.</p>\n<h2>Poker's Pareto Principle</h2>\n<p>The most interesting features of the model are the curve itself, relating effort and gain; and the \"waterline\".</p>\n<p>The curve isn't linear, but follows the same shape as a <a href=\"http://en.wikipedia.org/wiki/Pareto_distribution\">Pareto distribution</a>: it obeys the \"80/20\"principle most often associated with Pareto's original observation in the domain of economics - twenty percent of the population holds eighty percent of the wealth. Here, the idea is that twenty percent of the effort is enough to get you at a level of performance better than eighty percent of the population - not bad!</p>\n<blockquote>\n<p>In poker, for instance, simply learning to fold your worst hands, bet your best ones, and make some effort to consider what your opponent holds will substantially mitigate your losses. If you are willing to do this, then perhaps 80 percent of the time you will be making the same decision as one of the best poker players like Dwan&mdash;even if you have spent only 20 percent as much time studying the game.</p>\n</blockquote>\n<p>Silver dubs this the \"Pareto Principle of Prediction\" - it applies well beyond poker, to a large class of activities based on skills that require deliberate practice.</p>\n<h2>Raising the waterline</h2>\n<p>The curve divides into three parts: in the first part, progress is very rapid, disproportionate to the amount of effort you put in. In the middle, you are \"grinding\" - progress is steadier, requiring the accumulation and honing of a number of distinct techniques. Finally, as you near the top of the performance curve, ever-smaller gains in performance require ever-greater refinement of your existing skills and acquisition of subtle nuances of technique. This is the <a href=\"http://en.wikipedia.org/wiki/Ten_Thousand_Hour_Rule\">ten-thousand hour</a> domain, that of \"mastery\".</p>\n<p>The \"waterline\" represents the typical level of performance that you can expect to see in the player population. Silver represents it as a horizontal line, so we need to think of it as a \"gain\" level - the typical (say, median) poker player is earning (or losing) the amount of money per time period implied by the particular position of the waterline.</p>\n<p>The key idea in the waterline is that in many cases it's not how well you do in an absolute sense that matters - it's how well you do <em>relative to the competition</em>. This is especially true in a zero-sum game! If the waterline is low, then you can make very handsome gains at the cost of a limited investment in acquiring the basic skills. But if the waterline is high, there is no alternative, before you can beat the competition, to grinding your way through the lower insights, until you finally level up enough:</p>\n<blockquote>\n<p>When a field is highly competitive, it is only through this painstaking effort around the margin that you can make any money. There is a &ldquo;water level&rdquo; established by the competition and your profit will be like the tip of an iceberg: a small sliver of competitive advantage floating just above the surface, but concealing a vast bulwark of effort that went in to support it.</p>\n</blockquote>\n<p>(This, Silver argues, is what happened to online poker after 2006 and the Unlawful Internet Gambling Enforcement Act. The professional or semi-professional players, who derived an actual income stream from the game, continued playing, but the weaker amateurs quit, in the face of a tougher environment. Bad for poker, maybe; good for rationalists, at least insofar as it led Silver to start work on his election forecasting site FiveThirtyEight.com which has debunked a fair amount of madness about political polling, and ultimately to his book, which may introduce a broader population to largely Less Wrong compatible ideas.)</p>\n<h2>The (somewhat) bad news</h2>\n<p>Another important caveat here - this is, as far as I can tell, purely a conceptual model: I'm not aware that there's much hard empirical data that supports the curve having the shape pictured above. Silver does have statistics to show that in the case of poker, the population of mediocre players has a key role in \"feeding\" the better players, and based on before-and-after numbers, makes a good case that bad players drying up is bad for the better players. However, he doesn't cite anything that would suggest the precise relationship between effort and gain has been measured and shown to fit the curve.</p>\n<h2>The good news</h2>\n<p>This model and its more fleshed-out description of what a \"ridiculously low\" waterline entails are great news: it gives us some testable predictions. Suppose you find yourself in a competitive situation; conditional on the Pareto principle being applicable there, if you notice that applying a short set of uncomplicated techniques reliably results in outperforming your peers, that constitutes some evidence of a low waterline.&nbsp;</p>\n<p>For instance, if it is indeed the case that the waterline is very low in the skill of \"thinking probabilistically\", then those of us who have heeded the <a href=\"/lw/erp/skill_the_map_is_not_the_territory/\">lessons</a> of Less Wrong should be able to perform very well in a competitive forecasting environment, by applying only a few basic tools that unfortunately (for them) the general population doesn't yet possess.</p>\n<p>Failure to observe this would also have <em>interesting</em> consequences, raising the strength of alternate hypotheses: for instance a) we're not as good at applying even those basic techniques as we thought we were, or b) this particular competitive domain is not after all governed by a Pareto distribution, or c) the waterline is not as low as we thought it was.</p>\n<p>It turns out I was able to get some experience in just the kind of setting where this type of test could be performed: the Good Judgment Project. That's my next post.</p>\n<p>&nbsp;</p>\n<hr />\n<p style=\"font-size:90%\"><sup><a href=\"#f1c\">1</a></sup>&nbsp;This post is one way I hope to redeem those hours, turning them into a more productive effort after the fact.</p>\n<p style=\"font-size:90%\">P.S.: as this has been asked <a href=\"/lw/etk/book_review_the_signal_and_the_noise_why_so_many/\">elsewhere</a>, would I recommend Nate Silver's book? I certainly enjoyed it a lot, even though not all of it was new to me, and I often found myself wishing for clearer separation between the undoubtedly fascinating stories he tells - about financial panics, stock market crashes, baseball, poker, presidential elections, and so on - and the insights he draws from them, such as the waterline model. But enjoying it isn't quite the same as being ready to endorse it to others; I'm not quite sure yet how much value it would have, depending on the audience. Veteran Less Wrongers might not respond to it the same way as the general public, for instance. I found it valuable enough that I might invest some time in writing a short chapter-by-chapter summary and overall review, to answer that question for myself.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xexCWMyds6QLWognu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RiCkRACFwQPsH349u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 47, "extendedScore": null, "score": 0.000111, "legacy": true, "legacyId": "19209", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 47, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Among the goals of Less Wrong is to \"<a href=\"/lw/1e/raising_the_sanity_waterline/\">raise the sanity waterline</a>\" of humanity. We've also talked about \"raising the <a href=\"/lw/8j2/infographic_a_reminder_as_to_how_far_the/\">rationality waterline</a>\": the phrase is somewhat <a href=\"/search/results?cx=015839050583929870010%3A-802ptn4igi&amp;cof=FORID%3A11&amp;ie=UTF-8&amp;q=rationality+waterline&amp;sa=Search\">popular</a> around these parts, which suggests that the metaphor is catchy. But is that all there is to it, a catchy metaphor? Or can the phrase be more usefully cashed out?</p>\n<p>While reading Nate Silver's <a href=\"http://www.amazon.com/dp/159420411X\"><em>The Signal and the Noise</em></a>, I came across a discussion of \"raising the waterline\" which fleshes out the metaphor with a more substantial model. This model preserves some of the salient aspects of the metaphor as discussed on LW, for instance the perception that the current waterline (as regards sanity and rationality) is \"ridiculously low\". More interestingly, it fleshes out some of the specific ways that a \"waterline\" belief should constrain our future sensory experiences, maybe even to the point of <em>quantifying</em> what should result from low (or rising) waterlines.</p>\n<p>This is intended as a short series:</p>\n<ul>\n<li>\"<strong>Raising the waterline</strong>\", this introductory post, will summarize Nate Silver's \"waterline\" model, within its original context of playing Poker, which Silver frames as a game of prediction under <a href=\"/lw/vo/lawful_uncertainty/\">uncertainty</a>. Poker therefore serves as a \"toy model\" for a much more general class of problems.</li>\n<li>\"<strong>Raising the forecasting waterline</strong>\" will extend the discussion to the kind of forecasts studied by Philip Tetlock's <a href=\"/lw/6ya/link_get_paid_to_train_your_rationality/\">Good Judgement Project</a>, a prediction game somewhat similar to PredictionBook and related to prediction markets; I will leverage the waterline model to extract useful insights from my participation in GJP.</li>\n<li>\"<strong>Raising the discussion waterline</strong>\", a shamelessly speculative coda, will relate the previous two posts to the question of \"how do Internet discussions reliably lead to correct inferences from true beliefs, or fail to do so\"; I will argue that the waterline model brings some hope that a few basic tactics could nevertheless provide large wins, and raise the more general question of what other low waterlines we could aim to exploit.</li>\n</ul>\n<h2 id=\"The_Model\"><a id=\"more\"></a>The Model</h2>\n<p>The waterline model is introduced in Chaper 10, \"The Poker Bubble\", to explain how for a period of time in the 2000's Silver found it fairly easy to make a living from playing online poker, but this source of revenue later dried up altogether.</p>\n<blockquote>\n<p>I was one of those people. I lived the poker dream for a while, and then it died. I learned that poker sits at the muddy confluence of the signal and the noise. My years in the game taught me a great deal about the role that chance plays in our lives and the delusions it can produce when we seek to understand the world and predict its course.</p>\n</blockquote>\n<p>One graph neatly summarizes all features of the model, and strikes me as a good candidate for illlustrating the \"one picture worth a thousand words\" dictum:</p>\n<p><img src=\"http://i.imgur.com/ieQdX.png\" alt=\"\" width=\"600\" height=\"428\"></p>\n<p>The horizontal dimension is \"effort\" or \"experience\". One possible unit of measurement there could be \"hours\" - with the caveat that they should be hours of <em>deliberate</em> practice. (I'm not entirely sure why this is expressed as a percentage - I'll come back to taking the graph with a grain of salt.)</p>\n<p>The vertical dimension is labeled \"accuracy\", but we could more simply call it \"gain\". One possible unit of measurement could be \"money earned over some period of time playing the game\".</p>\n<h2 id=\"Chance__skill_and_practice\">Chance, skill and practice</h2>\n<p>I want to briefly come back to the distinction between experience and practice, as this is a very important but often missed point.</p>\n<p>Poker, like all games of chance, driven by random reinforcement, is a highly addictive activity. (In the days following having my interest piqued by Silver's description and wanting to give it a try, I found myself losing more hours to the game than I care to think about - and I was only playing computer opponents for virtual chips, poker's methadone compared to the crack-like properties of online play for real money&nbsp;<a style=\"background-color: #ffffff;\" name=\"f1c\"></a><sup><a href=\"#f1\">1</a></sup>.) It is entirely possible to spend a lot of time in actual play without ever having much to show for it in terms of improvement.</p>\n<p>Thus, a less easily measurable but more appropriate construct for the horizontal axis would be something like \"number of basic insights absorbed, in the appropriate order\". One way to operationalize this would be to devise a number of tests, for instance, and apply these tests externally to the behaviour of a player: do they fold under circumstance X, raise under circumstance Y, are they able to quantify this or that aspect of the game?</p>\n<p>This distinction is well-known in other domains, such as software engineering, where \"10 years of experience is not the same as one year of experience repeated 10 times\" turns out to be a useful mantra in hiring situations.</p>\n<h2 id=\"Poker_s_Pareto_Principle\">Poker's Pareto Principle</h2>\n<p>The most interesting features of the model are the curve itself, relating effort and gain; and the \"waterline\".</p>\n<p>The curve isn't linear, but follows the same shape as a <a href=\"http://en.wikipedia.org/wiki/Pareto_distribution\">Pareto distribution</a>: it obeys the \"80/20\"principle most often associated with Pareto's original observation in the domain of economics - twenty percent of the population holds eighty percent of the wealth. Here, the idea is that twenty percent of the effort is enough to get you at a level of performance better than eighty percent of the population - not bad!</p>\n<blockquote>\n<p>In poker, for instance, simply learning to fold your worst hands, bet your best ones, and make some effort to consider what your opponent holds will substantially mitigate your losses. If you are willing to do this, then perhaps 80 percent of the time you will be making the same decision as one of the best poker players like Dwan\u2014even if you have spent only 20 percent as much time studying the game.</p>\n</blockquote>\n<p>Silver dubs this the \"Pareto Principle of Prediction\" - it applies well beyond poker, to a large class of activities based on skills that require deliberate practice.</p>\n<h2 id=\"Raising_the_waterline\">Raising the waterline</h2>\n<p>The curve divides into three parts: in the first part, progress is very rapid, disproportionate to the amount of effort you put in. In the middle, you are \"grinding\" - progress is steadier, requiring the accumulation and honing of a number of distinct techniques. Finally, as you near the top of the performance curve, ever-smaller gains in performance require ever-greater refinement of your existing skills and acquisition of subtle nuances of technique. This is the <a href=\"http://en.wikipedia.org/wiki/Ten_Thousand_Hour_Rule\">ten-thousand hour</a> domain, that of \"mastery\".</p>\n<p>The \"waterline\" represents the typical level of performance that you can expect to see in the player population. Silver represents it as a horizontal line, so we need to think of it as a \"gain\" level - the typical (say, median) poker player is earning (or losing) the amount of money per time period implied by the particular position of the waterline.</p>\n<p>The key idea in the waterline is that in many cases it's not how well you do in an absolute sense that matters - it's how well you do <em>relative to the competition</em>. This is especially true in a zero-sum game! If the waterline is low, then you can make very handsome gains at the cost of a limited investment in acquiring the basic skills. But if the waterline is high, there is no alternative, before you can beat the competition, to grinding your way through the lower insights, until you finally level up enough:</p>\n<blockquote>\n<p>When a field is highly competitive, it is only through this painstaking effort around the margin that you can make any money. There is a \u201cwater level\u201d established by the competition and your profit will be like the tip of an iceberg: a small sliver of competitive advantage floating just above the surface, but concealing a vast bulwark of effort that went in to support it.</p>\n</blockquote>\n<p>(This, Silver argues, is what happened to online poker after 2006 and the Unlawful Internet Gambling Enforcement Act. The professional or semi-professional players, who derived an actual income stream from the game, continued playing, but the weaker amateurs quit, in the face of a tougher environment. Bad for poker, maybe; good for rationalists, at least insofar as it led Silver to start work on his election forecasting site FiveThirtyEight.com which has debunked a fair amount of madness about political polling, and ultimately to his book, which may introduce a broader population to largely Less Wrong compatible ideas.)</p>\n<h2 id=\"The__somewhat__bad_news\">The (somewhat) bad news</h2>\n<p>Another important caveat here - this is, as far as I can tell, purely a conceptual model: I'm not aware that there's much hard empirical data that supports the curve having the shape pictured above. Silver does have statistics to show that in the case of poker, the population of mediocre players has a key role in \"feeding\" the better players, and based on before-and-after numbers, makes a good case that bad players drying up is bad for the better players. However, he doesn't cite anything that would suggest the precise relationship between effort and gain has been measured and shown to fit the curve.</p>\n<h2 id=\"The_good_news\">The good news</h2>\n<p>This model and its more fleshed-out description of what a \"ridiculously low\" waterline entails are great news: it gives us some testable predictions. Suppose you find yourself in a competitive situation; conditional on the Pareto principle being applicable there, if you notice that applying a short set of uncomplicated techniques reliably results in outperforming your peers, that constitutes some evidence of a low waterline.&nbsp;</p>\n<p>For instance, if it is indeed the case that the waterline is very low in the skill of \"thinking probabilistically\", then those of us who have heeded the <a href=\"/lw/erp/skill_the_map_is_not_the_territory/\">lessons</a> of Less Wrong should be able to perform very well in a competitive forecasting environment, by applying only a few basic tools that unfortunately (for them) the general population doesn't yet possess.</p>\n<p>Failure to observe this would also have <em>interesting</em> consequences, raising the strength of alternate hypotheses: for instance a) we're not as good at applying even those basic techniques as we thought we were, or b) this particular competitive domain is not after all governed by a Pareto distribution, or c) the waterline is not as low as we thought it was.</p>\n<p>It turns out I was able to get some experience in just the kind of setting where this type of test could be performed: the Good Judgment Project. That's my next post.</p>\n<p>&nbsp;</p>\n<hr>\n<p style=\"font-size:90%\"><sup><a href=\"#f1c\">1</a></sup>&nbsp;This post is one way I hope to redeem those hours, turning them into a more productive effort after the fact.</p>\n<p style=\"font-size:90%\">P.S.: as this has been asked <a href=\"/lw/etk/book_review_the_signal_and_the_noise_why_so_many/\">elsewhere</a>, would I recommend Nate Silver's book? I certainly enjoyed it a lot, even though not all of it was new to me, and I often found myself wishing for clearer separation between the undoubtedly fascinating stories he tells - about financial panics, stock market crashes, baseball, poker, presidential elections, and so on - and the insights he draws from them, such as the waterline model. But enjoying it isn't quite the same as being ready to endorse it to others; I'm not quite sure yet how much value it would have, depending on the audience. Veteran Less Wrongers might not respond to it the same way as the general public, for instance. I found it valuable enough that I might invest some time in writing a short chapter-by-chapter summary and overall review, to answer that question for myself.</p>", "sections": [{"title": "The Model", "anchor": "The_Model", "level": 1}, {"title": "Chance, skill and practice", "anchor": "Chance__skill_and_practice", "level": 1}, {"title": "Poker's Pareto Principle", "anchor": "Poker_s_Pareto_Principle", "level": 1}, {"title": "Raising the waterline", "anchor": "Raising_the_waterline", "level": 1}, {"title": "The (somewhat) bad news", "anchor": "The__somewhat__bad_news", "level": 1}, {"title": "The good news", "anchor": "The_good_news", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqmjdBKa4ZaXJtNmf", "Xa5HA9idHygDK86KJ", "msJA6B9ZjiiZxT6EZ", "PyuRcbHvxXNdCHoG3", "KJ9MFBPwXGwNpadf2", "CSs3x3siBHqQeEWGE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-07T20:14:16.359Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels meetup", "slug": "meetup-brussels-meetup-3", "viewCount": null, "lastCommentedAt": "2012-10-07T20:14:16.359Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5ccaHFcombgbWGqy5/meetup-brussels-meetup-3", "pageUrlRelative": "/posts/5ccaHFcombgbWGqy5/meetup-brussels-meetup-3", "linkUrl": "https://www.lesswrong.com/posts/5ccaHFcombgbWGqy5/meetup-brussels-meetup-3", "postedAtFormatted": "Sunday, October 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ccaHFcombgbWGqy5%2Fmeetup-brussels-meetup-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ccaHFcombgbWGqy5%2Fmeetup-brussels-meetup-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ccaHFcombgbWGqy5%2Fmeetup-brussels-meetup-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/em'>Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 October 2012 01:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are once again meeting at 'la fleur en papier dor\u00e9' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. As always, I'll have a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/em'>Brussels meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5ccaHFcombgbWGqy5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0040038529103707e-06, "legacy": true, "legacyId": "19210", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup\">Discussion article for the meetup : <a href=\"/meetups/em\">Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 October 2012 01:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are once again meeting at 'la fleur en papier dor\u00e9' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. As always, I'll have a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup1\">Discussion article for the meetup : <a href=\"/meetups/em\">Brussels meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup", "level": 1}, {"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 0, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-07T20:26:15.247Z", "modifiedAt": null, "url": null, "title": "Meetup : Winter Solstice Megameetup - NYC", "slug": "meetup-winter-solstice-megameetup-nyc", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:58.222Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fbSEMZhmuBFJ8xn69/meetup-winter-solstice-megameetup-nyc", "pageUrlRelative": "/posts/fbSEMZhmuBFJ8xn69/meetup-winter-solstice-megameetup-nyc", "linkUrl": "https://www.lesswrong.com/posts/fbSEMZhmuBFJ8xn69/meetup-winter-solstice-megameetup-nyc", "postedAtFormatted": "Sunday, October 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Winter%20Solstice%20Megameetup%20-%20NYC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Winter%20Solstice%20Megameetup%20-%20NYC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfbSEMZhmuBFJ8xn69%2Fmeetup-winter-solstice-megameetup-nyc%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Winter%20Solstice%20Megameetup%20-%20NYC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfbSEMZhmuBFJ8xn69%2Fmeetup-winter-solstice-megameetup-nyc", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfbSEMZhmuBFJ8xn69%2Fmeetup-winter-solstice-megameetup-nyc", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 270, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/en\">Winter Solstice Megameetup - NYC</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">15 December 2012 05:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">316 W 138th Street New York, NY 10030</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Nearly a year ago, in NYC we held the first <a href=\"/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration/\">Less Wrong Winter Solstice</a> - a celebration of the past, present and future of winter, and the progress humankind has made over the years, and the progress we will continue to make.</p>\n<p>We'll be doing it again this year, on the weekend of December 15th and 16th. Our exact plans are still in motion, but people who want to come from out of state may want to start thinking about travel plans.</p>\n<p>It is likely that the communal music celebration will be on Saturday evening, and that Sunday afternoon will be freeform discussion with refreshments. Due to limited seating, my recommendation is that people who primarily want to meet a lot of Less Wrong folk should come to the Sunday party, and people who specifically want to participate in ritual should come on Saturday.</p>\n<p>(Don't shy away from coming to the evening celebration if you want to, we can make room. I just know that there are people who like big LW community events but don't care much for ritual/tribalism, and figure it's better not to try to cater to everyone all at once)</p>\n<p>The event will be held at Winterfell House, a three story apartment building that 5 of us recently acquired in Manhattan. It would be useful if we knew, sooner rather than later, how many people to expect, so we can plan the scale of the event accordingly.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/en\">Winter Solstice Megameetup - NYC</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fbSEMZhmuBFJ8xn69", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "19211", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Winter_Solstice_Megameetup___NYC\">Discussion article for the meetup : <a href=\"/meetups/en\">Winter Solstice Megameetup - NYC</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">15 December 2012 05:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">316 W 138th Street New York, NY 10030</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Nearly a year ago, in NYC we held the first <a href=\"/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration/\">Less Wrong Winter Solstice</a> - a celebration of the past, present and future of winter, and the progress humankind has made over the years, and the progress we will continue to make.</p>\n<p>We'll be doing it again this year, on the weekend of December 15th and 16th. Our exact plans are still in motion, but people who want to come from out of state may want to start thinking about travel plans.</p>\n<p>It is likely that the communal music celebration will be on Saturday evening, and that Sunday afternoon will be freeform discussion with refreshments. Due to limited seating, my recommendation is that people who primarily want to meet a lot of Less Wrong folk should come to the Sunday party, and people who specifically want to participate in ritual should come on Saturday.</p>\n<p>(Don't shy away from coming to the evening celebration if you want to, we can make room. I just know that there are people who like big LW community events but don't care much for ritual/tribalism, and figure it's better not to try to cater to everyone all at once)</p>\n<p>The event will be held at Winterfell House, a three story apartment building that 5 of us recently acquired in Manhattan. It would be useful if we knew, sooner rather than later, how many people to expect, so we can plan the scale of the event accordingly.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Winter_Solstice_Megameetup___NYC1\">Discussion article for the meetup : <a href=\"/meetups/en\">Winter Solstice Megameetup - NYC</a></h2>", "sections": [{"title": "Discussion article for the meetup : Winter Solstice Megameetup - NYC", "anchor": "Discussion_article_for_the_meetup___Winter_Solstice_Megameetup___NYC", "level": 1}, {"title": "Discussion article for the meetup : Winter Solstice Megameetup - NYC", "anchor": "Discussion_article_for_the_meetup___Winter_Solstice_Megameetup___NYC1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "26 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jES7mcPvKpfmzMTgC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-07T23:45:13.566Z", "modifiedAt": "2021-04-21T06:12:49.900Z", "url": null, "title": "LessWrong help desk - free paper downloads and more", "slug": "lesswrong-help-desk-free-paper-downloads-and-more", "viewCount": null, "lastCommentedAt": "2021-04-19T14:41:13.792Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2jfSi6HhTqWDKLQwm/lesswrong-help-desk-free-paper-downloads-and-more", "pageUrlRelative": "/posts/2jfSi6HhTqWDKLQwm/lesswrong-help-desk-free-paper-downloads-and-more", "linkUrl": "https://www.lesswrong.com/posts/2jfSi6HhTqWDKLQwm/lesswrong-help-desk-free-paper-downloads-and-more", "postedAtFormatted": "Sunday, October 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20help%20desk%20-%20free%20paper%20downloads%20and%20more&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20help%20desk%20-%20free%20paper%20downloads%20and%20more%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2jfSi6HhTqWDKLQwm%2Flesswrong-help-desk-free-paper-downloads-and-more%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20help%20desk%20-%20free%20paper%20downloads%20and%20more%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2jfSi6HhTqWDKLQwm%2Flesswrong-help-desk-free-paper-downloads-and-more", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2jfSi6HhTqWDKLQwm%2Flesswrong-help-desk-free-paper-downloads-and-more", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p>Over the last year, VincentYu, gwern, myself and others have provided 132 academic papers for the LessWrong community (out of 152 requests, a 87% success rate) through the <a href=\"/r/discussion/lw/7hi/free_research_help_editing_and_article_downloads/\">Free research, editing and articles thread</a>.&nbsp;We originally intended to provide editing, research and general troubleshooting help, but article downloads are by far the most requested service.</p>\n<p>If you're doing a LessWrong relevant project we want to help you. If you need help accessing a journal article or academic book chapter, we can get it for you. If you need some research or writing help, we can help there too.</p>\n<p>Turnaround times for articles published in the last 20 years or so is usually less than a day. Older articles often take a couple days.</p>\n<p>Please make new article requests in the comment section of this thread.</p>\n<p>If you would like to help out with finding papers, please monitor this thread for requests. If you want to monitor via RSS like I do, Google Reader will give you the comment feed if you give it the URL for this thread (or use <a href=\"/r/discussion/lw/eto/lesswrong_help_desk_free_paper_downloads_and_more/.rss\">this</a> link directly).&nbsp;</p>\n<p>If you have some special skills you want to volunteer, mention them in the comment section.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2jfSi6HhTqWDKLQwm", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 53, "extendedScore": null, "score": 0.000550405687542528, "legacy": true, "legacyId": "19212", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 752, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["q3wQGNicZZQPCmJXZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-10-07T23:45:13.566Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-08T03:16:12.828Z", "modifiedAt": null, "url": null, "title": "Meetup : Montr\u00e9al Meetup", "slug": "meetup-montreal-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Paul_G", "createdAt": "2012-06-08T01:42:32.451Z", "isAdmin": false, "displayName": "Paul_G"}, "userId": "g4WQoWHmq4HNzTJDC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ygef2abj4firgm5g5/meetup-montreal-meetup", "pageUrlRelative": "/posts/Ygef2abj4firgm5g5/meetup-montreal-meetup", "linkUrl": "https://www.lesswrong.com/posts/Ygef2abj4firgm5g5/meetup-montreal-meetup", "postedAtFormatted": "Monday, October 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montr%C3%A9al%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montr%C3%A9al%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgef2abj4firgm5g5%2Fmeetup-montreal-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montr%C3%A9al%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgef2abj4firgm5g5%2Fmeetup-montreal-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgef2abj4firgm5g5%2Fmeetup-montreal-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 89, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/eo'>Montr\u00e9al Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 October 2012 05:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Caffe Art Java - 645, av. du Pr\u00e9sident-Kennedy, Montr\u00e9al, QC H3A 1K1 </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Montr\u00e9al LessWrong Meetup</p>\n\n<p>The topic will be Bayesian vs Frequentist statistics, with a rundown of how Bayesian stats work in several contexts.</p>\n\n<p>We'll have some sort of sign saying \"LessWrong\".</p>\n\n<p>If the Caffe Art Java is full, we'll be nextdoor at the Broadway Cheesecake.</p>\n\n<p>Message me for my phone number in case you need to contact me for whatever reason!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/eo'>Montr\u00e9al Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ygef2abj4firgm5g5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.0042271150035457e-06, "legacy": true, "legacyId": "19214", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montr_al_Meetup\">Discussion article for the meetup : <a href=\"/meetups/eo\">Montr\u00e9al Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 October 2012 05:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Caffe Art Java - 645, av. du Pr\u00e9sident-Kennedy, Montr\u00e9al, QC H3A 1K1 </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Montr\u00e9al LessWrong Meetup</p>\n\n<p>The topic will be Bayesian vs Frequentist statistics, with a rundown of how Bayesian stats work in several contexts.</p>\n\n<p>We'll have some sort of sign saying \"LessWrong\".</p>\n\n<p>If the Caffe Art Java is full, we'll be nextdoor at the Broadway Cheesecake.</p>\n\n<p>Message me for my phone number in case you need to contact me for whatever reason!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montr_al_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/eo\">Montr\u00e9al Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Montr\u00e9al Meetup", "anchor": "Discussion_article_for_the_meetup___Montr_al_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Montr\u00e9al Meetup", "anchor": "Discussion_article_for_the_meetup___Montr_al_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-08T04:27:54.461Z", "modifiedAt": null, "url": null, "title": "Article sketch: When procrastination isn't akrasia", "slug": "article-sketch-when-procrastination-isn-t-akrasia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:26.858Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "odm3FHPzgbiGpWsSg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WDGcXwSZqabafvJJh/article-sketch-when-procrastination-isn-t-akrasia", "pageUrlRelative": "/posts/WDGcXwSZqabafvJJh/article-sketch-when-procrastination-isn-t-akrasia", "linkUrl": "https://www.lesswrong.com/posts/WDGcXwSZqabafvJJh/article-sketch-when-procrastination-isn-t-akrasia", "postedAtFormatted": "Monday, October 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Article%20sketch%3A%20When%20procrastination%20isn't%20akrasia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArticle%20sketch%3A%20When%20procrastination%20isn't%20akrasia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWDGcXwSZqabafvJJh%2Farticle-sketch-when-procrastination-isn-t-akrasia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Article%20sketch%3A%20When%20procrastination%20isn't%20akrasia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWDGcXwSZqabafvJJh%2Farticle-sketch-when-procrastination-isn-t-akrasia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWDGcXwSZqabafvJJh%2Farticle-sketch-when-procrastination-isn-t-akrasia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 867, "htmlBody": "<p>I have a lot to say on this topic, but I haven't written an article about it, for two reasons... come to think of it, more like for one reason, twice. The first reason is that, while I've learned quite a bit about procrastination in the past few months, I haven't learned <em>much</em>&nbsp;about it, and as a result, the essay I could write now wouldn't be as good as the article I could write a couple of months from now, after I've learned even more. The second reason is that, while I've learned quite a bit about procrastination in the past few months, I still do procrastinate, and besides that, I have a backlog of stuff I consider more important than writing this essay, and as a result, I've decided to postpone it until I've gotten all that other stuff done.</p>\n<p>I figure a sketch is better than nothing, so I'm just giving you this stream-of-consciousness business you're reading now.</p>\n<p>Procrastination, as far as I can tell, has two major causes. The first of these causes is akrasia: you know you should be doing something else, and you know <em>what</em>&nbsp;you should be doing, but you aren't doing it. This essay is about the second cause: disorganization.</p>\n<p>Now, it's likely that you can think of at least one thing right now&mdash;a major homework project, or cleaning the entire house, or something&mdash;that makes you think, <em>Dang it. I haven't done that yet, and I wish I had. I've been procrastinating.</em>&nbsp;Ask yourself this question: <em>when</em>&nbsp;should you have done it?</p>\n<p>Perhaps the answer is along the lines of \"Well, I should have done that Tuesday morning, when I looked on my to-do list and saw that it was the top item on there, but instead of doing it, I played Minecraft for three hours.\" In other words, perhaps you can think of a specific time you should have done it, <em>and</em> you remembered that you <em>could</em>&nbsp;have done it, <em>but</em> instead, you did something that was less important. If <em>all three</em>&nbsp;of those things are true, then your problem is akrasia.</p>\n<p>If any of those parts is missing, then your problem is <em>not</em>&nbsp;akrasia; your problem is disorganization. Let me explain.</p>\n<p>Suppose that instead of thinking of a specific time you should have done the task, you simply thought, \"I should have done that <em>at some point</em>\". Even if you have perfect willpower, knowing that you should be doing something \"at some point\" isn't enough, because at any given time, there are <em>lots</em>&nbsp;of things that you should do \"at some point\". If there are fifty such things, then it is impossible not to postpone at least forty-nine of them, so you can't blame yourself for not doing those forty-nine.</p>\n<p>Suppose that you think of a specific time you should have done it, but you didn't remember that you <em>could</em>&nbsp;have done it. Even if you have perfect willpower, you can't do the things you've completely forgotten to do.</p>\n<p>And suppose that you think of a specific time, and you remembered that you could have done it, but instead you did something that was <em>more</em>&nbsp;important. Obviously, that's not akrasia; you simply had better things to do.</p>\n<p>How can you fix these problems? The basic solution is simple: <em>write down</em>&nbsp;the things you have to do. All of them. Also write down the time frame in which you'd like to get them done, so that you can work on them in an order that makes sense, instead of just whenever they randomly pop into your mind.</p>\n<p>About working on things in an order that makes sense. What order should that be, exactly? Well, I think there are two things that are worth taking into consideration here: importance and ease. Working on more important tasks first can be a good idea for obvious reasons. Working on <em>easy</em>&nbsp;tasks first can be a good idea for less obvious reasons: that makes it easy to get started (and getting started tends to be the hardest part), it gives you a feeling of success right away, and it makes the work seem more manageable. (If you have 60 things to do, and the first 50 are easy, that seems manageable. If you have 10 things to do, and they're all hard, that also seems manageable. If you have 60 things to do, and the first 10 are hard, that seems far less manageable!)</p>\n<p>Assuming you have all your tasks written down and ordered sensibly, you're ready to begin executing your \"work algorithm\". What should that be? Simple enough. First, look at the first item on the list. If it's easy, do it. If it's hard, then break it into pieces, at least one of which is easy, and sort the pieces into a reasonable order. Repeat.</p>\n<p>Breaking a hard task into easy ones is outside the scope of this essay (or, at least, this sketch of it).</p>\n<p>Most what I think I know about organization is thanks to David Allen's \"Getting Things Done\". I highly recommend reading the first chapter, which will both give you some context, and convince you to read the second chapter. The second chapter will teach you organization in a nutshell.</p>\n<p>This page on my wiki may be useful or interesting to read:&nbsp;http://wiki.zbasu.net/procrastination This page may be interesting, but probably not particularly useful:&nbsp;http://wiki.zbasu.net/weekly-progress</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dqx5k65wjFfaiJ9sQ": 1, "r7qAjcbfhj2256EHH": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WDGcXwSZqabafvJJh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 28, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "19222", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-08T05:19:15.949Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Measuring Optimization Power", "slug": "seq-rerun-measuring-optimization-power", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:03.897Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N6CXdHxJHba5qHeav/seq-rerun-measuring-optimization-power", "pageUrlRelative": "/posts/N6CXdHxJHba5qHeav/seq-rerun-measuring-optimization-power", "linkUrl": "https://www.lesswrong.com/posts/N6CXdHxJHba5qHeav/seq-rerun-measuring-optimization-power", "postedAtFormatted": "Monday, October 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Measuring%20Optimization%20Power&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Measuring%20Optimization%20Power%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN6CXdHxJHba5qHeav%2Fseq-rerun-measuring-optimization-power%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Measuring%20Optimization%20Power%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN6CXdHxJHba5qHeav%2Fseq-rerun-measuring-optimization-power", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN6CXdHxJHba5qHeav%2Fseq-rerun-measuring-optimization-power", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<p>Today's post, <a href=\"/lw/va/measuring_optimization_power/\">Measuring Optimization Power</a> was originally published on 27 October 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>In order to measure the power of an optimization process, we can calculate how improbable it is that its goals would be fulfilled if that process were not present. The more unlikely they are, the more powerful the process is.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/eti/seq_rerun_aiming_at_the_target/\">Aiming at the Target</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N6CXdHxJHba5qHeav", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0042922416798214e-06, "legacy": true, "legacyId": "19223", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q4hLMDrFd8fbteeZ8", "7pW2FdS26yRdbC4ga", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-08T06:19:29.089Z", "modifiedAt": null, "url": null, "title": "Internal Availability", "slug": "internal-availability", "viewCount": null, "lastCommentedAt": "2017-11-16T22:39:07.205Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "royf", "createdAt": "2012-05-28T04:41:04.869Z", "isAdmin": false, "displayName": "royf"}, "userId": "Rrw92MqPSK6T8nSBg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Rn6gNpsnwaAbsn2Su/internal-availability", "pageUrlRelative": "/posts/Rn6gNpsnwaAbsn2Su/internal-availability", "linkUrl": "https://www.lesswrong.com/posts/Rn6gNpsnwaAbsn2Su/internal-availability", "postedAtFormatted": "Monday, October 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Internal%20Availability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInternal%20Availability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRn6gNpsnwaAbsn2Su%2Finternal-availability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Internal%20Availability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRn6gNpsnwaAbsn2Su%2Finternal-availability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRn6gNpsnwaAbsn2Su%2Finternal-availability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 864, "htmlBody": "<p><strong>Edit:</strong>&nbsp;Following mixed reception, I decided to split this part out of the <a href=\"/lw/eu1/pointbased_value_iteration/\">latest post</a> in my <a href=\"/lw/dsq/reinforcement_learning_a_nonstandard_introduction/\">sequence</a> <a href=\"/lw/dux/reinforcement_learning_a_nonstandard_introduction/\">on</a> <a href=\"/lw/dz4/reinforcement_preference_and_utility/\">reinforcement</a> <a href=\"/lw/e6a/the_bayesian_agent/\">learning</a>. It&nbsp;wasn't clear enough, and anyway&nbsp;didn't belong there.</p>\n<p>I'm posting this hopefully better version to Discussion, and welcome further comments on content and style.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>The <a href=\"http://wiki.lesswrong.com/wiki/Availability_heuristic\">availability heuristic</a>&nbsp;seems to be a mechanism inside our brains which uses the ease with which images, events and concepts come to mind as evidence for their prevalence or probability of occurrence. For this heuristic to be worth <a href=\"/lw/j5/availability/\">the trouble it causes</a>, there needs to be a counterpart, a second mechanism which actually makes things available to the first one in correlation with their likelihood. In this post I discuss why having such an <em>internal availability</em>&nbsp;mechanism can be a good idea, and outline some of the ways it can fail.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>You're playing Texas Hold'em poker against another player, and she has just bet all her chips on the flop (the 2nd of 4 betting rounds, when there are 2 more shared cards to draw). You estimate that with high probability she has a low pair (say, under 9) with a high kicker (A or K, hoping to hit a second pair). You hold Q-J off-suit. Do you call?</p>\n<p>One question this depends on is: what's the probability p that you will win this hand? An experienced player will know that your best hope is to hit a pair, without the other player hitting anything better than her low pair. This has probability of slightly less than 25%.</p>\n<p>We could compute or remember a better estimate if we notice the probability of&nbsp;<a href=\"http://en.wikipedia.org/wiki/Poker_probability_(Texas_hold_'em)#Runner-runner_outs\">runner-runner outs</a>,&nbsp;but is it worth it? It won't help us pin down p with amazing accuracy - we could be wrong about the opponent's hand to begin with. And anyway, the decision of whether to actually call depends on many other factors: the sizes of your stack, her stack, the blinds and so on. A 1% error in the estimate of the win probability is unlikely to change your decision.</p>\n<p>So instead of pointlessly trying to predict the future to an impossible and useless degree of accuracy, what we did was tell ourselves a bunch of likely stories about what might happen, then combine these scenarios into a simple probabilistic prediction of the future, and plan as best we can given this prediction.</p>\n<p>This may be the mechanism that makes the <a href=\"http://wiki.lesswrong.com/wiki/Availability_heuristic\">availability heuristic</a> a smart choice. The main observed effect of this heuristic is that past (subjective) prevalence seems to be highly linked to future predictions. Patches of stories we've heard may even work their way into the stories we tell of the future. The missing link is an <em>internal availability</em>&nbsp;mechanism which chooses which patches to make available for retelling. We seem to use such a mechanism to identify likely outcomes; before we forward them to the more commonly discussed process which integrates these stories of the future into a usable prediction.</p>\n<p>What events would be good candidates for becoming available? One thing to notice is that evaluation of the <a href=\"http://wiki.lesswrong.com/wiki/Expected_utility\">expected value</a> of our actions depends both on the probability and on the impact of their results; but for each specific future we don't need both these numbers, only their product. If the main function of the internal availability mechanism is to predict <em><a href=\"/lw/dz4/reinforcement_preference_and_utility/\">value</a></em>, rather than probability, it stands to reason that high-impact but improbable outcomes will become as available as mundane probable ones. Yes, concepts which were encountered most often in the past, in a context similar to the current one, come to mind easily. But one-in-a-hundred or -thousand outcomes should also become available if they are very important. One-in-a-million ones, on the other hand, are almost never worth the trouble.</p>\n<p>If something similar is indeed going on in our brains, then it seems to be working pretty well, usually.&nbsp;When I walk down the street, I give no mind to the possibility that there are invisible obstacles in my way. It is so remote, that even if I took it into account with adequately small probability, my actions would probably be roughly the same. It is therefore wise not to occupy my precious processing power with such nonsense.</p>\n<p>Even when the internal availability mechanism is working properly, it generates unavoidable errors in prediction. Strictly speaking, ignoring some unlikely and unimportant possibilities is wrong, however practical. And while it makes noticing things evidence for their higher probability, this heuristic&nbsp;<a href=\"/lw/j5/availability/\">could sometimes fail</a>, particularly if the internal availability mechanism is built for utility but used for probability.</p>\n<p>The mechanism itself can also fail. Availability doesn't seem to be binary, so one type of failure is to make certain scenarios over- or under-available, marking them as more or less likely and important than they are.&nbsp;There also appears to be some threshold, some minimal value for non-zero availability. Another type of failure is when an important outcome fails to meet this threshold, not becoming available.</p>\n<p>Or perhaps an unlikely future becomes available even though it shouldn't. This may explain why people&nbsp;are unable to estimate low probabilities. In their mind, the prospect of winning the lottery and becoming millionaires creates a vivid image of an exciting future. It's so immersive, that it really appears to be a real possibility - it could actually happen!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb19c": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Rn6gNpsnwaAbsn2Su", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 5, "extendedScore": null, "score": 1.0043241159031419e-06, "legacy": true, "legacyId": "18860", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JyPb4Wpty86tj7Xgn", "HPAjhrbYk6rPbpSXx", "xdjA6YtE7QBsLYQ3i", "bPeB6RT78k8dXKYKf", "G4XKiJ2Q93JGCJxCT", "R8cpqD3NA4rZxRdQ4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-08T06:19:34.355Z", "modifiedAt": null, "url": null, "title": "Point-Based Value Iteration", "slug": "point-based-value-iteration", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "royf", "createdAt": "2012-05-28T04:41:04.869Z", "isAdmin": false, "displayName": "royf"}, "userId": "Rrw92MqPSK6T8nSBg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JyPb4Wpty86tj7Xgn/point-based-value-iteration", "pageUrlRelative": "/posts/JyPb4Wpty86tj7Xgn/point-based-value-iteration", "linkUrl": "https://www.lesswrong.com/posts/JyPb4Wpty86tj7Xgn/point-based-value-iteration", "postedAtFormatted": "Monday, October 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Point-Based%20Value%20Iteration&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APoint-Based%20Value%20Iteration%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJyPb4Wpty86tj7Xgn%2Fpoint-based-value-iteration%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Point-Based%20Value%20Iteration%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJyPb4Wpty86tj7Xgn%2Fpoint-based-value-iteration", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJyPb4Wpty86tj7Xgn%2Fpoint-based-value-iteration", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1194, "htmlBody": "<p><strong>Followup to:</strong>&nbsp;<a href=\"/lw/e6a/the_bayesian_agent/\">The Bayesian Agent</a></p>\n<p>This post explains one interesting and influential algorithm for achieving high <a href=\"/lw/dz4/reinforcement_preference_and_utility/\">utility</a> of the actions of a <a href=\"/lw/e6a/the_bayesian_agent/\">Bayesian agent</a>, called Point-Based Value Iteration (<a href=\"http://robots.stanford.edu/papers/Pineau03a.pdf\">original paper</a>). Its main premise resembles some concept of <a href=\"/lw/ejw/internal_availability/\">internal availability</a>.</p>\n<p>A <a href=\"/lw/dux/reinforcement_learning_a_nonstandard_introduction/\">reinforcement-learning agent</a> chooses its actions based on its internal memory state. The memory doesn't have to include an exact account of all past observations - it's sufficient for the agent to <a href=\"/lw/e6a/the_bayesian_agent/\">keep track of a belief</a> of the current state of the world.</p>\n<p>This mitigates the problem of having the size of the memory state grow indefinitely. The memory-state space is now the set of all distributions over the world state. Importantly, this space doesn't grow exponentially with time like the space of observable histories. Unfortunately, it still has volume exponential in the number of world states.</p>\n<p>So we moved away from specifying the action to take after each observable history, in favor of specifying the action to take in each belief state. This was justified by the belief being a sufficient statistic for the world state, and motivated by the belief space being much smaller (for long processes) than the history space. But for the purpose of understanding this algorithm, we should go back to describing the action policy in terms of the action to take after each observable history, &pi;(A<sub>t</sub>|O<sub>1</sub>,...,O<sub>t</sub>).</p>\n<p>Now join us as we're watching the dynamics of the process unfold. We're at time t, and the agent has reached Bayesian belief B<sub>t</sub>, the distribution of the world state to the best of the agent's knowledge. What prospects does the agent still have for future rewards? What is its <em>value-to-go</em> of being in this belief state?</p>\n<p>If we know what action the agent will take after each observable history (if and when it comes to pass), it turns out that the expected total of future rewards is linear in B<sub>t</sub>. To see why this is so, consider a specific future (and present):</p>\n<p style=\"padding-left: 30px;\">w<sub>t</sub>, o<sub>t</sub>, a<sub>t</sub>, w<sub>t+1</sub>, o<sub>t+1</sub>, a<sub>t+1</sub>, ..., w<sub>n</sub>, o<sub>n</sub>, a<sub>n</sub></p>\n<p>The <a href=\"/lw/dz4/reinforcement_preference_and_utility/\">reward</a> of this future is:</p>\n<p style=\"padding-left: 30px;\">R(w<sub>t</sub>,a<sub>t</sub>) + R(w<sub>t+1</sub>,a<sub>t+1</sub>) + ... + R(w<sub>n</sub>,a<sub>n</sub>)</p>\n<p>The probability of this future, given that the agent already knows o<sub>1</sub>,...,o<sub>t</sub>, is:</p>\n<p style=\"padding-left: 30px;\">B<sub>t</sub>(w<sub>t</sub>)&middot;&pi;(a<sub>t</sub>|o<sub>1</sub>,...,o<sub>t</sub>)&middot;p(w<sub>t+1</sub>|w<sub>t</sub>,a<sub>t</sub>)&middot;&sigma;(o<sub>t+1</sub>|w<sub>t+1</sub>)&middot;&pi;(a<sub>t+1</sub>|o<sub>1</sub>,...,o<sub>t+1</sub>)&middot;&middot;&middot;</p>\n<p style=\"padding-left: 60px;\">&middot;&middot;&middot;p(w<sub>n</sub>|w<sub>n-1</sub>,a<sub>n-1</sub>)&middot;&sigma;(o<sub>n</sub>|w<sub>n</sub>)&middot;&pi;(a<sub>n</sub>|o<sub>1</sub>,...,o<sub>n</sub>)</p>\n<p>We arrive at this by starting with the agent's belief B<sub>t</sub>, which summarizes the distribution of w<sub>t</sub>&nbsp;given the observable history; then going on to multiply the probability for each new variable, given those before it that have part in causing it.</p>\n<p>Note how B<sub>t</sub>&nbsp;linearly determines the probability for w<sub>t</sub>, while the dynamics of the world and the agent determine the probability for the rest of the process. Now, to find the expected reward we sum, over all possible futures, the product of the future's reward and probability. This is all linear in the belief B<sub>t</sub>&nbsp;at time t.</p>\n<p>If the value of choosing a specific action policy is a linear function of B<sub>t</sub>, the value of choosing the <em>best</em> action policy is the maximum of many linear functions of B<sub>t</sub>. How many? If there are 2 possible observations, the observable history that will have unfolded in the next n-t steps can be any of 2<sup>n-t</sup> possible \"future histories\". If there are 2 possible actions to choose for each one, there are 2<sup>2<sup>n-t</sup></sup> possible policies. A scary number if the end is still some time ahead of us.</p>\n<p>Of course, we're not interested in the value of just any belief, only of those that can actually be reached at time t. Since the belief B<sub>t</sub>&nbsp;is determined by the observable history&nbsp;o<sub>1</sub>,...,o<sub>t</sub>, there are at most 2<sup>t</sup> such values. Less scary, but still not practical for long processes.</p>\n<p>The secret of the algorithm is that it limits both of the figures above to a fixed,&nbsp;manageable&nbsp;number. Suppose that we have a list of 100 belief states which are the most interesting to us, perhaps because they are the most likely to be reached at time t. Suppose that we also have a list &Pi;<sub>t</sub>&nbsp;of 100 policies: for each belief on the first list, we have in the second list a policy that gets a good reward if indeed that belief is reached at time t. Now what is the value, at time t, of choosing the best action policy of these 100? It's the maximum of, not many, but |&Pi;<sub>t</sub>|=100 linear functions of B<sub>t</sub>. Much more manageable.</p>\n<p>Ok, we're done with time t. What about time t-1? We are going to consider 100 belief states which can be reached at time t-1. For each such belief B<sub>t-1</sub>, we are going to consider each possible action A<sub>t-1</sub>. Then we are going to consider each possible observation O<sub>t</sub>&nbsp;that can come next - we can compute its probability from B<sub>t-1</sub>&nbsp;and the dynamics of the system. Given the observation, we will have a new belief B<sub>t</sub>. And the previous paragraph tells us what to do in this case: choose one of the policies in &Pi;<sub>t</sub>&nbsp;which is best for B<sub>t</sub>, to use for the rest of the process.</p>\n<p>(Note that this new B<sub>t</sub>&nbsp;doesn't have to be one of the 100 beliefs that were used to construct the 100 policies in &Pi;<sub>t</sub>.)</p>\n<p>So this is how the recursive step goes:</p>\n<ul>\n<li>For each of 100 possible values for B<sub>t-1</sub> \n<ul>\n<li>For each possible A<sub>t-1</sub> \n<ul>\n<li>For each possible O<sub>t</sub> \n<ul>\n<li>We compute B<sub>t</sub></li>\n<li>We choose for B<sub>t</sub>&nbsp;the best policy in &Pi;<sub>t</sub>, and find its value (it's a linear function of B<sub>t</sub>)</li>\n</ul>\n</li>\n<li>We go back and ask: how good is it really to choose A<sub>t-1</sub>? What is the expected value before knowing O<sub>t</sub>?</li>\n</ul>\n</li>\n<li>We choose the best A<sub>t-1</sub></li>\n<li>This gives us the best policy for B<sub>t-1</sub>, under the restriction that we are only willing to use one of the policies in&nbsp;&Pi;<sub>t</sub>&nbsp;from time t onward</li>\n</ul>\n</li>\n<li>After going over the 100 beliefs for time t-1, we now have a new stored list,&nbsp;&Pi;<sub>t-1</sub>,&nbsp;of 100 policies for time t-1.</li>\n</ul>\n<p>We continue backward in time, until we reach the starting time t<sub>0</sub>, where we choose the best policy in &Pi;<sub>t<sub>0</sub></sub>&nbsp;for the initial (prior) belief B<sub>t<sub>0</sub></sub>.</p>\n<p>We still need to say how to choose those special 100 beliefs to optimize for. Actually, this is not a fixed set. Most point-based algorithms start with only one or a few beliefs, and gradually expand the set to find better and better solutions.</p>\n<p>The original algorithm&nbsp;chooses beliefs in a similar way to an <a href=\"/r/discussion/lw/ejw/internal_availability/\">internal availability</a> mechanism that may be responsible for conjuring up images of likely futures in human brains. It&nbsp;starts with only one belief (the prior&nbsp;B<sub>t<sub>0</sub></sub>) and finds a solution where only one policy is allowed in each&nbsp;&Pi;<sub>t</sub>. Then it adds some beliefs which seem likely to be reached in this solution, and finds a new and improved solution. It repeats this computation until a good-enough solution is reached, or until it runs out of time.</p>\n<p>Point-based algorithms are today some of the best ones for finding good policies for Bayesian agents. They drive some real-world applications, and contribute to our understanding of how intelligent agents can act and react.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JyPb4Wpty86tj7Xgn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 1.0043241623338138e-06, "legacy": true, "legacyId": "19225", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["G4XKiJ2Q93JGCJxCT", "bPeB6RT78k8dXKYKf", "Rn6gNpsnwaAbsn2Su", "xdjA6YtE7QBsLYQ3i"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-08T08:01:13.095Z", "modifiedAt": null, "url": null, "title": "Firewalling the Optimal from the Rational", "slug": "firewalling-the-optimal-from-the-rational", "viewCount": null, "lastCommentedAt": "2015-07-16T18:04:54.009Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WbLAA8qZQNdbRgKte/firewalling-the-optimal-from-the-rational", "pageUrlRelative": "/posts/WbLAA8qZQNdbRgKte/firewalling-the-optimal-from-the-rational", "linkUrl": "https://www.lesswrong.com/posts/WbLAA8qZQNdbRgKte/firewalling-the-optimal-from-the-rational", "postedAtFormatted": "Monday, October 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Firewalling%20the%20Optimal%20from%20the%20Rational&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFirewalling%20the%20Optimal%20from%20the%20Rational%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWbLAA8qZQNdbRgKte%2Ffirewalling-the-optimal-from-the-rational%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Firewalling%20the%20Optimal%20from%20the%20Rational%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWbLAA8qZQNdbRgKte%2Ffirewalling-the-optimal-from-the-rational", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWbLAA8qZQNdbRgKte%2Ffirewalling-the-optimal-from-the-rational", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1515, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/eta/rationality_appreciating_cognitive_algorithms/\">Rationality: Appreciating Cognitive Algorithms</a>&nbsp;&nbsp;<em>(minor post)</em></p>\n<p>There's an old anecdote about <a href=\"/lw/m1/guardians_of_ayn_rand/\">Ayn Rand</a>, which Michael Shermer recounts in his \"The Unlikeliest Cult in History\" (<em>note: calling a fact unlikely is an insult to your prior model, not the fact itself</em>), which went as follows:</p>\n<p style=\"padding-left: 30px; \">Branden recalled an evening when a friend of Rand's remarked that he enjoyed the music of Richard Strauss. \"When he left at the end of the evening, Ayn said, in a reaction becoming increasingly typical, 'Now I understand why he and I can never be real soulmates. The distance in our sense of life is too great.' Often she did not wait until a friend had left to make such remarks.\"</p>\n<p>Many readers may already have appreciated this point, but one of the Go stones placed to block that failure mode is being careful what we bless with the great community-normative-keyword 'rational'. And one of the ways we do that is by trying to deflate the word 'rational' out of sentences, especially in post titles or critical comments, which can live without the word. &nbsp;As you hopefully recall from the previous post,&nbsp;we're only&nbsp;<em>forced</em>&nbsp;to use the word 'rational' when we talk about the&nbsp;<em>cognitive algorithms</em>&nbsp;which&nbsp;<em>systematically&nbsp;</em>promote goal achievement or map-territory correspondences. &nbsp;Otherwise the word can be deflated out of the sentence; e.g.&nbsp;\"It's rational to believe in anthropogenic global warming\" goes to \"Human activities are causing global temperatures to rise\"; or&nbsp;\"It's rational to vote for Party X\" deflates to \"It's optimal to vote for Party X\" or just \"I think you should vote for Party X\".</p>\n<p>If you're writing a post comparing the experimental evidence for four different diets, that's not \"Rational Dieting\", that's \"Optimal Dieting\". A post about&nbsp;<em>rational</em>&nbsp;dieting is if you're writing about how the sunk cost fallacy causes people to eat food they've already purchased even if they're not hungry, or if you're writing about how the typical mind fallacy or law of small numbers leads people to overestimate how likely it is that a diet which worked for them will work for a friend. And even then, your title is 'Dieting and the Sunk Cost Fallacy', unless it's an overview of four different cognitive biases affecting dieting. In which case a&nbsp;<em>better</em>&nbsp;title would be 'Four Biases Screwing Up Your Diet', since 'Rational Dieting' carries an implication that your post discusses <em>the</em>&nbsp;cognitive algorithm for dieting, as opposed to four contributing things to keep in mind.<a id=\"more\"></a></p>\n<p>By the same token, a post about Givewell's top charities and how they compare to existential-risk mitigation is a post about&nbsp;<em>optimal philanthropy,</em>&nbsp;while a post about <a href=\"http://wiki.lesswrong.com/wiki/Scope_insensitivity\">scope insensitivity</a> and <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">hedonic returns vs. marginal returns</a> is a post about&nbsp;<em>rational philanthropy,</em>&nbsp;because the first is discussing object-level outcomes while the second is discussing cognitive algorithms. And either way, if you can have a post title that doesn't include the word \"rational\", it's probably a good idea because the word gets a little less powerful every time it's used.</p>\n<p>Of course, it's still a good idea to include&nbsp;<em>concrete examples</em>&nbsp;when talking about general cognitive algorithms. A good writer won't discuss rational philanthropy without including some discussion of particular charities to illustrate the point. In general, the&nbsp;<em>concrete-abstract</em>&nbsp;writing pattern says that your opening paragraph should be a concrete example of a nonoptimal charity, and only afterward should you generalize to make the abstract point. (That's why the main post opened with the Ayn Rand anecdote.)</p>\n<p>And I'm not saying that we should never have posts about Optimal Dieting on LessWrong. What good is all that rationality if it never leads us to anything optimal?</p>\n<p>Nonetheless, the&nbsp;<em>second</em>&nbsp;Go stone placed to block the Objectivist Failure Mode is trying to&nbsp;<em>define ourselves as a community&nbsp;</em>around the cognitive algorithms; and trying to avoid membership tests (especially implicit&nbsp;<em>de facto</em>&nbsp;tests) that&nbsp;<em>aren't</em>&nbsp;about rational process, but just about some particular thing that a lot of us think is&nbsp;<em>optimal</em>.</p>\n<p>Like, say, paleo-inspired diets.</p>\n<p>Or having to love particular classical music composers, or hate dubstep, or something. &nbsp;(Does anyone know any good dubstep mixes of classical music, by the way?)</p>\n<p>Admittedly, a lot of the utility&nbsp;<em>in practice</em>&nbsp;from any community like this one, can and should come from sharing lifehacks. If you go around teaching people methods that they can allegedly use to distinguish&nbsp;<em>good</em>&nbsp;strange ideas from&nbsp;<em>bad</em>&nbsp;strange ideas,&nbsp;<em>and</em>&nbsp;there's some combination of successfully teaching <a href=\"http://wiki.lesswrong.com/wiki/Conformity_bias\">Cognitive Art: Resist Conformity</a>&nbsp;with the less lofty enhancer We Now Have Enough People Physically Present That You Don't Feel Nonconformist, that community will inevitably propagate what they&nbsp;<em>believe</em>&nbsp;to be good new ideas that haven't been mass-adopted by the general population.</p>\n<p>When I saw that Patri Friedman was wearing Vibrams (five-toed shoes) and that William Eden (then Will Ryan) was also wearing Vibrams, I got a pair myself to see if they'd work. They didn't work for me, which thanks to <a href=\"http://wiki.lesswrong.com/wiki/Oops\">Cognitive Art: Say Oops</a>&nbsp;I was able to admit without much fuss; and so I put my athletic shoes back on again. &nbsp;Paleo-inspired diets haven't done anything discernible for me, but have helped many other people in the community. Supplementing potassium (citrate) hasn't helped me much, but works dramatically for Anna, Kevin, and Vassar. &nbsp;Seth Roberts's \"Shangri-La diet\", which was propagating through econblogs, led me to lose twenty pounds that I've mostly kept off, and then it mysteriously stopped working...</p>\n<p>De facto, I&nbsp;<em>have</em>&nbsp;gotten a noticeable amount of mileage out of imitating things I've seen other rationalists do. In principle, this will work better than reading a lifehacking blog to whatever extent rationalist opinion leaders are better able to filter lifehacks - discern better and worse experimental evidence, avoid affective death spirals around things that sound cool, and give up faster when things don't work. In practice, I myself haven't gone particularly far into the mainstream lifehacking community, so I don't know how much of an advantage, if any, we've got (so far). My suspicion is that on average lifehackers should know more cool things than we do (by virtue of having invested more time and practice), and have more obviously bad things mixed in (due to only average levels of Cognitive Art: Resist Nonsense).</p>\n<p>But strange-to-the-mainstream yet oddly-effective ideas propagating through the community is something that happens if everything goes&nbsp;<em>right</em>. The danger of these things looking&nbsp;<em>weird...</em>&nbsp;is one that I think we just have to bite the bullet on, though opinions on this subject vary between myself and other community leaders.</p>\n<p>So a lot of real-world mileage in practice is likely to come out of us imitating each other...</p>\n<p>And yet&nbsp;<em>nonetheless,</em>&nbsp;I think it worth naming and resisting that dark temptation to think that somebody can't be a&nbsp;<em>real</em>&nbsp;community member if they aren't eating beef livers and supplementing potassium, or if they believe in a <a href=\"/lw/q7/if_manyworlds_had_come_first/\">collapse interpretation of QM</a>, etcetera. If a newcomer <em>also</em>&nbsp;doesn't show any particular, noticeable interest in the algorithms and the process, then sure, don't feed the trolls. It should be another matter if someone seems interested in the process, better yet the <a href=\"http://wiki.lesswrong.com/wiki/Bayes'_theorem\">math</a>, and has some non-zero grasp of it, and are just coming to different conclusions than the local consensus.</p>\n<p>Applied rationality counts for something, indeed; rationality that isn't applied might as well not exist. And if somebody believes in something really wacky, like Mormonism or that&nbsp;<a href=\"/lw/r9/quantum_mechanics_and_personal_identity/\">personal identity follows individual particles</a>, you'd&nbsp;<em>expect</em>&nbsp;to eventually find some flaw in reasoning - a departure from the rules - if you trace back their reasoning far enough. But there's a genuine and open question as to how much you should really assume - how much would be&nbsp;<em>actually true</em>&nbsp;to assume - about the general reasoning deficits of somebody who says they're Mormon, but who can solve Bayesian problems on a blackboard and explain what&nbsp;<a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">Governor Earl Warren was doing wrong</a>&nbsp;and&nbsp;<a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/\">analyzes the Amanda Knox case correctly</a>. Robert Aumann (Nobel laureate Bayesian guy) is a believing Orthodox Jew, after all.</p>\n<p>But the deeper danger isn't that of mistakenly excluding someone who's fairly good at a bunch of cognitive algorithms and still has some blind spots.</p>\n<p>The deeper danger is in allowing your&nbsp;<em>de facto</em>&nbsp;sense of rationalist community to start being defined by conformity to what people think is merely&nbsp;<em>optimal,</em>&nbsp;rather than the cognitive algorithms and thinking techniques that are supposed to be at the center.</p>\n<p>And then a purely metaphorical Ayn Rand starts kicking people out because they like suboptimal music. A sense of you-must-do-X-to-belong is also a kind of Authority.</p>\n<p>Not all Authority is bad - probability theory is also a kind of Authority and I try to be <a href=\"http://wiki.lesswrong.com/wiki/Lawful_intelligence\">ruled by it</a> as much as I can manage. But good Authority should generally be&nbsp;<em>modular;</em>&nbsp;having a sweeping cultural sense of lots and lots of mandatory things is also a failure mode. This is what I think of as the core Objectivist Failure Mode - why the heck is Ayn Rand talking about music?</p>\n<p>So let's all please be conservative about invoking the word 'rational', and try not to use it except when we're talking about cognitive algorithms and thinking techniques. And in general and as a reminder, let's continue exerting some pressure to adjust our intuitions about belonging-to-LW-ness in the direction of (a) deliberately not rejecting people who disagree with a particular point of mere optimality, and (b) deliberately extending hands to people who show&nbsp;<em>respect for the process</em>&nbsp;and&nbsp;<em>interest in the algorithms</em>&nbsp;even if they're disagreeing with the general consensus.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/eva/the_fabric_of_real_things/\">The Fabric of Real Things</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/eta/rationality_appreciating_cognitive_algorithms/\">Rationality: Appreciating Cognitive Algorithms</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "izp6eeJJEg9v5zcur": 1, "9YFoDPFwMoWthzgkY": 1, "AHK82ypfxF45rqh9D": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WbLAA8qZQNdbRgKte", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 114, "baseScore": 152, "extendedScore": null, "score": 0.000332, "legacy": true, "legacyId": "19203", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "the-fabric-of-real-things", "canonicalPrevPostSlug": "rationality-appreciating-cognitive-algorithms", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 152, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 337, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HcCpvYLoSFP4iAqSz", "96TBXaHwLbFyeAxrg", "3p3CYauiX8oLjmwRF", "WqGCaRhib42dhKWRL", "7HMSBiEiCfLKzd2gc", "mnS2WYLCGJP2kQkRn", "mkAcXPEJ7RZCJs8ry", "h6fzC6wFYFxxKDm8u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-08T19:50:36.426Z", "modifiedAt": null, "url": null, "title": "Reasons for someone to \"ignore\" you", "slug": "reasons-for-someone-to-ignore-you", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:07.156Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WmbHx5F3aiEWKD29Y/reasons-for-someone-to-ignore-you", "pageUrlRelative": "/posts/WmbHx5F3aiEWKD29Y/reasons-for-someone-to-ignore-you", "linkUrl": "https://www.lesswrong.com/posts/WmbHx5F3aiEWKD29Y/reasons-for-someone-to-ignore-you", "postedAtFormatted": "Monday, October 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reasons%20for%20someone%20to%20%22ignore%22%20you&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReasons%20for%20someone%20to%20%22ignore%22%20you%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmbHx5F3aiEWKD29Y%2Freasons-for-someone-to-ignore-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reasons%20for%20someone%20to%20%22ignore%22%20you%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmbHx5F3aiEWKD29Y%2Freasons-for-someone-to-ignore-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmbHx5F3aiEWKD29Y%2Freasons-for-someone-to-ignore-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 782, "htmlBody": "<p>I often feel guilty for <a href=\"/lw/1gg/agree_retort_or_ignore_a_post_from_the_future/\">ignoring</a> other people's comments or questions, and frustrated when other people seem to be ignoring me. If I can't indicate to someone exactly why I'm not answering, or can't receive such an indication myself, I can at least help my future selves and&nbsp;others&nbsp;obtain a better probability distribution over such reasons. To that end, I'm listing all of the reasons I can think of for someone to not respond to a comment/question, to save the effort of regenerating these hypotheses from scratch each time and prevent the possibility of failing to <em>consider</em>&nbsp;the actual reason. Note that these are not meant to be mutually exclusive.</p>\n<ul>\n<li>They haven't checked their inbox yet.</li>\n<li>They got too many responses in their inbox and didn't pay enough attention to yours.</li>\n<li>They are temporarily too busy to respond.</li>\n<li>They were planning to respond but then forgot to. </li>\n<li>They don't understand the comment yet and are still trying.</li>\n<li>They've stopped trying to understand the comment and don't expect further discussion to resolve the confusion. </li>\n<li>They think it's obvious that they agree.</li>\n<li>They think it's obvious that they disagree.</li>\n<li>They disagree and are planning to write up the reasons later.</li>\n<li>They don't know whether to agree or disagree and are still thinking about it.</li>\n<li>They think all useful information has been exchanged and it's not worth another comment just to indicate final agreement/disagreement.</li>\n<li>They think you just want to express your opinion and don't care what they think.</li>\n<li>They are&nbsp;tired of the discussion and don't want to think about it any more.</li>\n<li>The comment shows a level of intelligence and/or rationality and/or knowledge that makes it not worthwhile for them to engage you. </li>\n<li>They already addressed your question or point before but you missed it or didn't get it.</li>\n<li>They don't know how to answer your question and are too embarrassed&nbsp;to admit it.</li>\n<li>They interpreted your question as being addressed to the public rather than to them personally.</li>\n<li>They think most people already know the answer (or don't care to know) and don't want to bother answering just for you or a few other people.</li>\n<li>They think you are mainly signaling/status-seeking instead of truth-seeking.</li>\n<li>They are mainly&nbsp;signaling/status-seeking&nbsp;(perhaps subconsciously)&nbsp;and think not responding is optimal for that.</li>\n<li>They can't see how to respond honestly without causing or prolonging a personal&nbsp;enmity.</li>\n<li>They consider you a troll or potential troll and don't want to reinforce you with attention.</li>\n<li>They have an emotional aversion against talking to you. </li>\n<li>They have some other instrumental reason for not responding.</li>\n<li>Suggested by&nbsp;shminux: You're on a list of LWers they never reply to, because a number of prior conversations with you were invariably futile for one or more of the reasons described above, and their estimate of any future conversation going any better is very low. </li>\n<li>Suggested by wedrifid:&nbsp;Technical difficulties. They first read your comment via a mobile device, composed (mentally) a reply that would take too long to type on that medium and two days later they either forget to type it out via keyboard, no longer care about the subject or think that a late reply would be inappropriate given developments in the conversation.</li>\n<li>Suggested by wedrifid:&nbsp;Previous comments by them in the thread had been downvoted or otherwise opposed and they choose to accede to the implied wishes of the community rather than try to fight it or defy it.</li>\n<li>Suggested by cata: Not answering promptly caused them to feel guilty, which caused more delay and more guilt, so they never respond to hide their shame.</li>\n<li>Suggested by wedrifid: They think your comment missed the point of the context and so doesn't make sense but it is not important enough to embarrass you by explaining or challenging.</li>\n<li>Suggested by Morendil: Your post/comment didn't contain a single question mark, so there's no call to answer.</li>\n<li>Suggesetd by sixes_and_sevens:&nbsp;They think the discussion is going off topic.\n<li>Suggested by Airedale:&nbsp;They're purposefully trying to disengage early rather than getting into a fight about who has the \"last word\" on the subject, e.g., on some level they may want to respond or even to \"win\" the exchange, but they're purposefully telling themselves to step away from the computer.</li>\n</li>\n</ul>\n<p>If I missed any reasons (that happen often enough to be worth including in this list), please give them in the comments. See also <a href=\"/lw/5/issues_bugs_and_requested_features/11mr\">this related comment</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "MXcpQvaPGtXpB6vkM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WmbHx5F3aiEWKD29Y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 37, "extendedScore": null, "score": 1.004753625982663e-06, "legacy": true, "legacyId": "19234", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rH492M8T8pKK5763D"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-09T04:57:05.105Z", "modifiedAt": null, "url": null, "title": "Meetup : Berlin Meetup", "slug": "meetup-berlin-meetup-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:04.708Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PeoLN9ufEJeY8DKBX/meetup-berlin-meetup-3", "pageUrlRelative": "/posts/PeoLN9ufEJeY8DKBX/meetup-berlin-meetup-3", "linkUrl": "https://www.lesswrong.com/posts/PeoLN9ufEJeY8DKBX/meetup-berlin-meetup-3", "postedAtFormatted": "Tuesday, October 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berlin%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berlin%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPeoLN9ufEJeY8DKBX%2Fmeetup-berlin-meetup-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berlin%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPeoLN9ufEJeY8DKBX%2Fmeetup-berlin-meetup-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPeoLN9ufEJeY8DKBX%2Fmeetup-berlin-meetup-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 349, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ep'>Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 October 2012 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Spreegold ALEX, Rosa Luxemburg-Stra\u00dfe 2, 10178 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>NOTE: We switched venue to Spreegold ALEXANDERPLATZ, Rosa Luxemburg-Stra\u00dfe 2, 10178 Berlin.</p>\n\n<p>It looks like we'll be at least eight people, so it'll likely be our biggest gathering yet! I'm looking forward to meeting everyone.</p>\n\n<p>If you're attending for the first time: We have a <a href=\"http://groups.google.com/group/lw-berlin\" rel=\"nofollow\">mailing list</a> where the planning happens. Feel free to sign up. I'll also bring a sign saying 'LessWrong' in big, friendly - although red - letters.</p>\n\n<p>Here's the current plan and comments:</p>\n\n<ul>\n<li>Michael: Discuss recent sequence <a href=\"http://lesswrong.com/tag/seq_epistemology/\" rel=\"nofollow\">http://lesswrong.com/tag/seq_epistemology/</a></li>\n<li>Christian: Meta discussion limited to 30 minutes: How to improve the meetup? What do we want to achieve?</li>\n<li>Christian: Share goals and projects. Let\u2019s start with just sharing so the others know what interests us. No long discussion intended, so it shouldn\u2019t consume too much time.</li>\n<li>Jan: Calibration game using Gambit 7 questions</li>\n<li>Christian: rejection therapy follow-up</li>\n<li>Christian: evaluate commitments and make new ones</li>\n<li>Christian: agree on meetup scheduling for next time: suggestion: use doodle again, clarify ifneedbe, use highest yes+ifneedbe. If &gt;8 attendants again next time, switch to fixed-day schedule. (we'll make a new poll, but this one would suggest meeting every 2 weeks and alternating saturday and tuesday)</li>\n<li>Alexander: 'Hypothetical Apostasies' game from booklet. Choosing a belief in advance would help. The current candidate is 'taking untested recreational drugs is good'. Note feedback comment below.</li>\n</ul>\n\n<p>Semi-anonymous feedback:</p>\n\n<ul>\n<li>Some kind of moderation during focused discussion would be nice - to end or defer tangents and keep discussion moving</li>\n<li>Jan: Seconded. I volunteer to moderate.</li>\n<li>Jan: And I looked at the \u2018Hypothetical Apostasies\u2019 game. The current belief seems not too well suited (too easy to argue against and not important enough). But I don\u2019t think we need one in advance, since one of the purposes seems arguing against it on the spot. I have a few ideas we could use though.</li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ep'>Berlin Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PeoLN9ufEJeY8DKBX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0050431833203658e-06, "legacy": true, "legacyId": "19241", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup\">Discussion article for the meetup : <a href=\"/meetups/ep\">Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 October 2012 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Spreegold ALEX, Rosa Luxemburg-Stra\u00dfe 2, 10178 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>NOTE: We switched venue to Spreegold ALEXANDERPLATZ, Rosa Luxemburg-Stra\u00dfe 2, 10178 Berlin.</p>\n\n<p>It looks like we'll be at least eight people, so it'll likely be our biggest gathering yet! I'm looking forward to meeting everyone.</p>\n\n<p>If you're attending for the first time: We have a <a href=\"http://groups.google.com/group/lw-berlin\" rel=\"nofollow\">mailing list</a> where the planning happens. Feel free to sign up. I'll also bring a sign saying 'LessWrong' in big, friendly - although red - letters.</p>\n\n<p>Here's the current plan and comments:</p>\n\n<ul>\n<li>Michael: Discuss recent sequence <a href=\"http://lesswrong.com/tag/seq_epistemology/\" rel=\"nofollow\">http://lesswrong.com/tag/seq_epistemology/</a></li>\n<li>Christian: Meta discussion limited to 30 minutes: How to improve the meetup? What do we want to achieve?</li>\n<li>Christian: Share goals and projects. Let\u2019s start with just sharing so the others know what interests us. No long discussion intended, so it shouldn\u2019t consume too much time.</li>\n<li>Jan: Calibration game using Gambit 7 questions</li>\n<li>Christian: rejection therapy follow-up</li>\n<li>Christian: evaluate commitments and make new ones</li>\n<li>Christian: agree on meetup scheduling for next time: suggestion: use doodle again, clarify ifneedbe, use highest yes+ifneedbe. If &gt;8 attendants again next time, switch to fixed-day schedule. (we'll make a new poll, but this one would suggest meeting every 2 weeks and alternating saturday and tuesday)</li>\n<li>Alexander: 'Hypothetical Apostasies' game from booklet. Choosing a belief in advance would help. The current candidate is 'taking untested recreational drugs is good'. Note feedback comment below.</li>\n</ul>\n\n<p>Semi-anonymous feedback:</p>\n\n<ul>\n<li>Some kind of moderation during focused discussion would be nice - to end or defer tangents and keep discussion moving</li>\n<li>Jan: Seconded. I volunteer to moderate.</li>\n<li>Jan: And I looked at the \u2018Hypothetical Apostasies\u2019 game. The current belief seems not too well suited (too easy to argue against and not important enough). But I don\u2019t think we need one in advance, since one of the purposes seems arguing against it on the spot. I have a few ideas we could use though.</li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/ep\">Berlin Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-09T05:57:48.682Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Efficient Cross-Domain Optimization", "slug": "seq-rerun-efficient-cross-domain-optimization", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4tBerroMGG8xJG7J3/seq-rerun-efficient-cross-domain-optimization", "pageUrlRelative": "/posts/4tBerroMGG8xJG7J3/seq-rerun-efficient-cross-domain-optimization", "linkUrl": "https://www.lesswrong.com/posts/4tBerroMGG8xJG7J3/seq-rerun-efficient-cross-domain-optimization", "postedAtFormatted": "Tuesday, October 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Efficient%20Cross-Domain%20Optimization&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Efficient%20Cross-Domain%20Optimization%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4tBerroMGG8xJG7J3%2Fseq-rerun-efficient-cross-domain-optimization%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Efficient%20Cross-Domain%20Optimization%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4tBerroMGG8xJG7J3%2Fseq-rerun-efficient-cross-domain-optimization", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4tBerroMGG8xJG7J3%2Fseq-rerun-efficient-cross-domain-optimization", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>Today's post, <a href=\"/lw/vb/efficient_crossdomain_optimization/\">Efficient Cross-Domain Optimization</a> was originally published on 28 October 2008 .  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Efficient_Cross-Domain_Optimization\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>To speak of intelligence, rather than optimization power, we need to divide optimization power by the resources needed, or the amount of prior optimization that had to be done on the system.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/etz/seq_rerun_measuring_optimization_power/\">Measuring Optimization Power</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4tBerroMGG8xJG7J3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.005075368861509e-06, "legacy": true, "legacyId": "19245", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yLeEPFnnB9wE7KLx2", "N6CXdHxJHba5qHeav", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-09T15:49:57.335Z", "modifiedAt": null, "url": null, "title": "Raising the forecasting waterline (part 1)", "slug": "raising-the-forecasting-waterline-part-1", "viewCount": null, "lastCommentedAt": "2012-10-14T21:17:00.229Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YQ8H4e7z3q8ngev7J/raising-the-forecasting-waterline-part-1", "pageUrlRelative": "/posts/YQ8H4e7z3q8ngev7J/raising-the-forecasting-waterline-part-1", "linkUrl": "https://www.lesswrong.com/posts/YQ8H4e7z3q8ngev7J/raising-the-forecasting-waterline-part-1", "postedAtFormatted": "Tuesday, October 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Raising%20the%20forecasting%20waterline%20(part%201)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARaising%20the%20forecasting%20waterline%20(part%201)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQ8H4e7z3q8ngev7J%2Fraising-the-forecasting-waterline-part-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Raising%20the%20forecasting%20waterline%20(part%201)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQ8H4e7z3q8ngev7J%2Fraising-the-forecasting-waterline-part-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQ8H4e7z3q8ngev7J%2Fraising-the-forecasting-waterline-part-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1785, "htmlBody": "<p><strong>Previously:</strong> <a href=\"/lw/etl/raising_the_waterline/\">Raising the waterline</a>, <strong>see also:</strong>&nbsp;<a href=\"http://www.gwern.net/Prediction%20markets#predictionbook-nights\">1001 PredictionBook Nights</a>&nbsp;(<a href=\"/lw/7z9/1001_predictionbook_nights/\">LW copy</a>),&nbsp;<a href=\"/lw/3m6/techniques_for_probability_estimates/\">Techniques for probability estimates</a></p>\n<p>Low waterlines imply that it's relatively easy for a novice to outperform the competition. (In poker, as discussed in Nate Silver's book, the \"fish\" are those who can't master basic techniques such as folding when they have a poor hand, or calculating even roughly the expected value of a pot.) Does this apply to the domain of making predictions? It's early days, but it looks as if a smallish set of tools - a conscious status quo bias, respecting probability axioms when considering alternatives, considering references classes, leaving yourself a line of retreat, detaching from sunk costs, and a few more - can at least place you in a good position.&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<h2>A bit of backstory</h2>\n<p>Like perhaps many LessWrongers, my first encounter with the notion of calibrated confidence was \"<a href=\"http://yudkowsky.net/rational/technical\">A Technical Explanation of Technical Explanation</a>\". My first serious stab at publicly expressing my own beliefs as quantified probabilities was <a href=\"/lw/8p4/2011_survey_results/5do9\">the Amanda Knox case</a>&nbsp;-&nbsp;an eye-opener, waking me up to how everyday opinions could correspond to degrees of certainty, and how these had <em>consequences</em>. By the following year, I was trying to <a href=\"/lw/8p4/2011_survey_results/5do9\">improve</a> my calibration for work-related <a href=\"/lw/2qc/rationality_power_tools/2pj4\">purposes</a>, and playing with various Web sites, like <a href=\"http://predictionbook.com/\">PredictionBook</a> or Guessum (now defunct).</p>\n<p>Then the Good Judgment Project was <a href=\"/lw/6ya/link_get_paid_to_train_your_rationality/\">announced</a> on Less Wrong. Like several of us, I applied, <a href=\"/lw/6ya/link_get_paid_to_train_your_rationality/4lwm\">unexpectedly</a> got in, and started taking forecasting more seriously. (I tend to apply myself somewhat better to learning when there is a competitive element - not an attitude I'm particularly proud of, but being aware of that is useful.)</p>\n<p>The <a href=\"http://goodjudgmentproject.com/\">GJP</a> is both a contest and an experimental study, in fact a group of related studies: several distinct groups of researchers (<a href=\"http://goodjudgmentproject.com/\">1</a>,<a href=\"http://daggre.org/info/\">2</a>,<a href=\"http://www.ispade.net/\">3</a>,<a href=\"http://forecastingace.com/\">4</a>) are being <a href=\"http://www.iarpa.gov/solicitations_ace.html\">funded by IARPA</a> to each run their own experimental program. Within each, small or large number of participants have been recruited, allocated to different experimental conditions, and encouraged to compete with each other (or even, as far as I know, for some experimental conditions, collaborate with each other). The goal is to make predictions about \"world events\" - and if possible to get them more right, collectively, than we would individually.<a style=\"font-size: 11px; \" href=\"#f1\"><sup>1</sup></a><a name=\"fc1\"></a></p>\n<h2>Tool 1: Favor the status quo</h2>\n<p>The first hint I got that my approach to forecasting needed more explicit thinking tools was a <a href=\"http://torontopm.wordpress.com/2012/03/15/status-quo-predicting/\">blog post</a> by Paul Hewitt I came across late in the first season. My <a href=\"/\">scores</a> in that period (summer 2011 to spring 2012) had been decent but not fantastic; I ended up 5th on my team, which itself placed quite modestly in the contest.</p>\n<p>Hewitt pointed out that in general, you could do better than most other forecasters by favoring the status quo outcome.<sup><a href=\"#f2\">2</a><a name=\"fc2\"></a></sup>&nbsp;This may not quite be on the same order of effectiveness as the poker advice to \"err on the side of folding mediocre hands more often\", but it makes a lot of sense, at least for the Good Judgment Project (and possibly for many of the questions we might worry about). Many of the GJP questions refer to possibilities that loom large in the media at a given time, that are highly available&nbsp;- in the sense of the <a href=\"http://wiki.lesswrong.com/wiki/Availability_heuristic\">availability heuristic</a>. This results in a tendency to favor forecasts of change from status quo.</p>\n<p>For instance, one of the Season 1 questions was \"Will Marine LePen cease to be a candidate for President of France before 10 April 2012?\" (also <a href=\"http://predictionbook.com/predictions/5716\">on PredictionBook</a>). Just because the question is being asked doesn't mean that you should assign \"yes\" and \"no\" equal probabilities of 50%, or even close to 50%, any more than you should assign 50% to the proposition \"I will win the lottery\".</p>\n<p>Rather, you might start from a relatively low prior probability that anyone who undertakes something as significant as a bid for national presidency would throw in the towel before the contest even starts. Then, try to find evidence that positively favors a change. In this particular case, there was such evidence - &nbsp;the National Front, of which she was the candidate, consistently reports <a href=\"http://www.bbc.co.uk/news/world-europe-17296773\">difficulties</a> rounding up the endorsements required to register a candidate legally. However, only once in the past (1981) had this resulted in their candidate being barred (admittedly a very small sample). It would have been a mistake to weigh that evidence excessively. (I got a good score on that question, compared to the team, but definitely owing to a \"home ground advantage\" as a French citizen rather than my superior forecasting skills.)</p>\n<h2>Tool 2: Flip the question around</h2>\n<p>The next technique I try to apply consistently is respecting the axioms of probability. If the probability of event <strong>A</strong> is 70%, then the probability of <strong>not-A</strong> is 30%.</p>\n<p>This may strike everyone as obvious... it's not. In Season 2, several of my team-mates are on record as assigning a 75% probability to the proposition \"The number of registered <a href=\"http://data.unhcr.org/syrianrefugees/regional.php\">Syrian conflict refugees</a> reported by the UNHCR will exceed 250,000 at any point before 1 April 2013\".</p>\n<p>That number was reached today, six months in advance of the deadline. This was clear as early as August. The trend in the past few months has been an increase of 1000 to 2000 a day, and the UNHCR have recently provided estimates that this number will eventually reach 700,000. The kicker is that this number is <em>only</em> the count of people who are fully processed by the UNHCR administration and officially in their database; there are tens of thousands more in the camps who only have \"appointments to be registered\".</p>\n<p>I've been finding it hard to understand why my team-mates haven't been updating to, maybe not 100%, but at least 99%; and how one wouldn't see these as the only answers worth considering. At any point in the past few weeks, to state your probability as 85% or 91% (as some have quite recently) was to say, \"There is <em>still</em> a one in ten chance that the Syrian conflict will suddenly stop and all these people will go home, maybe next week?.\"</p>\n<p>This is kind of like saying \"There is a one in ten chance Santa Claus will be the one distributing the presents this year.\" It feels like a huge \"<a href=\"/lw/1mh/that_magical_click/1hmg\">clack</a>\".</p>\n<p>I can only speculate as to what's going on there. Queried for a probability, people are translating something like \"Sure, <strong>A</strong>&nbsp;is happening\" into a biggish number, and reporting that. They are totally failing to flip the question around and explicitly consider what it would take for <strong>not-A</strong> to happen. (Perhaps, too, people have been so strongly cautioned by cautions, from Tetlock and others, against being overconfident that they reflexively shy away from the extreme numbers.)</p>\n<p>Just because you're expressing beliefs as percentages doesn't mean that you are automatically applying the axioms of probability. Just because you use \"75%\" as a shorthand for \"I'm pretty sure\" doesn't mean you are thinking probabilistically; you must train the skill of seeing that for some events, its complement \"25%\" also counts as \"I'm pretty sure\". The axioms are more important than the use of numbers - in fact for this sort of forecast \"91%\" strikes me as needlessly precise; increments of 5% are more than enough, away from the extremes.</p>\n<h2>Tool 3: Reference class forecasting</h2>\n<p>The order in which I'm discussing these \"basics of forecasting\" reflects not so much their importance, as the order in which I tend to run through them when encountering a new question. (This might not be the optimal order, or even very good - but that should matter little if the waterline is indeed low.)</p>\n<p>Using <a href=\"/lw/1gw/contrarianism_and_reference_class_forecasting/\">reference classes</a> was actually part of the \"training package\" of the GJP. From the linked post comes the warning that \"deciding what's the proper reference class is not straightforward\". And in fact, this tool only applies in some cases, not systematically. One of our recently closed questions was \"Will any government force gain control of the Somali town of Kismayo before 1 November 2012?\". Clearly, you could spend quite a while trying to figure out an appropriate reference class here. (In fact, this question also stands as a counter-example to the \"Favor status quo\" tool, and flipping the question around might not have been too useful either. All these tools require some discrimination.)</p>\n<p>On the other hand, it came in rather handy in assessing the short-term question we got late september: \"What change will occur in the FAO Food Price index during September 2012?\" - with barely two weeks to go before the FAO was to post the updated index in early October. More generally, it's a useful tool when you're asked to make predictions regarding a numerical indicator, for which you can observe past data.&nbsp;</p>\n<p>The FAO price data can be retrieved as a <a href=\"http://www.fao.org/fileadmin/templates/worldfood/Reports_and_docs/Food_price_indices_data.xls\">spreadsheet</a>&nbsp;(.xsl download). Our forecast question divided the outcomes into four: <strong>A</strong>) an increase of 3% or more, <strong>B</strong>) an increase of less than 3%, <strong>C</strong>) a decrease of less than 3%, <strong>D</strong>) a decrease of more than 3%, <strong>E</strong>) \"no change\" - meaning a change too small to alter the value rounded to the nearest integer.</p>\n<p>It's not clear from the chart that there is any consistent seasonal variation. A change of 3% would have been about 6.4 points; since 8/2011 there had been four month-on-month changes of that magnitude, 3 decreases and 1 increase. Based on that reference class, the probability of a small change (B+C+E) came out to about 2/3. The probability for \"no change\" (E) to 1/12 - the August price was the same as the July price. The probability for an increase (A+B), roughly the same as for a decrease (C+D). My first-cut forecast allocated the probability mass as follows: 15/30/30/15/10.</p>\n<p>However, I figured I did need to apply a correction, based on reports of a drought in the US that could lead to some food shortages. I took 10% probability mass from the \"decrease\" outcomes and allocated it to the \"increase\" outcomes. My final forecast was 20/35/25/10/10. I didn't mess around with it any more than that. As it turned out, the actual outcome was <strong>B</strong>! My score was bettered by only 3 forecasters, out of a total of 9.</p>\n<h2>Next up: lines of retreat, ditching sunk costs, loss functions</h2>\n<p>This post has grown long enough, and I still have 3+ tools I want to cover. Stay tuned for Part 2!</p>\n<div><br /></div>\n<hr />\n<p style=\"font-size:90%\"><sup><a href=\"#fc1\">1</a><a name=\"f1\"></a></sup>&nbsp;The GJP is being run by Phil Tetlock, known for his \"hedgehog and fox\" analysis of forecasting. At that time I wasn't aware of the competing groups - one of them, DAGGRE, is run by Robin Hanson (of OB fame) among others, which might have made it an appealing alternate choice if I'd know about it.</p>\n<p style=\"font-size:90%\"><sup><a href=\"#fc2\">2</a><a name=\"f2\"></a></sup>&nbsp;Unfortunately, the experimental condition Paul belonged to used a prediction market where forecasters played virtual money by \"betting\" on predictions; this makes it hard to translate the numbers he provides into probabilities. The general point is still interesting.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8daMDi9NEShyLqxth": 1, "8hPTCJbwJnLBmfpCX": 1, "bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YQ8H4e7z3q8ngev7J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 51, "extendedScore": null, "score": 1.0053893074568985e-06, "legacy": true, "legacyId": "19249", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Previously:</strong> <a href=\"/lw/etl/raising_the_waterline/\">Raising the waterline</a>, <strong>see also:</strong>&nbsp;<a href=\"http://www.gwern.net/Prediction%20markets#predictionbook-nights\">1001 PredictionBook Nights</a>&nbsp;(<a href=\"/lw/7z9/1001_predictionbook_nights/\">LW copy</a>),&nbsp;<a href=\"/lw/3m6/techniques_for_probability_estimates/\">Techniques for probability estimates</a></p>\n<p>Low waterlines imply that it's relatively easy for a novice to outperform the competition. (In poker, as discussed in Nate Silver's book, the \"fish\" are those who can't master basic techniques such as folding when they have a poor hand, or calculating even roughly the expected value of a pot.) Does this apply to the domain of making predictions? It's early days, but it looks as if a smallish set of tools - a conscious status quo bias, respecting probability axioms when considering alternatives, considering references classes, leaving yourself a line of retreat, detaching from sunk costs, and a few more - can at least place you in a good position.&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"A_bit_of_backstory\">A bit of backstory</h2>\n<p>Like perhaps many LessWrongers, my first encounter with the notion of calibrated confidence was \"<a href=\"http://yudkowsky.net/rational/technical\">A Technical Explanation of Technical Explanation</a>\". My first serious stab at publicly expressing my own beliefs as quantified probabilities was <a href=\"/lw/8p4/2011_survey_results/5do9\">the Amanda Knox case</a>&nbsp;-&nbsp;an eye-opener, waking me up to how everyday opinions could correspond to degrees of certainty, and how these had <em>consequences</em>. By the following year, I was trying to <a href=\"/lw/8p4/2011_survey_results/5do9\">improve</a> my calibration for work-related <a href=\"/lw/2qc/rationality_power_tools/2pj4\">purposes</a>, and playing with various Web sites, like <a href=\"http://predictionbook.com/\">PredictionBook</a> or Guessum (now defunct).</p>\n<p>Then the Good Judgment Project was <a href=\"/lw/6ya/link_get_paid_to_train_your_rationality/\">announced</a> on Less Wrong. Like several of us, I applied, <a href=\"/lw/6ya/link_get_paid_to_train_your_rationality/4lwm\">unexpectedly</a> got in, and started taking forecasting more seriously. (I tend to apply myself somewhat better to learning when there is a competitive element - not an attitude I'm particularly proud of, but being aware of that is useful.)</p>\n<p>The <a href=\"http://goodjudgmentproject.com/\">GJP</a> is both a contest and an experimental study, in fact a group of related studies: several distinct groups of researchers (<a href=\"http://goodjudgmentproject.com/\">1</a>,<a href=\"http://daggre.org/info/\">2</a>,<a href=\"http://www.ispade.net/\">3</a>,<a href=\"http://forecastingace.com/\">4</a>) are being <a href=\"http://www.iarpa.gov/solicitations_ace.html\">funded by IARPA</a> to each run their own experimental program. Within each, small or large number of participants have been recruited, allocated to different experimental conditions, and encouraged to compete with each other (or even, as far as I know, for some experimental conditions, collaborate with each other). The goal is to make predictions about \"world events\" - and if possible to get them more right, collectively, than we would individually.<a style=\"font-size: 11px; \" href=\"#f1\"><sup>1</sup></a><a name=\"fc1\"></a></p>\n<h2 id=\"Tool_1__Favor_the_status_quo\">Tool 1: Favor the status quo</h2>\n<p>The first hint I got that my approach to forecasting needed more explicit thinking tools was a <a href=\"http://torontopm.wordpress.com/2012/03/15/status-quo-predicting/\">blog post</a> by Paul Hewitt I came across late in the first season. My <a href=\"/\">scores</a> in that period (summer 2011 to spring 2012) had been decent but not fantastic; I ended up 5th on my team, which itself placed quite modestly in the contest.</p>\n<p>Hewitt pointed out that in general, you could do better than most other forecasters by favoring the status quo outcome.<sup><a href=\"#f2\">2</a><a name=\"fc2\"></a></sup>&nbsp;This may not quite be on the same order of effectiveness as the poker advice to \"err on the side of folding mediocre hands more often\", but it makes a lot of sense, at least for the Good Judgment Project (and possibly for many of the questions we might worry about). Many of the GJP questions refer to possibilities that loom large in the media at a given time, that are highly available&nbsp;- in the sense of the <a href=\"http://wiki.lesswrong.com/wiki/Availability_heuristic\">availability heuristic</a>. This results in a tendency to favor forecasts of change from status quo.</p>\n<p>For instance, one of the Season 1 questions was \"Will Marine LePen cease to be a candidate for President of France before 10 April 2012?\" (also <a href=\"http://predictionbook.com/predictions/5716\">on PredictionBook</a>). Just because the question is being asked doesn't mean that you should assign \"yes\" and \"no\" equal probabilities of 50%, or even close to 50%, any more than you should assign 50% to the proposition \"I will win the lottery\".</p>\n<p>Rather, you might start from a relatively low prior probability that anyone who undertakes something as significant as a bid for national presidency would throw in the towel before the contest even starts. Then, try to find evidence that positively favors a change. In this particular case, there was such evidence - &nbsp;the National Front, of which she was the candidate, consistently reports <a href=\"http://www.bbc.co.uk/news/world-europe-17296773\">difficulties</a> rounding up the endorsements required to register a candidate legally. However, only once in the past (1981) had this resulted in their candidate being barred (admittedly a very small sample). It would have been a mistake to weigh that evidence excessively. (I got a good score on that question, compared to the team, but definitely owing to a \"home ground advantage\" as a French citizen rather than my superior forecasting skills.)</p>\n<h2 id=\"Tool_2__Flip_the_question_around\">Tool 2: Flip the question around</h2>\n<p>The next technique I try to apply consistently is respecting the axioms of probability. If the probability of event <strong>A</strong> is 70%, then the probability of <strong>not-A</strong> is 30%.</p>\n<p>This may strike everyone as obvious... it's not. In Season 2, several of my team-mates are on record as assigning a 75% probability to the proposition \"The number of registered <a href=\"http://data.unhcr.org/syrianrefugees/regional.php\">Syrian conflict refugees</a> reported by the UNHCR will exceed 250,000 at any point before 1 April 2013\".</p>\n<p>That number was reached today, six months in advance of the deadline. This was clear as early as August. The trend in the past few months has been an increase of 1000 to 2000 a day, and the UNHCR have recently provided estimates that this number will eventually reach 700,000. The kicker is that this number is <em>only</em> the count of people who are fully processed by the UNHCR administration and officially in their database; there are tens of thousands more in the camps who only have \"appointments to be registered\".</p>\n<p>I've been finding it hard to understand why my team-mates haven't been updating to, maybe not 100%, but at least 99%; and how one wouldn't see these as the only answers worth considering. At any point in the past few weeks, to state your probability as 85% or 91% (as some have quite recently) was to say, \"There is <em>still</em> a one in ten chance that the Syrian conflict will suddenly stop and all these people will go home, maybe next week?.\"</p>\n<p>This is kind of like saying \"There is a one in ten chance Santa Claus will be the one distributing the presents this year.\" It feels like a huge \"<a href=\"/lw/1mh/that_magical_click/1hmg\">clack</a>\".</p>\n<p>I can only speculate as to what's going on there. Queried for a probability, people are translating something like \"Sure, <strong>A</strong>&nbsp;is happening\" into a biggish number, and reporting that. They are totally failing to flip the question around and explicitly consider what it would take for <strong>not-A</strong> to happen. (Perhaps, too, people have been so strongly cautioned by cautions, from Tetlock and others, against being overconfident that they reflexively shy away from the extreme numbers.)</p>\n<p>Just because you're expressing beliefs as percentages doesn't mean that you are automatically applying the axioms of probability. Just because you use \"75%\" as a shorthand for \"I'm pretty sure\" doesn't mean you are thinking probabilistically; you must train the skill of seeing that for some events, its complement \"25%\" also counts as \"I'm pretty sure\". The axioms are more important than the use of numbers - in fact for this sort of forecast \"91%\" strikes me as needlessly precise; increments of 5% are more than enough, away from the extremes.</p>\n<h2 id=\"Tool_3__Reference_class_forecasting\">Tool 3: Reference class forecasting</h2>\n<p>The order in which I'm discussing these \"basics of forecasting\" reflects not so much their importance, as the order in which I tend to run through them when encountering a new question. (This might not be the optimal order, or even very good - but that should matter little if the waterline is indeed low.)</p>\n<p>Using <a href=\"/lw/1gw/contrarianism_and_reference_class_forecasting/\">reference classes</a> was actually part of the \"training package\" of the GJP. From the linked post comes the warning that \"deciding what's the proper reference class is not straightforward\". And in fact, this tool only applies in some cases, not systematically. One of our recently closed questions was \"Will any government force gain control of the Somali town of Kismayo before 1 November 2012?\". Clearly, you could spend quite a while trying to figure out an appropriate reference class here. (In fact, this question also stands as a counter-example to the \"Favor status quo\" tool, and flipping the question around might not have been too useful either. All these tools require some discrimination.)</p>\n<p>On the other hand, it came in rather handy in assessing the short-term question we got late september: \"What change will occur in the FAO Food Price index during September 2012?\" - with barely two weeks to go before the FAO was to post the updated index in early October. More generally, it's a useful tool when you're asked to make predictions regarding a numerical indicator, for which you can observe past data.&nbsp;</p>\n<p>The FAO price data can be retrieved as a <a href=\"http://www.fao.org/fileadmin/templates/worldfood/Reports_and_docs/Food_price_indices_data.xls\">spreadsheet</a>&nbsp;(.xsl download). Our forecast question divided the outcomes into four: <strong>A</strong>) an increase of 3% or more, <strong>B</strong>) an increase of less than 3%, <strong>C</strong>) a decrease of less than 3%, <strong>D</strong>) a decrease of more than 3%, <strong>E</strong>) \"no change\" - meaning a change too small to alter the value rounded to the nearest integer.</p>\n<p>It's not clear from the chart that there is any consistent seasonal variation. A change of 3% would have been about 6.4 points; since 8/2011 there had been four month-on-month changes of that magnitude, 3 decreases and 1 increase. Based on that reference class, the probability of a small change (B+C+E) came out to about 2/3. The probability for \"no change\" (E) to 1/12 - the August price was the same as the July price. The probability for an increase (A+B), roughly the same as for a decrease (C+D). My first-cut forecast allocated the probability mass as follows: 15/30/30/15/10.</p>\n<p>However, I figured I did need to apply a correction, based on reports of a drought in the US that could lead to some food shortages. I took 10% probability mass from the \"decrease\" outcomes and allocated it to the \"increase\" outcomes. My final forecast was 20/35/25/10/10. I didn't mess around with it any more than that. As it turned out, the actual outcome was <strong>B</strong>! My score was bettered by only 3 forecasters, out of a total of 9.</p>\n<h2 id=\"Next_up__lines_of_retreat__ditching_sunk_costs__loss_functions\">Next up: lines of retreat, ditching sunk costs, loss functions</h2>\n<p>This post has grown long enough, and I still have 3+ tools I want to cover. Stay tuned for Part 2!</p>\n<div><br></div>\n<hr>\n<p style=\"font-size:90%\"><sup><a href=\"#fc1\">1</a><a name=\"f1\"></a></sup>&nbsp;The GJP is being run by Phil Tetlock, known for his \"hedgehog and fox\" analysis of forecasting. At that time I wasn't aware of the competing groups - one of them, DAGGRE, is run by Robin Hanson (of OB fame) among others, which might have made it an appealing alternate choice if I'd know about it.</p>\n<p style=\"font-size:90%\"><sup><a href=\"#fc2\">2</a><a name=\"f2\"></a></sup>&nbsp;Unfortunately, the experimental condition Paul belonged to used a prediction market where forecasters played virtual money by \"betting\" on predictions; this makes it hard to translate the numbers he provides into probabilities. The general point is still interesting.</p>", "sections": [{"title": "A bit of backstory", "anchor": "A_bit_of_backstory", "level": 1}, {"title": "Tool 1: Favor the status quo", "anchor": "Tool_1__Favor_the_status_quo", "level": 1}, {"title": "Tool 2: Flip the question around", "anchor": "Tool_2__Flip_the_question_around", "level": 1}, {"title": "Tool 3: Reference class forecasting", "anchor": "Tool_3__Reference_class_forecasting", "level": 1}, {"title": "Next up: lines of retreat, ditching sunk costs, loss functions", "anchor": "Next_up__lines_of_retreat__ditching_sunk_costs__loss_functions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "107 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 106, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RiCkRACFwQPsH349u", "yE4Fdx4kYmQchBCek", "r8aAqSBeeeMNRtiYK", "PyuRcbHvxXNdCHoG3", "whxjfzFecFt4hAnBA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-09T18:02:03.685Z", "modifiedAt": null, "url": null, "title": "Signs you're on LW too much", "slug": "signs-you-re-on-lw-too-much", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:06.301Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mTNBLQ4rLmWrhqhJg/signs-you-re-on-lw-too-much", "pageUrlRelative": "/posts/mTNBLQ4rLmWrhqhJg/signs-you-re-on-lw-too-much", "linkUrl": "https://www.lesswrong.com/posts/mTNBLQ4rLmWrhqhJg/signs-you-re-on-lw-too-much", "postedAtFormatted": "Tuesday, October 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Signs%20you're%20on%20LW%20too%20much&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASigns%20you're%20on%20LW%20too%20much%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmTNBLQ4rLmWrhqhJg%2Fsigns-you-re-on-lw-too-much%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Signs%20you're%20on%20LW%20too%20much%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmTNBLQ4rLmWrhqhJg%2Fsigns-you-re-on-lw-too-much", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmTNBLQ4rLmWrhqhJg%2Fsigns-you-re-on-lw-too-much", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<p>In the style of Jeff Foxworthy's \"<a href=\"http://www.youtube.com/watch?v=T7E-isbgwpk\">You Might Be A Redneck If...</a>\"</p>\n<p>If you're waking up and the&nbsp;characters&nbsp;in your dream start asking you to stay asleep a bit longer because they're running on your brain and they don't want to be consigned to oblivion just yet... you might be on LW too much.</p>\n<p>&nbsp;</p>\n<p>Happened to me this morning, it was kinda weird, and I felt kinda sad. In fairness, it might just be my subconscious brain telling me I need to get more damn sleep.</p>\n<p>Anyone have similar stories?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mTNBLQ4rLmWrhqhJg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 17, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "19252", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-09T20:23:27.190Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Reasoning About Politics", "slug": "meetup-west-la-meetup-reasoning-about-politics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:05.237Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/b6q2oEX6hDiBum5bf/meetup-west-la-meetup-reasoning-about-politics", "pageUrlRelative": "/posts/b6q2oEX6hDiBum5bf/meetup-west-la-meetup-reasoning-about-politics", "linkUrl": "https://www.lesswrong.com/posts/b6q2oEX6hDiBum5bf/meetup-west-la-meetup-reasoning-about-politics", "postedAtFormatted": "Tuesday, October 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Reasoning%20About%20Politics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Reasoning%20About%20Politics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb6q2oEX6hDiBum5bf%2Fmeetup-west-la-meetup-reasoning-about-politics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Reasoning%20About%20Politics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb6q2oEX6hDiBum5bf%2Fmeetup-west-la-meetup-reasoning-about-politics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb6q2oEX6hDiBum5bf%2Fmeetup-west-la-meetup-reasoning-about-politics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 218, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/eq'>West LA Meetup - Reasoning About Politics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 October 2012 01:06:09PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, October 10th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> Many on this website would agree that <a href=\"http://lesswrong.com/lw/gw/politics_is_the_mindkiller/\">politics is the mind-killer</a>. But there are many important decisions we have to make that concern politics, and engaging in dialogue is a valuable strategy for figuring out complex topics. So how can we successfully talk and reason about politics? What precautions should we take? How do we talk politics with someone who hasn't stopped to think about this sort of thing?</p>\n\n<p>There will be general discussion too, and there are lots of interesting <a href=\"http://lesswrong.com/recentposts/\">recent posts</a> (also check out LW's sister site, <a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a> ). But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/eq'>West LA Meetup - Reasoning About Politics</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "b6q2oEX6hDiBum5bf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.005534366939011e-06, "legacy": true, "legacyId": "19253", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Reasoning_About_Politics\">Discussion article for the meetup : <a href=\"/meetups/eq\">West LA Meetup - Reasoning About Politics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 October 2012 01:06:09PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, October 10th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> Many on this website would agree that <a href=\"http://lesswrong.com/lw/gw/politics_is_the_mindkiller/\">politics is the mind-killer</a>. But there are many important decisions we have to make that concern politics, and engaging in dialogue is a valuable strategy for figuring out complex topics. So how can we successfully talk and reason about politics? What precautions should we take? How do we talk politics with someone who hasn't stopped to think about this sort of thing?</p>\n\n<p>There will be general discussion too, and there are lots of interesting <a href=\"http://lesswrong.com/recentposts/\">recent posts</a> (also check out LW's sister site, <a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a> ). But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Reasoning_About_Politics1\">Discussion article for the meetup : <a href=\"/meetups/eq\">West LA Meetup - Reasoning About Politics</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Reasoning About Politics", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Reasoning_About_Politics", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Reasoning About Politics", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Reasoning_About_Politics1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-09T22:05:48.090Z", "modifiedAt": null, "url": null, "title": "comment to dude", "slug": "comment-to-dude", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:04.616Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "moridinamael", "createdAt": "2011-01-20T18:55:32.068Z", "isAdmin": false, "displayName": "moridinamael"}, "userId": "sWA9eDCM9AgFaZho8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/skThFLj3uJhXSHPjb/comment-to-dude", "pageUrlRelative": "/posts/skThFLj3uJhXSHPjb/comment-to-dude", "linkUrl": "https://www.lesswrong.com/posts/skThFLj3uJhXSHPjb/comment-to-dude", "postedAtFormatted": "Tuesday, October 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20comment%20to%20dude&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Acomment%20to%20dude%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FskThFLj3uJhXSHPjb%2Fcomment-to-dude%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=comment%20to%20dude%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FskThFLj3uJhXSHPjb%2Fcomment-to-dude", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FskThFLj3uJhXSHPjb%2Fcomment-to-dude", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 146, "htmlBody": "<p>\n<p>Hi,</p>\n<p>&nbsp;</p>\n<p>Things have settled down somewhat and I'm still interested in talking with you about this project.</p>\n<p>&nbsp;</p>\n<p>I suppose, since you've told me some of your ideas in some detail, I should tell you some of mine, so we can get a back-and-forth going. &nbsp;</p>\n<p>&nbsp;</p>\n<p>My initial vision was essentially to create Bayesian Planning Tool. &nbsp;The central idea is that you input your desired goals and outcomes, and then you start working your way towards those outcomes by building a decision-tree framework, assisted by the tool.</p>\n<p>&nbsp;</p>\n<p>Let's say you have a goal of winning some particular contest, for example running a marathon. &nbsp;This gets input explicitly as a terminal goal. &nbsp;Next you break this goal down into the steps needed to reach the goal, but you do it in a probablistic fashion. &nbsp;For example, your intermediate goals are various training and nutritional sub-goals. &nbsp;If one of your sub-goals is \"run&nbsp;</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "skThFLj3uJhXSHPjb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -9, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "19256", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-10T02:21:52.685Z", "modifiedAt": null, "url": null, "title": "Meta-rationality", "slug": "meta-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:32.370Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tuukka_Virtaperko", "createdAt": "2012-01-05T21:47:14.522Z", "isAdmin": false, "displayName": "Tuukka_Virtaperko"}, "userId": "ekrbaZLNr6kQMS8EK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zMFcmKqbk4f8LkvBy/meta-rationality", "pageUrlRelative": "/posts/zMFcmKqbk4f8LkvBy/meta-rationality", "linkUrl": "https://www.lesswrong.com/posts/zMFcmKqbk4f8LkvBy/meta-rationality", "postedAtFormatted": "Wednesday, October 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meta-rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeta-rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzMFcmKqbk4f8LkvBy%2Fmeta-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meta-rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzMFcmKqbk4f8LkvBy%2Fmeta-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzMFcmKqbk4f8LkvBy%2Fmeta-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 314, "htmlBody": "<p>I've seen there's discussion on LW about rationality, namely, about what it means. I don't think a satisfactory answer can be found without defining what rationality is not. And this seems to be a problem. As far as I know, rationality on LW does not include systematic methods for categorizing and analyzing irrational things. Instead, the discussion seems to draw a circle around rationality. Everyone on LW is excepted to be inside this circle - think of it as a set in a Venn diagram. On the border of the circle there is a sign saying: \"Here be dragons\". And beyond the circle there is irrationality.</p>\n<p>How can we differentiate the irrational from the rational, if we do not know what the irrational is?</p>\n<p>But how can we approach the irrational, if we want to be rational?</p>\n<p>It seems to me there is no way to give a satisfactory account of rationality from within rationality itself. If we presuppose rationality is the only way to attain justification, and then try to find justification for rationalism (the doctrine according to which we should strive for rationality), we are simply making a circular argument. We already presupposed rationalism before trying to find justification for doing so.</p>\n<p>Therefore it seems to me we ought to make a metatheory of rationality in order to find out what is rational and what is irrational. The metatheory itself has to be as rational as possible. That would include having an analytically defined structure, which permits us to at least examine whether the metatheory is logically consistent or inconsistent. This would also allow us to also examine whether the metatheory is mathematically elegant, or whether the same thing could be expressed in a simpler form. The metatheory should also correspond with our actual observations so that we could figure out whether it contradicts empirical findings or not.</p>\n<p>How much interest is there for such a metatheory?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zMFcmKqbk4f8LkvBy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": -25, "extendedScore": null, "score": -5e-05, "legacy": true, "legacyId": "19258", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-10T04:17:43.044Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Economic Definition of Intelligence", "slug": "seq-rerun-economic-definition-of-intelligence", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BZJYoHR8JvHntA6Cu/seq-rerun-economic-definition-of-intelligence", "pageUrlRelative": "/posts/BZJYoHR8JvHntA6Cu/seq-rerun-economic-definition-of-intelligence", "linkUrl": "https://www.lesswrong.com/posts/BZJYoHR8JvHntA6Cu/seq-rerun-economic-definition-of-intelligence", "postedAtFormatted": "Wednesday, October 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Economic%20Definition%20of%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Economic%20Definition%20of%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBZJYoHR8JvHntA6Cu%2Fseq-rerun-economic-definition-of-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Economic%20Definition%20of%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBZJYoHR8JvHntA6Cu%2Fseq-rerun-economic-definition-of-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBZJYoHR8JvHntA6Cu%2Fseq-rerun-economic-definition-of-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p>Today's post, <a href=\"/lw/vc/economic_definition_of_intelligence/\">Economic Definition of Intelligence?</a> was originally published on 29 October 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Economic_Definition_of_Intelligence.3F\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Could economics help provide a definition and a general measure of intelligence?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/eul/seq_rerun_efficient_crossdomain_optimization/\">Efficient Cross-Domain Optimization</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BZJYoHR8JvHntA6Cu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.005785998203068e-06, "legacy": true, "legacyId": "19262", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uPCehCa7Ecidfn2Xe", "4tBerroMGG8xJG7J3", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-10T05:30:21.494Z", "modifiedAt": null, "url": null, "title": "Clarification: Behaviourism & Reinforcement", "slug": "clarification-behaviourism-and-reinforcement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:07.200Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Zaine", "createdAt": "2012-03-03T02:07:29.583Z", "isAdmin": false, "displayName": "Zaine"}, "userId": "KikX5wtGqDKSamkkC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6onoq24Py2rgXmJPE/clarification-behaviourism-and-reinforcement", "pageUrlRelative": "/posts/6onoq24Py2rgXmJPE/clarification-behaviourism-and-reinforcement", "linkUrl": "https://www.lesswrong.com/posts/6onoq24Py2rgXmJPE/clarification-behaviourism-and-reinforcement", "postedAtFormatted": "Wednesday, October 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Clarification%3A%20Behaviourism%20%26%20Reinforcement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AClarification%3A%20Behaviourism%20%26%20Reinforcement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6onoq24Py2rgXmJPE%2Fclarification-behaviourism-and-reinforcement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Clarification%3A%20Behaviourism%20%26%20Reinforcement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6onoq24Py2rgXmJPE%2Fclarification-behaviourism-and-reinforcement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6onoq24Py2rgXmJPE%2Fclarification-behaviourism-and-reinforcement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 601, "htmlBody": "<!-- h1 {text-align:center;} -->\n<p><em>Disclaimer</em>: The following is but a brief clarification on what the human brain does when one's behaviour is reinforced or punished. Thorough, exhaustive, and scholarly it is not.<br /><br /> <strong>Summary</strong>: Punishment, reinforcement, etc. of a behaviour creates an association in the mind of the affected party between the behaviour and the corresponding punishment, reinforcement, etc., the nature of which can only be known by the affected party. Take care when reinforcing or punishing others, as you may be effecting an unwanted association.</p>\n<hr />\n<p>I've noticed the behaviourist concept of reinforcement thrown around a great deal on this site, and am worried a fair number of those who frequent it develop a misconception or are simply ignorant of how reinforcement affects humans' brains, and why it is practically effective. <br /><br /> In the interest of time, I'm not going to go into much detail on classical black-box behaviourism and behavioural neuroscience; Luke already covered the how one can take advantage of <a href=\"/lw/cu2/the_power_of_reinforcement/\">positive reinforcement</a>.  Negative reinforcement and punishment are also important, but won't be covered here. <br /><a id=\"more\"></a></p>\n<h1>\n<div style=\"text-align:center;\">~</div>\n</h1>\n<p>The Couple has a three year old son. They are worried their son ingests too few and poorly varied micronutrients. They want their son to become a world conqueror someday, so a poor diet just won't do. Their son loves <a href=\"http://www.wired.com/geekdad/2012/06/dragonbox/all/\">DragonBox</a>. Every time they all sit down for a meal, their son will barely consume anything, so eager is he to play more DragonBox. <br /><br /> He also very much likes Dragons. <br /><br /> The Couple decides in favor of serving their son a balanced meal consisting solely of micronutrient dense, bioabsorptive foodstuffs. They tell him, \"Spawn, you must fletcherise and swallow all that is on your plate if you wish to play more DragonBox.\" The son understands, and acts accordingly; he eats all that is put before him so he may DragonBox. <br /><br /> What happens in the mind of The Couple's son when he is told thus? His brain creates a new association, or connexion, of concepts[2]; in this case, 'eating all that is put before me' becomes associated with 'more DragonBox!' Perhaps, though, he associates 'more DragonBox!' with a different concept: 'eating green things', say, or 'eating brown things'. <br /><br />In other words, one can never be certain of the precise association another creates when they are reinforced or punished.</p>\n<div style=\"text-align:right;\">[3]</div>\n<p><br /> I think I can make this explanation clearer. <br /><br /> Once a man named Watson wanted to investigate whether he could make a child fear something the child would otherwise not fear. He took a fluffy white rat, put it in front of a baby, Little Albert, and created a loud resounding metallic gong of a noise. After a while, Little Albert came to associate 'loud scary noise' with 'fluffy white thing' - not 'fluffy white rat'. Afterwards, when presented with a fluffy white bunny, dog, and even cotton balls, he displayed a fear response.[1] <br /><br /> Humans are constantly creating associations between anything that can be conceptualised - the color indigo, Herrenhausen, toothpick collecting - anything. When one is forced to link one concept to another, through any means, an association is created; one can never know with certainty the nature of another's association. <br /><br /> Be careful when using reinforcement and punishment on others. Ever be diligent.</p>\n<p>&nbsp;</p>\n<hr />\n<p>[1] This is called a generalised response in psychology; look into fear learning of the amygdala if keen. <br /><br /> [2] <a href=\"/lw/dxm/clarification_behaviourism_reinforcement/7ltc?context=1#7ltc\"><em>Vide</em></a><br /><br /> [3] Richard Kennaway provided a <a href=\"/lw/2ay/rationality_quotes_june_2010/2385\">quote</a> epitomising this concern, and further <a href=\"/lw/2ay/rationality_quotes_june_2010/238q\">elaborated</a> upon it admirably.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2d4": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6onoq24Py2rgXmJPE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 1.0058245491271307e-06, "legacy": true, "legacyId": "18058", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GGn8MBiY8Xz6NdNdH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-10T08:11:30.369Z", "modifiedAt": null, "url": null, "title": "[Link] Think Bayes: Bayesian Statistics Made Simple", "slug": "link-think-bayes-bayesian-statistics-made-simple", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:16.944Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/72pnYRabXvXRoh5zG/link-think-bayes-bayesian-statistics-made-simple", "pageUrlRelative": "/posts/72pnYRabXvXRoh5zG/link-think-bayes-bayesian-statistics-made-simple", "linkUrl": "https://www.lesswrong.com/posts/72pnYRabXvXRoh5zG/link-think-bayes-bayesian-statistics-made-simple", "postedAtFormatted": "Wednesday, October 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Think%20Bayes%3A%20Bayesian%20Statistics%20Made%20Simple&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Think%20Bayes%3A%20Bayesian%20Statistics%20Made%20Simple%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F72pnYRabXvXRoh5zG%2Flink-think-bayes-bayesian-statistics-made-simple%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Think%20Bayes%3A%20Bayesian%20Statistics%20Made%20Simple%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F72pnYRabXvXRoh5zG%2Flink-think-bayes-bayesian-statistics-made-simple", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F72pnYRabXvXRoh5zG%2Flink-think-bayes-bayesian-statistics-made-simple", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/Allen_Downey\">Allen B. Downey</a> has made a rough draft version of his new introduction to Bayesian statistics&nbsp;textbook available for free online. You can download the pdf version <a href=\"http://www.greenteapress.com/thinkbayes/thinkbayes.pdf\">here</a> or read the HTML <a href=\"http://www.greenteapress.com/thinkbayes/html/index.html\">here</a>.</p>\r\n<blockquote>\r\n<p><em>Think Bayes</em> is an introduction to Bayesian statistics using computational methods. This version of the book is a rough draft. I am making this draft available for comments, but it comes with the warning that it is probably full of errors.</p>\r\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "72pnYRabXvXRoh5zG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 1.0059100806365227e-06, "legacy": true, "legacyId": "19278", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-10T16:59:38.114Z", "modifiedAt": null, "url": null, "title": "[Link] \"Fewer than X% of Americans know Y\"", "slug": "link-fewer-than-x-of-americans-know-y", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:06.581Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bdwLHRzxsNkcbFQqX/link-fewer-than-x-of-americans-know-y", "pageUrlRelative": "/posts/bdwLHRzxsNkcbFQqX/link-fewer-than-x-of-americans-know-y", "linkUrl": "https://www.lesswrong.com/posts/bdwLHRzxsNkcbFQqX/link-fewer-than-x-of-americans-know-y", "postedAtFormatted": "Wednesday, October 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20%22Fewer%20than%20X%25%20of%20Americans%20know%20Y%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20%22Fewer%20than%20X%25%20of%20Americans%20know%20Y%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbdwLHRzxsNkcbFQqX%2Flink-fewer-than-x-of-americans-know-y%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20%22Fewer%20than%20X%25%20of%20Americans%20know%20Y%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbdwLHRzxsNkcbFQqX%2Flink-fewer-than-x-of-americans-know-y", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbdwLHRzxsNkcbFQqX%2Flink-fewer-than-x-of-americans-know-y", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 387, "htmlBody": "<p>How many times have you heard a claim from a somewhat reputable source like \"only 28 percent of Americans are able to name one of the constitutional freedoms, yet 52 percent are able to name at least two Simpsons family members\"?</p>\n<p>Mark Liberman over at Language Log wrote up a <a href=\"http://languagelog.ldc.upenn.edu/nll/?p=4243\">post</a> showing how even when such claims are based on actual studies, the methodology is biased to exaggerate ignorance:</p>\n<blockquote>\n<p>The way it works is that the survey designers craft a question like the following (asked at a time when William Rehnquist was the Chief Justice of the United States):</p>\n<p style=\"padding-left: 30px;\">\"Now we have a set of questions concerning various public figures. We want to see how much information about them gets out to the public from television, newspapers and the like&hellip;.<br />What about William Rehnquist &ndash; What job or political office does he NOW hold?\"</p>\n<p>The answers to such open-ended questions are recorded &mdash; as audio recordings and/or as notes taken by the interviewer &mdash; and these records are coded, later on, by hired coders.</p>\n<p>The survey designers give these coders very specific instructions about what counts as right and wrong in the answers. In the case of the question about William Rehnquist, the criteria for an answer to be judged correct were mentions of both \"chief justice\" and \"Supreme Court\". These terms had to be mentioned explicitly, so all of the following (actual answers) were counted as wrong:</p>\n<p>Supreme Court justice. The main one.<br />He&rsquo;s the senior judge on the Supreme Court.<br />He is the Supreme Court justice in charge.<br />He&rsquo;s the head of the Supreme Court.<br />He&rsquo;s top man in the Supreme Court.<br />Supreme Court justice, head.<br />Supreme Court justice. The head guy.<br />Head of Supreme Court.<br />Supreme Court justice head honcho.</p>\n<p>Similarly, the technically correct answer (\"Chief Justice of the United States\") would also have been scored as wrong (I'm not certain whether it actually occurred or not in the survey responses).</p>\n</blockquote>\n<p>If, every time you heard a claim of the form \"Only X% of Americans know Y\" you thought \"there's something strange about that\", then you get 1 rationality point. If you thought \"<a href=\"/lw/ig/i_defy_the_data/\">I don't believe that</a>\", then you get 2 rationality points.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vg4LDxjdwHLotCm8w": 1, "bh7uxTTqmsQ8jZJdB": 1, "3uE2pXvbcnS9nnZRE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bdwLHRzxsNkcbFQqX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 60, "extendedScore": null, "score": 0.000142, "legacy": true, "legacyId": "19280", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 60, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vrHRcEDMjZcx5Yfru"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-10T17:47:51.136Z", "modifiedAt": null, "url": null, "title": "Abandoning Cached Selves to Re-Write My Source Code Partially, I've Become Unstable", "slug": "abandoning-cached-selves-to-re-write-my-source-code", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:55.322Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uEnFDx7nQacQdf6Tg/abandoning-cached-selves-to-re-write-my-source-code", "pageUrlRelative": "/posts/uEnFDx7nQacQdf6Tg/abandoning-cached-selves-to-re-write-my-source-code", "linkUrl": "https://www.lesswrong.com/posts/uEnFDx7nQacQdf6Tg/abandoning-cached-selves-to-re-write-my-source-code", "postedAtFormatted": "Wednesday, October 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Abandoning%20Cached%20Selves%20to%20Re-Write%20My%20Source%20Code%20Partially%2C%20I've%20Become%20Unstable&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAbandoning%20Cached%20Selves%20to%20Re-Write%20My%20Source%20Code%20Partially%2C%20I've%20Become%20Unstable%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuEnFDx7nQacQdf6Tg%2Fabandoning-cached-selves-to-re-write-my-source-code%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Abandoning%20Cached%20Selves%20to%20Re-Write%20My%20Source%20Code%20Partially%2C%20I've%20Become%20Unstable%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuEnFDx7nQacQdf6Tg%2Fabandoning-cached-selves-to-re-write-my-source-code", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuEnFDx7nQacQdf6Tg%2Fabandoning-cached-selves-to-re-write-my-source-code", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 842, "htmlBody": "<p>For very long I've been caring a lot for the preferences of my past selves.&nbsp;</p>\n<p>Rules I established in childhood became sacred, much like laws are (can't <a href=\"/lw/xb/free_to_optimize/\">find</a> post in the sequences in which Yudkowsky is amazed by the fact that some things are good <em><a href=\"/lw/xb/free_to_optimize/\">just because they are old</a></em>), and that caused interesting unusual life choices, such as not wearing formal shoes and suits.&nbsp;</p>\n<p>I was spending more and more time doing what my previous selves thought I should, in a sense, I was composed mostly of something akin to what Anna Salomon and Steve Rayhawk called <a href=\"/lw/4e/cached_selves/\">Cached Selves</a>.</p>\n<p>That meant more dedication to long term issues (Longevity, Cryonics, Immortality). More dedication to spacially vast issues (Singularity, X-risk, Transhumanism). &nbsp;&nbsp;</p>\n<p>Less dedication to the parts of one's self that have a shorter life-span. &nbsp;Such as the instantaneous gratification of philosophical traditions of the east (buddhism, hinduism) and some hedonistic traditions of the west (psychedelism, selfish instantaneous hedonism, sex and masturbation-ism, drugs-isms, thrill-isms).&nbsp;</p>\n<p>Also less dedication to time spans such as three months. Personal projects visible, completable and doable in such scales.&nbsp;</p>\n<p>This process of letting your past decisions trump your current decisions/feelings/emotions/intuitions was very fruitful for me, and for very long I thought (and still think) it made my life greater than the life of most around me (schoolmates, university peers, theater friends etc... not necessarily the people I choose to hang out with, after all, I selected those!).&nbsp;</p>\n<p>At some point more recently, and I'm afraid this might happen to the Effective Altruist community and the immortalist community of Less Wrong, I started feeling overwhelmed, a slave of \"past me\". Even though a lot of \"past me\" orders were along the lines of \"maximize other people's utility, help everyone the most regardless of what those around you are doing\".</p>\n<p>Then the whole edifice crumbled, and I took 2 days off of all of life to go to a hotel in the woods and think/write alone to figure out what my current values are.&nbsp;</p>\n<p>I wrote several pages, thought about a lot of things. More importantly, I quantified the importance I give to different time-spans of my self (say 30 points to life-goals, 16 points to instantaneous gratification, 23 points to 3MonthGoals etc...). I also quantified differently sized circles of altruism/empathy &nbsp;(X points for immediate family, Y points for extended family, Z points for near friends, T points for smart people around the globe, U points for the bottom billion, K points for aliens, A points for animals etc...).&nbsp;</p>\n<p>Knowing my past commitment to past selves, I'd expect these new quantificatonal regulatory forces I had just created to take over me, and cause me to spend my time in proportion to their now known quantities. In other words, I allowed myself a major change, a rewriting which dug deeper into my source code than previous re-writings. And I expected the consequences to be of the same kind than those previous re-writings.&nbsp;</p>\n<p>Seems I was wrong. I've become unstable. Trying to give an outside description the algorithm as it feels from the inside, it seems that the natural order of attention allocation which I had, like a blacksmith, annealed over the years, has crumbled. Instead, I find myself being prone to an evolutionary fight between several distinct desires of internal selves. A mix of George Ainslie's piconomics and plain neural darwinism/multiple drafts.&nbsp;</p>\n<p>Such instability, if not for anything else, for hormonal reasons, is bound not to last long. But thus far it carried me into <a href=\"http://www.thegreatcourses.com/tgc/courses/course_detail.aspx?cid=437\">Existentialism</a> audiobooks, considering <a href=\"http://www.vagabonding.net/\">Vagabonding</a> lifestyle as an alternative to a&nbsp;<a href=\"http://www.leverageresearch.org/\">Utilitarian</a> lifestyle, and considering allowing a personality dissolution into whatever is left of one's personality when we \"allow it\" (emotionally) to dissolve and reforge itself.&nbsp;</p>\n<p>The instability doesn't cause anxiety, sadness, fear or any negative emotion (though I'm at the extreme tail of the happiness setpoint, the equivalent in happiness of having an IQ 145, or three standard deviations). Contrarywise. It is refreshing and gives a sense of freedom and choice.&nbsp;</p>\n<p>This post can be taken to be several distinct things for different readers.&nbsp;</p>\n<p>1) A warning for utilitarian life-style people that allowing deep changes causes an instability which you don't want to let your future self do.&nbsp;</p>\n<p>2) A tale of a self free of past enslavery (if only for a short period of time), who is feeling well and relieved and open to new experiences. That is, a kind of unusual suggestion for unusual people who are in an unusual time of their lives.&nbsp;</p>\n<p>(Note: because of the unusual set-point thing, positive psychology&nbsp;<a href=\"/lw/bq0/be_happier/\">advice</a>&nbsp;should be discarded as a basis for arguments, I've already achieved ~0 marginal returns after 2000pgs of it)</p>\n<p>3) This is the original intention of writing: I wanted to know the arguments in favor of a selfish vagabonding lifestyle, versus the arguments in favor of the Utilitarian lifestyle, because this is a particularly open-minded moment in my life, and I feel less biased than in most other times. For next semester, assume money is not an issue (both Vagabond and Utililtarian are cheap, as opposed to \"you have a million dollars\"). So, what are the arguments you'd use to decide that yourself?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uEnFDx7nQacQdf6Tg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 9, "extendedScore": null, "score": 1.0062160904975496e-06, "legacy": true, "legacyId": "19281", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 102, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EZ8GniEPSechjDYP9", "BHYBdijDcAKQ6e45Z", "JHcTP4Ad8QAmRTCZm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-10T23:45:17.260Z", "modifiedAt": null, "url": null, "title": "[Link] One in five American adults say they are atheist, agnostic or \"nothing in particular\"", "slug": "link-one-in-five-american-adults-say-they-are-atheist", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:24.651Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pablo_Stafforini", "createdAt": "2009-03-24T12:56:00.130Z", "isAdmin": false, "displayName": "Pablo"}, "userId": "W7ETRtvRMqYetyQE9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FriqiS4zQgf8ixAv9/link-one-in-five-american-adults-say-they-are-atheist", "pageUrlRelative": "/posts/FriqiS4zQgf8ixAv9/link-one-in-five-american-adults-say-they-are-atheist", "linkUrl": "https://www.lesswrong.com/posts/FriqiS4zQgf8ixAv9/link-one-in-five-american-adults-say-they-are-atheist", "postedAtFormatted": "Wednesday, October 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20One%20in%20five%20American%20adults%20say%20they%20are%20atheist%2C%20agnostic%20or%20%22nothing%20in%20particular%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20One%20in%20five%20American%20adults%20say%20they%20are%20atheist%2C%20agnostic%20or%20%22nothing%20in%20particular%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFriqiS4zQgf8ixAv9%2Flink-one-in-five-american-adults-say-they-are-atheist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20One%20in%20five%20American%20adults%20say%20they%20are%20atheist%2C%20agnostic%20or%20%22nothing%20in%20particular%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFriqiS4zQgf8ixAv9%2Flink-one-in-five-american-adults-say-they-are-atheist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFriqiS4zQgf8ixAv9%2Flink-one-in-five-american-adults-say-they-are-atheist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 645, "htmlBody": "<p>A new study by the Pew Forum on Religion and Public Life&nbsp;[<a href=\"http://www.pewforum.org/Unaffiliated/nones-on-the-rise.aspx\">executive summary</a>;&nbsp;<a href=\"http://www.pewforum.org/uploadedFiles/Topics/Religious_Affiliation/Unaffiliated/NonesOnTheRise-full.pdf\">full report</a>;&nbsp;<a href=\"http://www.nytimes.com/2012/10/10/us/study-finds-that-the-number-of-protestant-americans-is-declining.html\"><em>New York Times</em> cover story</a>]&nbsp;found that 19.6% of adult Americans answered a question about their current religion by saying they were&nbsp;atheist, agnostic or &ldquo;nothing in particular.&rdquo; &nbsp;In 2007, the figure was 15.3%; in 1972, it was about 7%.</p>\n<p><img src=\"http://images.lesswrong.com/t3_evn_0.png?v=45284c7ed0165a32836de108b83db8a1\" alt=\"Long-Term Trends in Religious Affiliation\" width=\"616\" height=\"477\" /></p>\n<p>Some relevant quotes from the executive summary<em>:</em></p>\n<blockquote>\n<p>However, a new survey by the Pew Research Center&rsquo;s Forum on Religion &amp; Public Life, conducted jointly with the PBS television program Religion &amp; Ethics NewsWeekly, finds that many of the country&rsquo;s 46 million unaffiliated adults are religious or spiritual in some way. Two-thirds of them say they believe in God (68%). More than half say they often feel a deep connection with nature and the earth (58%), while more than a third classify themselves as &ldquo;spiritual&rdquo; but not &ldquo;religious&rdquo; (37%), and one-in-five (21%) say they pray every day. In addition, most religiously unaffiliated Americans think that churches and other religious institutions benefit society by strengthening community bonds and aiding the poor.</p>\n<p>With few exceptions, though, the unaffiliated say they are not looking for a religion that would be right for them. Overwhelmingly, they think that religious organizations are too concerned with money and power, too focused on rules and too involved in politics.</p>\n</blockquote>\n<blockquote>\n<p>The growth in the number of religiously unaffiliated Americans &ndash; sometimes called the rise of the &ldquo;nones&rdquo; &ndash; is largely driven by generational replacement, the gradual supplanting of older generations by newer ones. A third of adults under 30 have no religious affiliation (32%), compared with just one-in-ten who are 65 and older (9%). And young adults today are much more likely to be unaffiliated than previous generations were at a similar stage in their lives.</p>\n</blockquote>\n<blockquote>\n<p>[T]he percentage of Americans who say they never doubt the existence of God has fallen modestly but noticeably over the past 25 years. In 1987, 88% of adults said they never doubt the existence of God. As of 2012, this figure was down 8 percentage points to 80%.</p>\n</blockquote>\n<blockquote>\n<p>The growth of the unaffiliated has taken place across a wide variety of demographic groups. The percentage of unaffiliated respondents has ticked up among men and women, college graduates and those without a college degree, people earning $75,000 or more and those making less than $30,000 annually, and residents of all major regions of the country.</p>\n<p>When it comes to race, however, the recent change has been concentrated in one group: whites. One-fifth of (non-Hispanic) whites now describe themselves as religiously unaffiliated, up five percentage points since 2007. By contrast, the share of blacks and Hispanics who are religiously unaffiliated has not changed by a statistically significant margin in recent years.</p>\n</blockquote>\n<blockquote>\n<p>[T]he ranks of the unaffiliated [are not] predominantly composed of practitioners of New Age spirituality or alternative forms of religion. Generally speaking, the unaffiliated are no more likely than members of the public as a whole to have such beliefs and practices. &nbsp;</p>\n<p>For example, roughly three-in-ten religiously unaffiliated adults say they believe in spiritual energy in physical objects and in yoga as a spiritual practice. About a quarter believe in astrology and reincarnation. In addition, nearly six-in-ten of the religiously unaffiliated say they often feel a deep connection with nature and the earth; about three-in-ten say they have felt in touch with someone who is dead; and 15% have consulted a psychic. All of these figures closely resemble the survey&rsquo;s findings among the public as a whole.</p>\n<p>On the other hand, the religiously unaffiliated are less inclined than Americans overall to say they often think about the meaning and purpose of life (53% vs. 67%). They also attach much less importance to belonging to a community of people with shared values and beliefs; 28% of the unaffiliated say this is very important to them, compared with 49% of all adults.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FriqiS4zQgf8ixAv9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 7, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "19283", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-11T04:16:09.807Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Intelligence in Economics", "slug": "seq-rerun-intelligence-in-economics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:05.938Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fRiKd5jYekRrzDJED/seq-rerun-intelligence-in-economics", "pageUrlRelative": "/posts/fRiKd5jYekRrzDJED/seq-rerun-intelligence-in-economics", "linkUrl": "https://www.lesswrong.com/posts/fRiKd5jYekRrzDJED/seq-rerun-intelligence-in-economics", "postedAtFormatted": "Thursday, October 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Intelligence%20in%20Economics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Intelligence%20in%20Economics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfRiKd5jYekRrzDJED%2Fseq-rerun-intelligence-in-economics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Intelligence%20in%20Economics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfRiKd5jYekRrzDJED%2Fseq-rerun-intelligence-in-economics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfRiKd5jYekRrzDJED%2Fseq-rerun-intelligence-in-economics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p>Today's post, <a href=\"/lw/vd/intelligence_in_economics/\">Intelligence in Economics</a> was originally published on 30 October 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Intelligence_in_Economics\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There are a few connections between economics and intelligence, so economics might have something to contribute to a definition of intelligence.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ev2/seq_rerun_economic_definition_of_intelligence/\">Economic Definition of Intelligence</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fRiKd5jYekRrzDJED", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0065498787003888e-06, "legacy": true, "legacyId": "19290", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4uDfpNTrdhEYEb2jm", "BZJYoHR8JvHntA6Cu", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-11T07:05:17.494Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne social meetup", "slug": "meetup-melbourne-social-meetup-18", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "toner", "createdAt": "2009-02-27T05:14:52.530Z", "isAdmin": false, "displayName": "toner"}, "userId": "XaYyFkpvhiiMscJJZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AsC2RrTJnJwux9sZA/meetup-melbourne-social-meetup-18", "pageUrlRelative": "/posts/AsC2RrTJnJwux9sZA/meetup-melbourne-social-meetup-18", "linkUrl": "https://www.lesswrong.com/posts/AsC2RrTJnJwux9sZA/meetup-melbourne-social-meetup-18", "postedAtFormatted": "Thursday, October 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAsC2RrTJnJwux9sZA%2Fmeetup-melbourne-social-meetup-18%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAsC2RrTJnJwux9sZA%2Fmeetup-melbourne-social-meetup-18", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAsC2RrTJnJwux9sZA%2Fmeetup-melbourne-social-meetup-18", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/er'>Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 October 2012 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">see mailing list, Carlton VIC 3053 Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next social meetup is on Friday 19 October, 6:30 for 7pm, at my house. If you have any trouble working out the location or getting in, you can call me on 0412 996 288.</p>\n\n<p>We'll get some snacks and organise some form of take-away for dinner. BYO drinks and games.</p>\n\n<p>We always look forward to meeting new people!</p>\n\n<p>This meetup is usually on the third Friday of the month. We also have a regular meetup about practical rationality on the first Friday of each month.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/er'>Melbourne social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AsC2RrTJnJwux9sZA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.0066397608114026e-06, "legacy": true, "legacyId": "19299", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/er\">Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 October 2012 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">see mailing list, Carlton VIC 3053 Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next social meetup is on Friday 19 October, 6:30 for 7pm, at my house. If you have any trouble working out the location or getting in, you can call me on 0412 996 288.</p>\n\n<p>We'll get some snacks and organise some form of take-away for dinner. BYO drinks and games.</p>\n\n<p>We always look forward to meeting new people!</p>\n\n<p>This meetup is usually on the third Friday of the month. We also have a regular meetup about practical rationality on the first Friday of each month.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/er\">Melbourne social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-11T08:03:14.779Z", "modifiedAt": null, "url": null, "title": "[Link] Learning New Languages Helps The Brain Grow", "slug": "link-learning-new-languages-helps-the-brain-grow", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:06.876Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EYyBqsMQJgdQS8K55/link-learning-new-languages-helps-the-brain-grow", "pageUrlRelative": "/posts/EYyBqsMQJgdQS8K55/link-learning-new-languages-helps-the-brain-grow", "linkUrl": "https://www.lesswrong.com/posts/EYyBqsMQJgdQS8K55/link-learning-new-languages-helps-the-brain-grow", "postedAtFormatted": "Thursday, October 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Learning%20New%20Languages%20Helps%20The%20Brain%20Grow&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Learning%20New%20Languages%20Helps%20The%20Brain%20Grow%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEYyBqsMQJgdQS8K55%2Flink-learning-new-languages-helps-the-brain-grow%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Learning%20New%20Languages%20Helps%20The%20Brain%20Grow%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEYyBqsMQJgdQS8K55%2Flink-learning-new-languages-helps-the-brain-grow", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEYyBqsMQJgdQS8K55%2Flink-learning-new-languages-helps-the-brain-grow", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 139, "htmlBody": "<p><a title=\"Article link\" href=\"http://www.lunduniversity.lu.se/o.o.i.s?news_item=5928&amp;id=24890\">http://www.lunduniversity.lu.se/o.o.i.s?news_item=5928&amp;id=24890</a></p>\n<p>According to Johan M&aring;rtensson from  Lund University, if you are learning new language quickly, it helps your brain to become bigger and increase its activity:</p>\n<blockquote>\n<p>This finding came from scientists at Lund University, after examining  young recruits with a talent for acquiring languages who were able to  speak in Arabic, Russian, or Dari fluently after just 13 months of  learning, before which they had no knowledge of the languages.</p>\n<p>After analyzing the results, the scientists saw no difference in the brain structure of the control group.  However, in  the language group, certain parts of the brain had grown, including the  hippocampus, responsible for learning new information, and three areas  in the cerebral cortex.</p>\n</blockquote>\n<p>And there is more:</p>\n<blockquote>\n<p>One particular study from 2011 provided evidence that <a href=\"http://www.medicalnewstoday.com/articles/216955.php\">Alzheimer's was delayed 5 years for bilingual patients</a>, compared to monolingual patients.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EYyBqsMQJgdQS8K55", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 3, "extendedScore": null, "score": 1.0066705637972175e-06, "legacy": true, "legacyId": "19300", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-11T17:05:22.204Z", "modifiedAt": null, "url": null, "title": "[Link] Are Children Natural Bayesians?", "slug": "link-are-children-natural-bayesians", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:06.368Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JQuinton", "createdAt": "2011-04-29T16:29:16.319Z", "isAdmin": false, "displayName": "JQuinton"}, "userId": "edHNo35soNo2w6ZwN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xxb6nxLcjbjrPtHda/link-are-children-natural-bayesians", "pageUrlRelative": "/posts/xxb6nxLcjbjrPtHda/link-are-children-natural-bayesians", "linkUrl": "https://www.lesswrong.com/posts/xxb6nxLcjbjrPtHda/link-are-children-natural-bayesians", "postedAtFormatted": "Thursday, October 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Are%20Children%20Natural%20Bayesians%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Are%20Children%20Natural%20Bayesians%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxxb6nxLcjbjrPtHda%2Flink-are-children-natural-bayesians%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Are%20Children%20Natural%20Bayesians%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxxb6nxLcjbjrPtHda%2Flink-are-children-natural-bayesians", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxxb6nxLcjbjrPtHda%2Flink-are-children-natural-bayesians", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 345, "htmlBody": "<p>This recent article at <em>Slate</em> thinks so:</p>\r\n<p><a href=\"http://www.slate.com/articles/health_and_science/science/2012/10/how_do_children_learn_so_quickly_bayesian_statistics_and_probabilities_help.single.html\">Why Your 4-Year-Old Is As Smart as Nate Silver</a></p>\r\n<div class=\"text parbase section\">\r\n<blockquote>\r\n<p>It turns out that even very young children reason [using Bayes Theorem]. <a href=\"http://alisongopnik.com/papers_Alison/Kushnir%20DevPsych.pdf\" target=\"_blank\">For example, my student Tamar Kushnir, now at Cornell, and I showed 4-year-olds a toy and told them that blocks made it light up.</a> Then we gave the kids a block and asked them how to make the toy light up. Almost all the children said you should put the block on the toy&mdash;they thought, sensibly, that touching the toy with the block was very likely to make it light up. That hypothesis had a high &ldquo;prior.&rdquo;</p>\r\n<div class=\"text parbase section\">\r\n<p>Then we showed 4-year-olds that when you put a block right on the toy it did indeed make it light up, but it did so only two out of six times. But when you waved a block over the top of the toy, it lit up two out of three times. Then we just asked the kids to make the toy light up.</p>\r\n</div>\r\n<div class=\"text parbase section\">\r\n<p>The children adjusted their hypotheses appropriately when they saw the statistical data, just like good Bayesians&mdash;they were now more likely to wave the block over the toy, and you could precisely predict how often they did so. What&rsquo;s more, even though both blocks made the machine light up twice, the 4-year-olds, only just learning to add, could unconsciously calculate that two out of three is more probable than two out of six. (<a href=\"http://alisongopnik.com/Papers_Alison/Probabilistic%20Causal%20Reasoning%20in%20Young%20Children.docx\" target=\"_blank\">In a current study, my colleagues and I have found that even 24-month-olds can do the same)</a>.</p>\r\n</div>\r\n</blockquote>\r\n</div>\r\n<div class=\"text parbase section\">\r\n<p>&nbsp;There also seems to be a reference to the Singularity Institute:</p>\r\n<blockquote>\r\n<p>The Bayesian idea is simple, but it turns out to be very powerful. It&rsquo;s so powerful, in fact, that computer scientists are using it to design intelligent learning machines, and more and more psychologists think that it might explain human intelligence.</p>\r\n</blockquote>\r\n<p>(Of course, I don't know how many other AI researchers are using Bayes Theorem, so the author also might not have the SI in mind)</p>\r\n<p>If children really are natural Bayesians, then why and how do you&nbsp;<a href=\"/lw/ul/my_bayesian_enlightenment/\">think we&nbsp;change</a>?</p>\r\n</div>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xxb6nxLcjbjrPtHda", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -7, "extendedScore": null, "score": 1.0069587839189021e-06, "legacy": true, "legacyId": "19301", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ti3Z7eZtud32LhGZT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-11T17:32:04.002Z", "modifiedAt": null, "url": null, "title": "Meetup : Bratislava meetup", "slug": "meetup-bratislava-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:27.773Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BarbaraB", "createdAt": "2012-04-04T15:11:52.435Z", "isAdmin": false, "displayName": "BarbaraB"}, "userId": "aHcDnFsxTnRpMEuXD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EXB6H8sTty4Wo8Szh/meetup-bratislava-meetup", "pageUrlRelative": "/posts/EXB6H8sTty4Wo8Szh/meetup-bratislava-meetup", "linkUrl": "https://www.lesswrong.com/posts/EXB6H8sTty4Wo8Szh/meetup-bratislava-meetup", "postedAtFormatted": "Thursday, October 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bratislava%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bratislava%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEXB6H8sTty4Wo8Szh%2Fmeetup-bratislava-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bratislava%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEXB6H8sTty4Wo8Szh%2Fmeetup-bratislava-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEXB6H8sTty4Wo8Szh%2Fmeetup-bratislava-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 238, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/es\">Bratislava meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">20 October 2012 05:30:00PM (+0200)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Venturska 9</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Hi everybody,</p>\n<p>the next LessWrong meetup will be in \"The tea room under-ground\" (Cajovna v podzemi) at Venturska 9, Bratislava, in the city center. For questions, contact me at barbarabrezna@gmail.com.</p>\n<p>Directions for people coming from Vienna: I recommend the connection by Blaguss bus from the busstop \"Wien,Erdbergstrasse 202\" departing at 16:10. The bus is directed towards Bratislava airport, but take off one stop earlier at 17:10. The name of the bus-stop is \"Bratislava, Nov&yacute; most\", also called \"Most SNP\" , translated as New Bridge or Bridge of Slovak National Uprising. (Why the hell did I choose the bus-stop, which has two synonymous names ???) Walk from there according to the map where \"Most SNP\" is marked with an arrow.</p>\n<p>Other options: The transportation ticket for 15 minutes (price 0,70 &euro;) should be enough from either train station or main bus station of the city. From the Main bus station \"Autobusova stanica Mlynske Nivy\", you can use the bus 70 to \"Most SNP\". From the Main train station \"Hlavna stanica\", take the bus 93 to the stop \"Zochova\". From the train station \"Petrzalka\", take the bus 93 or 94 to the stop \"Zochova\", or bus 91 or 191 to \"Most SNP\". \"Most SNP\", \"Zochova\" and tea-house are marked with an arrow on the map.</p>\n<p>See You there !</p>\n<p>Barbara</p>\n<p><img src=\"http://bur.sk/share/meetup201210.small.jpeg\" alt=\"\" /></p>\n<p>Click <a rel=\"nofollow\" href=\"http://bur.sk/share/meetup201210.jpeg\">here</a> for larger version.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/es\">Bratislava meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EXB6H8sTty4Wo8Szh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.0069729809315519e-06, "legacy": true, "legacyId": "19302", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bratislava_meetup\">Discussion article for the meetup : <a href=\"/meetups/es\">Bratislava meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">20 October 2012 05:30:00PM (+0200)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Venturska 9</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Hi everybody,</p>\n<p>the next LessWrong meetup will be in \"The tea room under-ground\" (Cajovna v podzemi) at Venturska 9, Bratislava, in the city center. For questions, contact me at barbarabrezna@gmail.com.</p>\n<p>Directions for people coming from Vienna: I recommend the connection by Blaguss bus from the busstop \"Wien,Erdbergstrasse 202\" departing at 16:10. The bus is directed towards Bratislava airport, but take off one stop earlier at 17:10. The name of the bus-stop is \"Bratislava, Nov\u00fd most\", also called \"Most SNP\" , translated as New Bridge or Bridge of Slovak National Uprising. (Why the hell did I choose the bus-stop, which has two synonymous names ???) Walk from there according to the map where \"Most SNP\" is marked with an arrow.</p>\n<p>Other options: The transportation ticket for 15 minutes (price 0,70 \u20ac) should be enough from either train station or main bus station of the city. From the Main bus station \"Autobusova stanica Mlynske Nivy\", you can use the bus 70 to \"Most SNP\". From the Main train station \"Hlavna stanica\", take the bus 93 to the stop \"Zochova\". From the train station \"Petrzalka\", take the bus 93 or 94 to the stop \"Zochova\", or bus 91 or 191 to \"Most SNP\". \"Most SNP\", \"Zochova\" and tea-house are marked with an arrow on the map.</p>\n<p>See You there !</p>\n<p>Barbara</p>\n<p><img src=\"http://bur.sk/share/meetup201210.small.jpeg\" alt=\"\"></p>\n<p>Click <a rel=\"nofollow\" href=\"http://bur.sk/share/meetup201210.jpeg\">here</a> for larger version.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Bratislava_meetup1\">Discussion article for the meetup : <a href=\"/meetups/es\">Bratislava meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bratislava meetup", "anchor": "Discussion_article_for_the_meetup___Bratislava_meetup", "level": 1}, {"title": "Discussion article for the meetup : Bratislava meetup", "anchor": "Discussion_article_for_the_meetup___Bratislava_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-12T02:05:50.537Z", "modifiedAt": null, "url": null, "title": "[Link] Free TEDYouth Event in NYC for High School Students", "slug": "link-free-tedyouth-event-in-nyc-for-high-school-students", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "nLPjP3PWrHDRy89ys", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wrjhwYFnAiAGguakv/link-free-tedyouth-event-in-nyc-for-high-school-students", "pageUrlRelative": "/posts/wrjhwYFnAiAGguakv/link-free-tedyouth-event-in-nyc-for-high-school-students", "linkUrl": "https://www.lesswrong.com/posts/wrjhwYFnAiAGguakv/link-free-tedyouth-event-in-nyc-for-high-school-students", "postedAtFormatted": "Friday, October 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Free%20TEDYouth%20Event%20in%20NYC%20for%20High%20School%20Students&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Free%20TEDYouth%20Event%20in%20NYC%20for%20High%20School%20Students%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwrjhwYFnAiAGguakv%2Flink-free-tedyouth-event-in-nyc-for-high-school-students%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Free%20TEDYouth%20Event%20in%20NYC%20for%20High%20School%20Students%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwrjhwYFnAiAGguakv%2Flink-free-tedyouth-event-in-nyc-for-high-school-students", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwrjhwYFnAiAGguakv%2Flink-free-tedyouth-event-in-nyc-for-high-school-students", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 213, "htmlBody": "<p>Taken from the <a href=\"http://www.ted.com/pages/tedyouth\">TEDYouth event description page</a>:</p>\n<blockquote>\n<div class=\"box intro\" style=\"margin: 0px 0px 15px; padding: 0px; border: 0px; outline: 0px; font-size: 12px; vertical-align: baseline; color: #3f3f3f; font-family: helvetica, arial, sans-serif; line-height: 15px;\">\n<p class=\"MsoNormal\" style=\"margin-bottom: 8.2pt; line-height: normal; vertical-align: baseline; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-size: 10pt; font-family: Arial, sans-serif;\">Held annually in New York City, TEDYouth is a day-long event for high school students that includes live speakers, hands-on activities, demonstrations and an opportunity for the youth attendees and speakers to connect. <span id=\"GRmark_40f6d386bb5ea3e77164f16debf50f52c91e656d_TEDYouth:0\" class=\"GRcorrect\">TEDYouth</span> coincides with more than 100 self-organized <a href=\"http://www.tedxyouthday.com/\"><span style=\"color: #ff2b06; text-decoration: none; text-underline: none;\">TEDxYouthDay</span></a>&nbsp;events happening worldwide over a 48-hour period.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt; line-height: normal;\"><span style=\"font-size: 10.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; mso-fareast-font-family: &quot;Times New Roman&quot;;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal; vertical-align: baseline; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-size: 10.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; mso-fareast-font-family: &quot;Times New Roman&quot;; color: #3f3f3f;\">This year&rsquo;s TEDYouth conference will be held on&nbsp;<strong>Saturday, November 17th, 2012, at the Times Center in Manhattan, from 1pm-6pm</strong>.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 12pt; line-height: normal; vertical-align: baseline; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-size: 10.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; mso-fareast-font-family: &quot;Times New Roman&quot;; color: #3f3f3f;\">More than 20 scientists, designers, technologists, explorers, artists, performers (and more!) will share short lessons on what they do best. They&rsquo;ll dazzle us with mind-shifting stories, inspire us with creativity and make us want to dive even deeper into this broad array of topics.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 12pt; line-height: normal; vertical-align: baseline; background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-size: 10.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; mso-fareast-font-family: &quot;Times New Roman&quot;; color: #3f3f3f;\">The program will be made up of two sessions and a break with engaging activities, demonstrations and even a chance to meet the speakers. Attendance is free of charge for 400 high school students from within the New York City area.</span></p>\n</div>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; padding: 0px; border: 0px; outline: 0px; font-size: 12px; vertical-align: baseline; line-height: 1.35;\">Students must apply by the 15th of October. I personally attended last year's TEDYouth conference and enjoyed it. One of my favorite things about it is that all the attendees were able to personally talk to all the speakers afterwards, including Adam Savage from MythBusters and <a href=\"http://www.thesciencebabe.com/\">The Science Babe</a>, Dr. Deborah Berebichez .&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wrjhwYFnAiAGguakv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "19303", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-12T02:11:13.686Z", "modifiedAt": null, "url": null, "title": "The Fabric of Real Things", "slug": "the-fabric-of-real-things", "viewCount": null, "lastCommentedAt": "2020-05-10T23:35:50.557Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h6fzC6wFYFxxKDm8u/the-fabric-of-real-things", "pageUrlRelative": "/posts/h6fzC6wFYFxxKDm8u/the-fabric-of-real-things", "linkUrl": "https://www.lesswrong.com/posts/h6fzC6wFYFxxKDm8u/the-fabric-of-real-things", "postedAtFormatted": "Friday, October 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Fabric%20of%20Real%20Things&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Fabric%20of%20Real%20Things%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh6fzC6wFYFxxKDm8u%2Fthe-fabric-of-real-things%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Fabric%20of%20Real%20Things%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh6fzC6wFYFxxKDm8u%2Fthe-fabric-of-real-things", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh6fzC6wFYFxxKDm8u%2Fthe-fabric-of-real-things", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1111, "htmlBody": "<p><strong>Followup to</strong>:&nbsp;<a href=\"/lw/eqn/the_useful_idea_of_truth/\">The Useful Concept of Truth</a></p>\n<p>We previously <a href=\"/lw/eqn/the_useful_idea_of_truth/7jlu\">asked</a>:</p>\n<p><em>What rule would restrict our beliefs to just statements that can be meaningful, without excluding a priori anything that could in principle be true?</em></p>\n<p>It doesn't work to require that the belief's truth or falsity make a sensory difference. It's true, but not testable, to say that a spaceship going over the cosmological horizon of an expanding universe does not suddenly blink out of existence. It's&nbsp;<em>meaningful and false</em>, rather than&nbsp;<em>meaningless</em>, to say that on March 22nd, 2003, the particles in the center of the Sun spontaneously arranged themselves into a short-lived chocolate cake. This statement's truth or falsity has no consequences we'll ever be able to test experientally. &nbsp;Nonetheless, it legitimately describes a way reality could be, but isn't; the atoms in our universe&nbsp;<em>could've</em>&nbsp;been arranged like that on March 22nd 2003, but they weren't.</p>\n<p>You can't say that there has to be some way to arrange the atoms in the universe so as to make the claim true or alternatively false. Then the theory of quantum mechanics is&nbsp;<em>a priori</em>&nbsp;meaningless, because there's no way to arrange&nbsp;<em>atoms</em>&nbsp;to make it true. And if you try to substitute quantum fields instead, well, what if they discover something else tomorrow? And is it&nbsp;<em>meaningless -</em>rather than&nbsp;<em>meaningful and false</em>&nbsp;- to imagine that physicists are lying about quantum mechanics in a grand organized conspiracy?</p>\n<p>Since claims are rendered true or false by how-the-universe-is, the question \"What claims can be meaningful?\" implies the question \"What sort of reality can exist for our statements to correspond to?\"</p>\n<p>If you rephrase it this way, the question probably sounds completely fruitless and pointless, the sort of thing that a philosopher would ponder for years before producing a long, incomprehensible book that would be studied by future generations of unhappy students while being of no conceivable interest to anyone with a real job.</p>\n<p>But while deep philosophical dilemmas such as these are never settled by philosophers, they&nbsp;<em>are</em>&nbsp;sometimes settled by people working on a related practical problem which happens to intersect the dilemma. There are a lot of people who think I'm being too harsh on philosophers when I express skepticism about mainstream philosophy; but in this case, at least, history clearly bears out the point. Philosophers have been discussing the nature of reality for literal millennia... and yet the people who first delineated and formalized a critical hint about the nature of reality, the people who first discovered&nbsp;<em>what sort of things seem to be real,</em>were trying to solve a completely different-sounding question.</p>\n<p>They were trying to figure out whether you can tell the direction of&nbsp;<em>cause and effect</em>&nbsp;from survey data.</p>\n<hr />\n<p><em>Please now read&nbsp;</em><em><a href=\"/lw/ev3/causal_diagrams_and_causal_models/\"><strong>Causal Diagrams and Causal Models</strong></a></em><em>, which was modularized out so that it could act as a standalone introduction. This post involves some simple math, but causality is so basic to key future posts that it's pretty important to get at least some grasp on the math involved. Once you are finished reading, continue with the rest of this post.<a id=\"more\"></a></em></p>\n<hr />\n<p>Okay, now suppose someone were to claim the following:</p>\n<p>\"A universe is a connected fabric of causes and effects.\"</p>\n<table border=\"0\" cellpadding=\"5\">\n<tbody>\n<tr valign=\"center\">\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/d/d4/Connected.jpg\" alt=\"\" width=\"275\" height=\"181\" /></td>\n<td>\n<h1>vs.</h1>\n</td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/5/58/NotConnected2.jpg\" alt=\"\" width=\"300\" height=\"137\" /></td>\n</tr>\n</tbody>\n</table>\n<p>(In the right-hand image we see a connected causal fabric; the sun raises the temperature, makes plants grow, and sends light into the eyes of the person eating from the plant. &nbsp;On the other hand, while \"post-utopian\" is linked to \"colonial alienation\" and vice versa, these two elements don't connect to the rest of the causal fabric - so that must not be a universe.)</p>\n<p>This same someone might further claim:</p>\n<p>\"For a statement to be comparable to your universe, so that it can be true or alternatively false, it must talk about stuff you can <em>find in relation to yourself</em> by tracing out causal links.\"</p>\n<p>To clarify the second claim, the idea here is that reference can trace causal links <em>forwards or backwards</em>. If a spaceship goes over the cosmological horizon, it may not cause anything&nbsp;<em>else</em>&nbsp;to happen to you after that. &nbsp;But you could still say, 'I saw the space shipyard - it affected my eyes - and the shipyard building was the cause of that ship existing and going over the horizon.' &nbsp;You know the second causal link exists, because you've previously observed the general law implementing links of that type - previously observed that objects continue to exist and do not violate Conservation of Energy by spontaneously vanishing.</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/8/81/Ship.svg\" alt=\"\" width=\"320\" height=\"192\" /></p>\n<p>And now I present three meditations, whose answers (or at least, what I think are the answers) will appear at later points in Highly Advanced Epistemology 101 For Beginners. &nbsp;Please take a shot at whispering the answers to yourself; or if you're bold enough to go on record, comments for collecting posted answers are linked.</p>\n<hr />\n<p><strong><a href=\"#7lqh\">Meditation 1:</a></strong></p>\n<p>\"You say that a&nbsp;<em>universe</em>&nbsp;is a connected fabric of causes and effects. Well, that's a very Western viewpoint - that it's all about mechanistic, deterministic stuff. I agree that anything else is outside the realm of science, but it can still be&nbsp;<em>real,</em>&nbsp;you know. My cousin is psychic - if you draw a card from his deck of cards, he can tell you the name of your card before he looks at it. There's no&nbsp;<em>mechanism</em>&nbsp;for it - it's not a&nbsp;<em>causal&nbsp;</em>thing that scientists could study - he just&nbsp;<em>does</em>&nbsp;it. Same thing when I commune on a deep level with the entire universe in order to realize that my partner truly loves me. I agree that purely spiritual phenomena are outside the realm of causal processes, which can be scientifically understood, but I don't agree that they can't be&nbsp;<em>real.</em>\"</p>\n<p>How would you reply?</p>\n<hr />\n<p><strong><a href=\"#7lqi\">Meditation 2:</a></strong></p>\n<p>\"Does your rule there forbid&nbsp;<a href=\"/lw/p7/zombies_zombies/\">epiphenomenalist theories of consciousness</a>&nbsp;- that consciousness is caused by neurons, but doesn't affect those neurons in turn? The classic argument for epiphenomenal consciousness has always been that we can imagine a universe in which all the atoms are in the same place and people behave exactly the same way, but there's nobody home - no awareness, no consciousness, inside the brain. The usual&nbsp;<em>effect</em>&nbsp;of the brain generating consciousness is missing, but consciousness doesn't cause anything else in turn - it's just a passive awareness - and so from the outside the universe looks the same. Now, I'm not so much interested in whether you think epiphenomenal theories of consciousness are true or false - rather, I want to know if you think they're impossible or meaningless&nbsp;<em>a priori</em>&nbsp;based on your rules.\"</p>\n<p>How would you reply?</p>\n<hr />\n<p><strong><a href=\"#7lqj\">Meditation 3</a></strong>:</p>\n<p>Does the idea that everything is made of causes and effects meaningfully constrain experience? Can you coherently say how reality might look, if our universe did <em>not </em>have the kind of structure that appears in a causal model?</p>\n<hr />\n<p><strong><a href=\"https://www.lesswrong.com/posts/h6fzC6wFYFxxKDm8u/the-fabric-of-real-things#Ffxk8hPC5CGsLmc5Q\">Mainstream status.</a></strong></p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/ev3/causal_diagrams_and_causal_models/\">Causal Diagrams and Causal Models</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/etf/firewalling_the_optimal_from_the_rational/\">Firewalling the Optimal from the Rational</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LnEEs8xGooYmQ8iLA": 1, "FtT2T9bRbECCGYxrL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h6fzC6wFYFxxKDm8u", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 34, "extendedScore": null, "score": 8.5e-05, "legacy": true, "legacyId": "19270", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "causal-diagrams-and-causal-models", "canonicalPrevPostSlug": "firewalling-the-optimal-from-the-rational", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Followup to</strong>:&nbsp;<a href=\"/lw/eqn/the_useful_idea_of_truth/\">The Useful Concept of Truth</a></p>\n<p>We previously <a href=\"/lw/eqn/the_useful_idea_of_truth/7jlu\">asked</a>:</p>\n<p><em>What rule would restrict our beliefs to just statements that can be meaningful, without excluding a priori anything that could in principle be true?</em></p>\n<p>It doesn't work to require that the belief's truth or falsity make a sensory difference. It's true, but not testable, to say that a spaceship going over the cosmological horizon of an expanding universe does not suddenly blink out of existence. It's&nbsp;<em>meaningful and false</em>, rather than&nbsp;<em>meaningless</em>, to say that on March 22nd, 2003, the particles in the center of the Sun spontaneously arranged themselves into a short-lived chocolate cake. This statement's truth or falsity has no consequences we'll ever be able to test experientally. &nbsp;Nonetheless, it legitimately describes a way reality could be, but isn't; the atoms in our universe&nbsp;<em>could've</em>&nbsp;been arranged like that on March 22nd 2003, but they weren't.</p>\n<p>You can't say that there has to be some way to arrange the atoms in the universe so as to make the claim true or alternatively false. Then the theory of quantum mechanics is&nbsp;<em>a priori</em>&nbsp;meaningless, because there's no way to arrange&nbsp;<em>atoms</em>&nbsp;to make it true. And if you try to substitute quantum fields instead, well, what if they discover something else tomorrow? And is it&nbsp;<em>meaningless -</em>rather than&nbsp;<em>meaningful and false</em>&nbsp;- to imagine that physicists are lying about quantum mechanics in a grand organized conspiracy?</p>\n<p>Since claims are rendered true or false by how-the-universe-is, the question \"What claims can be meaningful?\" implies the question \"What sort of reality can exist for our statements to correspond to?\"</p>\n<p>If you rephrase it this way, the question probably sounds completely fruitless and pointless, the sort of thing that a philosopher would ponder for years before producing a long, incomprehensible book that would be studied by future generations of unhappy students while being of no conceivable interest to anyone with a real job.</p>\n<p>But while deep philosophical dilemmas such as these are never settled by philosophers, they&nbsp;<em>are</em>&nbsp;sometimes settled by people working on a related practical problem which happens to intersect the dilemma. There are a lot of people who think I'm being too harsh on philosophers when I express skepticism about mainstream philosophy; but in this case, at least, history clearly bears out the point. Philosophers have been discussing the nature of reality for literal millennia... and yet the people who first delineated and formalized a critical hint about the nature of reality, the people who first discovered&nbsp;<em>what sort of things seem to be real,</em>were trying to solve a completely different-sounding question.</p>\n<p>They were trying to figure out whether you can tell the direction of&nbsp;<em>cause and effect</em>&nbsp;from survey data.</p>\n<hr>\n<p><em>Please now read&nbsp;</em><em><a href=\"/lw/ev3/causal_diagrams_and_causal_models/\"><strong>Causal Diagrams and Causal Models</strong></a></em><em>, which was modularized out so that it could act as a standalone introduction. This post involves some simple math, but causality is so basic to key future posts that it's pretty important to get at least some grasp on the math involved. Once you are finished reading, continue with the rest of this post.<a id=\"more\"></a></em></p>\n<hr>\n<p>Okay, now suppose someone were to claim the following:</p>\n<p>\"A universe is a connected fabric of causes and effects.\"</p>\n<table border=\"0\" cellpadding=\"5\">\n<tbody>\n<tr valign=\"center\">\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/d/d4/Connected.jpg\" alt=\"\" width=\"275\" height=\"181\"></td>\n<td>\n<h1 id=\"vs_\">vs.</h1>\n</td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/5/58/NotConnected2.jpg\" alt=\"\" width=\"300\" height=\"137\"></td>\n</tr>\n</tbody>\n</table>\n<p>(In the right-hand image we see a connected causal fabric; the sun raises the temperature, makes plants grow, and sends light into the eyes of the person eating from the plant. &nbsp;On the other hand, while \"post-utopian\" is linked to \"colonial alienation\" and vice versa, these two elements don't connect to the rest of the causal fabric - so that must not be a universe.)</p>\n<p>This same someone might further claim:</p>\n<p>\"For a statement to be comparable to your universe, so that it can be true or alternatively false, it must talk about stuff you can <em>find in relation to yourself</em> by tracing out causal links.\"</p>\n<p>To clarify the second claim, the idea here is that reference can trace causal links <em>forwards or backwards</em>. If a spaceship goes over the cosmological horizon, it may not cause anything&nbsp;<em>else</em>&nbsp;to happen to you after that. &nbsp;But you could still say, 'I saw the space shipyard - it affected my eyes - and the shipyard building was the cause of that ship existing and going over the horizon.' &nbsp;You know the second causal link exists, because you've previously observed the general law implementing links of that type - previously observed that objects continue to exist and do not violate Conservation of Energy by spontaneously vanishing.</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/8/81/Ship.svg\" alt=\"\" width=\"320\" height=\"192\"></p>\n<p>And now I present three meditations, whose answers (or at least, what I think are the answers) will appear at later points in Highly Advanced Epistemology 101 For Beginners. &nbsp;Please take a shot at whispering the answers to yourself; or if you're bold enough to go on record, comments for collecting posted answers are linked.</p>\n<hr>\n<p><strong id=\"Meditation_1_\"><a href=\"#7lqh\">Meditation 1:</a></strong></p>\n<p>\"You say that a&nbsp;<em>universe</em>&nbsp;is a connected fabric of causes and effects. Well, that's a very Western viewpoint - that it's all about mechanistic, deterministic stuff. I agree that anything else is outside the realm of science, but it can still be&nbsp;<em>real,</em>&nbsp;you know. My cousin is psychic - if you draw a card from his deck of cards, he can tell you the name of your card before he looks at it. There's no&nbsp;<em>mechanism</em>&nbsp;for it - it's not a&nbsp;<em>causal&nbsp;</em>thing that scientists could study - he just&nbsp;<em>does</em>&nbsp;it. Same thing when I commune on a deep level with the entire universe in order to realize that my partner truly loves me. I agree that purely spiritual phenomena are outside the realm of causal processes, which can be scientifically understood, but I don't agree that they can't be&nbsp;<em>real.</em>\"</p>\n<p>How would you reply?</p>\n<hr>\n<p><strong id=\"Meditation_2_\"><a href=\"#7lqi\">Meditation 2:</a></strong></p>\n<p>\"Does your rule there forbid&nbsp;<a href=\"/lw/p7/zombies_zombies/\">epiphenomenalist theories of consciousness</a>&nbsp;- that consciousness is caused by neurons, but doesn't affect those neurons in turn? The classic argument for epiphenomenal consciousness has always been that we can imagine a universe in which all the atoms are in the same place and people behave exactly the same way, but there's nobody home - no awareness, no consciousness, inside the brain. The usual&nbsp;<em>effect</em>&nbsp;of the brain generating consciousness is missing, but consciousness doesn't cause anything else in turn - it's just a passive awareness - and so from the outside the universe looks the same. Now, I'm not so much interested in whether you think epiphenomenal theories of consciousness are true or false - rather, I want to know if you think they're impossible or meaningless&nbsp;<em>a priori</em>&nbsp;based on your rules.\"</p>\n<p>How would you reply?</p>\n<hr>\n<p><strong><a href=\"#7lqj\">Meditation 3</a></strong>:</p>\n<p>Does the idea that everything is made of causes and effects meaningfully constrain experience? Can you coherently say how reality might look, if our universe did <em>not </em>have the kind of structure that appears in a causal model?</p>\n<hr>\n<p><strong id=\"Mainstream_status_\"><a href=\"https://www.lesswrong.com/posts/h6fzC6wFYFxxKDm8u/the-fabric-of-real-things#Ffxk8hPC5CGsLmc5Q\">Mainstream status.</a></strong></p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/ev3/causal_diagrams_and_causal_models/\">Causal Diagrams and Causal Models</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/etf/firewalling_the_optimal_from_the_rational/\">Firewalling the Optimal from the Rational</a>\"</p>", "sections": [{"title": "vs.", "anchor": "vs_", "level": 1}, {"title": "Meditation 1:", "anchor": "Meditation_1_", "level": 2}, {"title": "Meditation 2:", "anchor": "Meditation_2_", "level": 2}, {"title": "Mainstream status.", "anchor": "Mainstream_status_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "308 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 309, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["XqvnWFtRD2keJdwjX", "hzuSDMx7pd2uxFc5w", "fdEWWr8St59bXLbQr", "WbLAA8qZQNdbRgKte"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-12T04:24:46.456Z", "modifiedAt": null, "url": null, "title": "LessWrong, can you help me find an article I read a few months ago, I think here?", "slug": "lesswrong-can-you-help-me-find-an-article-i-read-a-few", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:07.738Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thecommexokid", "createdAt": "2012-10-09T20:26:45.933Z", "isAdmin": false, "displayName": "Thecommexokid"}, "userId": "wAJepargkGnWXDShb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W8yapTbtZbakEaKRk/lesswrong-can-you-help-me-find-an-article-i-read-a-few", "pageUrlRelative": "/posts/W8yapTbtZbakEaKRk/lesswrong-can-you-help-me-find-an-article-i-read-a-few", "linkUrl": "https://www.lesswrong.com/posts/W8yapTbtZbakEaKRk/lesswrong-can-you-help-me-find-an-article-i-read-a-few", "postedAtFormatted": "Friday, October 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%2C%20can%20you%20help%20me%20find%20an%20article%20I%20read%20a%20few%20months%20ago%2C%20I%20think%20here%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%2C%20can%20you%20help%20me%20find%20an%20article%20I%20read%20a%20few%20months%20ago%2C%20I%20think%20here%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW8yapTbtZbakEaKRk%2Flesswrong-can-you-help-me-find-an-article-i-read-a-few%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%2C%20can%20you%20help%20me%20find%20an%20article%20I%20read%20a%20few%20months%20ago%2C%20I%20think%20here%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW8yapTbtZbakEaKRk%2Flesswrong-can-you-help-me-find-an-article-i-read-a-few", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW8yapTbtZbakEaKRk%2Flesswrong-can-you-help-me-find-an-article-i-read-a-few", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 250, "htmlBody": "<p><span style=\"color: #aaaaaa; font-family: Helvetica; font-size: 19px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\"> </span></p>\n<div>I'm sorry in advance for a post that doesn't really offer the chance for any substantive discussion, but I am engaged in the most futile of all tasks &mdash; trying to find again something I read on the Internet six months ago &mdash; and I need all the help I can get. So onward to the point:</div>\n<div><br /></div>\n<div>Within the last 6 months I read an article. In my memory of the event I read it on LessWrong, but perhaps it was on Ovcoming Bias, or linked to from a LW post, or something. The main thesis of the piece was that American politics is covered in the press as a spectator sport, and issues that have enormous impact for millions of Americans are treated as though their only importance lies in their political ramifications. E.g., newspaper headlines say things like \"Victory for Obama as Congress passes healthcare bill,\" as if the only importance of the bill was that it constituted a political win, as if the American public ought to care more about the political victories and defeats of one guy in Washington than about the actual ramifications of the law on their own healthcare.</div>\n<div><br /></div>\n<div>Does anyone recognize the sound of this article? I have searched LessWrong in vain, but is it nonetheless here somewhere? Is anyone so familiar with the site that they can confirm definitively that it is <em>not </em>from here, so I can at least cross it off the list? In short, help?</div>\n<p>All my thanks.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W8yapTbtZbakEaKRk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 4, "extendedScore": null, "score": 1.0073201953431688e-06, "legacy": true, "legacyId": "19305", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-12T04:34:39.516Z", "modifiedAt": null, "url": null, "title": "Meetup : Madison: Prospect Theory", "slug": "meetup-madison-prospect-theory", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FggXNwJA9bsnA4pMJ/meetup-madison-prospect-theory", "pageUrlRelative": "/posts/FggXNwJA9bsnA4pMJ/meetup-madison-prospect-theory", "linkUrl": "https://www.lesswrong.com/posts/FggXNwJA9bsnA4pMJ/meetup-madison-prospect-theory", "postedAtFormatted": "Friday, October 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Madison%3A%20Prospect%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Madison%3A%20Prospect%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFggXNwJA9bsnA4pMJ%2Fmeetup-madison-prospect-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Madison%3A%20Prospect%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFggXNwJA9bsnA4pMJ%2Fmeetup-madison-prospect-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFggXNwJA9bsnA4pMJ%2Fmeetup-madison-prospect-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/et'>Madison: Prospect Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 October 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">325 Rethke Ave, Madison WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Prospect theory is a very simple, but reasonably predictive, model of actual human decisions. This Sunday, I'll give a short presentation on <a href=\"http://lesswrong.com/lw/6kf/prospect_theory_a_framework_for_understanding/\">prospect theory</a> -- in particular, what kinds of predictions it makes, what ways agents using it exhibit irrational behavior, and what we can do with this knowledge. Probably, I'll ramble about some other bits of behavioral economics, too. :)</p>\n\n<p>Also, of course: general discussion, and probably some board games.</p>\n\n<p>This Sunday, we'll meet at David's place. If you're in town, or near it, but you don't have an easy way to get to David's place, then ask for a ride, either here or on the <a href=\"https://groups.google.com/forum/#!forum/lesswrong-madison\">mailing list</a>. Someone will surely offer; if no one else does, I will pick you up. :)</p>\n\n<p>Also, also: if you're near Madison, do join our <a href=\"https://groups.google.com/forum/#!forum/lesswrong-madison\">mailing list</a>! There's some discussion there that never quite reaches this forum.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/et'>Madison: Prospect Theory</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FggXNwJA9bsnA4pMJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0073254550255844e-06, "legacy": true, "legacyId": "19306", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Madison__Prospect_Theory\">Discussion article for the meetup : <a href=\"/meetups/et\">Madison: Prospect Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 October 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">325 Rethke Ave, Madison WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Prospect theory is a very simple, but reasonably predictive, model of actual human decisions. This Sunday, I'll give a short presentation on <a href=\"http://lesswrong.com/lw/6kf/prospect_theory_a_framework_for_understanding/\">prospect theory</a> -- in particular, what kinds of predictions it makes, what ways agents using it exhibit irrational behavior, and what we can do with this knowledge. Probably, I'll ramble about some other bits of behavioral economics, too. :)</p>\n\n<p>Also, of course: general discussion, and probably some board games.</p>\n\n<p>This Sunday, we'll meet at David's place. If you're in town, or near it, but you don't have an easy way to get to David's place, then ask for a ride, either here or on the <a href=\"https://groups.google.com/forum/#!forum/lesswrong-madison\">mailing list</a>. Someone will surely offer; if no one else does, I will pick you up. :)</p>\n\n<p>Also, also: if you're near Madison, do join our <a href=\"https://groups.google.com/forum/#!forum/lesswrong-madison\">mailing list</a>! There's some discussion there that never quite reaches this forum.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Madison__Prospect_Theory1\">Discussion article for the meetup : <a href=\"/meetups/et\">Madison: Prospect Theory</a></h2>", "sections": [{"title": "Discussion article for the meetup : Madison: Prospect Theory", "anchor": "Discussion_article_for_the_meetup___Madison__Prospect_Theory", "level": 1}, {"title": "Discussion article for the meetup : Madison: Prospect Theory", "anchor": "Discussion_article_for_the_meetup___Madison__Prospect_Theory1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LQp9cZPzJncFKh5c8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-12T04:58:06.728Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Mundane Magic", "slug": "seq-rerun-mundane-magic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:05.840Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Eb4JzFv687o3F4sBA/seq-rerun-mundane-magic", "pageUrlRelative": "/posts/Eb4JzFv687o3F4sBA/seq-rerun-mundane-magic", "linkUrl": "https://www.lesswrong.com/posts/Eb4JzFv687o3F4sBA/seq-rerun-mundane-magic", "postedAtFormatted": "Friday, October 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Mundane%20Magic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Mundane%20Magic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEb4JzFv687o3F4sBA%2Fseq-rerun-mundane-magic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Mundane%20Magic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEb4JzFv687o3F4sBA%2Fseq-rerun-mundane-magic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEb4JzFv687o3F4sBA%2Fseq-rerun-mundane-magic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<p>Today's post, <a href=\"/lw/ve/mundane_magic/\">Mundane Magic</a> was originally published on 31 October 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Mundane_Magic\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A list of abilities that would be amazing if they were magic, or if only a few people had them.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/evu/seq_rerun_intelligence_in_economics/\">Intelligence in Economics</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Eb4JzFv687o3F4sBA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.007337935411989e-06, "legacy": true, "legacyId": "19307", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SXK87NgEPszhWkvQm", "fRiKd5jYekRrzDJED", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-12T15:25:18.835Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Berkeley, Bielefield, Cambridge MA, Chicago, Durham NC (2), Urbana-Champaign, Washington DC", "slug": "weekly-lw-meetups-austin-berkeley-bielefield-cambridge-ma", "viewCount": null, "lastCommentedAt": "2012-10-07T01:12:42.212Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E5EuJAe67GRPbpXBZ/weekly-lw-meetups-austin-berkeley-bielefield-cambridge-ma", "pageUrlRelative": "/posts/E5EuJAe67GRPbpXBZ/weekly-lw-meetups-austin-berkeley-bielefield-cambridge-ma", "linkUrl": "https://www.lesswrong.com/posts/E5EuJAe67GRPbpXBZ/weekly-lw-meetups-austin-berkeley-bielefield-cambridge-ma", "postedAtFormatted": "Friday, October 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Berkeley%2C%20Bielefield%2C%20Cambridge%20MA%2C%20Chicago%2C%20Durham%20NC%20(2)%2C%20Urbana-Champaign%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Berkeley%2C%20Bielefield%2C%20Cambridge%20MA%2C%20Chicago%2C%20Durham%20NC%20(2)%2C%20Urbana-Champaign%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE5EuJAe67GRPbpXBZ%2Fweekly-lw-meetups-austin-berkeley-bielefield-cambridge-ma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Berkeley%2C%20Bielefield%2C%20Cambridge%20MA%2C%20Chicago%2C%20Durham%20NC%20(2)%2C%20Urbana-Champaign%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE5EuJAe67GRPbpXBZ%2Fweekly-lw-meetups-austin-berkeley-bielefield-cambridge-ma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE5EuJAe67GRPbpXBZ%2Fweekly-lw-meetups-austin-berkeley-bielefield-cambridge-ma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 476, "htmlBody": "<p><strong>This summary was posted to LW main on October 5th, and has now been moved to discussion. The more recent summary is <a href=\"/lw/ewt/weekly_lw_meetups_austin_berlin_brussels_chicago/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/eb\">(Durham NC) HPMoR Discussion, chapters 4-7:&nbsp;<span class=\"date\">06 October 2012 11:00AM</span></a></li>\n<li><a href=\"/meetups/eg\">Urbana-Champaign, IL:&nbsp;<span class=\"date\">07 October 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/eh\">Bielefeld Meetup, October 9th :&nbsp;<span class=\"date\">09 October 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/e1\">(Durham NC) Research Triangle Area Less Wrong:&nbsp;<span class=\"date\">11 October 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/ed\">(Chicago) Zendo in the West Loop:&nbsp;<span class=\"date\">12 October 2012 06:30PM</span></a></li>\n<li><a href=\"/meetups/e9\">Munich Meetup, EDIT: October 27th/28th:&nbsp;<span class=\"date\">27 October 2012 02:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">06 October 2018 01:30PM</span></a></li>\n<li><a href=\"/meetups/df\">Cambridge (MA) first-Sundays meetup:&nbsp;<span class=\"date\">07 October 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/ef\">DC Meetup: Boardgames:&nbsp;<span class=\"date\">07 October 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/e5\">(Berkeley CA) Pre-Singularity Summit Overcoming Bias / Less Wrong Meetup Party:&nbsp;<span class=\"date\">11 October 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/dg\">Cambridge (MA) third-Sundays Meetup:&nbsp;<span class=\"date\">21 October 2012 02:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E5EuJAe67GRPbpXBZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.0076717915220425e-06, "legacy": true, "legacyId": "19179", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zv2ttqCuqZjEBcCXB", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-12T15:56:44.168Z", "modifiedAt": null, "url": null, "title": "Raising the forecasting waterline (part 2)", "slug": "raising-the-forecasting-waterline-part-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:23.500Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YEKHh5nyqhpE3E4Bm/raising-the-forecasting-waterline-part-2", "pageUrlRelative": "/posts/YEKHh5nyqhpE3E4Bm/raising-the-forecasting-waterline-part-2", "linkUrl": "https://www.lesswrong.com/posts/YEKHh5nyqhpE3E4Bm/raising-the-forecasting-waterline-part-2", "postedAtFormatted": "Friday, October 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Raising%20the%20forecasting%20waterline%20(part%202)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARaising%20the%20forecasting%20waterline%20(part%202)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYEKHh5nyqhpE3E4Bm%2Fraising-the-forecasting-waterline-part-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Raising%20the%20forecasting%20waterline%20(part%202)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYEKHh5nyqhpE3E4Bm%2Fraising-the-forecasting-waterline-part-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYEKHh5nyqhpE3E4Bm%2Fraising-the-forecasting-waterline-part-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1643, "htmlBody": "<p><strong>Previously:</strong> <a href=\"/lw/eup/raising_the_forecasting_waterline_part_1/\">part 1</a></p>\n<p>The three tactics I described in part 1 are most suited to making an initial forecast. I will now turn to a question that was raised in <a href=\"/lw/eup/raising_the_forecasting_waterline_part_1/7m3f\">comments</a> on part 1 - that of updating when new evidence arrives. But first, I'd like to discuss the notion of a \"well-specified forecast\".</p>\n<h2>Well-specified forecasts</h2>\n<p>It is often surprisingly hard to frame a question in terms that make a forecast reasonably easy to verify and score. &nbsp;Questions can be ambiguous (consider \"X will win the U.S. presidential election\" - do we mean win the popular vote, or win re-election in the electoral college?). They can fail to cover all possible outcomes (so \"which of the candidates will win the election\" needs a catch-all \"Other\").<a name=\"fc1\"></a><sup><a href=\"#f1\">1</a></sup></p>\n<p><a id=\"more\"></a>Another way to make questions ambiguous is to leave out their sell-by date. Consider the <a href=\"http://predictionbook.com/predictions/1549\">question</a>, \"Google Earth / Street View will become a video gaming platform.\" This seems designed to prompt a \"by when?\" On both PredictionBook and the Good Judgment Project, questions come with a \"known on\" or a \"closing\" date respectively. But the question itself generally includes a date: \"<a href=\"http://predictionbook.com/predictions/5557\">Super Mario Bros. Z</a> episode 9 will be released on or before November 24, 2012\". The \"known on\" or \"closing\" date often leaves a margin of safety, for cases where some time may pass between the event happening and the outcome becoming known.&nbsp;<a name=\"fc2\"></a><sup><a href=\"#f2\">2</a></sup></p>\n<p>Questions for GJP are provided by IARPA, the tournament sponsor. Both IARPA and the GJP research team go to some lengths to make sure that questions leave little room for interpretation or ambiguity: a deadline is always specified, and a \"judge's statement\" clarifies the meaning of terms (even such obvious-seeming ones as \"by\" a certain date, which is expanded into \"by midnight of that date\") and which sources will be taken as authoritative (for instance, \"gain control of the Somali town of Kismayo\" was to be verified by one of BBC, Reuters or the Economist announcing a control transition and failing to then announce a reversal within 48 hours). Some questions have been voided (not scored) due to ambiguity in the wording. This is one of the things I appreciate about GJP.</p>\n<h2>Tool 4 - Prepare lines of retreat</h2>\n<p>Many forecasts are long-range, and many unexpected things might happen between making your initial forecast and reaching the deadline, or the occurrence of one of the specified outcomes. There are two pitfalls to avoid: one is that you will over-react to new information, swinging between \"almost certain\" to \"cannot happen\" every time you hear something in the news; the other is that you will find a way to interpret any new information as confirming your initial forecast (<a href=\"/lw/iw/positive_bias_look_into_the_dark/\">confirmation bias</a>).</p>\n<p>One of my recent breakthroughs with GJP was when I started laying out my <a href=\"/lw/o4/leave_a_line_of_retreat/\">lines of retreat</a> in advance: I now try to ask myself, \"What would change my mind about this\", and write that in the comments that you can optionally leave on a forecast, as a non-repudiable reminder. For instance, on the question \"Will the Colombian government and FARC commence official talks before 1 January 2013?\", I entered a 90% \"Yes\" forecast on 9/18 when a date for the talks was set, but added: \"Will revise significantly towards \"No\" if the meeting fails to happen on October 8 or is pushed back.\" This was to prevent my thinking later \"Oh, it's just a small delay, it will surely happen anyway\". On October 1st, a delay was announced, and I duly scaled my forecast back to 75%.</p>\n<p>Advice of this sort was part of the \"process feedback\" that we received from the GJP team at the start of Season 2, pointing out behaviors that they associated with high-performing forecasters, and in particular the quantity and quality of the comments these participants posted with their forecasts. I only recently started really getting the hang of this, but now, more likely than not, mine are accompanied with a mini-summary (a few paragraphs) where I briefly summarize the status quo, reference classes or base rates if applicable, current&nbsp;evidence pointing to likely change, and what kind of future reports might change my mind. These are generally not based on my background knowledge, which is often embarrassingly scant, but between a few minutes and an hour of Googling and Wikipedia-ing.</p>\n<h2>Tool 5 - Abandon sunk costs</h2>\n<p>The GJP scores forecasts using what's known as the \"Brier score\", which consists of taking your probability and squaring it if the event <em>did not</em> happen, or the complement of your probability and squaring it if the event <em>did</em> happen. (That's for binary outcomes; for multiple outcomes, it's the sum of these squares over each outcome.)</p>\n<p>You'll notice that the best you can hope for is 0: you assign 100% probability to something that happens, or 0% to something that doesn't happen. In any other situation your score is positive; so a <em>good</em> score is a <em>low</em> score.&nbsp;</p>\n<p>The sunk cost fallacy <a href=\"/lw/9hb/position_design_and_write_rationality_curriculum/\">consists</a> of letting past costs (or losses) affect decisions on future bets. Say you have entered a forecast of 90% on the question \"Will 1 Euro buy less than $1.20 US dollars at any point before 1 January 2013?\", as I did in mid-July. If this fails to happen, the penalty is a fairly large .81. A few months later, propped up by governmental decisions in the Euro zone, the continent's currency is still strong.</p>\n<p>You may find yourself thinking \"Well if I change my mind now, half-way through, I'm only getting at best the average of .81 and the Brier score of whatever I change it to; .4 in the best case. But really, there's still a chance it will hit that low... then I'd get a very nice .01 for sticking to my guns.\" That's the sunk cost fallacy. It's silly because \"sticking to my guns\" will penalize me even worse if my forecast does fail. Whatever is going to happen to the Euro will happen; the loss from my past forecast is already determined. The Brier score is a \"proper\" scoring rule, which can only be optimized by accurately stating my degree of uncertainty.</p>\n<p>What's scary is that, while I knew about the sunk cost fallacy in theory, I think it pretty much describes my thoughts on the Euro question. I only retreated to 80% at first, then 70% - before finally biting the bullet and admitting my new belief, predominantly on the \"No\" side. (That question isn't scored yet.)</p>\n<p>Detach from your sunk costs: treat your past forecasts as if they'd been made by someone else, and if you now have grounds to form a completely different forecast, go with that.</p>\n<h2>Tool 6 - Consider your loss function</h2>\n<p>The Brier score is also known as the \"squared error loss\" and can be seen as a \"loss function\": informally, you can think of a \"loss function\" as \"what is at stake if I call this wrong\". In poker, the loss function would be not the probability that your hand loses, or the Brier score associated with your estimate of that probability, but the probability multiplied by the size of the pot - the real-world consequences of your estimate, in other words. This is why you may play the same hand aggressively or conservatively according to the circumstances.</p>\n<p>The Brier score is more \"forgiving\" in one sense than another common loss function, the logarithmic scoring rule - which instead of the square takes the log of your probability (or of its complement). If you use probabilities close to 0 or 1, you can end up with huge negative scores! With the Brier score, on the other hand, the worst you can do is a score of 1, and a forecast of 100% isn't much worse than 95%, even if the event fails to happen.</p>\n<p>The GJP system computes your Brier score for each day that the question is open, according to what your forecast is on that day, and average over all days. Forecasts have a sell-by date, which is somewhat artificially imposed by the contest rather than intrinsic to the situation. This means there is an asymmetry to some situations, such that the best forecast may not reflect your actual beliefs. One example was the question \"Will any government force gain control of the Somali town of Kismayo before 1 November 2012?\".</p>\n<p>When this question opened, I quickly learned that government forces were preparing an attack on the town. *If* the assault succeeded, then the question would probably resolve in a few days, and the Brier score would be dominated by my initial short-term forecast. If, on the other hand, the assault failed, the status quo would likely continue for quite some time; I could then change my forecast, and the initial would be \"diluted\" over the following weeks. So I picked a \"Yes\" value more extreme (90%) than my actual confidence, planning to quickly \"retreat\" back to the \"No\" side if the assault failed or gave any sign of turning into a protracted battle.</p>\n<p>This can feel a little like <a href=\"/lw/eup/raising_the_forecasting_waterline_part_1/7m4y\">cheating</a> - but, if your objective is to do well in the forecasting contest (as opposed to having correct beliefs on the more general question \"will Kismayo fall\", which does not have a real deadline), it's perfectly sensible.</p>\n<p>&nbsp;</p>\n<p>We have reached something that feels a little like a \"trick\" of forecasting, and thus are probably leaving the domain of \"basic skills to raise yourself above a low waterline\". I'll leave you with these, and hope this summary has encouraged you to try your hand at forecasting, if you hadn't done so before.</p>\n<p>If you do: have fun! And please report back here with whatever new and useful tricks you've learned.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p style=\"font-size:90%\"><sup><a name=\"f1\"></a><a name=\"f1\"></a><a href=\"#fc1\">1</a></sup> PB only has binary questions, so that isn't an issue, but it is one on GJP where multiple-choice questions are common.</p>\n<p style=\"font-size:90%\"><a name=\"f2\"></a><sup><a href=\"#fc2\">2</a></sup> On PB, there appears to be a tacit convention that the \"known on\" date also serves as a deadline for the question, if no such deadline was specified.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YEKHh5nyqhpE3E4Bm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 22, "extendedScore": null, "score": 1.0076885225355243e-06, "legacy": true, "legacyId": "19324", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Previously:</strong> <a href=\"/lw/eup/raising_the_forecasting_waterline_part_1/\">part 1</a></p>\n<p>The three tactics I described in part 1 are most suited to making an initial forecast. I will now turn to a question that was raised in <a href=\"/lw/eup/raising_the_forecasting_waterline_part_1/7m3f\">comments</a> on part 1 - that of updating when new evidence arrives. But first, I'd like to discuss the notion of a \"well-specified forecast\".</p>\n<h2 id=\"Well_specified_forecasts\">Well-specified forecasts</h2>\n<p>It is often surprisingly hard to frame a question in terms that make a forecast reasonably easy to verify and score. &nbsp;Questions can be ambiguous (consider \"X will win the U.S. presidential election\" - do we mean win the popular vote, or win re-election in the electoral college?). They can fail to cover all possible outcomes (so \"which of the candidates will win the election\" needs a catch-all \"Other\").<a name=\"fc1\"></a><sup><a href=\"#f1\">1</a></sup></p>\n<p><a id=\"more\"></a>Another way to make questions ambiguous is to leave out their sell-by date. Consider the <a href=\"http://predictionbook.com/predictions/1549\">question</a>, \"Google Earth / Street View will become a video gaming platform.\" This seems designed to prompt a \"by when?\" On both PredictionBook and the Good Judgment Project, questions come with a \"known on\" or a \"closing\" date respectively. But the question itself generally includes a date: \"<a href=\"http://predictionbook.com/predictions/5557\">Super Mario Bros. Z</a> episode 9 will be released on or before November 24, 2012\". The \"known on\" or \"closing\" date often leaves a margin of safety, for cases where some time may pass between the event happening and the outcome becoming known.&nbsp;<a name=\"fc2\"></a><sup><a href=\"#f2\">2</a></sup></p>\n<p>Questions for GJP are provided by IARPA, the tournament sponsor. Both IARPA and the GJP research team go to some lengths to make sure that questions leave little room for interpretation or ambiguity: a deadline is always specified, and a \"judge's statement\" clarifies the meaning of terms (even such obvious-seeming ones as \"by\" a certain date, which is expanded into \"by midnight of that date\") and which sources will be taken as authoritative (for instance, \"gain control of the Somali town of Kismayo\" was to be verified by one of BBC, Reuters or the Economist announcing a control transition and failing to then announce a reversal within 48 hours). Some questions have been voided (not scored) due to ambiguity in the wording. This is one of the things I appreciate about GJP.</p>\n<h2 id=\"Tool_4___Prepare_lines_of_retreat\">Tool 4 - Prepare lines of retreat</h2>\n<p>Many forecasts are long-range, and many unexpected things might happen between making your initial forecast and reaching the deadline, or the occurrence of one of the specified outcomes. There are two pitfalls to avoid: one is that you will over-react to new information, swinging between \"almost certain\" to \"cannot happen\" every time you hear something in the news; the other is that you will find a way to interpret any new information as confirming your initial forecast (<a href=\"/lw/iw/positive_bias_look_into_the_dark/\">confirmation bias</a>).</p>\n<p>One of my recent breakthroughs with GJP was when I started laying out my <a href=\"/lw/o4/leave_a_line_of_retreat/\">lines of retreat</a> in advance: I now try to ask myself, \"What would change my mind about this\", and write that in the comments that you can optionally leave on a forecast, as a non-repudiable reminder. For instance, on the question \"Will the Colombian government and FARC commence official talks before 1 January 2013?\", I entered a 90% \"Yes\" forecast on 9/18 when a date for the talks was set, but added: \"Will revise significantly towards \"No\" if the meeting fails to happen on October 8 or is pushed back.\" This was to prevent my thinking later \"Oh, it's just a small delay, it will surely happen anyway\". On October 1st, a delay was announced, and I duly scaled my forecast back to 75%.</p>\n<p>Advice of this sort was part of the \"process feedback\" that we received from the GJP team at the start of Season 2, pointing out behaviors that they associated with high-performing forecasters, and in particular the quantity and quality of the comments these participants posted with their forecasts. I only recently started really getting the hang of this, but now, more likely than not, mine are accompanied with a mini-summary (a few paragraphs) where I briefly summarize the status quo, reference classes or base rates if applicable, current&nbsp;evidence pointing to likely change, and what kind of future reports might change my mind. These are generally not based on my background knowledge, which is often embarrassingly scant, but between a few minutes and an hour of Googling and Wikipedia-ing.</p>\n<h2 id=\"Tool_5___Abandon_sunk_costs\">Tool 5 - Abandon sunk costs</h2>\n<p>The GJP scores forecasts using what's known as the \"Brier score\", which consists of taking your probability and squaring it if the event <em>did not</em> happen, or the complement of your probability and squaring it if the event <em>did</em> happen. (That's for binary outcomes; for multiple outcomes, it's the sum of these squares over each outcome.)</p>\n<p>You'll notice that the best you can hope for is 0: you assign 100% probability to something that happens, or 0% to something that doesn't happen. In any other situation your score is positive; so a <em>good</em> score is a <em>low</em> score.&nbsp;</p>\n<p>The sunk cost fallacy <a href=\"/lw/9hb/position_design_and_write_rationality_curriculum/\">consists</a> of letting past costs (or losses) affect decisions on future bets. Say you have entered a forecast of 90% on the question \"Will 1 Euro buy less than $1.20 US dollars at any point before 1 January 2013?\", as I did in mid-July. If this fails to happen, the penalty is a fairly large .81. A few months later, propped up by governmental decisions in the Euro zone, the continent's currency is still strong.</p>\n<p>You may find yourself thinking \"Well if I change my mind now, half-way through, I'm only getting at best the average of .81 and the Brier score of whatever I change it to; .4 in the best case. But really, there's still a chance it will hit that low... then I'd get a very nice .01 for sticking to my guns.\" That's the sunk cost fallacy. It's silly because \"sticking to my guns\" will penalize me even worse if my forecast does fail. Whatever is going to happen to the Euro will happen; the loss from my past forecast is already determined. The Brier score is a \"proper\" scoring rule, which can only be optimized by accurately stating my degree of uncertainty.</p>\n<p>What's scary is that, while I knew about the sunk cost fallacy in theory, I think it pretty much describes my thoughts on the Euro question. I only retreated to 80% at first, then 70% - before finally biting the bullet and admitting my new belief, predominantly on the \"No\" side. (That question isn't scored yet.)</p>\n<p>Detach from your sunk costs: treat your past forecasts as if they'd been made by someone else, and if you now have grounds to form a completely different forecast, go with that.</p>\n<h2 id=\"Tool_6___Consider_your_loss_function\">Tool 6 - Consider your loss function</h2>\n<p>The Brier score is also known as the \"squared error loss\" and can be seen as a \"loss function\": informally, you can think of a \"loss function\" as \"what is at stake if I call this wrong\". In poker, the loss function would be not the probability that your hand loses, or the Brier score associated with your estimate of that probability, but the probability multiplied by the size of the pot - the real-world consequences of your estimate, in other words. This is why you may play the same hand aggressively or conservatively according to the circumstances.</p>\n<p>The Brier score is more \"forgiving\" in one sense than another common loss function, the logarithmic scoring rule - which instead of the square takes the log of your probability (or of its complement). If you use probabilities close to 0 or 1, you can end up with huge negative scores! With the Brier score, on the other hand, the worst you can do is a score of 1, and a forecast of 100% isn't much worse than 95%, even if the event fails to happen.</p>\n<p>The GJP system computes your Brier score for each day that the question is open, according to what your forecast is on that day, and average over all days. Forecasts have a sell-by date, which is somewhat artificially imposed by the contest rather than intrinsic to the situation. This means there is an asymmetry to some situations, such that the best forecast may not reflect your actual beliefs. One example was the question \"Will any government force gain control of the Somali town of Kismayo before 1 November 2012?\".</p>\n<p>When this question opened, I quickly learned that government forces were preparing an attack on the town. *If* the assault succeeded, then the question would probably resolve in a few days, and the Brier score would be dominated by my initial short-term forecast. If, on the other hand, the assault failed, the status quo would likely continue for quite some time; I could then change my forecast, and the initial would be \"diluted\" over the following weeks. So I picked a \"Yes\" value more extreme (90%) than my actual confidence, planning to quickly \"retreat\" back to the \"No\" side if the assault failed or gave any sign of turning into a protracted battle.</p>\n<p>This can feel a little like <a href=\"/lw/eup/raising_the_forecasting_waterline_part_1/7m4y\">cheating</a> - but, if your objective is to do well in the forecasting contest (as opposed to having correct beliefs on the more general question \"will Kismayo fall\", which does not have a real deadline), it's perfectly sensible.</p>\n<p>&nbsp;</p>\n<p>We have reached something that feels a little like a \"trick\" of forecasting, and thus are probably leaving the domain of \"basic skills to raise yourself above a low waterline\". I'll leave you with these, and hope this summary has encouraged you to try your hand at forecasting, if you hadn't done so before.</p>\n<p>If you do: have fun! And please report back here with whatever new and useful tricks you've learned.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<hr>\n<p style=\"font-size:90%\"><sup><a name=\"f1\"></a><a name=\"f1\"></a><a href=\"#fc1\">1</a></sup> PB only has binary questions, so that isn't an issue, but it is one on GJP where multiple-choice questions are common.</p>\n<p style=\"font-size:90%\"><a name=\"f2\"></a><sup><a href=\"#fc2\">2</a></sup> On PB, there appears to be a tacit convention that the \"known on\" date also serves as a deadline for the question, if no such deadline was specified.</p>\n<p>&nbsp;</p>", "sections": [{"title": "Well-specified forecasts", "anchor": "Well_specified_forecasts", "level": 1}, {"title": "Tool 4 - Prepare lines of retreat", "anchor": "Tool_4___Prepare_lines_of_retreat", "level": 1}, {"title": "Tool 5 - Abandon sunk costs", "anchor": "Tool_5___Abandon_sunk_costs", "level": 1}, {"title": "Tool 6 - Consider your loss function", "anchor": "Tool_6___Consider_your_loss_function", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "22 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YQ8H4e7z3q8ngev7J", "rmAbiEKQDpDnZzcRf", "3XgYbghWruBMrPTAL", "ifL8f4Xzy2D9Bb6zs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-12T20:50:30.737Z", "modifiedAt": null, "url": null, "title": "Meetup : SLC Meetup: Free Will, Minicamp Braindump, and Group Goals", "slug": "meetup-slc-meetup-free-will-minicamp-braindump-and-group", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hamnox", "createdAt": "2011-01-17T01:16:28.722Z", "isAdmin": false, "displayName": "hamnox"}, "userId": "EY9o6qtXvYhS5CTHD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7f8DYtBsFQic9WFgs/meetup-slc-meetup-free-will-minicamp-braindump-and-group", "pageUrlRelative": "/posts/7f8DYtBsFQic9WFgs/meetup-slc-meetup-free-will-minicamp-braindump-and-group", "linkUrl": "https://www.lesswrong.com/posts/7f8DYtBsFQic9WFgs/meetup-slc-meetup-free-will-minicamp-braindump-and-group", "postedAtFormatted": "Friday, October 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20SLC%20Meetup%3A%20Free%20Will%2C%20Minicamp%20Braindump%2C%20and%20Group%20Goals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20SLC%20Meetup%3A%20Free%20Will%2C%20Minicamp%20Braindump%2C%20and%20Group%20Goals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7f8DYtBsFQic9WFgs%2Fmeetup-slc-meetup-free-will-minicamp-braindump-and-group%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20SLC%20Meetup%3A%20Free%20Will%2C%20Minicamp%20Braindump%2C%20and%20Group%20Goals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7f8DYtBsFQic9WFgs%2Fmeetup-slc-meetup-free-will-minicamp-braindump-and-group", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7f8DYtBsFQic9WFgs%2Fmeetup-slc-meetup-free-will-minicamp-braindump-and-group", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/eu'>SLC Meetup: Free Will, Minicamp Braindump, and Group Goals</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 October 2012 03:00:38PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Calvin E. Smith Library: 810 East 3300 South, Salt Lake City, UT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>810 East 3300 South, Salt Lake City, UT\nWe'll be in the small conference room at the back.</p>\n\n<p>Here's the plan breakdown:\n    3:00 Greetings, Introduction and Catchup\n    3:10 Minicamp Brain Dump (Kevin V.)\n    3:30 discuss\n    3:40 Free Will (Kip W.)\n    4:00 discuss\n    4:10 Group Goals (Mel H.)\n    4:30 discuss\n    4:40 Wrap up and Final Thoughts, Sign up for next meetup\n    Free Discussion</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/eu'>SLC Meetup: Free Will, Minicamp Braindump, and Group Goals</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7f8DYtBsFQic9WFgs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "19326", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___SLC_Meetup__Free_Will__Minicamp_Braindump__and_Group_Goals\">Discussion article for the meetup : <a href=\"/meetups/eu\">SLC Meetup: Free Will, Minicamp Braindump, and Group Goals</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 October 2012 03:00:38PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Calvin E. Smith Library: 810 East 3300 South, Salt Lake City, UT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>810 East 3300 South, Salt Lake City, UT\nWe'll be in the small conference room at the back.</p>\n\n<p>Here's the plan breakdown:\n    3:00 Greetings, Introduction and Catchup\n    3:10 Minicamp Brain Dump (Kevin V.)\n    3:30 discuss\n    3:40 Free Will (Kip W.)\n    4:00 discuss\n    4:10 Group Goals (Mel H.)\n    4:30 discuss\n    4:40 Wrap up and Final Thoughts, Sign up for next meetup\n    Free Discussion</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___SLC_Meetup__Free_Will__Minicamp_Braindump__and_Group_Goals1\">Discussion article for the meetup : <a href=\"/meetups/eu\">SLC Meetup: Free Will, Minicamp Braindump, and Group Goals</a></h2>", "sections": [{"title": "Discussion article for the meetup : SLC Meetup: Free Will, Minicamp Braindump, and Group Goals", "anchor": "Discussion_article_for_the_meetup___SLC_Meetup__Free_Will__Minicamp_Braindump__and_Group_Goals", "level": 1}, {"title": "Discussion article for the meetup : SLC Meetup: Free Will, Minicamp Braindump, and Group Goals", "anchor": "Discussion_article_for_the_meetup___SLC_Meetup__Free_Will__Minicamp_Braindump__and_Group_Goals1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-12T21:49:16.747Z", "modifiedAt": "2022-02-24T01:51:19.919Z", "url": null, "title": "Causal Diagrams and Causal Models", "slug": "causal-diagrams-and-causal-models", "viewCount": null, "lastCommentedAt": "2019-12-12T18:53:10.351Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hzuSDMx7pd2uxFc5w/causal-diagrams-and-causal-models", "pageUrlRelative": "/posts/hzuSDMx7pd2uxFc5w/causal-diagrams-and-causal-models", "linkUrl": "https://www.lesswrong.com/posts/hzuSDMx7pd2uxFc5w/causal-diagrams-and-causal-models", "postedAtFormatted": "Friday, October 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Causal%20Diagrams%20and%20Causal%20Models&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACausal%20Diagrams%20and%20Causal%20Models%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhzuSDMx7pd2uxFc5w%2Fcausal-diagrams-and-causal-models%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Causal%20Diagrams%20and%20Causal%20Models%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhzuSDMx7pd2uxFc5w%2Fcausal-diagrams-and-causal-models", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhzuSDMx7pd2uxFc5w%2Fcausal-diagrams-and-causal-models", "socialPreviewImageUrl": "http://wiki.lesswrong.com/mediawiki/images/f/fc/EY1.svg", "question": false, "authorIsUnreviewed": false, "wordCount": 4622, "htmlBody": "<p>Suppose a general-population survey shows that people who exercise less, weigh more. You don't have any known direction of <em>time</em> in the data - you don't know which came first, the increased weight or the diminished exercise. And you didn't randomly assign half the population to exercise less; you just surveyed an existing population.</p>\n<p>The statisticians who discovered causality were trying to find a way to distinguish, within survey data, the direction of cause and effect - whether, as common sense would have it, more obese people exercise less <em>because</em> they find physical activity less rewarding; or whether, as in the <a href=\"http://en.wikipedia.org/wiki/Just-world_hypothesis\">virtue theory of metabolism</a>, lack of exercise actually <em>causes</em> weight gain due to divine punishment for the sin of sloth.</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/f/fc/EY1.svg\" alt=\"\"></td>\n<td valign=\"center\">\n<h1><strong>&nbsp;vs.&nbsp;</strong></h1>\n</td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/0/09/EY2.svg\" alt=\"\"></td>\n</tr>\n</tbody>\n</table>\n<p>The usual way to resolve this sort of question is by <em>randomized</em> <em>intervention.</em> If you randomly assign half your experimental subjects to exercise more, and afterward the increased-exercise group doesn't lose any weight compared to the control group&nbsp;<a name=\"note1back\"></a><a href=\"#note1\">[1]</a>, you could rule out causality <em>from</em> exercise <em>to</em> weight, and conclude that the correlation between weight and exercise is probably due to physical activity being less fun when you're overweight <a name=\"note3back\"></a><a href=\"#note3\">[3]</a>. The question is whether you can get causal data <em>without</em> interventions.</p>\n<p>For a long time, the conventional wisdom in philosophy was that this was impossible unless you knew the direction of time and knew which event had happened first. Among some philosophers of science, there was a belief that the \"direction of causality\" was a <em>meaningless</em> question, and that in the universe itself there were <em>only</em> correlations - that \"cause and effect\" was something unobservable and undefinable, that only unsophisticated non-statisticians believed in due to their lack of formal training:</p>\n<p style=\"padding-left: 30px;\">\"The law of causality, I believe, like much that passes muster among philosophers, is a relic of a bygone age, surviving, like the monarchy, only because it is erroneously supposed to do no harm.\" -- Bertrand Russell (he later changed his mind)</p>\n<p style=\"padding-left: 30px;\">\"Beyond such discarded fundamentals as 'matter' and 'force' lies still another fetish among the inscrutable arcana of modern science, namely, the category of cause and effect.\" -- Karl Pearson</p>\n<p>The famous statistician Fisher, who was also a smoker, testified before Congress that the correlation between smoking and lung cancer couldn't prove that the former caused the latter. &nbsp;We have remnants of this type of reasoning in old-school \"Correlation does not imply causation\", without the now-standard appendix, \"<a href=\"http://xkcd.com/552/\">But it sure is a hint</a>\".</p>\n<p>This skepticism was overturned by a surprisingly simple mathematical observation.<a id=\"more\"></a></p>\n<p>Let's say there are three variables in the survey data: Weight, how much the person exercises, and how much time they spend on the Internet.</p>\n<p>For simplicity, we'll have these three variables be binary, yes-or-no observations: Y or N for whether the person has a BMI over 25, Y or N for whether they exercised at least twice in the last week, and Y or N for whether they've checked Reddit in the last 72 hours.</p>\n<p>Now let's say our gathered data looks like this:</p>\n<table border=\"1\">\n<tbody>\n<!-- Results table headers --> \n<tr>\n<th>Overweight</th> <th>Exercise</th> <th>Internet</th> <th>#</th>\n</tr>\n<tr>\n<td>Y</td>\n<td>Y</td>\n<td>Y</td>\n<td>&nbsp;1,119</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>Y</td>\n<td>N</td>\n<td>&nbsp;16,104</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>N</td>\n<td>Y</td>\n<td>&nbsp;11,121</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>N</td>\n<td>N</td>\n<td>&nbsp;60,032</td>\n</tr>\n<tr>\n<td>N</td>\n<td>Y</td>\n<td>Y</td>\n<td>&nbsp;18,102</td>\n</tr>\n<tr>\n<td>N</td>\n<td>Y</td>\n<td>N</td>\n<td>&nbsp;132,111</td>\n</tr>\n<tr>\n<td>N</td>\n<td>N</td>\n<td>Y</td>\n<td>&nbsp;29,120</td>\n</tr>\n<tr>\n<td>N</td>\n<td>N</td>\n<td>N</td>\n<td>&nbsp;155,033</td>\n</tr>\n</tbody>\n</table>\n<p>And lo, merely by eyeballing this data -</p>\n<p>(which is <em>totally made up</em>, so don't go actually <em>believing </em>the conclusion I'm about to draw)</p>\n<p>- we now realize that <em>being overweight and spending time on the Internet both cause you to exercise less,</em> presumably because exercise is less fun and you have more alternative things to do,<em> but exercising has no causal influence on body weight or Internet use.</em></p>\n<p>\"What!\" you cry. \"How can you tell <em>that</em> just by inspecting those numbers? You can't say that exercise isn't <em>correlated</em> to body weight - if you just look at all the members of the population who exercise, they clearly have lower weights. 10%&nbsp;of exercisers are overweight, vs. 28%&nbsp;of non-exercisers. &nbsp;How could you rule out the obvious causal explanation for that correlation, just by looking at this data?\"</p>\n<hr>\n<p>There's a wee bit of math involved. &nbsp;It's <em>simple</em> math - the part we'll use doesn't involve solving equations or complicated proofs -but we do have to introduce a wee bit of novel math to explain how the heck we got there from here.</p>\n<p>Let me start with a question that turned out - to the surprise of many investigators involved - to be highly related to the issue we've just addressed.</p>\n<p>Suppose that earthquakes and burglars can both set off burglar alarms. &nbsp;If the burglar alarm in your house goes off, it might be because of an actual burglar, but it might <em>also </em>be because a minor earthquake rocked your house and triggered a few sensors. Early investigators in Artificial Intelligence, who were trying to represent all high-level events using primitive tokens in a first-order logic (for reasons of historical stupidity we won't go into) were stymied by the following apparent paradox:</p>\n<ul>\n<li>\n<p>If you tell me that my burglar alarm went off, I infer a burglar, which I will represent in my first-order-logical database using a theorem&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\u22a2</span>&nbsp;ALARM&nbsp;\u2192&nbsp;BURGLAR. (The symbol \"<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\u22a2</span>\" is called \"<a href=\"http://en.wikipedia.org/wiki/Turnstile_(symbol)\">turnstile</a>\" and means \"the logical system asserts that\".)</p>\n</li>\n<li>\n<p>If an earthquake occurs, it will set off burglar alarms. I shall represent this using the theorem&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\u22a2</span>&nbsp;EARTHQUAKE&nbsp;\u2192&nbsp;ALARM, or \"earthquake implies alarm\".</p>\n</li>\n<li>\n<p>If you tell me that my alarm went off, and then further tell me that an earthquake occurred, it <em>explains away</em> my burglar alarm going off. I don't need to explain the alarm by a burglar, because the alarm has already been explained by the earthquake. I conclude there was no burglar. I shall represent this by adding a theorem which says&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\u22a2</span>&nbsp;(EARTHQUAKE &amp; ALARM)&nbsp;\u2192&nbsp;NOT BURGLAR.</p>\n</li>\n</ul>\n<p>Which represents a logical contradiction, and for a while there were attempts to develop \"non-monotonic logics\" so that you could retract conclusions given additional data. <a href=\"/lw/vt/the_nature_of_logic/\">This didn't work very well, since the underlying structure of reasoning was a terrible fit for the structure of classical logic, even when mutated.</a></p>\n<p>Just changing certainties to quantitative probabilities can fix many problems with classical logic, and one might think that this case was likewise easily fixed.</p>\n<p>Namely, just write a probability table of all possible combinations of earthquake or \u00acearthquake, burglar or \u00acburglar, and alarm or \u00acalarm (where&nbsp;\u00ac is the logical negation symbol), with the following entries:</p>\n<table border=\"1\">\n<tbody>\n<!-- Results table headers --> \n<tr>\n<th>Burglar</th> <th>Earthquake</th> <th>Alarm</th> <th>%</th>\n</tr>\n<tr>\n<td>b</td>\n<td>e</td>\n<td>a</td>\n<td>.000162</td>\n</tr>\n<tr>\n<td>b</td>\n<td>e</td>\n<td>\u00aca</td>\n<td>.0000085</td>\n</tr>\n<tr>\n<td>b</td>\n<td>\u00ace</td>\n<td>a</td>\n<td>.0151</td>\n</tr>\n<tr>\n<td>b</td>\n<td>\u00ace</td>\n<td>\u00aca</td>\n<td>.00168</td>\n</tr>\n<tr>\n<td>\u00acb</td>\n<td>e</td>\n<td>a</td>\n<td>.0078</td>\n</tr>\n<tr>\n<td>\u00acb</td>\n<td>e</td>\n<td>\u00aca</td>\n<td>.002</td>\n</tr>\n<tr>\n<td>\u00acb</td>\n<td>\u00ace</td>\n<td>a</td>\n<td>.00097</td>\n</tr>\n<tr>\n<td>\u00acb</td>\n<td>\u00ace</td>\n<td>\u00aca</td>\n<td>.972</td>\n</tr>\n</tbody>\n</table>\n<p>Using the operations of <em>marginalization</em> and <em>conditionalization,</em> we get the desired reasoning back out:</p>\n<p>Let's start with the <em>probability of a burglar given an alarm,</em> p(burglar|alarm). By the law of conditional probability,</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(b|a)%20=%20\\frac{p(ab)}{p(a)}\" alt=\"\" width=\"112\" height=\"44\"></p>\n<p>i.e. the relative fraction of cases where there's an alarm <em>and</em> a burglar, within the set of all cases where there's an alarm.</p>\n<p>The table doesn't directly tell us p(alarm &amp; burglar)/p(alarm), but by the law of marginal probability,</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(ab)%20=%20p(abe)%20+%20p(ab\\neg%20e)%20=%20.000162%20+%20.0151%20=%20.0153\" alt=\"\" width=\"405\" height=\"19\"></p>\n<p>Similarly, to get the probability of an alarm going off, p(alarm), we add up all the different sets of events that involve an alarm going off - entries 1, 3, 5, and 7 in the table.</p>\n<p>So the entire set of calculations looks like this:</p>\n<ul>\n<li>\n<p>If I hear a burglar alarm, I conclude there was probably (63%) a burglar.&nbsp;</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(b|a)%20=%20\\frac{p(ab)}{p(a)}%20=%20\\frac{.0153}{.000162%20+%20.0151%20+%20.0078%20+%20.00097}%20=%20.63\" alt=\"\" width=\"443\" height=\"44\"></p>\n</li>\n<li>\n<p>If I learn about an earthquake, I conclude there was probably (80%) an alarm.&nbsp;</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(a|e)%20=%20\\frac{p(ae)}{p(e)}%20=%20\\frac{.000162%20+%20.0078}{.000162%20+%20.0078%20+%20.0000085%20+%20.002}%20=%20.8\" alt=\"\" width=\"445\" height=\"44\"></p>\n</li>\n<li>\n<p>I hear about an alarm and then hear about an earthquake; I conclude there was probably (98%) no burglar.&nbsp;</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?\\frac{p(ae\\neg%20b)}{p(ae)}%20=%20\\frac{p(ae\\neg%20b)}{p(aeb)%20+%20p(ae\\neg%20b)}%20=%20\\frac{.0078}{.000162%20+%20.0078}%20=%20.98\" alt=\"\" width=\"417\" height=\"44\"></p>\n</li>\n</ul>\n<p>Thus, a joint probability distribution is indeed capable of <em>representing</em> the reasoning-behaviors we want.</p>\n<p>So is our problem solved? Our work done?</p>\n<p>Not in real life or real Artificial Intelligence work. The problem is that this solution doesn't scale. <em>Boy howdy</em>, does it not scale! If you have a model containing <em>forty</em> binary variables - alert readers may notice that the observed physical universe contains at least forty things - and you try to write out the <em>joint probability distribution</em> over all combinations of those variables, it looks like this:</p>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>.0000000000112</td>\n<td>YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY</td>\n</tr>\n<tr>\n<td>.000000000000034</td>\n<td>YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYN</td>\n</tr>\n<tr>\n<td>.00000000000991</td>\n<td>YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYNY</td>\n</tr>\n<tr>\n<td>.00000000000532</td>\n<td>YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYNN</td>\n</tr>\n<tr>\n<td>.000000000145</td>\n<td>YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYNYY</td>\n</tr>\n<tr>\n<td>&nbsp;...</td>\n<td>...</td>\n</tr>\n</tbody>\n</table>\n<p>(1,099,511,627,776 entries)</p>\n<p>This isn't merely a storage problem. In terms of storage, a trillion entries is just a terabyte or three. The real problem is <em>learning</em> a table like that. You have to deduce 1,099,511,627,776 floating-point probabilities from observed data, and the only constraint on this giant table is that all the probabilities must sum to exactly 1.0, a problem with 1,099,511,627,775 degrees of freedom. (If you know the first 1,099,511,627,775 numbers, you can deduce the 1,099,511,627,776th number using the constraint that they all sum to exactly 1.0.) It's not the storage cost that kills you in a problem with forty variables, it's the difficulty of gathering enough observational data to constrain a trillion different parameters. And in a universe containing <em>seventy</em> things, things are even worse.</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/b/b3/Boromir.png\" alt=\"\" width=\"551\" height=\"325\"></p>\n<p>So instead, suppose we approached the earthquake-burglar problem by trying to specify probabilities in a format where... never mind, it's easier to just give an example before stating abstract rules.</p>\n<p>First let's add, for purposes of further illustration, a new variable, \"Recession\", whether or not there's a depressed economy at the time. Now suppose that:</p>\n<ul>\n<li>\n<p>The probability of an earthquake is 0.01.</p>\n</li>\n<li>\n<p>The probability of a recession at any given time is 0.33 (or 1/3).</p>\n</li>\n<li>\n<p>The probability of a burglary given a recession is 0.04; or, given no recession, 0.01.</p>\n</li>\n<li>\n<p>An earthquake is 0.8 likely to set off your burglar alarm; a burglar is 0.9 likely to set off your burglar alarm. &nbsp;<em>And </em>- we can't compute this model fully without this info - the combination of a burglar <em>and</em> an earthquake is 0.95 likely to set off the alarm; and in the absence of either burglars or earthquakes, your alarm has a 0.001 chance of going off anyway.</p>\n</li>\n</ul>\n<p>\n<table border=\"0\" cellpadding=\"3\">\n<tbody>\n<tr valign=\"top\">\n<td rowspan=\"3\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/8/8d/Alarm.svg\" alt=\"\" width=\"358\" height=\"259\"></td>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(r)</td>\n<td>.33</td>\n</tr>\n<tr>\n<td>p(\u00acr)</td>\n<td>.67</td>\n</tr>\n</tbody>\n</table>\n</td>\n<td rowspan=\"3\">\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(a|be)</td>\n<td>.95</td>\n</tr>\n<tr>\n<td>p(a|b\u00ace)</td>\n<td>.9</td>\n</tr>\n<tr>\n<td>p(a|\u00acbe)</td>\n<td>.797</td>\n</tr>\n<tr>\n<td>p(a|\u00acb\u00ace)</td>\n<td>.001</td>\n</tr>\n<tr>\n<td>p(\u00aca|be)</td>\n<td>.05</td>\n</tr>\n<tr>\n<td>p(\u00aca|b\u00ace)</td>\n<td>.1</td>\n</tr>\n<tr>\n<td>p(\u00aca|\u00acbe)</td>\n<td>.203</td>\n</tr>\n<tr>\n<td>p(\u00aca|\u00acb\u00ace)</td>\n<td>.999</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr valign=\"top\">\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(e)</td>\n<td>.01</td>\n</tr>\n<tr>\n<td>p(\u00ace)</td>\n<td>.99</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr valign=\"top\">\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(b|r)</td>\n<td>.04</td>\n</tr>\n<tr>\n<td>p(b|\u00acr)</td>\n<td>.01</td>\n</tr>\n<tr>\n<td>p(\u00acb|r)</td>\n<td>.96</td>\n</tr>\n<tr>\n<td>p(\u00acb|\u00acr)</td>\n<td>.99</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>According to this model, if you want to know \"The probability that an earthquake occurs\" - just the probability of that one variable, without talking about any others - you can directly look up p(e) = .01. On the other hand, if you want to know the probability of a burglar striking, you have to first look up the probability of a recession (.33), and then p(b|r) and p(b|\u00acr), and sum up p(b|r)*p(r) + p(b|\u00acr)*p(\u00acr) to get a net probability of .01*.66 + .04*.33 = .02 = p(b), a 2% probability that a burglar is around at some random time.</p>\n<p>If we want to compute the joint probability of four values for all four variables - for example, the probability that there is no earthquake <em>and</em> no recession<em> and</em> a burglar <em>and</em> the alarm goes off - this causal model computes this joint probability as the product:</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(\\neg e)p(\\neg r)p(b|\\neg r)p(a|b\\neg e) = .99 * .67 * .01 * .9 = .006 = 0.6%\" alt=\"\" width=\"470\" height=\"19\"></p>\n<p>In general, to go from a <em>causal model</em> to a <em>probability distribution,</em> we compute, for each setting of all the variables, the product</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(\\mathbf{X}=\\mathbf{x})= \\prod_i p(X_i=x_i|\\mathbf{PA_i}=\\mathbf{pa_i})\" alt=\"\"></p>\n<p>multiplying together the conditional probability of each variable <em>given the values of its immediate parents.</em>&nbsp; (If a node has no parents, the probability table for it has just an unconditional probability, like \"the chance of an earthquake is .01\".)</p>\n<p>This is a <em>causal</em> model because it corresponds to a world in which each event is <em>directly</em> caused by only a small set of other events, its parent nodes in the graph. In this model, a recession can <em>indirectly</em> cause an alarm to go off - the recession increases the probability of a burglar, who in turn sets off an alarm - but the recession <em>only</em> acts on the alarm through the <em>intermediate cause</em> of the burglar. &nbsp;(Contrast to a model where recessions set off burglar alarms directly.)</p>\n<table border=\"0\" cellpadding=\"3\">\n<tbody>\n<tr valign=\"center\">\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/71/Indirect.svg\" alt=\"\" width=\"166\" height=\"259\"></td>\n<td>\n<h1>vs.</h1>\n</td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/7a/Direct.svg\" alt=\"\" width=\"171\" height=\"259\"></td>\n</tr>\n</tbody>\n</table>\n<p>The first diagram implies that once we <em>already know</em> whether or not there's a burglar, we don't learn <em>anything more</em> about the probability of a burglar alarm, if we find out that there's a recession:</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(a|b) = p(a|br)\" alt=\"\" width=\"122\" height=\"19\"></p>\n<p>This is a fundamental illustration of <em>the locality of causality -</em>&nbsp;once I know there's a burglar, I know <em>everything I need to know</em>&nbsp;to calculate the probability that there's an alarm. &nbsp;Knowing the state of Burglar&nbsp;<em>screens off</em>&nbsp;anything that Recession could tell me about Alarm - even though, if I <em>didn't </em>know the value of the Burglar variable, Recessions would appear to be statistically correlated with Alarms. &nbsp;The present screens off the past from the future; in a causal system, if you know the <em>exact, complete</em> state of the present, the state of the past has no further physical relevance to computing the future. &nbsp;It's how, in a system containing many correlations (like the recession-alarm correlation), it's still possible to compute each variable just by looking at a small number of immediate neighbors.</p>\n<p>Constraints like this are also how we can store a causal model - and much more importantly,&nbsp;<em>learn</em>&nbsp;a causal model - with many fewer parameters than the naked, raw, joint probability distribution.</p>\n<p>Let's illustrate this using a simplified version of this graph, which only talks about earthquakes and recessions. We could consider three hypothetical causal diagrams over only these two variables:</p>\n<table border=\"0\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td rowspan=\"2\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/6/60/EY13.svg\" alt=\"\" width=\"371\" height=\"62\"></td>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(r)</td>\n<td>0.03</td>\n</tr>\n<tr>\n<td>p(\u00acr)</td>\n<td>0.97</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(e)</td>\n<td>0.29</td>\n</tr>\n<tr>\n<td>p(\u00ace)</td>\n<td>0.71</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" align=\"center\">\n<h2>p(E&amp;R)=p(E)p(R)</h2>\n</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<table border=\"0\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td rowspan=\"2\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/0/02/EY14.svg\" alt=\"\" width=\"192\" height=\"160\"></td>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(e)</td>\n<td>0.29</td>\n</tr>\n<tr>\n<td>p(\u00ace)</td>\n<td>0.71</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(r|e)</td>\n<td>0.15</td>\n</tr>\n<tr>\n<td>p(\u00acr|e)</td>\n<td>0.85</td>\n</tr>\n<tr>\n<td>p(r|\u00ace)</td>\n<td>0.03</td>\n</tr>\n<tr>\n<td>p(\u00acr|\u00ace)</td>\n<td>0.97</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr align=\"center\">\n<td colspan=\"2\">\n<h2>p(E&amp;R) = p(E)p(R|E)</h2>\n</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<table border=\"0\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td rowspan=\"2\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/0/01/EY15.svg\" alt=\"\" width=\"192\" height=\"160\"></td>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(r)</td>\n<td>0.03</td>\n</tr>\n<tr>\n<td>p(\u00acr)</td>\n<td>0.97</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(e|r)</td>\n<td>0.24</td>\n</tr>\n<tr>\n<td>p(\u00ace|r)</td>\n<td>0.76</td>\n</tr>\n<tr>\n<td>p(e|\u00acr)</td>\n<td>0.09</td>\n</tr>\n<tr>\n<td>p(\u00ace|\u00acr)</td>\n<td>0.91</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" align=\"center\">\n<h2>p(E&amp;R) = p(R)p(E|R)</h2>\n</td>\n</tr>\n</tbody>\n</table>\n<p>Let's consider the first hypothesis - that there's no causal arrows connecting earthquakes and recessions. If we build a <em>causal model</em> around this diagram, it has 2 real degrees of freedom - a degree of freedom for saying that the probability of an earthquake is, say, 29% (and hence that the probability of not-earthquake is necessarily 71%), and another degree of freedom for saying that the probability of a recession is 3% (and hence the probability of not-recession is constrained to be 97%).</p>\n<p>On the other hand, the full joint probability distribution would have 3 degrees of freedom - a free choice of (earthquake&amp;recession), a choice of p(earthquake&amp;\u00acrecession), a choice of p(\u00acearthquake&amp;recession), and then a constrained p(\u00acearthquake&amp;\u00acrecession) which must be equal to 1 minus the sum of the other three, so that all four probabilities sum to 1.0.</p>\n<p>By the pigeonhole principle (you can't fit 3 pigeons into 2 pigeonholes) there must be some joint probability distributions which <em>cannot be represented</em> in the first causal structure. This means the first causal structure is <em>falsifiable;</em> there's survey data we can get which would lead us to reject it as a hypothesis. In particular, the first causal model requires:</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(er) = p(e)p(r)\" alt=\"\" width=\"126\" height=\"19\"></p>\n<p>or equivalently</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(e|r) = p(e)\" alt=\"\"></p>\n<p>or equivalently</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(r|e) = p(r)\" alt=\"\" width=\"100\" height=\"19\"></p>\n<p>which is a <em>conditional independence</em> constraint - it says that learning about recessions doesn't tell us anything about the probability of an earthquake or vice versa. If we find that earthquakes and recessions are highly correlated in the observed data - if earthquakes and recessions go together, or earthquakes and the <em>absence</em> of recessions go together - it falsifies the first causal model.</p>\n<p>For example, let's say that in your state, an earthquake is 0.1 probable per year and a recession is 0.2 probable. If we suppose that earthquakes don't cause recessions, earthquakes are not an effect of recessions, and that there aren't hidden aliens which produce both earthquakes and recessions, then we should find that years in which there are <em>earthquakes and recessions</em> happen around 0.02 of the time. If instead earthquakes and recessions happen 0.08 of the time, then the probability of a recession <em>given</em> an earthquake is 0.8 instead of 0.2, and we should much more strongly expect a recession any time we are told that an earthquake has occurred. Given enough samples, this falsifies the theory that these factors are unconnected; or rather, the more samples we have, the more we disbelieve that the two events are unconnected.</p>\n<p>On the other hand, we can't tell apart the second two possibilities from survey data, because both causal models have 3 degrees of freedom, which is the size of the full joint probability distribution. (In general, <em>fully connected</em> causal graphs in which there's a line between every pair of nodes, have the same number of degrees of freedom as a raw joint distribution - and 2 nodes connected by 1 line are \"fully connected\".) We can't tell if earthquakes are 0.1 likely and cause recessions with 0.8 probability, or recessions are 0.2 likely and cause earthquakes with 0.4 probability (or if there are hidden aliens which on 6% of years show up and cause earthquakes and recessions with probability 1).</p>\n<p>With larger universes, the difference between <em>causal models</em> and <em>joint probability distributions</em> becomes a lot more striking. If we're trying to reason about a million binary variables connected in a huge causal model, and each variable could have up to four direct 'parents' - four other variables that <em>directly</em> exert a causal effect on it - then the total number of free parameters would be at most... 16 million!</p>\n<p>The number of free parameters in a raw joint probability distribution over a million binary variables would be 2<sup>1,000,000</sup>. Minus one.</p>\n<p>So causal models which are <em>less</em> than fully connected - in which most objects in the universe are not the direct cause or direct effect of everything else in the universe - are very strongly <em>falsifiable;</em> they only allow probability distributions (hence, observed frequencies) in an infinitesimally tiny range of all possible joint probability tables. Causal models very strongly <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">constrain anticipation</a>&nbsp;- disallow almost all possible patterns of observed frequencies - and gain mighty <a href=\"http://yudkowsky.net/rational/technical\">Bayesian advantages</a>&nbsp;when these predictions come true.</p>\n<p>To see this effect at work, let's consider the <em>three</em> variables Recession, Burglar, and Alarm. \n<table border=\"1\">\n<tbody>\n<!-- Results table headers --> \n<tr>\n<th>Alarm</th> <th>Burglar</th> <th>Recession</th> <th>%</th>\n</tr>\n<tr>\n<td>Y</td>\n<td>Y</td>\n<td>Y</td>\n<td>.012</td>\n</tr>\n<tr>\n<td>N</td>\n<td>Y</td>\n<td>Y</td>\n<td>.0013</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>N</td>\n<td>Y</td>\n<td>.00287</td>\n</tr>\n<tr>\n<td>N</td>\n<td>N</td>\n<td>Y</td>\n<td>.317</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>Y</td>\n<td>N</td>\n<td>.003</td>\n</tr>\n<tr>\n<td>N</td>\n<td>Y</td>\n<td>N</td>\n<td>.000333</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>N</td>\n<td>N</td>\n<td>.00591</td>\n</tr>\n<tr>\n<td>N</td>\n<td>N</td>\n<td>N</td>\n<td>.654</td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>All three variables seem correlated to each other when considered two at a time. For example, if we consider Recessions and Alarms, they should seem correlated because recessions cause burglars which cause alarms. If we learn there was an alarm, for example, we conclude it's more probable that there was a recession. So since all three variables are correlated, can we distinguish between, say, these three causal models?</p>\n<table border=\"0\" cellpadding=\"5\">\n<tbody>\n<tr valign=\"center\">\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/71/Indirect.svg\" alt=\"\" width=\"166\" height=\"259\"></td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/75/EY17.svg\" alt=\"\" width=\"296\" height=\"160\"></td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/c/cf/EY19.svg\" alt=\"\" width=\"166\" height=\"259\"></td>\n</tr>\n<tr valign=\"center\">\n<td align=\"center\"><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(rba) = p(r)p(b|r)p(a|b)\" alt=\"\" width=\"192\" height=\"19\">&nbsp;&nbsp;</td>\n<td align=\"center\"><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(rba)=p(r)p(a)p(b|ra)\" alt=\"\" width=\"189\" height=\"19\">&nbsp;&nbsp;</td>\n<td align=\"center\"><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(rab)=p(r)p(a|r)p(b|a)\" alt=\"\" width=\"194\" height=\"19\">&nbsp;&nbsp;</td>\n</tr>\n</tbody>\n</table>\n<p>Yes we can! Among these causal models, the prediction which only the first model makes, which is not shared by either of the other two, is that <em>once we know whether a burglar is there, </em>we learn nothing <em>more</em> about whether there was an alarm by finding out that there was a recession, since recessions only affect alarms through the intermediary of burglars:</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(a|br) = p(a|b)\" alt=\"\" width=\"122\" height=\"19\"></p>\n<p>But the third model, in which recessions directly cause alarms, which only then cause burglars, does <em>not</em> have this property. If I know that a burglar has appeared, it's likely that an alarm caused the burglar - but it's even <em>more</em> likely that there was an alarm, if there was a recession around to cause the alarm! So the third model predicts:</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(a|br) \\neq p(a|b)\" alt=\"\" width=\"122\" height=\"19\"></p>\n<p>And in the second model, where alarms and recessions both cause burglars, we again don't have the conditional independence. If we know that there's a burglar, then we think that either an alarm or a recession caused it; and if we're told that there's an alarm, we'd conclude it was less likely that there was a recession, since the recession had been explained away.</p>\n<p>(This may seem a bit clearer by considering the scenario B-&gt;A&lt;-E, where burglars and earthquakes both cause alarms. If we're told the value of the bottom node, that there was an alarm, the probability of there being a burglar is <em>not</em> independent of whether we're told there was an earthquake - the two top nodes are <em>not</em> conditionally independent <em>once we condition on the bottom node.</em>)</p>\n<p>On the other hand, we can't tell the difference between:</p>\n<table border=\"0\" cellpadding=\"3\">\n<tbody>\n<tr valign=\"center\">\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/71/Indirect.svg\" alt=\"\" width=\"166\" height=\"259\"></td>\n<td>\n<h1>vs.</h1>\n</td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/d/db/EY3.svg\" alt=\"\" width=\"166\" height=\"259\"></td>\n<td>\n<h1>vs.</h1>\n</td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/3/39/EY4.svg\" alt=\"\" width=\"296\" height=\"160\"></td>\n</tr>\n</tbody>\n</table>\n<p>using <em>only</em> this data and no other variables, because all three causal structures predict the same pattern of conditional dependence and independence - three variables which all appear mutually correlated, but Alarm and Recession become independent once you condition on Burglar.</p>\n<p>Being able to read off patterns of conditional dependence and independence is an art known as \"D-separation\", and if you're good at it you can glance at a diagram like this...</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/2/20/Season.svg\" alt=\"\" width=\"272\" height=\"358\"></p>\n<p>...and see that, once we already know the Season, whether the Sprinkler is on and whether it is Raining are conditionally independent of each other - if we're told that it's Raining we conclude nothing about whether or not the Sprinkler is on. But if we then further observe that the sidewalk is Slippery, then Sprinkler and Rain become conditionally dependent once more, because if the Sidewalk is Slippery then it is probably Wet and this can be explained by either the Sprinkler or the Rain but probably not both, i.e. if we're told that it's Raining we conclude that it's less likely that the Sprinkler was on.</p>\n<hr>\n<p>Okay, back to the obesity-exercise-Internet example. You may recall that we had the following observed frequencies:</p>\n<table border=\"0\">\n</table>\n<table border=\"1\">\n<tbody>\n<!-- Results table headers --> \n<tr>\n<th>Overweight</th> <th>Exercise</th> <th>Internet</th> <th>#</th>\n</tr>\n<tr>\n<td>Y</td>\n<td>Y</td>\n<td>Y</td>\n<td>&nbsp;1,119</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>Y</td>\n<td>N</td>\n<td>&nbsp;16,104</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>N</td>\n<td>Y</td>\n<td>&nbsp;11,121</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>N</td>\n<td>N</td>\n<td>&nbsp;60,032</td>\n</tr>\n<tr>\n<td>N</td>\n<td>Y</td>\n<td>Y</td>\n<td>&nbsp;18,102</td>\n</tr>\n<tr>\n<td>N</td>\n<td>Y</td>\n<td>N</td>\n<td>&nbsp;132,111</td>\n</tr>\n<tr>\n<td>N</td>\n<td>N</td>\n<td>Y</td>\n<td>&nbsp;29,120</td>\n</tr>\n<tr>\n<td>N</td>\n<td>N</td>\n<td>N</td>\n<td>&nbsp;155,033</td>\n</tr>\n</tbody>\n</table>\n<p>Do you see where this is going?</p>\n<p>\"Er,\" you reply, \"Maybe if I had a calculator and ten minutes... you want to just go ahead and spell it out?\"</p>\n<p>Sure! First, we <em>marginalize</em> over the 'exercise' variable to get the table for just weight and Internet use. We do this by taking the 1,119 people who are YYY, overweight and Reddit users and exercising, and the 11,121 people who are overweight and&nbsp;non-exercising and&nbsp;Reddit users, YNY, and adding them together to get 12,240 total people who are overweight Reddit users:</p>\n<table border=\"1\">\n<tbody>\n<!-- Results table headers --> \n<tr>\n<th>Overweight</th> <th>Internet</th> <th>#</th>\n</tr>\n<tr>\n<td>Y</td>\n<td>Y</td>\n<td>12,240</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>N</td>\n<td>76,136</td>\n</tr>\n<tr>\n<td>N</td>\n<td>Y</td>\n<td>47,222</td>\n</tr>\n<tr>\n<td>N</td>\n<td>N</td>\n<td>287,144</td>\n</tr>\n</tbody>\n</table>\n<p>\"And then?\"</p>\n<p>Well, that suggests that the <em>probability</em> of using Reddit, given that your weight is normal, is the <em>same</em> as the probability that you use Reddit, given that you're overweight. 47,222 out of 334,366 normal-weight people use Reddit, and 12,240 out of 88,376 overweight people use Reddit. That's about 14% either way.</p>\n<p>\"And so we conclude?\"</p>\n<p>Well, first we conclude it's not particularly likely that using Reddit causes weight gain, or that being overweight causes people to use Reddit:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/1/15/EY5.svg\" alt=\"\">&nbsp;<img src=\"http://wiki.lesswrong.com/mediawiki/images/4/4a/EY6.svg\" alt=\"\" width=\"195\" height=\"160\"></p>\n<p>If either of those causal links existed, those two variables should be <em>correlated.</em> We shouldn't find the <em>lack of correlation</em> or <em>conditional independence</em> that we just discovered.</p>\n<p>Next, imagine that the real causal graph looked like this:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/5/55/EY7.svg\" alt=\"\" width=\"358\" height=\"160\"></p>\n<p>In this graph, exercising <em>causes</em> you to be less likely to be overweight (due to the virtue theory of metabolism), and exercising <em>causes</em> you to spend less time on the Internet (because you have less time for it).</p>\n<p>But in this case we should <em>not</em> see that the groups who are/aren't overweight have the same probability of spending time on Reddit. There should be an outsized group of people who are both normal-weight and non-Redditors (because they exercise), and an outsized group of non-exercisers who are overweight and Reddit-using.</p>\n<p>So that causal graph is also <em>ruled out</em> by the data, as are others like:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/c/c3/EY8.svg\" alt=\"\" width=\"195\" height=\"259\">&nbsp;<img src=\"http://wiki.lesswrong.com/mediawiki/images/a/a5/EY9.svg\" alt=\"\" width=\"195\" height=\"259\">&nbsp;<img src=\"http://wiki.lesswrong.com/mediawiki/images/0/06/EY18.svg\" alt=\"\" width=\"195\" height=\"259\"></p>\n<p>&nbsp;</p>\n<p>&nbsp;<img src=\"http://wiki.lesswrong.com/mediawiki/images/5/5e/EY11.svg\" alt=\"\"></p>\n<p>Leaving <em>only</em> this causal graph:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/6/66/EY12.svg\" alt=\"\" width=\"358\" height=\"160\"></p>\n<p>Which says that weight and Internet use exert causal effects on exercise, but exercise doesn't causally affect either.</p>\n<p>All this discipline was invented and systematized by Judea Pearl, Peter Spirtes, Thomas Verma, and a number of other people in the 1980s and you should be quite impressed by their accomplishment, because before then, inferring causality from correlation was thought to be a fundamentally unsolvable problem. The standard volume on causal structure is <em>Causality</em>&nbsp;by Judea Pearl.</p>\n<p>Causal <em>models</em> (with specific probabilities attached) are sometimes known as \"Bayesian networks\" or \"Bayes nets\", since they were invented by Bayesians and make use of Bayes's Theorem. They have all sorts of neat computational advantages which are far beyond the scope of this introduction - e.g. in many cases you can split up a Bayesian network into parts, put each of the parts on its own computer processor, and then update on three different pieces of evidence at once using a neatly local message-passing algorithm in which each node talks only to its immediate neighbors and when all the updates are finished propagating the whole network has settled into the correct state. For more on this see Judea Pearl's <em>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</em> which is the original book on Bayes nets and still the best introduction I've personally happened to read.</p>\n<hr>\n<p><a href=\"#note1back\"></a><a name=\"note1\"></a>&nbsp;<a href=\"#note1back\">[1]</a> Somewhat to my own shame, I must admit to ignoring my own observations in this department - even after I saw no discernible effect on my weight or my musculature from aerobic exercise and strength training 2 hours a day 3 times a week, I didn't really start believing that the virtue theory of metabolism was <em>wrong&nbsp;<a name=\"note2back\"></a></em><a href=\"#note2\">[2]</a> until after <a href=\"http://en.wikipedia.org/wiki/Gary_Taubes\">other</a> <a href=\"http://sethroberts.net/\">people</a>&nbsp;had <a href=\"/lw/mb/lonely_dissent/\">started</a> the skeptical dogpile.</p>\n<p><a href=\"#note2back\"></a><a name=\"note2\"></a>&nbsp;<a href=\"#note2back\">[2]</a> I should mention, though, that I have confirmed a personal effect where eating <em>enough </em>cookies (at a convention where no protein is available) will cause weight gain afterward. There's no other discernible correlation between my carbs/protein/fat allocations and weight gain, <em>just</em> that eating sweets in large quantities can cause weight gain afterward. This admittedly does bear with the straight-out virtue theory of metabolism, i.e., eating pleasurable foods is sinful weakness and hence punished with fat.</p>\n<p><a href=\"#note3back\"></a><a name=\"note3\"></a>&nbsp;<a href=\"#note3back\">[3]</a> Or there might be some hidden third factor, a gene which causes both fat and non-exercise. By Occam's Razor this is more complicated and its probability is penalized accordingly, but we can't actually rule it out. It is obviously impossible to do the converse experiment where half the subjects are randomly assigned lower weights, since there's no known intervention which can cause weight loss.</p>\n<hr>\n<p><strong>Mainstream status</strong>: &nbsp;This is meant to be an introduction to completely bog-standard Bayesian networks, causal models, and causal diagrams. &nbsp;Any departures from mainstream academic views are errors and should be flagged accordingly.</p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/ezu/stuff_that_makes_stuff_happen/\">Stuff That Makes Stuff Happen</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/eva/the_fabric_of_real_things/\">The Fabric of Real Things</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"cq69M9ceLNA35ShTR": 9}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hzuSDMx7pd2uxFc5w", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 114, "baseScore": 122, "extendedScore": null, "score": 0.000265, "legacy": true, "legacyId": "19263", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "http://wiki.lesswrong.com/mediawiki/images/f/fc/EY1.svg", "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "stuff-that-makes-stuff-happen", "canonicalPrevPostSlug": "the-fabric-of-real-things", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 122, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Suppose a general-population survey shows that people who exercise less, weigh more. You don't have any known direction of <em>time</em> in the data - you don't know which came first, the increased weight or the diminished exercise. And you didn't randomly assign half the population to exercise less; you just surveyed an existing population.</p>\n<p>The statisticians who discovered causality were trying to find a way to distinguish, within survey data, the direction of cause and effect - whether, as common sense would have it, more obese people exercise less <em>because</em> they find physical activity less rewarding; or whether, as in the <a href=\"http://en.wikipedia.org/wiki/Just-world_hypothesis\">virtue theory of metabolism</a>, lack of exercise actually <em>causes</em> weight gain due to divine punishment for the sin of sloth.</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/f/fc/EY1.svg\" alt=\"\"></td>\n<td valign=\"center\">\n<h1 id=\"_vs__\"><strong>&nbsp;vs.&nbsp;</strong></h1>\n</td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/0/09/EY2.svg\" alt=\"\"></td>\n</tr>\n</tbody>\n</table>\n<p>The usual way to resolve this sort of question is by <em>randomized</em> <em>intervention.</em> If you randomly assign half your experimental subjects to exercise more, and afterward the increased-exercise group doesn't lose any weight compared to the control group&nbsp;<a name=\"note1back\"></a><a href=\"#note1\">[1]</a>, you could rule out causality <em>from</em> exercise <em>to</em> weight, and conclude that the correlation between weight and exercise is probably due to physical activity being less fun when you're overweight <a name=\"note3back\"></a><a href=\"#note3\">[3]</a>. The question is whether you can get causal data <em>without</em> interventions.</p>\n<p>For a long time, the conventional wisdom in philosophy was that this was impossible unless you knew the direction of time and knew which event had happened first. Among some philosophers of science, there was a belief that the \"direction of causality\" was a <em>meaningless</em> question, and that in the universe itself there were <em>only</em> correlations - that \"cause and effect\" was something unobservable and undefinable, that only unsophisticated non-statisticians believed in due to their lack of formal training:</p>\n<p style=\"padding-left: 30px;\">\"The law of causality, I believe, like much that passes muster among philosophers, is a relic of a bygone age, surviving, like the monarchy, only because it is erroneously supposed to do no harm.\" -- Bertrand Russell (he later changed his mind)</p>\n<p style=\"padding-left: 30px;\">\"Beyond such discarded fundamentals as 'matter' and 'force' lies still another fetish among the inscrutable arcana of modern science, namely, the category of cause and effect.\" -- Karl Pearson</p>\n<p>The famous statistician Fisher, who was also a smoker, testified before Congress that the correlation between smoking and lung cancer couldn't prove that the former caused the latter. &nbsp;We have remnants of this type of reasoning in old-school \"Correlation does not imply causation\", without the now-standard appendix, \"<a href=\"http://xkcd.com/552/\">But it sure is a hint</a>\".</p>\n<p>This skepticism was overturned by a surprisingly simple mathematical observation.<a id=\"more\"></a></p>\n<p>Let's say there are three variables in the survey data: Weight, how much the person exercises, and how much time they spend on the Internet.</p>\n<p>For simplicity, we'll have these three variables be binary, yes-or-no observations: Y or N for whether the person has a BMI over 25, Y or N for whether they exercised at least twice in the last week, and Y or N for whether they've checked Reddit in the last 72 hours.</p>\n<p>Now let's say our gathered data looks like this:</p>\n<table border=\"1\">\n<tbody>\n<!-- Results table headers --> \n<tr>\n<th>Overweight</th> <th>Exercise</th> <th>Internet</th> <th>#</th>\n</tr>\n<tr>\n<td>Y</td>\n<td>Y</td>\n<td>Y</td>\n<td>&nbsp;1,119</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>Y</td>\n<td>N</td>\n<td>&nbsp;16,104</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>N</td>\n<td>Y</td>\n<td>&nbsp;11,121</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>N</td>\n<td>N</td>\n<td>&nbsp;60,032</td>\n</tr>\n<tr>\n<td>N</td>\n<td>Y</td>\n<td>Y</td>\n<td>&nbsp;18,102</td>\n</tr>\n<tr>\n<td>N</td>\n<td>Y</td>\n<td>N</td>\n<td>&nbsp;132,111</td>\n</tr>\n<tr>\n<td>N</td>\n<td>N</td>\n<td>Y</td>\n<td>&nbsp;29,120</td>\n</tr>\n<tr>\n<td>N</td>\n<td>N</td>\n<td>N</td>\n<td>&nbsp;155,033</td>\n</tr>\n</tbody>\n</table>\n<p>And lo, merely by eyeballing this data -</p>\n<p>(which is <em>totally made up</em>, so don't go actually <em>believing </em>the conclusion I'm about to draw)</p>\n<p>- we now realize that <em>being overweight and spending time on the Internet both cause you to exercise less,</em> presumably because exercise is less fun and you have more alternative things to do,<em> but exercising has no causal influence on body weight or Internet use.</em></p>\n<p>\"What!\" you cry. \"How can you tell <em>that</em> just by inspecting those numbers? You can't say that exercise isn't <em>correlated</em> to body weight - if you just look at all the members of the population who exercise, they clearly have lower weights. 10%&nbsp;of exercisers are overweight, vs. 28%&nbsp;of non-exercisers. &nbsp;How could you rule out the obvious causal explanation for that correlation, just by looking at this data?\"</p>\n<hr>\n<p>There's a wee bit of math involved. &nbsp;It's <em>simple</em> math - the part we'll use doesn't involve solving equations or complicated proofs -but we do have to introduce a wee bit of novel math to explain how the heck we got there from here.</p>\n<p>Let me start with a question that turned out - to the surprise of many investigators involved - to be highly related to the issue we've just addressed.</p>\n<p>Suppose that earthquakes and burglars can both set off burglar alarms. &nbsp;If the burglar alarm in your house goes off, it might be because of an actual burglar, but it might <em>also </em>be because a minor earthquake rocked your house and triggered a few sensors. Early investigators in Artificial Intelligence, who were trying to represent all high-level events using primitive tokens in a first-order logic (for reasons of historical stupidity we won't go into) were stymied by the following apparent paradox:</p>\n<ul>\n<li>\n<p>If you tell me that my burglar alarm went off, I infer a burglar, which I will represent in my first-order-logical database using a theorem&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\u22a2</span>&nbsp;ALARM&nbsp;\u2192&nbsp;BURGLAR. (The symbol \"<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\u22a2</span>\" is called \"<a href=\"http://en.wikipedia.org/wiki/Turnstile_(symbol)\">turnstile</a>\" and means \"the logical system asserts that\".)</p>\n</li>\n<li>\n<p>If an earthquake occurs, it will set off burglar alarms. I shall represent this using the theorem&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\u22a2</span>&nbsp;EARTHQUAKE&nbsp;\u2192&nbsp;ALARM, or \"earthquake implies alarm\".</p>\n</li>\n<li>\n<p>If you tell me that my alarm went off, and then further tell me that an earthquake occurred, it <em>explains away</em> my burglar alarm going off. I don't need to explain the alarm by a burglar, because the alarm has already been explained by the earthquake. I conclude there was no burglar. I shall represent this by adding a theorem which says&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\u22a2</span>&nbsp;(EARTHQUAKE &amp; ALARM)&nbsp;\u2192&nbsp;NOT BURGLAR.</p>\n</li>\n</ul>\n<p>Which represents a logical contradiction, and for a while there were attempts to develop \"non-monotonic logics\" so that you could retract conclusions given additional data. <a href=\"/lw/vt/the_nature_of_logic/\">This didn't work very well, since the underlying structure of reasoning was a terrible fit for the structure of classical logic, even when mutated.</a></p>\n<p>Just changing certainties to quantitative probabilities can fix many problems with classical logic, and one might think that this case was likewise easily fixed.</p>\n<p>Namely, just write a probability table of all possible combinations of earthquake or \u00acearthquake, burglar or \u00acburglar, and alarm or \u00acalarm (where&nbsp;\u00ac is the logical negation symbol), with the following entries:</p>\n<table border=\"1\">\n<tbody>\n<!-- Results table headers --> \n<tr>\n<th>Burglar</th> <th>Earthquake</th> <th>Alarm</th> <th>%</th>\n</tr>\n<tr>\n<td>b</td>\n<td>e</td>\n<td>a</td>\n<td>.000162</td>\n</tr>\n<tr>\n<td>b</td>\n<td>e</td>\n<td>\u00aca</td>\n<td>.0000085</td>\n</tr>\n<tr>\n<td>b</td>\n<td>\u00ace</td>\n<td>a</td>\n<td>.0151</td>\n</tr>\n<tr>\n<td>b</td>\n<td>\u00ace</td>\n<td>\u00aca</td>\n<td>.00168</td>\n</tr>\n<tr>\n<td>\u00acb</td>\n<td>e</td>\n<td>a</td>\n<td>.0078</td>\n</tr>\n<tr>\n<td>\u00acb</td>\n<td>e</td>\n<td>\u00aca</td>\n<td>.002</td>\n</tr>\n<tr>\n<td>\u00acb</td>\n<td>\u00ace</td>\n<td>a</td>\n<td>.00097</td>\n</tr>\n<tr>\n<td>\u00acb</td>\n<td>\u00ace</td>\n<td>\u00aca</td>\n<td>.972</td>\n</tr>\n</tbody>\n</table>\n<p>Using the operations of <em>marginalization</em> and <em>conditionalization,</em> we get the desired reasoning back out:</p>\n<p>Let's start with the <em>probability of a burglar given an alarm,</em> p(burglar|alarm). By the law of conditional probability,</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(b|a)%20=%20\\frac{p(ab)}{p(a)}\" alt=\"\" width=\"112\" height=\"44\"></p>\n<p>i.e. the relative fraction of cases where there's an alarm <em>and</em> a burglar, within the set of all cases where there's an alarm.</p>\n<p>The table doesn't directly tell us p(alarm &amp; burglar)/p(alarm), but by the law of marginal probability,</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(ab)%20=%20p(abe)%20+%20p(ab\\neg%20e)%20=%20.000162%20+%20.0151%20=%20.0153\" alt=\"\" width=\"405\" height=\"19\"></p>\n<p>Similarly, to get the probability of an alarm going off, p(alarm), we add up all the different sets of events that involve an alarm going off - entries 1, 3, 5, and 7 in the table.</p>\n<p>So the entire set of calculations looks like this:</p>\n<ul>\n<li>\n<p>If I hear a burglar alarm, I conclude there was probably (63%) a burglar.&nbsp;</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(b|a)%20=%20\\frac{p(ab)}{p(a)}%20=%20\\frac{.0153}{.000162%20+%20.0151%20+%20.0078%20+%20.00097}%20=%20.63\" alt=\"\" width=\"443\" height=\"44\"></p>\n</li>\n<li>\n<p>If I learn about an earthquake, I conclude there was probably (80%) an alarm.&nbsp;</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(a|e)%20=%20\\frac{p(ae)}{p(e)}%20=%20\\frac{.000162%20+%20.0078}{.000162%20+%20.0078%20+%20.0000085%20+%20.002}%20=%20.8\" alt=\"\" width=\"445\" height=\"44\"></p>\n</li>\n<li>\n<p>I hear about an alarm and then hear about an earthquake; I conclude there was probably (98%) no burglar.&nbsp;</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?\\frac{p(ae\\neg%20b)}{p(ae)}%20=%20\\frac{p(ae\\neg%20b)}{p(aeb)%20+%20p(ae\\neg%20b)}%20=%20\\frac{.0078}{.000162%20+%20.0078}%20=%20.98\" alt=\"\" width=\"417\" height=\"44\"></p>\n</li>\n</ul>\n<p>Thus, a joint probability distribution is indeed capable of <em>representing</em> the reasoning-behaviors we want.</p>\n<p>So is our problem solved? Our work done?</p>\n<p>Not in real life or real Artificial Intelligence work. The problem is that this solution doesn't scale. <em>Boy howdy</em>, does it not scale! If you have a model containing <em>forty</em> binary variables - alert readers may notice that the observed physical universe contains at least forty things - and you try to write out the <em>joint probability distribution</em> over all combinations of those variables, it looks like this:</p>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>.0000000000112</td>\n<td>YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY</td>\n</tr>\n<tr>\n<td>.000000000000034</td>\n<td>YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYN</td>\n</tr>\n<tr>\n<td>.00000000000991</td>\n<td>YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYNY</td>\n</tr>\n<tr>\n<td>.00000000000532</td>\n<td>YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYNN</td>\n</tr>\n<tr>\n<td>.000000000145</td>\n<td>YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYNYY</td>\n</tr>\n<tr>\n<td>&nbsp;...</td>\n<td>...</td>\n</tr>\n</tbody>\n</table>\n<p>(1,099,511,627,776 entries)</p>\n<p>This isn't merely a storage problem. In terms of storage, a trillion entries is just a terabyte or three. The real problem is <em>learning</em> a table like that. You have to deduce 1,099,511,627,776 floating-point probabilities from observed data, and the only constraint on this giant table is that all the probabilities must sum to exactly 1.0, a problem with 1,099,511,627,775 degrees of freedom. (If you know the first 1,099,511,627,775 numbers, you can deduce the 1,099,511,627,776th number using the constraint that they all sum to exactly 1.0.) It's not the storage cost that kills you in a problem with forty variables, it's the difficulty of gathering enough observational data to constrain a trillion different parameters. And in a universe containing <em>seventy</em> things, things are even worse.</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/b/b3/Boromir.png\" alt=\"\" width=\"551\" height=\"325\"></p>\n<p>So instead, suppose we approached the earthquake-burglar problem by trying to specify probabilities in a format where... never mind, it's easier to just give an example before stating abstract rules.</p>\n<p>First let's add, for purposes of further illustration, a new variable, \"Recession\", whether or not there's a depressed economy at the time. Now suppose that:</p>\n<ul>\n<li>\n<p>The probability of an earthquake is 0.01.</p>\n</li>\n<li>\n<p>The probability of a recession at any given time is 0.33 (or 1/3).</p>\n</li>\n<li>\n<p>The probability of a burglary given a recession is 0.04; or, given no recession, 0.01.</p>\n</li>\n<li>\n<p>An earthquake is 0.8 likely to set off your burglar alarm; a burglar is 0.9 likely to set off your burglar alarm. &nbsp;<em>And </em>- we can't compute this model fully without this info - the combination of a burglar <em>and</em> an earthquake is 0.95 likely to set off the alarm; and in the absence of either burglars or earthquakes, your alarm has a 0.001 chance of going off anyway.</p>\n</li>\n</ul>\n<p>\n</p><table border=\"0\" cellpadding=\"3\">\n<tbody>\n<tr valign=\"top\">\n<td rowspan=\"3\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/8/8d/Alarm.svg\" alt=\"\" width=\"358\" height=\"259\"></td>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(r)</td>\n<td>.33</td>\n</tr>\n<tr>\n<td>p(\u00acr)</td>\n<td>.67</td>\n</tr>\n</tbody>\n</table>\n</td>\n<td rowspan=\"3\">\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(a|be)</td>\n<td>.95</td>\n</tr>\n<tr>\n<td>p(a|b\u00ace)</td>\n<td>.9</td>\n</tr>\n<tr>\n<td>p(a|\u00acbe)</td>\n<td>.797</td>\n</tr>\n<tr>\n<td>p(a|\u00acb\u00ace)</td>\n<td>.001</td>\n</tr>\n<tr>\n<td>p(\u00aca|be)</td>\n<td>.05</td>\n</tr>\n<tr>\n<td>p(\u00aca|b\u00ace)</td>\n<td>.1</td>\n</tr>\n<tr>\n<td>p(\u00aca|\u00acbe)</td>\n<td>.203</td>\n</tr>\n<tr>\n<td>p(\u00aca|\u00acb\u00ace)</td>\n<td>.999</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr valign=\"top\">\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(e)</td>\n<td>.01</td>\n</tr>\n<tr>\n<td>p(\u00ace)</td>\n<td>.99</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr valign=\"top\">\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(b|r)</td>\n<td>.04</td>\n</tr>\n<tr>\n<td>p(b|\u00acr)</td>\n<td>.01</td>\n</tr>\n<tr>\n<td>p(\u00acb|r)</td>\n<td>.96</td>\n</tr>\n<tr>\n<td>p(\u00acb|\u00acr)</td>\n<td>.99</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n</tbody>\n</table>\n<p></p>\n<p>According to this model, if you want to know \"The probability that an earthquake occurs\" - just the probability of that one variable, without talking about any others - you can directly look up p(e) = .01. On the other hand, if you want to know the probability of a burglar striking, you have to first look up the probability of a recession (.33), and then p(b|r) and p(b|\u00acr), and sum up p(b|r)*p(r) + p(b|\u00acr)*p(\u00acr) to get a net probability of .01*.66 + .04*.33 = .02 = p(b), a 2% probability that a burglar is around at some random time.</p>\n<p>If we want to compute the joint probability of four values for all four variables - for example, the probability that there is no earthquake <em>and</em> no recession<em> and</em> a burglar <em>and</em> the alarm goes off - this causal model computes this joint probability as the product:</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(\\neg e)p(\\neg r)p(b|\\neg r)p(a|b\\neg e) = .99 * .67 * .01 * .9 = .006 = 0.6%\" alt=\"\" width=\"470\" height=\"19\"></p>\n<p>In general, to go from a <em>causal model</em> to a <em>probability distribution,</em> we compute, for each setting of all the variables, the product</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(\\mathbf{X}=\\mathbf{x})= \\prod_i p(X_i=x_i|\\mathbf{PA_i}=\\mathbf{pa_i})\" alt=\"\"></p>\n<p>multiplying together the conditional probability of each variable <em>given the values of its immediate parents.</em>&nbsp; (If a node has no parents, the probability table for it has just an unconditional probability, like \"the chance of an earthquake is .01\".)</p>\n<p>This is a <em>causal</em> model because it corresponds to a world in which each event is <em>directly</em> caused by only a small set of other events, its parent nodes in the graph. In this model, a recession can <em>indirectly</em> cause an alarm to go off - the recession increases the probability of a burglar, who in turn sets off an alarm - but the recession <em>only</em> acts on the alarm through the <em>intermediate cause</em> of the burglar. &nbsp;(Contrast to a model where recessions set off burglar alarms directly.)</p>\n<table border=\"0\" cellpadding=\"3\">\n<tbody>\n<tr valign=\"center\">\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/71/Indirect.svg\" alt=\"\" width=\"166\" height=\"259\"></td>\n<td>\n<h1 id=\"vs_\">vs.</h1>\n</td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/7a/Direct.svg\" alt=\"\" width=\"171\" height=\"259\"></td>\n</tr>\n</tbody>\n</table>\n<p>The first diagram implies that once we <em>already know</em> whether or not there's a burglar, we don't learn <em>anything more</em> about the probability of a burglar alarm, if we find out that there's a recession:</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(a|b) = p(a|br)\" alt=\"\" width=\"122\" height=\"19\"></p>\n<p>This is a fundamental illustration of <em>the locality of causality -</em>&nbsp;once I know there's a burglar, I know <em>everything I need to know</em>&nbsp;to calculate the probability that there's an alarm. &nbsp;Knowing the state of Burglar&nbsp;<em>screens off</em>&nbsp;anything that Recession could tell me about Alarm - even though, if I <em>didn't </em>know the value of the Burglar variable, Recessions would appear to be statistically correlated with Alarms. &nbsp;The present screens off the past from the future; in a causal system, if you know the <em>exact, complete</em> state of the present, the state of the past has no further physical relevance to computing the future. &nbsp;It's how, in a system containing many correlations (like the recession-alarm correlation), it's still possible to compute each variable just by looking at a small number of immediate neighbors.</p>\n<p>Constraints like this are also how we can store a causal model - and much more importantly,&nbsp;<em>learn</em>&nbsp;a causal model - with many fewer parameters than the naked, raw, joint probability distribution.</p>\n<p>Let's illustrate this using a simplified version of this graph, which only talks about earthquakes and recessions. We could consider three hypothetical causal diagrams over only these two variables:</p>\n<table border=\"0\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td rowspan=\"2\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/6/60/EY13.svg\" alt=\"\" width=\"371\" height=\"62\"></td>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(r)</td>\n<td>0.03</td>\n</tr>\n<tr>\n<td>p(\u00acr)</td>\n<td>0.97</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(e)</td>\n<td>0.29</td>\n</tr>\n<tr>\n<td>p(\u00ace)</td>\n<td>0.71</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" align=\"center\">\n<h2 id=\"p_E_R__p_E_p_R_\">p(E&amp;R)=p(E)p(R)</h2>\n</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<table border=\"0\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td rowspan=\"2\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/0/02/EY14.svg\" alt=\"\" width=\"192\" height=\"160\"></td>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(e)</td>\n<td>0.29</td>\n</tr>\n<tr>\n<td>p(\u00ace)</td>\n<td>0.71</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(r|e)</td>\n<td>0.15</td>\n</tr>\n<tr>\n<td>p(\u00acr|e)</td>\n<td>0.85</td>\n</tr>\n<tr>\n<td>p(r|\u00ace)</td>\n<td>0.03</td>\n</tr>\n<tr>\n<td>p(\u00acr|\u00ace)</td>\n<td>0.97</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr align=\"center\">\n<td colspan=\"2\">\n<h2 id=\"p_E_R____p_E_p_R_E_\">p(E&amp;R) = p(E)p(R|E)</h2>\n</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<table border=\"0\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td rowspan=\"2\"><img src=\"http://wiki.lesswrong.com/mediawiki/images/0/01/EY15.svg\" alt=\"\" width=\"192\" height=\"160\"></td>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(r)</td>\n<td>0.03</td>\n</tr>\n<tr>\n<td>p(\u00acr)</td>\n<td>0.97</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(e|r)</td>\n<td>0.24</td>\n</tr>\n<tr>\n<td>p(\u00ace|r)</td>\n<td>0.76</td>\n</tr>\n<tr>\n<td>p(e|\u00acr)</td>\n<td>0.09</td>\n</tr>\n<tr>\n<td>p(\u00ace|\u00acr)</td>\n<td>0.91</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\" align=\"center\">\n<h2 id=\"p_E_R____p_R_p_E_R_\">p(E&amp;R) = p(R)p(E|R)</h2>\n</td>\n</tr>\n</tbody>\n</table>\n<p>Let's consider the first hypothesis - that there's no causal arrows connecting earthquakes and recessions. If we build a <em>causal model</em> around this diagram, it has 2 real degrees of freedom - a degree of freedom for saying that the probability of an earthquake is, say, 29% (and hence that the probability of not-earthquake is necessarily 71%), and another degree of freedom for saying that the probability of a recession is 3% (and hence the probability of not-recession is constrained to be 97%).</p>\n<p>On the other hand, the full joint probability distribution would have 3 degrees of freedom - a free choice of (earthquake&amp;recession), a choice of p(earthquake&amp;\u00acrecession), a choice of p(\u00acearthquake&amp;recession), and then a constrained p(\u00acearthquake&amp;\u00acrecession) which must be equal to 1 minus the sum of the other three, so that all four probabilities sum to 1.0.</p>\n<p>By the pigeonhole principle (you can't fit 3 pigeons into 2 pigeonholes) there must be some joint probability distributions which <em>cannot be represented</em> in the first causal structure. This means the first causal structure is <em>falsifiable;</em> there's survey data we can get which would lead us to reject it as a hypothesis. In particular, the first causal model requires:</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(er) = p(e)p(r)\" alt=\"\" width=\"126\" height=\"19\"></p>\n<p>or equivalently</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(e|r) = p(e)\" alt=\"\"></p>\n<p>or equivalently</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(r|e) = p(r)\" alt=\"\" width=\"100\" height=\"19\"></p>\n<p>which is a <em>conditional independence</em> constraint - it says that learning about recessions doesn't tell us anything about the probability of an earthquake or vice versa. If we find that earthquakes and recessions are highly correlated in the observed data - if earthquakes and recessions go together, or earthquakes and the <em>absence</em> of recessions go together - it falsifies the first causal model.</p>\n<p>For example, let's say that in your state, an earthquake is 0.1 probable per year and a recession is 0.2 probable. If we suppose that earthquakes don't cause recessions, earthquakes are not an effect of recessions, and that there aren't hidden aliens which produce both earthquakes and recessions, then we should find that years in which there are <em>earthquakes and recessions</em> happen around 0.02 of the time. If instead earthquakes and recessions happen 0.08 of the time, then the probability of a recession <em>given</em> an earthquake is 0.8 instead of 0.2, and we should much more strongly expect a recession any time we are told that an earthquake has occurred. Given enough samples, this falsifies the theory that these factors are unconnected; or rather, the more samples we have, the more we disbelieve that the two events are unconnected.</p>\n<p>On the other hand, we can't tell apart the second two possibilities from survey data, because both causal models have 3 degrees of freedom, which is the size of the full joint probability distribution. (In general, <em>fully connected</em> causal graphs in which there's a line between every pair of nodes, have the same number of degrees of freedom as a raw joint distribution - and 2 nodes connected by 1 line are \"fully connected\".) We can't tell if earthquakes are 0.1 likely and cause recessions with 0.8 probability, or recessions are 0.2 likely and cause earthquakes with 0.4 probability (or if there are hidden aliens which on 6% of years show up and cause earthquakes and recessions with probability 1).</p>\n<p>With larger universes, the difference between <em>causal models</em> and <em>joint probability distributions</em> becomes a lot more striking. If we're trying to reason about a million binary variables connected in a huge causal model, and each variable could have up to four direct 'parents' - four other variables that <em>directly</em> exert a causal effect on it - then the total number of free parameters would be at most... 16 million!</p>\n<p>The number of free parameters in a raw joint probability distribution over a million binary variables would be 2<sup>1,000,000</sup>. Minus one.</p>\n<p>So causal models which are <em>less</em> than fully connected - in which most objects in the universe are not the direct cause or direct effect of everything else in the universe - are very strongly <em>falsifiable;</em> they only allow probability distributions (hence, observed frequencies) in an infinitesimally tiny range of all possible joint probability tables. Causal models very strongly <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">constrain anticipation</a>&nbsp;- disallow almost all possible patterns of observed frequencies - and gain mighty <a href=\"http://yudkowsky.net/rational/technical\">Bayesian advantages</a>&nbsp;when these predictions come true.</p>\n<p>To see this effect at work, let's consider the <em>three</em> variables Recession, Burglar, and Alarm. \n</p><table border=\"1\">\n<tbody>\n<!-- Results table headers --> \n<tr>\n<th>Alarm</th> <th>Burglar</th> <th>Recession</th> <th>%</th>\n</tr>\n<tr>\n<td>Y</td>\n<td>Y</td>\n<td>Y</td>\n<td>.012</td>\n</tr>\n<tr>\n<td>N</td>\n<td>Y</td>\n<td>Y</td>\n<td>.0013</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>N</td>\n<td>Y</td>\n<td>.00287</td>\n</tr>\n<tr>\n<td>N</td>\n<td>N</td>\n<td>Y</td>\n<td>.317</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>Y</td>\n<td>N</td>\n<td>.003</td>\n</tr>\n<tr>\n<td>N</td>\n<td>Y</td>\n<td>N</td>\n<td>.000333</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>N</td>\n<td>N</td>\n<td>.00591</td>\n</tr>\n<tr>\n<td>N</td>\n<td>N</td>\n<td>N</td>\n<td>.654</td>\n</tr>\n</tbody>\n</table>\n<p></p>\n<p>All three variables seem correlated to each other when considered two at a time. For example, if we consider Recessions and Alarms, they should seem correlated because recessions cause burglars which cause alarms. If we learn there was an alarm, for example, we conclude it's more probable that there was a recession. So since all three variables are correlated, can we distinguish between, say, these three causal models?</p>\n<table border=\"0\" cellpadding=\"5\">\n<tbody>\n<tr valign=\"center\">\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/71/Indirect.svg\" alt=\"\" width=\"166\" height=\"259\"></td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/75/EY17.svg\" alt=\"\" width=\"296\" height=\"160\"></td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/c/cf/EY19.svg\" alt=\"\" width=\"166\" height=\"259\"></td>\n</tr>\n<tr valign=\"center\">\n<td align=\"center\"><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(rba) = p(r)p(b|r)p(a|b)\" alt=\"\" width=\"192\" height=\"19\">&nbsp;&nbsp;</td>\n<td align=\"center\"><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(rba)=p(r)p(a)p(b|ra)\" alt=\"\" width=\"189\" height=\"19\">&nbsp;&nbsp;</td>\n<td align=\"center\"><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(rab)=p(r)p(a|r)p(b|a)\" alt=\"\" width=\"194\" height=\"19\">&nbsp;&nbsp;</td>\n</tr>\n</tbody>\n</table>\n<p>Yes we can! Among these causal models, the prediction which only the first model makes, which is not shared by either of the other two, is that <em>once we know whether a burglar is there, </em>we learn nothing <em>more</em> about whether there was an alarm by finding out that there was a recession, since recessions only affect alarms through the intermediary of burglars:</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(a|br) = p(a|b)\" alt=\"\" width=\"122\" height=\"19\"></p>\n<p>But the third model, in which recessions directly cause alarms, which only then cause burglars, does <em>not</em> have this property. If I know that a burglar has appeared, it's likely that an alarm caused the burglar - but it's even <em>more</em> likely that there was an alarm, if there was a recession around to cause the alarm! So the third model predicts:</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(a|br) \\neq p(a|b)\" alt=\"\" width=\"122\" height=\"19\"></p>\n<p>And in the second model, where alarms and recessions both cause burglars, we again don't have the conditional independence. If we know that there's a burglar, then we think that either an alarm or a recession caused it; and if we're told that there's an alarm, we'd conclude it was less likely that there was a recession, since the recession had been explained away.</p>\n<p>(This may seem a bit clearer by considering the scenario B-&gt;A&lt;-E, where burglars and earthquakes both cause alarms. If we're told the value of the bottom node, that there was an alarm, the probability of there being a burglar is <em>not</em> independent of whether we're told there was an earthquake - the two top nodes are <em>not</em> conditionally independent <em>once we condition on the bottom node.</em>)</p>\n<p>On the other hand, we can't tell the difference between:</p>\n<table border=\"0\" cellpadding=\"3\">\n<tbody>\n<tr valign=\"center\">\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/71/Indirect.svg\" alt=\"\" width=\"166\" height=\"259\"></td>\n<td>\n<h1 id=\"vs_1\">vs.</h1>\n</td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/d/db/EY3.svg\" alt=\"\" width=\"166\" height=\"259\"></td>\n<td>\n<h1 id=\"vs_2\">vs.</h1>\n</td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/3/39/EY4.svg\" alt=\"\" width=\"296\" height=\"160\"></td>\n</tr>\n</tbody>\n</table>\n<p>using <em>only</em> this data and no other variables, because all three causal structures predict the same pattern of conditional dependence and independence - three variables which all appear mutually correlated, but Alarm and Recession become independent once you condition on Burglar.</p>\n<p>Being able to read off patterns of conditional dependence and independence is an art known as \"D-separation\", and if you're good at it you can glance at a diagram like this...</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/2/20/Season.svg\" alt=\"\" width=\"272\" height=\"358\"></p>\n<p>...and see that, once we already know the Season, whether the Sprinkler is on and whether it is Raining are conditionally independent of each other - if we're told that it's Raining we conclude nothing about whether or not the Sprinkler is on. But if we then further observe that the sidewalk is Slippery, then Sprinkler and Rain become conditionally dependent once more, because if the Sidewalk is Slippery then it is probably Wet and this can be explained by either the Sprinkler or the Rain but probably not both, i.e. if we're told that it's Raining we conclude that it's less likely that the Sprinkler was on.</p>\n<hr>\n<p>Okay, back to the obesity-exercise-Internet example. You may recall that we had the following observed frequencies:</p>\n<table border=\"0\">\n</table>\n<table border=\"1\">\n<tbody>\n<!-- Results table headers --> \n<tr>\n<th>Overweight</th> <th>Exercise</th> <th>Internet</th> <th>#</th>\n</tr>\n<tr>\n<td>Y</td>\n<td>Y</td>\n<td>Y</td>\n<td>&nbsp;1,119</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>Y</td>\n<td>N</td>\n<td>&nbsp;16,104</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>N</td>\n<td>Y</td>\n<td>&nbsp;11,121</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>N</td>\n<td>N</td>\n<td>&nbsp;60,032</td>\n</tr>\n<tr>\n<td>N</td>\n<td>Y</td>\n<td>Y</td>\n<td>&nbsp;18,102</td>\n</tr>\n<tr>\n<td>N</td>\n<td>Y</td>\n<td>N</td>\n<td>&nbsp;132,111</td>\n</tr>\n<tr>\n<td>N</td>\n<td>N</td>\n<td>Y</td>\n<td>&nbsp;29,120</td>\n</tr>\n<tr>\n<td>N</td>\n<td>N</td>\n<td>N</td>\n<td>&nbsp;155,033</td>\n</tr>\n</tbody>\n</table>\n<p>Do you see where this is going?</p>\n<p>\"Er,\" you reply, \"Maybe if I had a calculator and ten minutes... you want to just go ahead and spell it out?\"</p>\n<p>Sure! First, we <em>marginalize</em> over the 'exercise' variable to get the table for just weight and Internet use. We do this by taking the 1,119 people who are YYY, overweight and Reddit users and exercising, and the 11,121 people who are overweight and&nbsp;non-exercising and&nbsp;Reddit users, YNY, and adding them together to get 12,240 total people who are overweight Reddit users:</p>\n<table border=\"1\">\n<tbody>\n<!-- Results table headers --> \n<tr>\n<th>Overweight</th> <th>Internet</th> <th>#</th>\n</tr>\n<tr>\n<td>Y</td>\n<td>Y</td>\n<td>12,240</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>N</td>\n<td>76,136</td>\n</tr>\n<tr>\n<td>N</td>\n<td>Y</td>\n<td>47,222</td>\n</tr>\n<tr>\n<td>N</td>\n<td>N</td>\n<td>287,144</td>\n</tr>\n</tbody>\n</table>\n<p>\"And then?\"</p>\n<p>Well, that suggests that the <em>probability</em> of using Reddit, given that your weight is normal, is the <em>same</em> as the probability that you use Reddit, given that you're overweight. 47,222 out of 334,366 normal-weight people use Reddit, and 12,240 out of 88,376 overweight people use Reddit. That's about 14% either way.</p>\n<p>\"And so we conclude?\"</p>\n<p>Well, first we conclude it's not particularly likely that using Reddit causes weight gain, or that being overweight causes people to use Reddit:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/1/15/EY5.svg\" alt=\"\">&nbsp;<img src=\"http://wiki.lesswrong.com/mediawiki/images/4/4a/EY6.svg\" alt=\"\" width=\"195\" height=\"160\"></p>\n<p>If either of those causal links existed, those two variables should be <em>correlated.</em> We shouldn't find the <em>lack of correlation</em> or <em>conditional independence</em> that we just discovered.</p>\n<p>Next, imagine that the real causal graph looked like this:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/5/55/EY7.svg\" alt=\"\" width=\"358\" height=\"160\"></p>\n<p>In this graph, exercising <em>causes</em> you to be less likely to be overweight (due to the virtue theory of metabolism), and exercising <em>causes</em> you to spend less time on the Internet (because you have less time for it).</p>\n<p>But in this case we should <em>not</em> see that the groups who are/aren't overweight have the same probability of spending time on Reddit. There should be an outsized group of people who are both normal-weight and non-Redditors (because they exercise), and an outsized group of non-exercisers who are overweight and Reddit-using.</p>\n<p>So that causal graph is also <em>ruled out</em> by the data, as are others like:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/c/c3/EY8.svg\" alt=\"\" width=\"195\" height=\"259\">&nbsp;<img src=\"http://wiki.lesswrong.com/mediawiki/images/a/a5/EY9.svg\" alt=\"\" width=\"195\" height=\"259\">&nbsp;<img src=\"http://wiki.lesswrong.com/mediawiki/images/0/06/EY18.svg\" alt=\"\" width=\"195\" height=\"259\"></p>\n<p>&nbsp;</p>\n<p>&nbsp;<img src=\"http://wiki.lesswrong.com/mediawiki/images/5/5e/EY11.svg\" alt=\"\"></p>\n<p>Leaving <em>only</em> this causal graph:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/6/66/EY12.svg\" alt=\"\" width=\"358\" height=\"160\"></p>\n<p>Which says that weight and Internet use exert causal effects on exercise, but exercise doesn't causally affect either.</p>\n<p>All this discipline was invented and systematized by Judea Pearl, Peter Spirtes, Thomas Verma, and a number of other people in the 1980s and you should be quite impressed by their accomplishment, because before then, inferring causality from correlation was thought to be a fundamentally unsolvable problem. The standard volume on causal structure is <em>Causality</em>&nbsp;by Judea Pearl.</p>\n<p>Causal <em>models</em> (with specific probabilities attached) are sometimes known as \"Bayesian networks\" or \"Bayes nets\", since they were invented by Bayesians and make use of Bayes's Theorem. They have all sorts of neat computational advantages which are far beyond the scope of this introduction - e.g. in many cases you can split up a Bayesian network into parts, put each of the parts on its own computer processor, and then update on three different pieces of evidence at once using a neatly local message-passing algorithm in which each node talks only to its immediate neighbors and when all the updates are finished propagating the whole network has settled into the correct state. For more on this see Judea Pearl's <em>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</em> which is the original book on Bayes nets and still the best introduction I've personally happened to read.</p>\n<hr>\n<p><a href=\"#note1back\"></a><a name=\"note1\"></a>&nbsp;<a href=\"#note1back\">[1]</a> Somewhat to my own shame, I must admit to ignoring my own observations in this department - even after I saw no discernible effect on my weight or my musculature from aerobic exercise and strength training 2 hours a day 3 times a week, I didn't really start believing that the virtue theory of metabolism was <em>wrong&nbsp;<a name=\"note2back\"></a></em><a href=\"#note2\">[2]</a> until after <a href=\"http://en.wikipedia.org/wiki/Gary_Taubes\">other</a> <a href=\"http://sethroberts.net/\">people</a>&nbsp;had <a href=\"/lw/mb/lonely_dissent/\">started</a> the skeptical dogpile.</p>\n<p><a href=\"#note2back\"></a><a name=\"note2\"></a>&nbsp;<a href=\"#note2back\">[2]</a> I should mention, though, that I have confirmed a personal effect where eating <em>enough </em>cookies (at a convention where no protein is available) will cause weight gain afterward. There's no other discernible correlation between my carbs/protein/fat allocations and weight gain, <em>just</em> that eating sweets in large quantities can cause weight gain afterward. This admittedly does bear with the straight-out virtue theory of metabolism, i.e., eating pleasurable foods is sinful weakness and hence punished with fat.</p>\n<p><a href=\"#note3back\"></a><a name=\"note3\"></a>&nbsp;<a href=\"#note3back\">[3]</a> Or there might be some hidden third factor, a gene which causes both fat and non-exercise. By Occam's Razor this is more complicated and its probability is penalized accordingly, but we can't actually rule it out. It is obviously impossible to do the converse experiment where half the subjects are randomly assigned lower weights, since there's no known intervention which can cause weight loss.</p>\n<hr>\n<p><strong>Mainstream status</strong>: &nbsp;This is meant to be an introduction to completely bog-standard Bayesian networks, causal models, and causal diagrams. &nbsp;Any departures from mainstream academic views are errors and should be flagged accordingly.</p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/ezu/stuff_that_makes_stuff_happen/\">Stuff That Makes Stuff Happen</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/eva/the_fabric_of_real_things/\">The Fabric of Real Things</a>\"</p>", "sections": [{"title": "\u00a0vs.\u00a0", "anchor": "_vs__", "level": 1}, {"title": "vs.", "anchor": "vs_", "level": 1}, {"title": "p(E&R)=p(E)p(R)", "anchor": "p_E_R__p_E_p_R_", "level": 2}, {"title": "p(E&R) = p(E)p(R|E)", "anchor": "p_E_R____p_E_p_R_E_", "level": 2}, {"title": "p(E&R) = p(R)p(E|R)", "anchor": "p_E_R____p_R_p_E_R_", "level": 2}, {"title": "vs.", "anchor": "vs_1", "level": 1}, {"title": "vs.", "anchor": "vs_2", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "282 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 283, "af": false, "version": "1.2.0", "pingbacks": {"Posts": ["c93eRh3mPaN62qrD2", "a7n8GdKiAZRX86T5A", "CEGnJBHmkcwPTysb7", "NhQju3htS9W6p6wE6", "h6fzC6wFYFxxKDm8u"]}, "moderationGuidelinesVersion": "1.1.0", "customHighlightVersion": "1.1.0", "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-13T00:00:38.843Z", "modifiedAt": null, "url": null, "title": "A possible solution to pascals mugging.", "slug": "a-possible-solution-to-pascals-mugging", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:06.921Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "staticIP", "createdAt": "2012-03-18T02:14:47.707Z", "isAdmin": false, "displayName": "staticIP"}, "userId": "wAQ2cTMb2K7a6R8D6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mQwgYPxAenLo2vaq4/a-possible-solution-to-pascals-mugging", "pageUrlRelative": "/posts/mQwgYPxAenLo2vaq4/a-possible-solution-to-pascals-mugging", "linkUrl": "https://www.lesswrong.com/posts/mQwgYPxAenLo2vaq4/a-possible-solution-to-pascals-mugging", "postedAtFormatted": "Saturday, October 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20possible%20solution%20to%20pascals%20mugging.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20possible%20solution%20to%20pascals%20mugging.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmQwgYPxAenLo2vaq4%2Fa-possible-solution-to-pascals-mugging%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20possible%20solution%20to%20pascals%20mugging.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmQwgYPxAenLo2vaq4%2Fa-possible-solution-to-pascals-mugging", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmQwgYPxAenLo2vaq4%2Fa-possible-solution-to-pascals-mugging", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 111, "htmlBody": "<p>Now I tend not to follow this form very much, so please excuse me if this has been suggested before. Still, I don't know that there's anyone else on this board who could actually carry out these threats.</p>\n<p>&nbsp;</p>\n<p>If anyone accepts a pascals mugging style trade off with full knowledge of the problem, then I will slowly torture to death 3^^^^3 sentient minds. Or a suitably higher number if they include a higher (plausible, from my external viewpoint) number. Rest assured I can at least match their raw computing power from where I am. Good luck.</p>\n<p>&nbsp;</p>\n<p>EDIT: I'm told that Eleizer proposed a similar solution over <a href=\"http://www.overcomingbias.com/2012/07/life-after-death-for-pascals-wager.html\">here</a>, although more eloquently then I have.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mQwgYPxAenLo2vaq4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": -29, "extendedScore": null, "score": -6.6e-05, "legacy": true, "legacyId": "19327", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-13T02:59:59.472Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Building Something Smarter", "slug": "seq-rerun-building-something-smarter", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:06.736Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HgJ5iA6AgBbajcmA9/seq-rerun-building-something-smarter", "pageUrlRelative": "/posts/HgJ5iA6AgBbajcmA9/seq-rerun-building-something-smarter", "linkUrl": "https://www.lesswrong.com/posts/HgJ5iA6AgBbajcmA9/seq-rerun-building-something-smarter", "postedAtFormatted": "Saturday, October 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Building%20Something%20Smarter&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Building%20Something%20Smarter%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHgJ5iA6AgBbajcmA9%2Fseq-rerun-building-something-smarter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Building%20Something%20Smarter%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHgJ5iA6AgBbajcmA9%2Fseq-rerun-building-something-smarter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHgJ5iA6AgBbajcmA9%2Fseq-rerun-building-something-smarter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>Today's post, <a href=\"/lw/vg/building_something_smarter/\">Building Something Smarter</a> was originally published on 02 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Building_Something_Smarter\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It is possible for humans to create something better than ourselves. It's been done. It's not paradoxical.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ewb/seq_rerun_mundane_magic/\">Mundane Magic</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HgJ5iA6AgBbajcmA9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0080417931023663e-06, "legacy": true, "legacyId": "19336", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GpvvQzf3pPyPsBepr", "Eb4JzFv687o3F4sBA", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-13T05:36:29.297Z", "modifiedAt": null, "url": null, "title": "Notes from Online Optimal Philanthropy Meetup: 12-10-09", "slug": "notes-from-online-optimal-philanthropy-meetup-12-10-09", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:30.844Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Giles", "createdAt": "2011-02-11T02:30:16.999Z", "isAdmin": false, "displayName": "Giles"}, "userId": "H347ba3KZMP8XoDt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Gjsxhyewy2A3GCQY3/notes-from-online-optimal-philanthropy-meetup-12-10-09", "pageUrlRelative": "/posts/Gjsxhyewy2A3GCQY3/notes-from-online-optimal-philanthropy-meetup-12-10-09", "linkUrl": "https://www.lesswrong.com/posts/Gjsxhyewy2A3GCQY3/notes-from-online-optimal-philanthropy-meetup-12-10-09", "postedAtFormatted": "Saturday, October 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Notes%20from%20Online%20Optimal%20Philanthropy%20Meetup%3A%2012-10-09&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANotes%20from%20Online%20Optimal%20Philanthropy%20Meetup%3A%2012-10-09%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGjsxhyewy2A3GCQY3%2Fnotes-from-online-optimal-philanthropy-meetup-12-10-09%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Notes%20from%20Online%20Optimal%20Philanthropy%20Meetup%3A%2012-10-09%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGjsxhyewy2A3GCQY3%2Fnotes-from-online-optimal-philanthropy-meetup-12-10-09", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGjsxhyewy2A3GCQY3%2Fnotes-from-online-optimal-philanthropy-meetup-12-10-09", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3361, "htmlBody": "<div id=\"output\" class=\"markdown\">\n<p>Here are my notes from the <a href=\"/r/discussion/lw/etc/online_optimal_philanthropy_meetup_tue_109_8pm_et/\">Optimal Philanthropy online meeting</a>. Things in square brackets are my later additions and corrections. Let me know if there are factual errors or if I've incorrectly captured the drift of what you were saying.</p>\n<p>Nick:</p>\n<ul>\n<li>existential risk argument goes as follows: \n<ul>\n<li>small chance that people will be around for a really long time</li>\n<li>if so, increasing probability of that happening by a small amount then really good</li>\n<li>therefore, you should focus on this rather than things that have short-run impacts</li>\n<li>focus only on things that reduce xrisk</li>\n</ul>\n</li>\n<li>Similar property: make things a little bit better for a really long time. Might be easier to do that. \n<ul>\n<li>name for these: trajectory change. Different from just economic growth &ndash; changes default trajectory along which things go</li>\n<li>interesting for same reason as xrisk reduction but people haven&rsquo;t talked much about them.</li>\n</ul>\n</li>\n</ul>\n<p>[sorry, didn&rsquo;t note who said this]: education as an example. Particular kind of education?</p>\n<p>Nick:</p>\n<ul>\n<li>not really thought through this idea yet. [This idea is at the same stage] as if you&rsquo;d just stumbled across xrisk concept</li>\n<li>another example is just changing people&rsquo;s values for a better \n<ul>\n<li>ordinary &ndash; e.g. campaigning for particular political party</li>\n</ul>\n</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>calls this &ldquo;static&rdquo; [I assume this means as in &ldquo;noise&rdquo; rather than as in &ldquo;stationary&rdquo;]</li>\n<li>I&rsquo;m doing something &ndash; tends to break down after a month or so, very difficult to predict long term results</li>\n<li>should focus on things that <strong>can</strong> be predicted, build better models</li>\n</ul>\n<p>Nick:</p>\n<ul>\n<li>things that look short run could have long run impacts</li>\n<li>e.g. thinking of buying bednets for people as having short-term impact &ndash; has short run impact today, no impact in 100 years time</li>\n<li>but if you think about all downstream consequences, actually making small impact</li>\n</ul>\n<p>Scott agrees.</p>\n<ul>\n<li>both good and bad long-term consequences included in static</li>\n<li>it&rsquo;s a wash</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>what makes you think bednets have good long term consequences?</li>\n<li>currently thinks of bednet as short term good, long term wash</li>\n<li>benefits of having more people, having more healthy kids?</li>\n<li>really hard to predict long term value</li>\n</ul>\n<p>Nick:</p>\n<ul>\n<li>Four classes of thing \n<ul>\n<li>things that reduce xrisk</li>\n<li>things that speed up progress</li>\n<li>things that make trajectory change</li>\n<li>only short run effect</li>\n</ul>\n</li>\n<li>bednet not a trajectory change.</li>\n<li>buying bednet increases economic growth &ndash; long term effects</li>\n<li>economic growth has been compounding for a long time.</li>\n<li>Plausible story for why this happens &ndash; people get richer, specialization, help each other out. Overall a positive process, and a process that ripples far into the future.</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>really hard to get good predictions. Maybe you could figure it out</li>\n<li>even evaluating short-term effects of individual charity now is hard.</li>\n<li>long term effects incredibly difficult</li>\n</ul>\n<p>Nick:</p>\n<ul>\n<li>when comparing within categories, makes sense to ignore these thing.</li>\n<li>e.g. two things that contribute to economic growth &ndash; just ask which has better predictable impacts.</li>\n<li>but if you&rsquo;re comparing something that reduces xrisk with something that increases economic growth</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>funding education versus funding health.</li>\n<li>wouldn&rsquo;t surprise him if someone got handle on their long term effects and they were very different</li>\n</ul>\n<p>Nick:</p>\n<ul>\n<li>expects one of them to contribute to trajectory change more</li>\n<li>hard to believe one of them has a very different effect on long term economic growth</li>\n<li>e.g. education appeared on a par with health intervention, but expecting 1000x more economic growth seems wild.</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>[if optimizing for] economic growth &ndash; would do it very differently from doing health interventions</li>\n</ul>\n<p>Nick:</p>\n<ul>\n<li>can&rsquo;t use economic growth argument to promote <span class=\"caps\">AMF</span>: \"AMF is best charity to</li>\n<li>more plausible candidate: political or meta-research (some GiveWell thing)</li>\n</ul>\n<p>At this point everyone on the call introduces themselves.</p>\n<ul>\n<li>Giles Edkins: Toronto LW, <span class=\"caps\">THINK</span></li>\n<li>Jeff Kaufman: married to Julia Wise. Both into earning to give stuff</li>\n<li>Nick Beckstead: <span class=\"caps\">PHD</span> in Philosophy at Rutgers. Population ethics and xrisks. Involved in Centre for Effective Altruism.</li>\n<li>Nisan Stiennon: Berkeley. <span class=\"caps\">PHD</span> in Math. Teaches for <span class=\"caps\">CFAR</span>. Confused about whether to focus on xrisk or other kinds of philanthropy e.g. <span class=\"caps\">AMF</span></li>\n<li>Peter Hurford: Ohio. Political Science &amp; Psych. Joined <span class=\"caps\">GWWC</span> &ndash; giving 10% of meager college income. New to smart giving stuff, on smart giving subreddit, on LW.</li>\n<li>Raymond Arnold: does a lot of work for <span class=\"caps\">THINK</span>. LW for past two years. General boat of xrisk &amp; GiveWell stuff, only things he has a lot of info about</li>\n<li>Scott Dickey: LWer, gives 25% of income to charity, trying to make that go as far as he can &ndash; SingInst, GiveWell. Following, reading them, watching where things are going</li>\n<li>Boris Yakubchik: president of <span class=\"caps\">GWWC</span> Rutgers. Giving 50% of income, high school math teacher. Don&rsquo;t know what to expect from this hangout &ndash; needs to run off!</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>assumes everyone has read GW, Nick Bostrom&rsquo;s xrisk stuff</li>\n<li>anyone read anything else?</li>\n</ul>\n<p>Nick:</p>\n<ul>\n<li>a lot of my dissertation is not empirical, instead it&rsquo;s moral philosophy</li>\n<li>has been thinking on the side on comparison between xrisk and other kinds of things, talking to people</li>\n<li>feeling is there&rsquo;s not huge amount of writing</li>\n<li>e.g. economics of climate change &ndash; most similar to xrisk. Not a whole lot out there that&rsquo;s helpful</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>earlier this summer (interrupted by other things), wanted breadth-first look at potentially promising high impact stuff</li>\n<li>3 books on global poverty, each with a different thesis \n<ul>\n<li>The End of Poverty &ndash; Jeffrey Sachs</li>\n<li>White man&rsquo;s burden &ndash; William Easterly</li>\n<li>The Bottom Billion &ndash; [Paul Collier]</li>\n</ul>\n</li>\n<li>would be good for people in our community to start compiling summaries of this information</li>\n</ul>\n<p>Giles:</p>\n<ul>\n<li>GW prioritizes things that are easier to understand &ndash; targets beginners</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>And where there&rsquo;s good information</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>GW are reasonably honest in not just choosing charities that are accessible, but that the people involved are convinced about the best ones &ndash; has talked to Holden on Skype 8 months ago.</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>Only recently with GW Labs have they made an effort to look at harder questions, not just looking at charities with easy-to-obtain information</li>\n</ul>\n<p>Giles:</p>\n<ul>\n<li>Any info on how GW Labs is going?</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>100k to Cochrane. Not a whole lot of money! Still question on what else is out there</li>\n</ul>\n<p>Peter:</p>\n<ul>\n<li>Thought they were going to move away from GW Labs, were going to pursue this as main funding strategy</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>Thought they were going to integrate things into one effort</li>\n</ul>\n<p>Peter:</p>\n<ul>\n<li>target big funders</li>\n<li>away from health related issues</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>[Should we] give to GiveWell itself?</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>[If we give to GiveWell], a small amount to funding goes to themselves, the rest goes to their favorite charity</li>\n</ul>\n<p>Peter:</p>\n<ul>\n<li>can give to Clear Fund &ndash; fund for GW activities.</li>\n<li>But they strongly discourage that &ndash; want to give to their top charity instead.</li>\n</ul>\n<p>Giles:</p>\n<ul>\n<li>can we talk to GW and find out why?</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>if you give to charity, tell them it&rsquo;s because of GW</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>has anyone talked to non-LWers about optimal philanthropy?</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>talked to college friends, not LWers. Some people think it makes sense &ndash; they are the people most similar to LWers, computer programmers and other engineers. Job makes them a bit more money than they know what to do with. &ldquo;If I started giving away more than 20% of my money it wouldn&rsquo;t hurt.&rdquo; Helps people to think about it dispassionately.</li>\n<li>Other people not inclined to this approach &ndash; doesn&rsquo;t like quantifying. Shouldn&rsquo;t be prioritising one thing over another</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>talk to people at work, highly diverse set of backgrounds. Vast majority doesn&rsquo;t like analytic side &ndash; they call me &ldquo;cold&rdquo;. Work fundraising drive &ndash; literally had picture of crying puppy. Very emotional, very community oriented, didn&rsquo;t like analysis. Could be because I&rsquo;m a bad arguer.</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>tries really hard, really hard to have discussions with people that people aren&rsquo;t giving to best charity. People used to get mad at him. Now usually agrees to disagree but not losing any more friends</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>requested charity Christmas gift &ndash; think about who you&rsquo;re giving to. Conversation over Christmas dinner &ndash; worst idea ever! Grandmother most likely to donate, donated to local causes. She prioritizes giving locally not because she believes its the most effective but she prioritises her own community [did I get this right?]</li>\n<li>nowadays asks what being a good person means to someone before having that conversation</li>\n<li>has been having conversation at work not as &ldquo;you should be doing it too&rdquo; but &ldquo;here&rsquo;s this thing I&rsquo;m doing&rdquo;</li>\n<li>has laid groundwork.</li>\n<li>one person has said she now gives to more global causes [is this right?] as a result of what I was saying</li>\n<li>long-term non-confrontational strategy</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>another thing GW is doing right is targeting high-wealth individuals</li>\n<li>spent a lot of time talking to my Dad about it. After 10 conversations he says &ldquo;fine, I&rsquo;ll give to what you think are more effective causes&rdquo; &ndash; gave $100, and that was it forever.</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>convincing someone to give $10000 a year is not something to be sneezed at. Need to accept both the &ldquo;give effectively&rdquo; and &ldquo;give substantially&rdquo; ideas.</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>not many people [in effective altruism movement] older than I am. (late 30&rsquo;s or higher)</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>Bill Gates and Warren Buffett started own movement. Get rich people to give 50% of resources</li>\n<li>Jaan Tallin &ndash; level 2 &ndash; looking at &ldquo;malaria&rdquo; instead of &ldquo;human disease&rdquo;</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>B&amp;M Gates Foundation does a lot of stuff with malaria because it&rsquo;s effective &ndash; same reason as GiveWell.</li>\n</ul>\n<p>Scott: agrees</p>\n<ul>\n<li>goes back to Nick&rsquo;s thing of smaller things making larger impacts over time</li>\n<li>not trying to change human condition</li>\n<li>what inspired Scott: &ldquo;what can you do today that people can remember your name 50000 years for now&rdquo;</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>if goal is to be remembered, won&rsquo;t get there by donating. Founding agency or inventing something.</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>Or Hitler, but would rather not be on that list.</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>new ways of thinking &ndash; Darwin, Galileo.</li>\n<li>optimizing giving is a different question</li>\n<li>even &ldquo;change the way governments work&rdquo; &ndash; won&rsquo;t be remembered for donating to that. But that&rsquo;s OK.</li>\n<li>agrees with sentiment though</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>certainly an inspiring sentiment</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>what is the thing you&rsquo;d want to remembered in 50000 years for, even if your name isn&rsquo;t remembered?</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>some really important things don&rsquo;t get remembered, e.g. eradicating Polio and Smallpox. Huge amounts of work, people don&rsquo;t think about it because it&rsquo;s no longer a problem.</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>amend it to &ldquo;should&rdquo; be remembered for</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>something I&rsquo;ve been trying to do is put together a list of inspiring things that humans have done that we should care about.</li>\n<li>evidence that things are getting better</li>\n<li>ways of dealing with stress &ndash; global poverty is really depressing</li>\n<li>&ldquo;we should do more stuff like that&rdquo;</li>\n<li>polio &amp; smallpox on list</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>green revolution</li>\n<li>one guy, saved or created billions of lives</li>\n</ul>\n<p>Nick:</p>\n<ul>\n<li>book: &ldquo;millions saved&rdquo; [Ruth Levine?], success stories in philanthropy in particular global health</li>\n<li>Steven Pinker: &ldquo;better angels of our nature, chapter 1&rdquo;, things that went surprisingly well in last few decades</li>\n<li>&ldquo;Scientists greater than Einstein&rdquo; [Billy Woodward?]</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>80000hours blog</li>\n<li>have read it occasionally</li>\n<li>first few times, changed how I looked at that side of things</li>\n<li>since then, haven&rsquo;t found a whole lot of content</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>stopped reading 80k</li>\n<li>they deferred their choosing of charities to GW</li>\n<li>[I had] already read up on income maximization</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>am subscribed to 80k blog</li>\n<li>haven&rsquo;t read things that are wholly new ideas that I&rsquo;m excited to read</li>\n<li>mostly been summaries of things</li>\n<li>remaining questions are really hard. People won&rsquo;t make good progress sitting down for an afternoon reading [writing?] a blog post</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>waiting for Nick Bostrom to come up with magnum opus?</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>happy with what I&rsquo;m doing so far</li>\n<li>important not to get attached to really high leverage factors where it&rsquo;s only worth it if impact is too huge to think about</li>\n<li>good to realize how much good you can do with a <span class=\"caps\">GWWC</span>-style 10% given to GW charities</li>\n<li>and try and do better than that!</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>Jeff: you and Julia are highest% giving couple that I know of. On the giving a lot side&hellip;</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>keeping a donations page, very numbersy</li>\n<li>Bolder Giving talks about us as if it&rsquo;s still 2009 and we have changes to make, titled Julia Wise and should be both of us&hellip;</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>question is &ldquo;are they living in Manhattan&rdquo;?</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>rent in Boston is still high</li>\n<li>studio apartment costing $1000 a month</li>\n<li>even if we&rsquo;d be spending twice as much, it wouldn&rsquo;t have had a huge effect</li>\n<li>actually, yes it would have a huge effect! [but could still give away large percentage]</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>and do they have kids?</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>yes, planning to</li>\n<li>yearly expenses will change by $10000 for each kid we have [note I may have written down Jeff&rsquo;s numbers wrong]</li>\n<li>living at parents house, pay them rent but it&rsquo;s low</li>\n<li>own house with kids: $40000 on ourselves, giving at least that much away, still pretty good</li>\n<li>looking forward to say &ldquo;yes, you can have kids and a house and still give away lots!&rdquo;</li>\n</ul>\n<p>Giles:</p>\n<ul>\n<li><a href=\"/lw/wj/is_that_your_true_rejection/\">&ldquo;is that your true reason for rejection&rdquo;</a></li>\n<li>people still might manage to find a reason why you&rsquo;re weird and not want to emulate you?</li>\n<li>have house &amp; kids because you want them, not for signaling!</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>yes, of course!</li>\n<li>more normal and conventional our life seems, convincing in a non-logical way</li>\n<li>&ldquo;oh, I could live a life like that. Not so different from the life I was thinking of living&rdquo;.</li>\n</ul>\n<p>Nick:</p>\n<ul>\n<li>a few people had a question about how to compare xrisk reduction and GW charities</li>\n<li>are there specific aspects of that that people want to know about?</li>\n</ul>\n<p>Giles:</p>\n<ul>\n<li>how do we know xrisk orgs are having any impact at all? How do we know they&rsquo;re not making things worse?</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li><a href=\"/lw/ehd/the_yudkowsky_ambition_scale/\">Yudkowksy&rsquo;s scale of ambition</a>. Went into hacker news post, restructured entire conversation where highest you could go was &ldquo;bigger than apple&rdquo;, he put that at 2. His 10 was &ldquo;you can hack the computer universe is running on&rdquo;</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>expected value calculation &ndash; don&rsquo;t need to <strong>know</strong> they [xrisk orgs] are doing something meaningful, 1% chance is good enough. But how can we even be sure of that?</li>\n<li>without some calculation, running on intuition.</li>\n<li>intuitively sounds plausible that donating to xrisk org is higher impact than giving to <span class=\"caps\">AMF</span></li>\n<li>whole point of GW is that our intuitions aren&rsquo;t necessarily reliable</li>\n<li>what are the numbers for xrisk?</li>\n<li>one thing that&rsquo;s bothering him &ndash; people say xrisk, only think of SingInst, <span class=\"caps\">FHI</span></li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>Methuselah Foundation</li>\n<li>Lifeboat Foundation (even they sound amateurish)</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>SingInst is most reputable of the various xrisk orgs [what about <span class=\"caps\">FHI</span>?], still has a lot of reasons to be skeptical of SingInst</li>\n<li>changing a bit since Luke Muelhauser took the reins, moving in more reliable direction</li>\n</ul>\n<p>[These didn't all come up in the discussion, but I'll give lukeprog's <a href=\"/lw/bze/new_xrisk_organization_at_cambridge_university/\">list of x-risk orgs</a>: <span class=\"s1\"><a href=\"http://www.fhi.ox.ac.uk/\"><span class=\"s2\">FHI</span></a>, <a href=\"/lw/8gg/new_ai_risks_research_institute_at_oxford/\"><span class=\"s2\">FutureTech</span></a>, the&nbsp;<a href=\"http://intelligence.org/\"><span class=\"s2\">Singularity Institute</span></a>, <a href=\"http://www.leverageresearch.org/\"><span class=\"s2\">Leverage Research</span></a></span>, the <a href=\"http://www.gcrinstitute.org/\">Global Catastrophic Risk Institute</a>, <a href=\"http://cser.org/\">CSER</a>]<a href=\"http://cser.org/\"></a></p>\n<p>Scott:</p>\n<ul>\n<li>Growing pains</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>OK, I can see SingInst is improving. Not at point I&rsquo;d be willing to donate to them yet</li>\n<li>set up criteria: if they meet these criteria, I&rsquo;d consider them reliable</li>\n</ul>\n<p>Giles:</p>\n<ul>\n<li><a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">Holden done some of the groundwork for that?</a></li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>part of what influenced me</li>\n</ul>\n<p>Nick:</p>\n<ul>\n<li>do you wish there was more like the Holden post?</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>Holden should target any other organization in the field [i.e. should address other xrisk orgs besides SingInst]</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>Holden&rsquo;s post on Singinst was mostly organisation critique. Looking at what they&rsquo;re doing, things they&rsquo;re doing that seem to be working and not.</li>\n<li>even if they passed all of those things, I would still be unsure it would make more sense [to donate] than an easily measurable intervention [such as <span class=\"caps\">AMF</span>]</li>\n<li>SingInst still doing incredibly hard to evaluate work</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>asteroid impact &ndash; already solved/diminishing returns point</li>\n<li>AI risk</li>\n<li>nuclear proliferation</li>\n<li>bioterrorism</li>\n<li><span class=\"caps\">CDC</span></li>\n</ul>\n<p>[question came up on what other xrisk mitigation efforts might there be that we don&rsquo;t know about, in particular AI related]</p>\n<p>Scott:</p>\n<ul>\n<li>Google, Microsoft Research</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>Google mostly public with google.org stuff</li>\n<li>what secret but massively positive stuff would there be?</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>Google cars, save 30000 lives a year, not too big. Bigger if roll out to world</li>\n<li>at that point, Google is AI company and should be focusing on AI safety</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>people in Google mostly think that kind of thing is silly?</li>\n<li>very far from machine learning systems that are scary</li>\n<li>so hard to get them to do anything at all intelligent</li>\n<li>hard for people to think about possibility of recursively self-improving anything.</li>\n<li>not my impression that they are excited about AI safety.</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>not saying that they are, but that they should be</li>\n<li>keep a lot of it locked down</li>\n<li>we don&rsquo;t know what&rsquo;s happening in China, Korea, Japan had a push for AI recently</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>wouldn&rsquo;t be surprised if no-one at Google knows more about AI safety than what you can read on LW</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>one thing that&rsquo;s potentially high impact is donating to scientific research</li>\n<li>there&rsquo;s a thing called Petridish &ndash; Kickstarter for research projects</li>\n<li>not optimized for &ldquo;these are the projects that will help the world the most&rdquo;</li>\n<li>wonder if there&rsquo;s a way we can push this in that direction?</li>\n</ul>\n<p>Jeff:</p>\n<ul>\n<li>amazing how animal-focused they are</li>\n<li>specific to biology, or is that what people like to fund?</li>\n</ul>\n<p>Giles:</p>\n<ul>\n<li>find people good at communicating effective altruism message and put them in touch with orgs [such as Petridish] that need it?</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>finding marketers</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>some of that is in progress at Leverage. At pre-planning stage</li>\n</ul>\n<p>Scott:</p>\n<ul>\n<li>campaign Red or Pink, generate millions of dollars by having campaigns. Doesn&rsquo;t seem to be having much impact, but can we co-opt or create something like that?</li>\n</ul>\n<p>Ray:</p>\n<ul>\n<li>am definitely looking for info on xrisk in a more concise form</li>\n<li>&ldquo;I don&rsquo;t have a degree in computer science or AI research, wouldn&rsquo;t be able to analyze at Eliezer level&rdquo;</li>\n<li>so layman can feel like they&rsquo;re making an informed decision, has important information but not too much information</li>\n</ul>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Gjsxhyewy2A3GCQY3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 1.0081251800086527e-06, "legacy": true, "legacyId": "19349", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8wfAnjrxRG4oBG3An", "TGux5Fhcd7GmTfNGC", "mxDAPAuy2b3tkeJRc", "ewgw6iEx4ihGeJdck", "wTgS6J73XMjYCKGC7", "6SGqkCgHuNr7d4yJm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-13T14:47:16.367Z", "modifiedAt": null, "url": null, "title": "Good transhumanist fiction?", "slug": "good-transhumanist-fiction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:02.479Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5uZiajKCrg2r9yA7z/good-transhumanist-fiction", "pageUrlRelative": "/posts/5uZiajKCrg2r9yA7z/good-transhumanist-fiction", "linkUrl": "https://www.lesswrong.com/posts/5uZiajKCrg2r9yA7z/good-transhumanist-fiction", "postedAtFormatted": "Saturday, October 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Good%20transhumanist%20fiction%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGood%20transhumanist%20fiction%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5uZiajKCrg2r9yA7z%2Fgood-transhumanist-fiction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Good%20transhumanist%20fiction%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5uZiajKCrg2r9yA7z%2Fgood-transhumanist-fiction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5uZiajKCrg2r9yA7z%2Fgood-transhumanist-fiction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 221, "htmlBody": "<p>I just watched <a href=\"http://vimeo.com/6765616\">this</a>, a very pretty version of \"don't try to make yourself different, just accept who you are\", and I realized that self-directed change in fiction is a worthwhile topic.</p>\n<p>What I'm looking for is stories where main characters change themselves in ways which are basically improvements-- getting beyond the usual human is a plus, but for purposes of this discussion I'm including any significant positive change.</p>\n<p>Another big plus would be the character needing to learn which of their goals make sense, and which methods work.</p>\n<p>**ETA:** That was a bit of a stub-- HPMOR is partly about Harry and Hermione changing themselves, generally for the better I think (I've only read it once). It would be interesting to see what happens if Quirrell decides he needs to upgrade himself.</p>\n<p>*Stranger in a Strange Land* is an interesting partial example-- the Martian language is presumably an upgrade for the human race, but it was developed by and for Martians, and needs some modification.</p>\n<p>A *lot* of relatively recent fiction has people learning martial arts. I think appearance makeovers (typically for women) have become less common. I don't think there's a lot of fiction about appearance makeovers for men-- *The Stars My Destination* has one, but it's offstage. It wouldn't surprise me if *The Count of Monte Cristo* (frequently referenced with TSMD) has one.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jiuackr7B5JAetbF6": 2, "etDohXtBrXd8WqCtR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5uZiajKCrg2r9yA7z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 13, "extendedScore": null, "score": 1.0084187548323236e-06, "legacy": true, "legacyId": "19353", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-13T15:03:18.480Z", "modifiedAt": null, "url": null, "title": "Alternate approaches to Pascal's Mugging", "slug": "alternate-approaches-to-pascal-s-mugging", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:22.628Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NKqa3mWsnoohrwvfb/alternate-approaches-to-pascal-s-mugging", "pageUrlRelative": "/posts/NKqa3mWsnoohrwvfb/alternate-approaches-to-pascal-s-mugging", "linkUrl": "https://www.lesswrong.com/posts/NKqa3mWsnoohrwvfb/alternate-approaches-to-pascal-s-mugging", "postedAtFormatted": "Saturday, October 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Alternate%20approaches%20to%20Pascal's%20Mugging&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAlternate%20approaches%20to%20Pascal's%20Mugging%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNKqa3mWsnoohrwvfb%2Falternate-approaches-to-pascal-s-mugging%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Alternate%20approaches%20to%20Pascal's%20Mugging%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNKqa3mWsnoohrwvfb%2Falternate-approaches-to-pascal-s-mugging", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNKqa3mWsnoohrwvfb%2Falternate-approaches-to-pascal-s-mugging", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 668, "htmlBody": "<p>A lot's been said about Pascal's Mugging, counter-muggings, adding extra arrows to the hyper-power stack, and so forth; but if anyone's said anything about my own reaction, I've yet to read it; and, in case it might spur some useful discussion, I'll try explaining it.</p>\n<p>Over the years, I've worked out a rough rule-of-thumb to figure out useful answers to most everyday sorts of ethical quandaries, one which seems to do at least as well as any other I've seen: I call it the \"I'm a selfish bastard\" rule, though my present formulation of it continues with the clauses, \"but I'm a <em>smart</em> selfish bastard, interested in my <em>long-term</em> self-interest.\" This seems to give enough guidance to cover anything from \"should I steal this or not?\" to \"is it worth respecting other peoples' rights in order to maximize the odds that my rights will be respected?\" to \"exactly whose rights should I respect, anyway?\". From this latter question, I ended up with a 'Trader's Definition' for personhood: if some other entity can make a choice about whether or not to make an exchange with me of a banana for a backrub, or playtime for programming, or anything of the sort, then at least generally, it's in my own self-interest to treat them <em>as if</em> they were a person, whether or not they match any other criteria for personhood.</p>\n<p>Which brings us to <a href=\"http://wiki.lesswrong.com/wiki/Pascal%27s_mugging\">Pascal's Mugging</a> itself:  \"Give me five dollars, or I'll use my magic powers from outside the Matrix to run a Turing machine that simulates and kills 3^^^^3 people.\"</p>\n<p>To put it bluntly... why should I care? Even if the Mugger's claim is accurate, the entities he threatens to simulate and calls 'people' don't seem as if they will have any opportunity to interact with my portion of the Matrix; they will never have any opportunity to offer me any benefit or any harm. How would it benefit myself to treat such entities as if they not only had a right to life, but that I had an obligation to try to defend their right?</p>\n<p>&nbsp;</p>\n<p>For an alternate approach: Most forms of ethics that have been built, have been created with an unstated assumption about the number of possible beings someone can interact with in their life - on the outside, someone who lived 120 years and met with a new person every second would meet less than 4 billion individuals. It's only in the past few centuries that hard, physical experience has given us sufficient insights into some of the rather more basic foundations of economics for humanity to have even developed enough competing theories of ethics for some to start being winnowed out; and we're still a long way away from getting a broad consensus on an ethical theory that can deal with the existence of a mere 10 billion individuals. What are the odds that we possess sufficient information to have any inkling of the assumptions required to deal with a mere 3^^^3 individuals existing? What knowledge would be required so that when the answer is figured out, we would actually be able to tell that that was it?</p>\n<p>&nbsp;</p>\n<p>For another alternate approach: Assuming that the Mugger is telling the truth... is what he threatens to do <em>actually</em> a bad thing? He doesn't say anything about the nature of the lives the people he simulates; one approach he might take could be to simulate a large number of copies of the universe, which eventually peters out in heat-death; would the potential inhabitants of such a simulated universe really object to their existence being created in the first place?</p>\n<p>&nbsp;</p>\n<p>For yet another alternate approach: \"You have outside-the-Matrix powers <em>capable</em> of simulating 3^^^^3 people? Whoah, that implies so much about the nature of reality that this particular bet is nearly meaningless. Which answer could I give that would induce you to tell me more about the true substrate of existence?\"</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Do any of these seem to be a worthwhile way of looking at PM? Do you have any out-of-left-field approaches of your own?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NKqa3mWsnoohrwvfb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -8, "extendedScore": null, "score": 1.0084273040127954e-06, "legacy": true, "legacyId": "19355", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-13T15:28:27.190Z", "modifiedAt": null, "url": null, "title": "[Link] Scientists to simulate human brain inside a supercomputer ", "slug": "link-scientists-to-simulate-human-brain-inside-a", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:06.657Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tuxedage", "createdAt": "2012-03-22T17:13:05.551Z", "isAdmin": false, "displayName": "Tuxedage"}, "userId": "Ezvcs6nqmgXbpD5bN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vHSEjq3mMaeh5RhWp/link-scientists-to-simulate-human-brain-inside-a", "pageUrlRelative": "/posts/vHSEjq3mMaeh5RhWp/link-scientists-to-simulate-human-brain-inside-a", "linkUrl": "https://www.lesswrong.com/posts/vHSEjq3mMaeh5RhWp/link-scientists-to-simulate-human-brain-inside-a", "postedAtFormatted": "Saturday, October 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Scientists%20to%20simulate%20human%20brain%20inside%20a%20supercomputer%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Scientists%20to%20simulate%20human%20brain%20inside%20a%20supercomputer%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvHSEjq3mMaeh5RhWp%2Flink-scientists-to-simulate-human-brain-inside-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Scientists%20to%20simulate%20human%20brain%20inside%20a%20supercomputer%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvHSEjq3mMaeh5RhWp%2Flink-scientists-to-simulate-human-brain-inside-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvHSEjq3mMaeh5RhWp%2Flink-scientists-to-simulate-human-brain-inside-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 12, "htmlBody": "<p>I saw this on Reddit, I thought I'd share it here.</p>\n<p>http://edition.cnn.com/2012/10/12/tech/human-brain-computer/index.html?hpt=hp_t3</p>\n<p>&nbsp;</p>\n<p>Any thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vHSEjq3mMaeh5RhWp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -4, "extendedScore": null, "score": -4e-06, "legacy": true, "legacyId": "19356", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-14T08:04:08.557Z", "modifiedAt": null, "url": null, "title": "The basic argument for the feasibility of transhumanism", "slug": "the-basic-argument-for-the-feasibility-of-transhumanism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:24.083Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KGnJ3BakF3E5SYE7q/the-basic-argument-for-the-feasibility-of-transhumanism", "pageUrlRelative": "/posts/KGnJ3BakF3E5SYE7q/the-basic-argument-for-the-feasibility-of-transhumanism", "linkUrl": "https://www.lesswrong.com/posts/KGnJ3BakF3E5SYE7q/the-basic-argument-for-the-feasibility-of-transhumanism", "postedAtFormatted": "Sunday, October 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20basic%20argument%20for%20the%20feasibility%20of%20transhumanism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20basic%20argument%20for%20the%20feasibility%20of%20transhumanism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKGnJ3BakF3E5SYE7q%2Fthe-basic-argument-for-the-feasibility-of-transhumanism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20basic%20argument%20for%20the%20feasibility%20of%20transhumanism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKGnJ3BakF3E5SYE7q%2Fthe-basic-argument-for-the-feasibility-of-transhumanism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKGnJ3BakF3E5SYE7q%2Fthe-basic-argument-for-the-feasibility-of-transhumanism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 635, "htmlBody": "<p>Eliezer sometimes talks about how animals on earth are but a tiny dot in the \"mind design space.\" For example, in \"Artificial Intelligence as a Positive and Negative Factor in Global Risk,\" he writes:</p>\n<blockquote>\n<p>The term &ldquo;Artificial Intelligence&rdquo; refers to a vastly greater space of possibilities than&nbsp;does the term &ldquo;Homo sapiens.&rdquo; When we talk about &ldquo;AIs&rdquo; we are really talking about&nbsp;minds-in-general, or optimization processes in general. Imagine a map of mind design&nbsp;space. In one corner, a tiny little circle contains all humans; within a larger tiny circle&nbsp;containing all biological life; and all the rest of the huge map is the space of minds-ingeneral. The entire map floats in a still vaster space, the space of optimization processes.&nbsp;Natural selection creates complex functional machinery without mindfulness; evolution&nbsp;lies inside the space of optimization processes but outside the circle of minds.</p>\n</blockquote>\n<p>Though Eliezer doesn't stress this point, this argument applies as much to biotechnology as Artificial Intelligence. You could say, paralleling Eliezer, that when we talk about \"biotechnology\" we are really talking about living things in general, because life on Earth represents just a tiny subset of all life that could have evolved anywhere in the universe. Biotechnology may allow to create some of that life that could have evolved but didn't. Extending the point, there's probably an even vaster space of life that's recognizably life but couldn't have evolved, because it exists in a tiny island of life not connected to other possible life by a chain of small, beneficial mutations, and therefore is effectively impossible to reach without the conscious planning of a bioengineer.</p>\n<p>The argument can further be extended to nanotechnology. Nanotechnology is like life in that they both involve doing interesting things with complex arrangements of matter on a very small scale, it's just that visions of nanotechnology tend to involve things which don't otherwise look very much like life at all. So we've got this huge space of \"doing interesting this with complex arrangements of matter on a very small scale,\" of which existing life on earth is a tiny, tiny fraction, and in which \"Artificial Intelligence,\" \"biotechnology,\" and so on represent much large subsets.</p>\n<p>Generalized in this way, this argument seems to me to be an extremely important one, enough to make it a serious contender for the title \"the basic argument for the feasibility* of transhumanism.\" &nbsp;It suggests a vast space of unexplored possibilities, some of which would involve life on earth being very different than it is right now. Short of some catastrophe putting a halt to scientific progress, it seems hard to imagine how we could avoid having some significant changes of this sort not taking place, even without considering specifics involving superhuman AI, mind uploading, and so on.</p>\n<p>On&nbsp;<em>Star Trek, </em>this outcome is avoided because a war with genetically enhanced supermen led to the banning of genetic enhancement, but in the real world such regulation is likely to be far from totally effective, no more than current bans on recreational drugs, performance enhancers, or copyright violation are totally effective. Of course, the real reason for the genetic engineering ban on <em>Star Trek </em>is that stories about people fundamentally like us are easier for writers to write and viewers to relate to.</p>\n<p>I could ramble on about this for some time, but my reason for writing this post is to bounce ideas off people. In particular:</p>\n<p>&nbsp;</p>\n<ol>\n<li>Is there a better candidate for the title \"the basic argument for the feasibility of transhumanism\"?--and--</li>\n<li>What objections can be raised against this argument? I'm looking both for <em>good </em>objections and objections that many people are likely to raise, even if they aren't really any good.</li>\n</ol>\n<p>&nbsp;</p>\n<p>*I don't call it an argument for transhumanism, because transhumanism is often defined to involve claims about the&nbsp;<em>desirability&nbsp;</em>of certain developments, which this argument doesn't show anything about one way or the other.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jiuackr7B5JAetbF6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KGnJ3BakF3E5SYE7q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 9, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "19358", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-14T09:11:25.757Z", "modifiedAt": null, "url": null, "title": "Rationality, Transhumanism, and Mental Health", "slug": "rationality-transhumanism-and-mental-health", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:57.125Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ialdabaoth", "createdAt": "2012-10-11T00:04:32.345Z", "isAdmin": false, "displayName": "ialdabaoth"}, "userId": "335oJYQyzcd85RAoe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ptz3JKGZ7dgNkGYZT/rationality-transhumanism-and-mental-health", "pageUrlRelative": "/posts/ptz3JKGZ7dgNkGYZT/rationality-transhumanism-and-mental-health", "linkUrl": "https://www.lesswrong.com/posts/ptz3JKGZ7dgNkGYZT/rationality-transhumanism-and-mental-health", "postedAtFormatted": "Sunday, October 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%2C%20Transhumanism%2C%20and%20Mental%20Health&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%2C%20Transhumanism%2C%20and%20Mental%20Health%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fptz3JKGZ7dgNkGYZT%2Frationality-transhumanism-and-mental-health%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%2C%20Transhumanism%2C%20and%20Mental%20Health%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fptz3JKGZ7dgNkGYZT%2Frationality-transhumanism-and-mental-health", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fptz3JKGZ7dgNkGYZT%2Frationality-transhumanism-and-mental-health", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 462, "htmlBody": "<p>My name is Brent, and I'm probably insane.</p>\n<p>I can perform various experimental tests to verify that<em>&nbsp;</em>I do not perform primate pack-bonding rituals correctly, which is about half of what we mean by \"insane\". This concerns me simply from a utilitarian perspective (separation from pack makes ego-depletion problems harder; it makes resources harder to come by; and it simply sucks to experience \"from the inside\"), but these are not the things that concern me most.</p>\n<p>The thing that concerns me most is this:</p>\n<p>What if the very tools that I use to make decisions are flawed?</p>\n<p>I stumbled upon Bayesian techniques as a young child; I was lucky enough to have the opportunity to perform a lot of self-guided artificial intelligence \"research\" in Junior High and High School, due to growing up in a time and place when computers were utterly mysterious, so no one could really tell me what I was \"supposed\" to be doing with them - so I started making simple video games, had no opponents to play them against due to the aforementioned failures to correctly perform pack-bonding rituals, decided to create my own, became dissatisfied with the quality of my opponents, and suddenly found myself chewing on Hopfstaedter and Wiener and Minsky.</p>\n<p>I'm filling in that bit of detail to explain that I have been attempting to operate as a rational intelligence for quite some time, so I believe that I've become very familiar with the kinds of \"bugs\" that I will tend to exhibit.</p>\n<p>I've spent a very long time attempting to correct for my cognitive biases, edit out tendencies to seek comfortable-but-misleading inputs, and otherwise \"force\" myself to be rational, and often, the result is that my \"will\" will crack under the strain. My entire utility-table will suddenly flip on its head, and attempt to maximize my own self-destruction rather than allow me to continue to torture it with endlessly recursive, unsolvable problems that all tend to boil down to \"you do not have sufficient social power, and humans are savage and cruel no matter how much you care about them.\"</p>\n<p>Most of my energy is spent attempting to maintain positive, rational, long-term goals in the face of some kind of regedit-hack of my utility table itself, coming from somewhere in my subconscious that I can't seem to gain write-access to.</p>\n<p>Clearly, the transhumanist solution would be to identify the underlying physical storage where the bug is occurring, and replace it with a less-malfunctioning piece of hardware.</p>\n<p>Hopefully someday someone with more self-control, financial resources, and social resources than I will invent a method to do that, and I can get enough of a partial personectomy to create something viable with the remaining subroutines.</p>\n<p>In the meantime, what is someone who wishes to be rational supposed to do, when the underlying hardware simply <em>won't</em> cooperate?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ptz3JKGZ7dgNkGYZT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 13, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "19359", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 129, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-14T13:21:36.086Z", "modifiedAt": null, "url": null, "title": "The deeper solution to the mystery of moralism\u2014Believing in morality and free will are hazardous to your mental health", "slug": "the-deeper-solution-to-the-mystery-of-moralism-believing-in", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:07.341Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "metaphysicist", "createdAt": "2012-02-17T23:36:50.395Z", "isAdmin": false, "displayName": "metaphysicist"}, "userId": "WhJB5nfwSBQu7wjhz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w6jtBqbyJ3SfgAyGc/the-deeper-solution-to-the-mystery-of-moralism-believing-in", "pageUrlRelative": "/posts/w6jtBqbyJ3SfgAyGc/the-deeper-solution-to-the-mystery-of-moralism-believing-in", "linkUrl": "https://www.lesswrong.com/posts/w6jtBqbyJ3SfgAyGc/the-deeper-solution-to-the-mystery-of-moralism-believing-in", "postedAtFormatted": "Sunday, October 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20deeper%20solution%20to%20the%20mystery%20of%20moralism%E2%80%94Believing%20in%20morality%20and%20free%20will%20are%20hazardous%20to%20your%20mental%20health&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20deeper%20solution%20to%20the%20mystery%20of%20moralism%E2%80%94Believing%20in%20morality%20and%20free%20will%20are%20hazardous%20to%20your%20mental%20health%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw6jtBqbyJ3SfgAyGc%2Fthe-deeper-solution-to-the-mystery-of-moralism-believing-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20deeper%20solution%20to%20the%20mystery%20of%20moralism%E2%80%94Believing%20in%20morality%20and%20free%20will%20are%20hazardous%20to%20your%20mental%20health%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw6jtBqbyJ3SfgAyGc%2Fthe-deeper-solution-to-the-mystery-of-moralism-believing-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw6jtBqbyJ3SfgAyGc%2Fthe-deeper-solution-to-the-mystery-of-moralism-believing-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1355, "htmlBody": "<p>[<a href=\"http://juridicalcoherence.blogspot.com/2012/10/normal-0-false-false-false-en-us-x-none.html\">Crossposted</a>.]</p>\n<div class=\"MsoNormal\" style=\"text-align: center;\"><strong><span style=\"font-family: Verdana, sans-serif;\">The<span>&nbsp;</span>complex relationship between Systems 1 and 2 and construal level</span></strong></div>\n<div class=\"MsoNormal\" style=\"text-align: center;\"><br /></div>\n<div class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif;\">The distinction between pre-attentive and focal-attentive mental processes<span>&nbsp;<span>&nbsp;</span></span>has dominated cognitive psychology for some 35 years. In the past decade has arisen<span>&nbsp;</span><a style=\"text-decoration: none; color: #800080;\" href=\"http://disputedissues.blogspot.com/2012/02/construal-level-theory-and-matching.html\">another cognitive dichotomy</a><span>&nbsp;</span>specific to social psychology: processes of<span>&nbsp;</span><em>abstract construal</em><span>&nbsp;</span>(<em>far</em><span>&nbsp;</span>cognition) versus<em>concrete construal</em><span>&nbsp;</span>(<em>near</em><span>&nbsp;</span>cognition).<span>&nbsp;</span><strong><span style=\"color: red;\">This essay will theorize about the relationship between these dichotomies to clarify further how believing in the existence of free will and in the objective existence of morality can thwart reason by causing you to choose what you don&rsquo;t want.</span></strong></span></div>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif;\">The state of the art on pre-attentive and focal-attentive processes is Daniel Kahneman&rsquo;s book<em>Thinking, Fast and Slow</em>, where he calls pre-attentive processes<span>&nbsp;</span><em>System 1</em><span>&nbsp;</span>and focal-attentive processes<span>&nbsp;</span><em>System 2</em>. The reification of processes into fictional systems also resembles Freud&rsquo;s<em>System Csc</em><span>&nbsp;</span>(Conscious) and<span>&nbsp;</span><em>System Pcs</em><span>&nbsp;</span>(Preconscious). I&rsquo;ll adopt the language<span>&nbsp;</span><em>System 1</em><span>&nbsp;</span>and<em>System 2</em>, but readers can apply their understanding of<span>&nbsp;</span><em>conscious &ndash;preconscious</em>,<span>&nbsp;</span><em>pre-attentive &ndash; focal-attentive</em>, or<span>&nbsp;</span><em>automatic processes &ndash; controlled processes</em><span>&nbsp;</span>dichotomies. They name the same distinction, in which System 1 consists of processes occurring quickly and effortlessly in parallel outside awareness; System 2 consists of processes occurring slowly and effortfully in sequential<em>awareness</em>, which in this context refers to the contents of working memory rather than<span>&nbsp;</span><a style=\"text-decoration: none; color: #800080;\" href=\"http://juridicalcoherence.blogspot.com/2012/09/161-raw-experience-dogma-dissolving.html\">raw experience</a><span>&nbsp;</span>and accompanies System 2 activity.</span></div>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif;\">To integrate Systems 1 and 2 with construal-level theory, we note that System 2&mdash;the conscious part of our minds&mdash;can perform any of three routines in making a decision about taking some action,<span>&nbsp;</span><strong>such as whether to vote in an election</strong>, a good example not just for timeliness but also for linkages to our main concern with morality: voting is a clear example of an action without tangible benefit.<span>&nbsp;</span><strong>The</strong><span>&nbsp;</span><strong>potential voter might:</strong></span></div>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\" style=\"margin-left: 0.5in;\"><strong><span style=\"color: #0070c0; font-family: Verdana, sans-serif;\">Case 1.</span></strong><span style=\"color: #0070c0; font-family: Verdana, sans-serif;\"><span>&nbsp;</span>Make a conscious decision to vote based on applying the principle that citizens owe a duty to vote in elections.</span></div>\n<div class=\"MsoNormal\" style=\"margin-left: 0.5in;\"><strong><span style=\"color: #00b050; font-family: Verdana, sans-serif;\">Case 2.</span></strong><span style=\"color: #00b050; font-family: Verdana, sans-serif;\"><span>&nbsp;</span>Decide to be open to the candidates&rsquo; substantive positions and vote only if either candidate seems worthy of support.</span></div>\n<div class=\"MsoNormal\" style=\"margin-left: 0.5in;\"><strong><span style=\"color: #7030a0; font-family: Verdana, sans-serif;\">Case 3.</span></strong><span style=\"color: #7030a0; font-family: Verdana, sans-serif;\"><span>&nbsp;</span>Experience a change of mind between 1 and 2.</span></div>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\"><strong><span style=\"font-family: Verdana, sans-serif;\">The preceding were examples of the three routines System 2 can perform:</span></strong></div>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\" style=\"margin-left: 0.5in;\"><strong><span style=\"color: #0070c0; font-family: Verdana, sans-serif;\">Case 1.</span></strong><span style=\"color: #0070c0; font-family: Verdana, sans-serif;\"><span>&nbsp;</span>Make the choice.</span></div>\n<div class=\"MsoNormal\" style=\"margin-left: 0.5in;\"><strong><span style=\"color: #00b050; font-family: Verdana, sans-serif;\">Case 2.</span></strong><span style=\"color: #00b050; font-family: Verdana, sans-serif;\"><span>&nbsp;</span>&ldquo;Program&rdquo; System 1 to make the choice based on automatic criteria that don&rsquo;t require sequential thinking.</span></div>\n<div class=\"MsoNormal\" style=\"margin-left: 0.5in;\"><strong><span style=\"color: #7030a0; font-family: Verdana, sans-serif;\">Case 3.</span></strong><span style=\"color: #7030a0; font-family: Verdana, sans-serif;\"><span>&nbsp;</span>Interrupt System 1 in the face of anomalies.</span></div>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif;\"><strong><span style=\"color: red;\">When System 2 initiates action, whether it retains the power to decide or passes to System 1 is the difference between concrete and abstract construal.<span>&nbsp;</span></span></strong>The second routine is key to understanding how Systems 1 and 2 work to produce the effects construal-level theory predicts. Keep in mind that the unconscious, automatic System 1 includes not just hardwired patterns but also skilled habits. Meanwhile, System 2 is notoriously &ldquo;lazy,&rdquo; unwilling to interrupt System 1, as in<span>&nbsp;</span><span style=\"color: #741b47;\">Case 3</span>, but despite the perennial biases that plague system 1, resulting from letting System 1 have its way, the highest levels of expertise also occur in System 1.</span></div>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif;\">A delegate System 1 operates with potentially complex holistic patterns typifying<span>&nbsp;</span><em>far</em><span>&nbsp;</span>cognition. This pattern is<span>&nbsp;</span><em>far</em><span>&nbsp;</span>because we offload distant matter to System 1 but exercise sequential control under System 2 as immediacy looms&mdash;although there are many exceptions. It is critical to distinguish<span>&nbsp;</span><em>far</em><span>&nbsp;</span>cognition from the lazy failure of System 2 to perform properly in<span>&nbsp;</span><span style=\"color: #7030a0;\">Case 3</span>. Such failure isn&rsquo;t specific to mode.<span>&nbsp;</span><em>Far</em><span>&nbsp;</span>cognition, System 1 acting as delegate for System 2, is a narrower concept than automatic cognition, but<span>&nbsp;</span><em>far</em><span>&nbsp;</span>cognition<span>&nbsp;</span><em>is</em><span>&nbsp;</span>automatic cognition.<span>&nbsp;</span><em>Near</em>cognition admits no easy cross-classification.</span></div>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\" style=\"text-align: center;\"><strong><span style=\"font-family: Verdana, sans-serif;\">Belief in free will and moral realism undermine our &ldquo;fast and frugal heuristics&rdquo;</span></strong></div>\n<div class=\"MsoNormal\" style=\"text-align: center;\"><br /></div>\n<div class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif;\">The two most important recent books on the cognitive psychology of decision and judgment are<em>Thinking, Fast and Slow<span>&nbsp;</span></em>by Daniel Kahneman and<span>&nbsp;</span><em>Gut Reactions: The Intelligence of the Unconscious</em><span>&nbsp;</span>by Gerd Gigerenzer, and both insist on the<span>&nbsp;</span><em>contrast</em><span>&nbsp;</span>between their positions, although conflicts aren&rsquo;t obvious. Kahneman explains System 1 biases as due to the mechanisms employed outside the range of evolutionary usefulness; Gigerenzer describes &ldquo;fast and frugal heuristics&rdquo; that sometimes misfire to produce biases. Where these half-empty versus half-full positions on heuristics and biases really differ is their overall appraisal of<span>&nbsp;</span><em>near</em><span>&nbsp;</span>and<span>&nbsp;</span><em>far</em><span>&nbsp;</span>processes, as Gigerenzer is a<span>&nbsp;</span><em>far</em><span>&nbsp;</span>thinker and Kahneman a<span>&nbsp;</span><em>near</em><span>&nbsp;</span>thinker, and they are both naturally biased for their preferred modes.<span>&nbsp;</span><em>Far</em><span>&nbsp;</span>thought shows more confidence in fast-and-frugal heuristics, since it offloads to System 1, whose province is to employ them.</span></div>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif;\">The fast-and-frugal-heuristics way of thinking is particularly useful in understanding the effect of moral realism and free will: they cause System 2 to supplant System 1 in decision-making. When we apply principles of integrity to regulate our conduct, sometimes we do better in<span>&nbsp;</span><em>far mode</em>, where System 2 offloads the task of determining compliance to System 1. To the contrary, if you have a principle of integrity that includes an absolute obligation to vote, you act as in<span>&nbsp;</span><span style=\"color: blue;\">Case 1</span>: on a conscious decision. But principles of integrity do not really take this absolute form, an illusion begotten by moral realism. A principle of integrity flexible enough for actual use might favor voting (based, say, on a general principle embracing an obligation to perform duties) but disfavor it for &ldquo;lowering the bar&rdquo; when there&rsquo;s only a choice between the lesser of evils. To practice the art of objectively applying these principles depends on your honest appraisal of the strength of your commitment to each virtue. System 2 is incapable of this feat; when it can be accomplished, it&rsquo;s due to System 1&rsquo;s automatic skills, operating unconsciously.<span style=\"color: red;\"><strong>Principles of integrity are applied more accurately in<span>&nbsp;</span><em>far-mode</em><span>&nbsp;</span>than<span>&nbsp;</span><em>near-mode</em>.</strong></span><span>&nbsp;</span>[Hat Tip to<span>&nbsp;</span><a style=\"text-decoration: none; color: #800080;\" href=\"http://www.overcomingbias.com/\">Overcoming Bias</a><span>&nbsp;</span>for these convenient phrases.]</span></div>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif;\">But belief in moral realism and free will impel moral actors to apply their principles in<span>&nbsp;</span><em>near-mode</em>. Objective morality and moral realism imply that compliance with morality results from freely willed acts. I&rsquo;m not going to defend this premise thoroughly here, but this thought experiment might carry some persuasive weight.<span>&nbsp;</span><strong>Read the following in near mode, and introspect your emotions:</strong></span></div>\n<div class=\"MsoQuote\"><br /></div>\n<p>&nbsp;</p>\n<blockquote class=\"tr_bq\">\n<div class=\"MsoQuote\"><strong><span style=\"color: #538135; font-family: Verdana, sans-serif;\">Sexual predator Jerry Sandusky will serve his time in a minimal security prison, where he&rsquo;s allowed groups of visitors five days a week.</span></strong></div>\n</blockquote>\n<p>&nbsp;</p>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif;\">Some readers will experience a sense of outrage. Then remind yourself:<span>&nbsp;</span><strong>There&rsquo;s no free will.</strong>If you believe the reminder, your outrage will subside; if you&rsquo;ve long been a convinced and consistent determinist, you might not need to remind yourself.<span>&nbsp;</span><em>Morality</em><span>&nbsp;</span>inculpates based on acts of<span>&nbsp;</span><em>free will</em>: morality and free will are inseparable.</span></div>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif;\">A point I must emphasize because of its novelty: it&rsquo;s System 1 that ordinarily determines what you want. System 2 doesn&rsquo;t ordinarily deliberate about the subject directly; it deliberates about relevant facts, but in the end, you can only<span>&nbsp;</span><em>intuit</em><span>&nbsp;</span>your volition. You can&rsquo;t deduce it.</span></div>\n<div class=\"MsoNormal\"><br /><span style=\"font-family: Verdana, sans-serif;\">What a belief in moral realism and free will do is nothing less than change the architecture of decision-making. When we practice principles of integrity and internalize them, they and nonmoral considerations<span>&nbsp;</span><em>co-determine</em><span>&nbsp;</span>our System 1 judgments, whereas according to moral realism and free will, moral good is the product of conscious free choice, so System 2<span>&nbsp;</span><em>contrasts</em>its moral opinion to System 1&rsquo;s intuition, for which System 2 compensates&mdash;and usually overcompensates. The voter had to weigh the imperatives of the duty to vote and the duty to avoid &ldquo;lowering the bar&rdquo; when both candidates are ideologically and programmatically distasteful. System 2 can prime and program System 1 by studying the issues, but the multifaceted decision is itself best made by System 1. What happens when System 2 tries to decide these propositions? System 2 makes the qualitative judgment that System 1 is biased one way or the other and<span>&nbsp;</span><em>corrects</em><span>&nbsp;</span>System 1. This will implicate the<span>&nbsp;</span><a style=\"text-decoration: none; color: #800080;\" href=\"http://disputedissues.blogspot.com/2008/11/overcompensation-for-persuasion.html\">overcompensation bias</a>, in which conscious attempts to counteract biases usually overcorrect. A voter who thinks correction is needed for a bias toward shirking duty will vote when not really wanting to, all things considered. A voter biased toward \"lowering the bar\" will be<span>&nbsp;</span><em>excessively</em><span>&nbsp;</span>purist. Whatever standard the voter uses will be taken too far.</span></div>\n<div class=\"MsoNormal\" style=\"text-align: center;\"><br /></div>\n<div class=\"MsoNormal\" style=\"text-align: center;\"><strong><span style=\"font-family: Verdana, sans-serif;\">Belief in moral realism and free will biases practical reasoning</span></strong></div>\n<div class=\"MsoNormal\" style=\"text-align: center;\"><br /></div>\n<div class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif;\">This essay presents the third of three ways that b</span><span style=\"font-family: Verdana, sans-serif;\">elief in objective morality and free will can cause people to do what they don&rsquo;t want to do:</span></div>\n<p>&nbsp;</p>\n<ol>\n<li><span style=\"font-family: Verdana, sans-serif;\">It retards people in<span>&nbsp;</span><a style=\"text-decoration: none; color: #800080;\" href=\"http://juridicalcoherence.blogspot.com/2011/12/14-why-do-what-oughta-habit-theory-of.html\">adaptively changing</a><span>&nbsp;</span>their principles of integrity.</span></li>\n<li><span style=\"font-family: Verdana, sans-serif;\">It prevents people from<span>&nbsp;</span><a style=\"text-decoration: none; color: #800080;\" href=\"http://juridicalcoherence.blogspot.com/2012/04/143-unraveling-mystery-of-morality.html\">questioning<span>&nbsp;</span></a>their so-called foundations.</span></li>\n<li><strong><span style=\"color: red;\"><span style=\"font-family: Verdana, sans-serif;\">It systematically exaggerates the compellingness of moral claims.</span></span></strong></li>\n</ol>\n<p>&nbsp;</p>\n<div class=\"MsoNormal\"><span style=\"font-family: Verdana, sans-serif;\">Some will be tempted to think that the third either is contrary to experience or is socially desirable. It&rsquo;s neither. In moralism, an exaggerated subjective sense of duty and excessive sense of guilt<span>&nbsp;</span><em>co-exist</em><span>&nbsp;</span>with unresponsiveness to morality&rsquo;s practical demands.</span></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w6jtBqbyJ3SfgAyGc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 57, "baseScore": -34, "extendedScore": null, "score": -6.6e-05, "legacy": true, "legacyId": "19357", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-15T14:51:33.293Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Complexity and Intelligence", "slug": "seq-rerun-complexity-and-intelligence", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GhdmNSXNqghGkvx76/seq-rerun-complexity-and-intelligence", "pageUrlRelative": "/posts/GhdmNSXNqghGkvx76/seq-rerun-complexity-and-intelligence", "linkUrl": "https://www.lesswrong.com/posts/GhdmNSXNqghGkvx76/seq-rerun-complexity-and-intelligence", "postedAtFormatted": "Monday, October 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Complexity%20and%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Complexity%20and%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGhdmNSXNqghGkvx76%2Fseq-rerun-complexity-and-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Complexity%20and%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGhdmNSXNqghGkvx76%2Fseq-rerun-complexity-and-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGhdmNSXNqghGkvx76%2Fseq-rerun-complexity-and-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p>Today's post, <a href=\"/lw/vh/complexity_and_intelligence/\">Complexity and Intelligence</a> was originally published on 03 November 2008.  A summary:</p>\n<blockquote>One of the Godel-inspired challenges to the idea of self-improving minds is based on the notion of \"complexity\". But everyday usage and technical usage differ in some really important ways.</blockquote>\n<p>&nbsp;</p>\n<p>Discuss the post here (rather than in the comments to the original post).</p>\n<p>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/ex4/seq_rerun_building_something_smarter/\">Building Something Smarter</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.</p>\n<p>Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GhdmNSXNqghGkvx76", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0099585851422648e-06, "legacy": true, "legacyId": "19381", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rELc88PvDkhetQzqx", "HgJ5iA6AgBbajcmA9", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-15T19:51:50.285Z", "modifiedAt": null, "url": null, "title": "A presentation about Cox's Theorem made for my English class", "slug": "a-presentation-about-cox-s-theorem-made-for-my-english-class", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:07.664Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BT_Uytya", "createdAt": "2011-12-03T16:41:14.863Z", "isAdmin": false, "displayName": "BT_Uytya"}, "userId": "Enh7Ap3zRTQDR4gMH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YKeZJMMx42qMQKWSw/a-presentation-about-cox-s-theorem-made-for-my-english-class", "pageUrlRelative": "/posts/YKeZJMMx42qMQKWSw/a-presentation-about-cox-s-theorem-made-for-my-english-class", "linkUrl": "https://www.lesswrong.com/posts/YKeZJMMx42qMQKWSw/a-presentation-about-cox-s-theorem-made-for-my-english-class", "postedAtFormatted": "Monday, October 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20presentation%20about%20Cox's%20Theorem%20made%20for%20my%20English%20class&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20presentation%20about%20Cox's%20Theorem%20made%20for%20my%20English%20class%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYKeZJMMx42qMQKWSw%2Fa-presentation-about-cox-s-theorem-made-for-my-english-class%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20presentation%20about%20Cox's%20Theorem%20made%20for%20my%20English%20class%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYKeZJMMx42qMQKWSw%2Fa-presentation-about-cox-s-theorem-made-for-my-english-class", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYKeZJMMx42qMQKWSw%2Fa-presentation-about-cox-s-theorem-made-for-my-english-class", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>At my English class everybody was supposed to make a short presentation about subject of one's choice. I decided to tell people about Cox's Theorem (heavily based on the introduction in \"Probability theory: The Logic Of Science\" by E T Jaynes and \"<span class=\"st\">Constructing a logic of plausible inference: a <em>guide</em> to <em>Cox's theorem</em></span>\" by Kevin S. Van Horn). Thought someone might find that interesting or useful.</p>\n<p>&nbsp;</p>\n<p>Make sure that you have speaker notes visible.</p>\n<p>https://docs.google.com/file/d/0BwJocL_GupTsNnMtdWFLT3RYWGs/edit</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YKeZJMMx42qMQKWSw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.0101191363192813e-06, "legacy": true, "legacyId": "19382", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-15T21:42:33.679Z", "modifiedAt": null, "url": null, "title": "Problem of Optimal False Information", "slug": "problem-of-optimal-false-information", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:39.746Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Endovior", "createdAt": "2011-03-14T14:29:56.346Z", "isAdmin": false, "displayName": "Endovior"}, "userId": "2SGSP9KceMqSsjQWj", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/acTr9pk9PFT9j6tyL/problem-of-optimal-false-information", "pageUrlRelative": "/posts/acTr9pk9PFT9j6tyL/problem-of-optimal-false-information", "linkUrl": "https://www.lesswrong.com/posts/acTr9pk9PFT9j6tyL/problem-of-optimal-false-information", "postedAtFormatted": "Monday, October 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Problem%20of%20Optimal%20False%20Information&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProblem%20of%20Optimal%20False%20Information%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FacTr9pk9PFT9j6tyL%2Fproblem-of-optimal-false-information%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Problem%20of%20Optimal%20False%20Information%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FacTr9pk9PFT9j6tyL%2Fproblem-of-optimal-false-information", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FacTr9pk9PFT9j6tyL%2Fproblem-of-optimal-false-information", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 415, "htmlBody": "<p>This is a thought that occured to me on my way to classes today; sharing it for feedback.</p>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Omega\">Omega</a> appears before you, and after presenting an arbitrary proof that it is, in fact, a completely trustworthy superintelligence of the caliber needed to play these kinds of games, presents you with a choice between two boxes. &nbsp;These boxes do not contain money, they contain information. &nbsp;One box is white and contains a true fact that you do not currently know; the other is black and contains false information that you do not currently believe. &nbsp;Omega advises you that the the true fact is not misleading in any way (ie: not a fact that will cause you to make incorrect assumptions and lower the accuracy of your probability estimates), and is fully supported with enough evidence to both prove to you that it is true, and enable you to&nbsp;independently&nbsp;verify&nbsp;its truth for yourself within a month. &nbsp;The false information is demonstrably false, and is something that you would disbelieve if presented outright, but if you open the box to discover it, a machine inside the box will reprogram your mind such that you will believe it completely, thus leading you to believe other related falsehoods, as you rationalize away discrepancies.</p>\n<p>Omega further advises that, within those constraints, the true fact is one that has been optimized to inflict upon you the maximum amount of long-term disutility for a fact in its class, should you now become aware of it, and the false information has been optimized to provide you with the maximum amount of long-term utility for a belief in its class, should you now begin to believe it over the truth. &nbsp;You are required to choose one of the boxes; if you refuse to do so, Omega will kill you outright and try again on another <a href=\"http://wiki.lesswrong.com/wiki/Everett_branch\">Everett branch</a>.&nbsp; Which box do you choose, and why?</p>\n<p>&nbsp;</p>\n<p>(This example is obviously hypothetical, but for a simple and practical case, consider the use of <a href=\"http://en.wikipedia.org/wiki/Drug-induced_amnesia\">amnesia-inducing drugs</a> to selectively eliminate traumatic memories; it would be more accurate to still have those memories, taking the time and effort to come to terms with the trauma... but present much greater utility to be without them, and thus without the trauma altogether. &nbsp;Obviously related to the <a href=\"http://wiki.lesswrong.com/wiki/Valley_of_bad_rationality\">valley of bad rationality</a>, but since there clearly exist most optimal lies and least optimal truths, it'd be useful to know which categories of facts are <a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">generally hazardous</a>, and whether or not there are categories of lies which are generally helpful.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1f2": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "acTr9pk9PFT9j6tyL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 21, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "19383", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 113, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AdYdLP2sRqPMoe8fb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-15T23:48:36.533Z", "modifiedAt": null, "url": null, "title": "Singularity Summit 2012, discuss it here", "slug": "singularity-summit-2012-discuss-it-here", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:29.339Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TkQf24Q9jmPkWKmg8/singularity-summit-2012-discuss-it-here", "pageUrlRelative": "/posts/TkQf24Q9jmPkWKmg8/singularity-summit-2012-discuss-it-here", "linkUrl": "https://www.lesswrong.com/posts/TkQf24Q9jmPkWKmg8/singularity-summit-2012-discuss-it-here", "postedAtFormatted": "Monday, October 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20Summit%202012%2C%20discuss%20it%20here&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20Summit%202012%2C%20discuss%20it%20here%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTkQf24Q9jmPkWKmg8%2Fsingularity-summit-2012-discuss-it-here%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20Summit%202012%2C%20discuss%20it%20here%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTkQf24Q9jmPkWKmg8%2Fsingularity-summit-2012-discuss-it-here", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTkQf24Q9jmPkWKmg8%2Fsingularity-summit-2012-discuss-it-here", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<p>How was it?&nbsp; Which speakers delivered according to expectations?</p>\n<p>Which topics were left unresolved?</p>\n<p>Were any topics resolved?</p>\n<p>Whatever you have to say about it, say it here.</p>\n<p>Suggestion: if you are going to comment, mention \"I was there\" just so we know who was or wasn't.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TkQf24Q9jmPkWKmg8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 8, "extendedScore": null, "score": 1.010245761427528e-06, "legacy": true, "legacyId": "19385", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-16T00:35:17.261Z", "modifiedAt": null, "url": null, "title": "Decision theory and \"winning\"", "slug": "decision-theory-and-winning", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:08.722Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MDq7vgnbyQJQAEeuT/decision-theory-and-winning", "pageUrlRelative": "/posts/MDq7vgnbyQJQAEeuT/decision-theory-and-winning", "linkUrl": "https://www.lesswrong.com/posts/MDq7vgnbyQJQAEeuT/decision-theory-and-winning", "postedAtFormatted": "Tuesday, October 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Decision%20theory%20and%20%22winning%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecision%20theory%20and%20%22winning%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMDq7vgnbyQJQAEeuT%2Fdecision-theory-and-winning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Decision%20theory%20and%20%22winning%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMDq7vgnbyQJQAEeuT%2Fdecision-theory-and-winning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMDq7vgnbyQJQAEeuT%2Fdecision-theory-and-winning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1039, "htmlBody": "<p>With much help from <a href=\"/user/crazy88/\">crazy88</a>, I'm still developing my Decision Theory FAQ. Here's the current section on <em>Decision Theory and \"Winning\".</em>&nbsp;I feel pretty uncertain about it, so I'm posting it here for feedback. (In the FAQ, CDT and EDT and TDT and Newcomblike problems have already been explained.)</p>\n<blockquote>\n<p>One of the primary motivations for developing TDT is a sense that both CDT and EDT fail to reason in a desirable manner in some decision scenarios. However, despite acknowledging that CDT agents end up worse off in Newcomb's Problem, many (and perhaps the majority of) decision theorists are proponents of CDT. On the face of it, this may seem to suggest that these decision theorists aren't interested in developing a decision algorithm that \"wins\" but rather have some other aim in mind. If so then this might lead us to question the value of developing one-boxing decision algorithms.</p>\n<p>However, the claim that most decision theorists don&rsquo;t care about finding an algorithm that &ldquo;wins&rdquo; mischaracterizes their position. After all, proponents of CDT tend to take the challenge posed by the fact that CDT agents &ldquo;lose&rdquo; in Newcomb's problem seriously (in the philosophical literature, it's often referred to as the <em>Why ain'cha rich?</em> problem). A common reaction to this challenge is neatly summarized in <a href=\"http://www.amazon.com/Foundations-Decision-Cambridge-Probability-Induction/dp/0521641640\">Joyce (1999, p. 153-154 )</a> as a response to a hypothetical question about why, if two-boxing is rational, the CDT agent does not end up as rich as an agent that one-boxes:</p>\n<blockquote>\n<p>Rachel has a perfectly good answer to the \"Why ain't you rich?\" question. \"I am not rich,\" she will say, \"because I am not the kind of person [Omega] thinks will refuse the money. I'm just not like you, Irene [the one-boxer]. Given that I know that I am the type who takes the money, and given that [Omega] knows that I am this type, it was reasonable of me to think that the $1,000,000 was not in [the box]. The $1,000 was the most I was going to get no matter what I did. So the only reasonable thing for me to do was to take it.\"</p>\n<p>Irene may want to press the point here by asking, \"But don't you wish you were like me, Rachel?\"... Rachael can and should admit that she <em>does</em> wish she were more like Irene... At this point, Irene will exclaim, \"You've admitted it! It wasn't so smart to take the money after all.\" Unfortunately for Irene, her conclusion does not follow from Rachel's premise. Rachel will patiently explain that wishing to be a [one-boxer] in a Newcomb problem is not inconsistent with thinking that one should take the $1,000 <em>whatever type one is</em>. When Rachel wishes she was Irene's type she is wishing for <em>Irene's options</em>, not sanctioning her choice... While a person who knows she will face (has faced) a Newcomb problem might wish that she were (had been) the type that [Omega] labels a [one-boxer], this wish does not provide a reason for <em>being</em> a [one-boxer]. It might provide a reason to try (before [the boxes are filled]) to change her type <em>if she thinks this might affect [Omega's] prediction</em>, but it gives her no reason for doing anything other than taking the money once she comes to believes that she will be unable to influence what [Omega] does.</p>\n</blockquote>\n<p>In other words, this response distinguishes between the <em>winning decision</em> and the <em>winning type of agent</em> and claims that two-boxing is the winning decision in Newcomb&rsquo;s problem (even if one-boxers are the winning type of agent). Consequently, insofar as decision theory is about determining which <em>decision</em> is rational, on this account CDT reasons correctly in Newcomb&rsquo;s problem.</p>\n<p>For those that find this response perplexing, an analogy could be drawn to the <em>chewing gum problem</em>. In this scenario, there is near unanimous agreement that the rational decision is to chew gum. However, statistically, non-chewers will be better off than chewers. As such, the non-chewer could ask, &ldquo;if you&rsquo;re so smart, why aren&rsquo;t you healthy?&rdquo;. In this case, the above response seems particularly appropriate. The chewers are less healthy not because of their decision but rather because they&rsquo;re more likely to have an undesirable gene. Having good genes doesn&rsquo;t make the non-chewer more rational but simply more lucky. The proponent of CDT simply extends this response to Newcomb&rsquo;s problem.</p>\n<p>One final point about this response is worth nothing. A proponent of CDT can accept the above argument but still acknowledge that, if given the choice before the boxes are filled, they would be rational to choose to modify themselves to be a one-boxing type of agent (as Joyce acknowledged in the above passage and as argued for in <a href=\"http://www.jstor.org/stable/20118389\">Burgess, 2004</a>). To the proponent of CDT, this is unproblematic: if we are sometimes rewarded not for the rationality of our decisions in the moment but for the type of agent we were at some past moment then it should be unsurprising that changing to a different type of agent might be beneficial.</p>\n<p>The response to this defense of two-boxing in Newcomb&rsquo;s problem has been divided. Many find it compelling but others, like <a href=\"http://link.springer.com/article/10.1007%2Fs10670-011-9355-2\">Ahmed and Price (2012)</a> think it does not adequately address to the challenge:</p>\n<blockquote>\n<p>It is no use the causalist's whining that foreseeably, Newcomb problems do in fact reward irrationality, or rather CDT-irrationality. The point of the argument is that if everyone knows that the CDT-irrational strategy will in fact do better on average than the CDT-rational strategy, then it's rational to play the CDT-irrational strategy.</p>\n</blockquote>\n<p>Given this, there seem to be two positions one could take on these issues. If the response given by the proponent of CDT is compelling, then we should be attempting to develop a decision theory that two-boxes on Newcomb&rsquo;s problem. Perhaps the best theory for this role is CDT but perhaps it is instead BT, which many people think reasons better in the psychopath button scenario. On the other hand, if the response given by the proponents of CDT is not compelling, then we should be developing a theory that one-boxes in Newcomb&rsquo;s problem. In this case, TDT, or something like it, seems like the most promising theory currently on offer.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MDq7vgnbyQJQAEeuT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.0102707285679523e-06, "legacy": true, "legacyId": "19386", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-16T04:30:50.665Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Back Up and Ask Whether, Not Why", "slug": "seq-rerun-back-up-and-ask-whether-not-why", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o32Fyn3Z4mne7N2Ts/seq-rerun-back-up-and-ask-whether-not-why", "pageUrlRelative": "/posts/o32Fyn3Z4mne7N2Ts/seq-rerun-back-up-and-ask-whether-not-why", "linkUrl": "https://www.lesswrong.com/posts/o32Fyn3Z4mne7N2Ts/seq-rerun-back-up-and-ask-whether-not-why", "postedAtFormatted": "Tuesday, October 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Back%20Up%20and%20Ask%20Whether%2C%20Not%20Why&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Back%20Up%20and%20Ask%20Whether%2C%20Not%20Why%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo32Fyn3Z4mne7N2Ts%2Fseq-rerun-back-up-and-ask-whether-not-why%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Back%20Up%20and%20Ask%20Whether%2C%20Not%20Why%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo32Fyn3Z4mne7N2Ts%2Fseq-rerun-back-up-and-ask-whether-not-why", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo32Fyn3Z4mne7N2Ts%2Fseq-rerun-back-up-and-ask-whether-not-why", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<p>Today's post, <a href=\"/lw/vk/back_up_and_ask_whether_not_why/\">Back Up and Ask Whether, Not Why</a> was originally published on 06 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Back_Up_and_Ask_Whether.2C_Not_Why\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When someone asks you why you're doing \"X\", don't ask yourself why you're doing \"X\". Ask yourself whether someone should do \"X\".</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/eyd/seq_rerun_complexity_and_intelligence/\">Complexity and Intelligence</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o32Fyn3Z4mne7N2Ts", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0103967377552855e-06, "legacy": true, "legacyId": "19388", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nHRri2F49MZbcqmkY", "GhdmNSXNqghGkvx76", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-16T04:33:52.859Z", "modifiedAt": null, "url": null, "title": "Thinking soberly about the context and consequences of Friendly AI", "slug": "thinking-soberly-about-the-context-and-consequences-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:22.124Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/czxjKohS7RQjkBiSD/thinking-soberly-about-the-context-and-consequences-of", "pageUrlRelative": "/posts/czxjKohS7RQjkBiSD/thinking-soberly-about-the-context-and-consequences-of", "linkUrl": "https://www.lesswrong.com/posts/czxjKohS7RQjkBiSD/thinking-soberly-about-the-context-and-consequences-of", "postedAtFormatted": "Tuesday, October 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thinking%20soberly%20about%20the%20context%20and%20consequences%20of%20Friendly%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThinking%20soberly%20about%20the%20context%20and%20consequences%20of%20Friendly%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FczxjKohS7RQjkBiSD%2Fthinking-soberly-about-the-context-and-consequences-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thinking%20soberly%20about%20the%20context%20and%20consequences%20of%20Friendly%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FczxjKohS7RQjkBiSD%2Fthinking-soberly-about-the-context-and-consequences-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FczxjKohS7RQjkBiSD%2Fthinking-soberly-about-the-context-and-consequences-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 297, "htmlBody": "<p>The project of Friendly AI would benefit from being approached in a much more down-to-earth way. Discourse about the subject seems to be dominated by a set of possibilities which are given far too much credence:</p>\n<ul>\n<li>A single AI will take over the world </li>\n<li>A future galactic civilization depends on 21st-century Earth </li>\n<li>10<sup>n</sup>-year lifespans are at stake, n greater than or equal to 3&nbsp;</li>\n<li>We might be living in a simulation </li>\n<li>Acausal deal-making </li>\n<li>Multiverse theory&nbsp;</li>\n</ul>\n<p>Add up all of that, and you have a great recipe for enjoyable irrelevance. Negate every single one of those ideas, and you have an alternative set of working assumptions that are still consistent with the idea that Friendly AI matters, and which are much more suited to practical success:</p>\n<ul>\n<li>There will always be multiple centers of power</li>\n<li>What's at stake is, at most, the future centuries of a solar-system civilization </li>\n<li>No assumption that individual humans can survive even for hundreds of years, or that they would want to</li>\n<li>Assume that the visible world is the real world</li>\n<li>Assume that life and intelligence are about causal interaction</li>\n<li>Assume that the single visible world is the only world we affect or have reason to care about&nbsp;</li>\n</ul>\n<p>The simplest reason to care about Friendly AI is that we are going to be coexisting with AI, and so we should want it to be something we can live with. I don't see that anything important would be lost by strongly foregrounding the second set of assumptions, and treating the first set of possibilities just as possibilities, rather than as the working hypothesis about reality.</p>\n<p>[Earlier posts on related themes: <a href=\"/lw/772/what_a_practical_plan_for_friendly_ai_looks_like/\">practical FAI</a>, <a href=\"/lw/c1x/extrapolating_values_without_outsourcing/\">FAI without \"outsourcing\"</a>.]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 2, "ZFrgTgzwEfStg26JL": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "czxjKohS7RQjkBiSD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 20, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "19389", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kv7gvQ9AkDisCL2kg", "MWjtBRnWvrEGB22KB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-16T05:29:24.231Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 10/15/12", "slug": "group-rationality-diary-10-15-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:23.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ekdyBuErNG9MDKYeg/group-rationality-diary-10-15-12", "pageUrlRelative": "/posts/ekdyBuErNG9MDKYeg/group-rationality-diary-10-15-12", "linkUrl": "https://www.lesswrong.com/posts/ekdyBuErNG9MDKYeg/group-rationality-diary-10-15-12", "postedAtFormatted": "Tuesday, October 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%2010%2F15%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%2010%2F15%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FekdyBuErNG9MDKYeg%2Fgroup-rationality-diary-10-15-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%2010%2F15%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FekdyBuErNG9MDKYeg%2Fgroup-rationality-diary-10-15-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FekdyBuErNG9MDKYeg%2Fgroup-rationality-diary-10-15-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">T<span style=\"color: #333333;\">his is the public group instrumental rationality diary for the week of October 15th. It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<div id=\"entry_t3_drj\" class=\"content clear\" style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; font-size: 12px; line-height: 18px;\">\n<div class=\"md\">\n<ul style=\"padding: 0px; line-height: 19px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Thanks to everyone who contributes!</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\"><a style=\"color: #8a8a8b;\" href=\"/lw/eqv/group_rationality_diary_10112/\">Previous diary</a>;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">archive of prior diaries</a>.</p>\n</div>\n</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ekdyBuErNG9MDKYeg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0104280678391256e-06, "legacy": true, "legacyId": "19396", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EjEBNhNoeKBEoqXy6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-16T13:54:38.675Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow: Biases, Applied Rationality, Visual Thinking", "slug": "meetup-moscow-biases-applied-rationality-visual-thinking", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4qGa7Q5zY7ogBhrki/meetup-moscow-biases-applied-rationality-visual-thinking", "pageUrlRelative": "/posts/4qGa7Q5zY7ogBhrki/meetup-moscow-biases-applied-rationality-visual-thinking", "linkUrl": "https://www.lesswrong.com/posts/4qGa7Q5zY7ogBhrki/meetup-moscow-biases-applied-rationality-visual-thinking", "postedAtFormatted": "Tuesday, October 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%3A%20Biases%2C%20Applied%20Rationality%2C%20Visual%20Thinking&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%3A%20Biases%2C%20Applied%20Rationality%2C%20Visual%20Thinking%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4qGa7Q5zY7ogBhrki%2Fmeetup-moscow-biases-applied-rationality-visual-thinking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%3A%20Biases%2C%20Applied%20Rationality%2C%20Visual%20Thinking%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4qGa7Q5zY7ogBhrki%2Fmeetup-moscow-biases-applied-rationality-visual-thinking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4qGa7Q5zY7ogBhrki%2Fmeetup-moscow-biases-applied-rationality-visual-thinking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ev'>Moscow: Biases, Applied Rationality, Visual Thinking</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 October 2012 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rossiya, Moscow, ulitsa Ostozhenka 14/2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant, entrance from Lopukhinskiy pereulok. Look for a table with \u201cLW\u201d banner, I will be there from 16:00 MSK.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Cognitive biases analysis. You can read about some bias and tell us all.</p></li>\n<li><p>Applied rationality: practice. You can also propose exercises or rationality skill to practice.</p></li>\n<li><p>Visual information processing and visual thinking.</p></li>\n<li><p>How to formulate an assignment or problem to make it solvable.</p></li>\n<li><p>Text analysis, what can we extract from it.</p></li>\n</ul>\n\n<p>If you are going for the first time, please fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dERxM3oyWlRLUThKclo3cnVHa05vY0E6MQ\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>N. B. Google shows incorrect location, please use <a href=\"http://maps.yandex.ru/?text=%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D1%8F%2C%20%D0%9C%D0%BE%D1%81%D0%BA%D0%B2%D0%B0%2C%20%D1%83%D0%BB%D0%B8%D1%86%D0%B0%20%D0%9E%D1%81%D1%82%D0%BE%D0%B6%D0%B5%D0%BD%D0%BA%D0%B0%2C%2014%2F2&amp;sll=37.599139%2C55.74185&amp;ll=37.599139%2C55.741850&amp;spn=0.014377%2C0.004449&amp;z=17&amp;l=map\" rel=\"nofollow\">Yandex maps</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ev'>Moscow: Biases, Applied Rationality, Visual Thinking</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4qGa7Q5zY7ogBhrki", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0106984508146915e-06, "legacy": true, "legacyId": "19412", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Biases__Applied_Rationality__Visual_Thinking\">Discussion article for the meetup : <a href=\"/meetups/ev\">Moscow: Biases, Applied Rationality, Visual Thinking</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 October 2012 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rossiya, Moscow, ulitsa Ostozhenka 14/2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant, entrance from Lopukhinskiy pereulok. Look for a table with \u201cLW\u201d banner, I will be there from 16:00 MSK.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Cognitive biases analysis. You can read about some bias and tell us all.</p></li>\n<li><p>Applied rationality: practice. You can also propose exercises or rationality skill to practice.</p></li>\n<li><p>Visual information processing and visual thinking.</p></li>\n<li><p>How to formulate an assignment or problem to make it solvable.</p></li>\n<li><p>Text analysis, what can we extract from it.</p></li>\n</ul>\n\n<p>If you are going for the first time, please fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dERxM3oyWlRLUThKclo3cnVHa05vY0E6MQ\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>N. B. Google shows incorrect location, please use <a href=\"http://maps.yandex.ru/?text=%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D1%8F%2C%20%D0%9C%D0%BE%D1%81%D0%BA%D0%B2%D0%B0%2C%20%D1%83%D0%BB%D0%B8%D1%86%D0%B0%20%D0%9E%D1%81%D1%82%D0%BE%D0%B6%D0%B5%D0%BD%D0%BA%D0%B0%2C%2014%2F2&amp;sll=37.599139%2C55.74185&amp;ll=37.599139%2C55.741850&amp;spn=0.014377%2C0.004449&amp;z=17&amp;l=map\" rel=\"nofollow\">Yandex maps</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Biases__Applied_Rationality__Visual_Thinking1\">Discussion article for the meetup : <a href=\"/meetups/ev\">Moscow: Biases, Applied Rationality, Visual Thinking</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow: Biases, Applied Rationality, Visual Thinking", "anchor": "Discussion_article_for_the_meetup___Moscow__Biases__Applied_Rationality__Visual_Thinking", "level": 1}, {"title": "Discussion article for the meetup : Moscow: Biases, Applied Rationality, Visual Thinking", "anchor": "Discussion_article_for_the_meetup___Moscow__Biases__Applied_Rationality__Visual_Thinking1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-16T16:05:33.084Z", "modifiedAt": null, "url": null, "title": "[LINK] Willpower Is All In Your Head", "slug": "link-willpower-is-all-in-your-head", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:07.718Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "maia", "createdAt": "2012-02-08T20:28:42.643Z", "isAdmin": false, "displayName": "maia"}, "userId": "QW72Lk68mKnTfmdAB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JLDWrTc2H5YhFeejs/link-willpower-is-all-in-your-head", "pageUrlRelative": "/posts/JLDWrTc2H5YhFeejs/link-willpower-is-all-in-your-head", "linkUrl": "https://www.lesswrong.com/posts/JLDWrTc2H5YhFeejs/link-willpower-is-all-in-your-head", "postedAtFormatted": "Tuesday, October 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Willpower%20Is%20All%20In%20Your%20Head&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Willpower%20Is%20All%20In%20Your%20Head%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJLDWrTc2H5YhFeejs%2Flink-willpower-is-all-in-your-head%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Willpower%20Is%20All%20In%20Your%20Head%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJLDWrTc2H5YhFeejs%2Flink-willpower-is-all-in-your-head", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJLDWrTc2H5YhFeejs%2Flink-willpower-is-all-in-your-head", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 204, "htmlBody": "<p>The authors find that ego depletion is lessened if you believe that willpower is not a limited resource. Abstract:</p>\n<blockquote>\n<p>Building cognitive abilities often requires sustained engagement with effortful tasks. We demonstrate that beliefs about willpower&ndash;whether willpower is viewed as a limited or non-limited resource&ndash;impact sustained learning on a strenuous mental task. As predicted, beliefs about willpower did not affect accuracy or improvement during the initial phases of learning; however, participants who were led to view willpower as non-limited showed greater sustained learning over the full duration of the task. These findings highlight the interactive nature of motivational and cognitive processes: motivational factors can substantially affect people&rsquo;s ability to recruit their cognitive resources to sustain learning over time.</p>\n</blockquote>\n<p>NY Times article:&nbsp;<a href=\"http://www.nytimes.com/2011/11/27/opinion/sunday/willpower-its-in-your-head.html?_r=0\">http://www.nytimes.com/2011/11/27/opinion/sunday/willpower-its-in-your-head.html?_r=0</a></p>\n<p>Recent study by the authors:&nbsp;<a href=\"http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0038680\">http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0038680</a></p>\n<p>A related, slightly older study by the same authors which reviews several experiments:&nbsp;<a href=\"http://intl-pss.sagepub.com/content/21/11/1686.full\">http://intl-pss.sagepub.com/content/21/11/1686.full</a></p>\n<p>(Not sure if the second link is openable for everyone; I have access to most things through my university library.)</p>\n<p>The NY Times article seems to be saying that studies showing an increase in willpower after consuming sugar are incorrect in some way. What I'm left wondering is: what if you believe that willpower is unlimited, AND you ingest glucose shortly before doing a difficult task? Would that be even better?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JLDWrTc2H5YhFeejs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": -2, "extendedScore": null, "score": -2e-06, "legacy": true, "legacyId": "19413", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-16T21:42:56.998Z", "modifiedAt": null, "url": null, "title": "Happy Ada Lovelace Day", "slug": "happy-ada-lovelace-day", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:22.824Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "palladias", "createdAt": "2012-04-03T13:45:53.766Z", "isAdmin": false, "displayName": "palladias"}, "userId": "Bv2LXWzZf96WGpqJ5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZpNvwH4CiGtKuyatG/happy-ada-lovelace-day", "pageUrlRelative": "/posts/ZpNvwH4CiGtKuyatG/happy-ada-lovelace-day", "linkUrl": "https://www.lesswrong.com/posts/ZpNvwH4CiGtKuyatG/happy-ada-lovelace-day", "postedAtFormatted": "Tuesday, October 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Happy%20Ada%20Lovelace%20Day&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHappy%20Ada%20Lovelace%20Day%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZpNvwH4CiGtKuyatG%2Fhappy-ada-lovelace-day%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Happy%20Ada%20Lovelace%20Day%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZpNvwH4CiGtKuyatG%2Fhappy-ada-lovelace-day", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZpNvwH4CiGtKuyatG%2Fhappy-ada-lovelace-day", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<p>Today is <a href=\"http://findingada.com/\">Ada Lovelace Day</a>, when STEM enthusiasts highlight the work of modern and historical women scientists, engineers, and mathematicians. &nbsp;If you run a blog, you may want to participate by posting about a woman in a STEM field whom you admire. &nbsp;But I'd love to have people share women scientists/mathematicians/authors&nbsp;in the comments that&nbsp;they think we could all stand to read more about.&nbsp;</p>\n<p>\n<p>\n<ul>\n<li>Women in STEM fields (living or dead, fiction or nonfictional) that you'd like us to know more about (preferably with a little precis and a link</li>\n<li>Books about women in STEM fields that are awesome</li>\n<li>Books written by women about STEM subjects that are awesome</li>\n<li>Studies about sexism (or ways to combat it) in STEM fields (and anywhere else)</li>\n<li>Practical things you or organizations you're with have done to cut down on careless or intentional sexism. (how did you implement it, how did you measure the effects, etc)</li>\n</ul>\n</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZpNvwH4CiGtKuyatG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 17, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "19415", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-16T22:32:33.393Z", "modifiedAt": null, "url": null, "title": "Looking for alteration suggestions for the official Sequences ebook", "slug": "looking-for-alteration-suggestions-for-the-official", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:30.546Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexvermeer", "createdAt": "2010-08-13T16:28:34.576Z", "isAdmin": false, "displayName": "alexvermeer"}, "userId": "3bK6aDQviGG3ovuDJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BpCiHdBzAEHumxWav/looking-for-alteration-suggestions-for-the-official", "pageUrlRelative": "/posts/BpCiHdBzAEHumxWav/looking-for-alteration-suggestions-for-the-official", "linkUrl": "https://www.lesswrong.com/posts/BpCiHdBzAEHumxWav/looking-for-alteration-suggestions-for-the-official", "postedAtFormatted": "Tuesday, October 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Looking%20for%20alteration%20suggestions%20for%20the%20official%20Sequences%20ebook&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALooking%20for%20alteration%20suggestions%20for%20the%20official%20Sequences%20ebook%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBpCiHdBzAEHumxWav%2Flooking-for-alteration-suggestions-for-the-official%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Looking%20for%20alteration%20suggestions%20for%20the%20official%20Sequences%20ebook%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBpCiHdBzAEHumxWav%2Flooking-for-alteration-suggestions-for-the-official", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBpCiHdBzAEHumxWav%2Flooking-for-alteration-suggestions-for-the-official", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 248, "htmlBody": "<p>As you may have heard, the Singularity Institute is in the process of creating an official ebook version of The Sequences (specifically, Eliezer's Major Sequences written between 2006 and 2009).&nbsp;</p>\n<p>Now is an opportune time to make any alterations to the contents of the Sequences.&nbsp;We're looking for suggestions about:</p>\n<ol>\n<li>Posts to <em>add</em> to the Sequences. E.g., \"scope insensitivity\" is not currently a part of any sequence, perhaps it should be? Preferably suggest a specific location, or at least a specific sequence where you think the addition would logically go.</li>\n<li>Posts to <em>remove</em> from the Sequences. Are there redundant or unnecessary posts? To call the Sequences long is a bit of an understatement.</li>\n<li>Alternatives to \"The Sequences\" as a title, such as \"How to be Less Wrong: The Sequences, 2006--2009.\"</li>\n</ol>\n<p>Put separate suggestions in separate comments so that specific changes can be discussed.&nbsp;All suggestions will be reviewed, with final changes made by Eliezer.&nbsp;Next thing you know, you'll be sipping a hot mocha in your favorite chair while reading about Death Spirals on your handy e-reader.</p>\n<p>The Sequences that will be present in the ebook:</p>\n<ul>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Map_and_Territory_(sequence)\">Map and Territory</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">Mysterious Answers to Mysterious Questions</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/A_Human%27s_Guide_to_Words\">A Human's Guide to Words</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">How to Actually Change Your Mind</a> (includes all posts from all its subsequences)</li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence)\">Reductionism</a>&nbsp;(includes all posts from <a href=\"http://wiki.lesswrong.com/wiki/Joy_in_the_Merely_Real\">Joy in the Merely Real</a> and <a href=\"http://wiki.lesswrong.com/wiki/Zombies_(sequence)\">Zombies</a>)</li>\n<li><a href=\"/lw/r5/the_quantum_physics_sequence/\">Quantum Physics</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">Metaethics</a></li>\n<li><a href=\"/lw/xy/the_fun_theory_sequence/\">Fun Theory</a></li>\n<li><a href=\"/lw/cz/the_craft_and_the_community/\">The Craft and the Community</a></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BpCiHdBzAEHumxWav", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 20, "extendedScore": null, "score": 1.0109757480073125e-06, "legacy": true, "legacyId": "19257", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 73, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hc9Eg6erp6hk9bWhn", "K4aGvLnHvYgX9pZHS", "YdcF6WbBmJhaaDqoD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-16T22:34:31.727Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham HPMoR Discussion group", "slug": "meetup-durham-hpmor-discussion-group", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cPANcibtJ8xJRX3jR/meetup-durham-hpmor-discussion-group", "pageUrlRelative": "/posts/cPANcibtJ8xJRX3jR/meetup-durham-hpmor-discussion-group", "linkUrl": "https://www.lesswrong.com/posts/cPANcibtJ8xJRX3jR/meetup-durham-hpmor-discussion-group", "postedAtFormatted": "Tuesday, October 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20HPMoR%20Discussion%20group&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20HPMoR%20Discussion%20group%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcPANcibtJ8xJRX3jR%2Fmeetup-durham-hpmor-discussion-group%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20HPMoR%20Discussion%20group%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcPANcibtJ8xJRX3jR%2Fmeetup-durham-hpmor-discussion-group", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcPANcibtJ8xJRX3jR%2Fmeetup-durham-hpmor-discussion-group", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 79, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ew'>Durham HPMoR Discussion group</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 October 2012 11:00:00AM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Parker and Otis, 112 S Duke St, Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be discussing Harry Potter and the Methods of Methods of Rationality, chapters 8-11.</p>\n\n<p>While we encourage everyone to read the chapters in question, please feel free to come even if you haven't. We'll summarize the main points of the chapters as we discuss them.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ew'>Durham HPMoR Discussion group</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cPANcibtJ8xJRX3jR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0109768041987223e-06, "legacy": true, "legacyId": "19416", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion_group\">Discussion article for the meetup : <a href=\"/meetups/ew\">Durham HPMoR Discussion group</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 October 2012 11:00:00AM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Parker and Otis, 112 S Duke St, Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be discussing Harry Potter and the Methods of Methods of Rationality, chapters 8-11.</p>\n\n<p>While we encourage everyone to read the chapters in question, please feel free to come even if you haven't. We'll summarize the main points of the chapters as we discuss them.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion_group1\">Discussion article for the meetup : <a href=\"/meetups/ew\">Durham HPMoR Discussion group</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham HPMoR Discussion group", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion_group", "level": 1}, {"title": "Discussion article for the meetup : Durham HPMoR Discussion group", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion_group1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-16T22:43:18.801Z", "modifiedAt": null, "url": null, "title": "Open Thread, October 16-31, 2012", "slug": "open-thread-october-16-31-2012", "viewCount": null, "lastCommentedAt": "2012-11-09T07:06:36.481Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jG74S37Y6x8odJ3BF/open-thread-october-16-31-2012", "pageUrlRelative": "/posts/jG74S37Y6x8odJ3BF/open-thread-october-16-31-2012", "linkUrl": "https://www.lesswrong.com/posts/jG74S37Y6x8odJ3BF/open-thread-october-16-31-2012", "postedAtFormatted": "Tuesday, October 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20October%2016-31%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20October%2016-31%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjG74S37Y6x8odJ3BF%2Fopen-thread-october-16-31-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20October%2016-31%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjG74S37Y6x8odJ3BF%2Fopen-thread-october-16-31-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjG74S37Y6x8odJ3BF%2Fopen-thread-october-16-31-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post, even in Discussion, it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jG74S37Y6x8odJ3BF", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 1.0109815087285989e-06, "legacy": true, "legacyId": "19417", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 280, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-10-16T22:43:18.801Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-16T22:59:56.428Z", "modifiedAt": null, "url": null, "title": "Using existing Strong AIs as case studies", "slug": "using-existing-strong-ais-as-case-studies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:09.106Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ialdabaoth", "createdAt": "2012-10-11T00:04:32.345Z", "isAdmin": false, "displayName": "ialdabaoth"}, "userId": "335oJYQyzcd85RAoe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nT7zEMXJgBFbaEBgH/using-existing-strong-ais-as-case-studies", "pageUrlRelative": "/posts/nT7zEMXJgBFbaEBgH/using-existing-strong-ais-as-case-studies", "linkUrl": "https://www.lesswrong.com/posts/nT7zEMXJgBFbaEBgH/using-existing-strong-ais-as-case-studies", "postedAtFormatted": "Tuesday, October 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Using%20existing%20Strong%20AIs%20as%20case%20studies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUsing%20existing%20Strong%20AIs%20as%20case%20studies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnT7zEMXJgBFbaEBgH%2Fusing-existing-strong-ais-as-case-studies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Using%20existing%20Strong%20AIs%20as%20case%20studies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnT7zEMXJgBFbaEBgH%2Fusing-existing-strong-ais-as-case-studies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnT7zEMXJgBFbaEBgH%2Fusing-existing-strong-ais-as-case-studies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 323, "htmlBody": "<p>I would like to put forth the argument that we already have multiple human-programmed \"Strong AI\" operating among us, they already exhibits clearly \"intelligent\", rational, self-modifying goal-seeking behavior, and we should systematically study these entities before engaging in any particularly detailed debates about \"designing\" AI with particular goals.</p>\n<p>They're called \"Bureaucracies\".</p>\n<p>Essentially, a modern bureaucracy - whether it is operating as the decision-making system for a capitalist corporation, a government, a non-profit charity, or a political party, is an artificial intelligence that uses human brains as its basic hardware and firmware, allowing it to \"borrow\" a lot of human computational algorithms to do its own processing.</p>\n<p>The fact that bureaucratic decisions can be traced back to individual human decisions is irrelevant - even within a human or computer AI, a decision can theoretically be traced back to single neurons or subroutines - the fact is that bureaucracies have evolved to guide and exploit human decision-making towards their own ends, often to the detriment of the individual humans that comprise said bureaucracy.</p>\n<p>Note that when I say \"I would like to put forth the argument\", I am at least partially admitting that I'm speaking from hunch, rather than already having a huge collection of empirical data to work from - part of the point of putting this forward is to acknowledge that I'm not yet very good at \"avalanche of empirical evidence\"-style argument. But I would *greatly* appreciate anyone who suspects that they might be able to demonstrate evidence for or against this idea, presenting said evidence so I can solidify my reasoning.&nbsp;</p>\n<p>As a \"step 2\": assuming the evidence weighs in towards my notion, what would it take to develop a systematic approach to studying bureaucracy from the perspective of AI or even xenosapience, such that bureaucracies could be either \"programmed\" or communicated with directly by the human agents that comprise them (and ideally by the larger pool of human stakeholders that are forced to interact with them?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nT7zEMXJgBFbaEBgH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": -12, "extendedScore": null, "score": -2.3e-05, "legacy": true, "legacyId": "19418", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-16T23:00:14.776Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham Meetup: Article discussions", "slug": "meetup-durham-meetup-article-discussions", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ifWF8hFCqYriGQdK8/meetup-durham-meetup-article-discussions", "pageUrlRelative": "/posts/ifWF8hFCqYriGQdK8/meetup-durham-meetup-article-discussions", "linkUrl": "https://www.lesswrong.com/posts/ifWF8hFCqYriGQdK8/meetup-durham-meetup-article-discussions", "postedAtFormatted": "Tuesday, October 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20Meetup%3A%20Article%20discussions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20Meetup%3A%20Article%20discussions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FifWF8hFCqYriGQdK8%2Fmeetup-durham-meetup-article-discussions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20Meetup%3A%20Article%20discussions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FifWF8hFCqYriGQdK8%2Fmeetup-durham-meetup-article-discussions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FifWF8hFCqYriGQdK8%2Fmeetup-durham-meetup-article-discussions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ex'>Durham Meetup: Article discussions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 October 2012 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Francesca's 706B 9th St Durham, NC 27705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be discussing some articles over coffee and pastries. In particular:</p>\n\n<p><a href=\"http://lesswrong.com/lw/dr/generalizing_from_one_example/\">Generalizing From One Example</a></p>\n\n<p><a href=\"http://lesswrong.com/lw/aw/its_okay_to_be_at_least_a_little_irrational/\">It&#39;s okay to be (at least a little) irrational</a></p>\n\n<p><a href=\"http://yudkowsky.net/rational/technical\" rel=\"nofollow\">A Technical Explanation of Technical Explanation</a></p>\n\n<p>Reading at least some of these in advance is encouraged, but not required. We will summarize and discuss the articles, and in particular how we can apply them in the real world.\nAlso on the agenda: discussion of rationality goals from the previous meeting, and discussion of future meetup topics.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ex'>Durham Meetup: Article discussions</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ifWF8hFCqYriGQdK8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0109905771826815e-06, "legacy": true, "legacyId": "19419", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_Meetup__Article_discussions\">Discussion article for the meetup : <a href=\"/meetups/ex\">Durham Meetup: Article discussions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 October 2012 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Francesca's 706B 9th St Durham, NC 27705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be discussing some articles over coffee and pastries. In particular:</p>\n\n<p><a href=\"http://lesswrong.com/lw/dr/generalizing_from_one_example/\">Generalizing From One Example</a></p>\n\n<p><a href=\"http://lesswrong.com/lw/aw/its_okay_to_be_at_least_a_little_irrational/\">It's okay to be (at least a little) irrational</a></p>\n\n<p><a href=\"http://yudkowsky.net/rational/technical\" rel=\"nofollow\">A Technical Explanation of Technical Explanation</a></p>\n\n<p>Reading at least some of these in advance is encouraged, but not required. We will summarize and discuss the articles, and in particular how we can apply them in the real world.\nAlso on the agenda: discussion of rationality goals from the previous meeting, and discussion of future meetup topics.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_Meetup__Article_discussions1\">Discussion article for the meetup : <a href=\"/meetups/ex\">Durham Meetup: Article discussions</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham Meetup: Article discussions", "anchor": "Discussion_article_for_the_meetup___Durham_Meetup__Article_discussions", "level": 1}, {"title": "Discussion article for the meetup : Durham Meetup: Article discussions", "anchor": "Discussion_article_for_the_meetup___Durham_Meetup__Article_discussions1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["baTWMegR42PAsH9qJ", "Yiv9BeroBhJC6zqSs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-17T04:44:46.602Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Recognizing Intelligence", "slug": "seq-rerun-recognizing-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:08.932Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m46jnehmKAtay7AJu/seq-rerun-recognizing-intelligence", "pageUrlRelative": "/posts/m46jnehmKAtay7AJu/seq-rerun-recognizing-intelligence", "linkUrl": "https://www.lesswrong.com/posts/m46jnehmKAtay7AJu/seq-rerun-recognizing-intelligence", "postedAtFormatted": "Wednesday, October 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Recognizing%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Recognizing%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm46jnehmKAtay7AJu%2Fseq-rerun-recognizing-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Recognizing%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm46jnehmKAtay7AJu%2Fseq-rerun-recognizing-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm46jnehmKAtay7AJu%2Fseq-rerun-recognizing-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>Today's post, <a href=\"/lw/vl/recognizing_intelligence/\">Recognizing Intelligence</a> was originally published on 07 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Recognizing_Intelligence\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Suppose we landed on another planet and found a large metal object that contained wires made of superconductors, and hundreds of tightly matched gears. Would we be able to infer the presence of an optimization process?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/eyk/seq_rerun_back_up_and_ask_whether_not_why/\">Back Up and Ask Whether, Not Why</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m46jnehmKAtay7AJu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0111751228283731e-06, "legacy": true, "legacyId": "19422", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LZMeuRGQhSw77XewC", "o32Fyn3Z4mne7N2Ts", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-17T06:10:34.625Z", "modifiedAt": "2020-07-05T21:55:55.564Z", "url": null, "title": "How To Have Things Correctly", "slug": "how-to-have-things-correctly", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:07.323Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Alicorn", "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KT8Mf3ey6uwQAkWek/how-to-have-things-correctly", "pageUrlRelative": "/posts/KT8Mf3ey6uwQAkWek/how-to-have-things-correctly", "linkUrl": "https://www.lesswrong.com/posts/KT8Mf3ey6uwQAkWek/how-to-have-things-correctly", "postedAtFormatted": "Wednesday, October 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20To%20Have%20Things%20Correctly&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20To%20Have%20Things%20Correctly%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKT8Mf3ey6uwQAkWek%2Fhow-to-have-things-correctly%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20To%20Have%20Things%20Correctly%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKT8Mf3ey6uwQAkWek%2Fhow-to-have-things-correctly", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKT8Mf3ey6uwQAkWek%2Fhow-to-have-things-correctly", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1709, "htmlBody": "<p><em>I think people who are not made happier by having things either have the wrong things, or have them incorrectly.&nbsp; Here is how I get the most out of my stuff.</em></p>\n<p>Money doesn't buy happiness.&nbsp; If you want to try throwing money at the problem anyway, you should buy experiences like vacations or services, rather than purchasing objects.&nbsp; If you have to buy objects, they should be absolute and not positional goods; positional goods just put you on a treadmill and you're never going to catch up.</p>\n<p>Supposedly.</p>\n<p>I think getting value out of spending money, owning objects, and having positional goods are all three of them <em>skills</em>, that people often don't have naturally but can develop.&nbsp; I'm going to focus mostly on the middle skill: how to have things correctly<sup>1</sup>.</p>\n<p><strong><a id=\"more\"></a> 1. Obtain more-correct things in the first place.</strong></p>\n<p>If you and I are personal friends, you probably know that I have <a href=\"http://alicorn24.livejournal.com/47721.html\">weird gift-receiving protocols</a>.&nbsp; This is partly because I hate surprises.&nbsp; But it's also because I don't want to have incorrect things, cluttering my space, generating guilt because they're gifts that I never use, and generally having high opportunity cost because the giver could have gotten me something else.</p>\n<p>This problem isn't only with gifts.&nbsp; People get themselves incorrect things all the time - seduced by marketing, seized by impulse, or too hurried to think about which of several choices is the best one for their wants and needs.&nbsp; I have some incorrect clothes, which I got because I was sick of shopping and needed <em>a</em> new pair of pants even if it was terrible; as soon as I found better pants (or whatever) those clothes were never worn again and now they're just waiting for my next haul to Goodwill.&nbsp; I bet a lot of people have incorrect printers, mostly because printers in general are evil, but partly because it's irritating and dull to investigate them ahead of time.&nbsp; Cars may also tend to fall into this category, with a lot of people neutral or ambivalent about their self-selected objects that cost thousands of dollars.</p>\n<p>If you are not currently living in a cluttered space, or feeling guilty about not using your objects enough, or tending to dislike the things that you have, or finding yourself wanting things that you \"can't\" get because you already have an inferior item in the same reference class, or just buying too many not-strict-necessities than is appropriate for your budget - then this might not be a step you need to focus on.&nbsp; If you have objects you don't like (not just aren't getting a lot out of, that's for later steps, but actually dislike) then you might need to change your thresholds for object-acquisition.</p>\n<p>This doesn't mean something stodgy like \"before you get something, think carefully about whether you will actually use and enjoy it, using outside view information about items in this reference class\".&nbsp; Or, well, it can mean that, but that's not the only criterion!&nbsp; You can also increase the amount of sheer emotional want that you allow to move you to action - wait until you more-than-idly desire it.&nbsp; If I were good at math, I would try to operationalize this as some sort of formula, but suffice it to say that the cost of the object (in money, but also social capital and storage space and inconvenience and whatnot) should interact with how much you just-plain-want-it and also with how much use you will likely get out of it.</p>\n<p>Speaking of how much use you get out of it...</p>\n<p><strong>2. Find excuses to use your stuff.</strong></p>\n<p>I have a cloak.&nbsp; It cost me about $80 on <a href=\"http://www.etsy.com/shop/mcostlow\">Etsy</a>.&nbsp; It is custom made, and reversible between black and gray, and made out of my favorite fabric, and falls all the way to the floor from my shoulders, and has a hood so deep that I can hide in it if I want.&nbsp; If I run while I wear it, it swoops out behind me.&nbsp; It's soft and warm but not <em>too</em> warm.&nbsp; I <em>like</em> my cloak.</p>\n<p>I also have sweaters.&nbsp; They didn't cost me anywhere near $80, not a one of them.</p>\n<p>When it's chilly, I reach for the cloak first.</p>\n<p>I'm playing a game with my brain: I will let it make me spend $80 on a cloak, if it will produce enough impetus towards cloak-wearing and cloak-enjoying that I actually get $80 of value out of it.&nbsp; If it can't follow through, then I later trust its wants less (\"last time I bought something like this, it just hung in my closet forever and I only pulled it out on Halloween!\"), and then it doesn't get to make me buy any more cloaklike objects, which it really wants to be able to do.&nbsp; (I don't know if everyone's brain is wired to play this sort of game, but if yours is, it's worth doing.)&nbsp; My brain is doing a very nice job of helping me enjoy my cloak.&nbsp; Eventually I may let it buy another cloak in a different pair of colors, if it demonstrates that it really can keep this up long-term.</p>\n<p>People sometimes treat not using their stuff like something that happens to them.&nbsp; \"I never wound up using it.\"&nbsp; \"It turned out that I just left it in the basement.\"&nbsp; This is silly.&nbsp; If I'm going to use my cloak - or my miniature cheesecake pan or my snazzy letter opener - then this is because at some point I will decide to put on my cloak, make miniature cheesecakes, or open letters with my snazzy dedicated device instead of my nail file.&nbsp; You know, on purpose.</p>\n<p>Sure, some things seem to prompt you to use them more easily.&nbsp; If you get a new video game, and you really like it, it's probably not going turn out that you never think to play it.&nbsp; If you get a <em>cat</em> or something sufficiently autonomous like that, you will <em>know</em> if you are not paying it sufficient attention.</p>\n<p>But if you get a muffin tin and you have no pre-installed prompts for \"I could make muffins\" because that impulse was extinguished due to lack of muffin tin, it will be easy to ignore.&nbsp; You're going to need to train yourself to think of muffins as a makeable thing.&nbsp; And you can train yourself to do that!&nbsp; Put the muffins on your to-do list.&nbsp; Lead your friends to expect baked goods.&nbsp; Preheat the oven and leave a stick of butter out to soften so you're committed.&nbsp; If that doesn't sound appealing to you - if you don't want to bake muffins - then you shouldn't have acquired a muffin tin.</p>\n<p>Speaking of your friends...</p>\n<p><strong>3. Invite others to benefit from your thing.</strong></p>\n<p>I've got a pet snake.&nbsp; Six days of the week, she is just <em>my</em> pet snake.&nbsp; On Saturdays, during my famous dinner parties at which the Illuminati congregate, I often pass her around to interested visitors, too.&nbsp; The dinner parties themselves allow my friends to benefit from my stuff, too - kitchen implements and appliances and the very table at which my guests sit.&nbsp; It would be less useful to own a stand mixer or a giant wok if I only ever cooked for myself.&nbsp; It would be less pleasant to have a pet snake if I had no chance to share her.&nbsp; It would be less important to have pretty clothes if no one ever saw me wearing them.</p>\n<p>You're a social ape.&nbsp; If you're trying to get more out of something, an obvious first hypothesis to test is to see if adding other social apes helps:</p>\n<ul>\n<li>Loan your stuff out.&nbsp; (People seem to acquire physical copies of books for this motivation; it is good.&nbsp; Do more of that.)</li>\n<li>Acquire more stuff that can be used cooperatively.&nbsp; (Own games you like, for instance.)</li>\n<li>Find creative ways to use stuff cooperatively where it was not intended.</li>\n<li>Tell people stories about your stuff, if you have interesting stories about it.</li>\n<li>Fetch it when it is a useful tool for someone else's task.</li>\n<li>Accept compliments on your stuff gleefully.&nbsp; Let people have experiences of your stuff so that they will produce same.</li>\n</ul>\n<p>Also, circling back to the bit about gifts: I bet you own some gifts.&nbsp; Use them as excuses to think about who gave them to you!&nbsp; My grandmother got me my blender, my mom made me my purse, my best friend gave me the entire signed Fablehaven series.&nbsp; Interacting with those objects now produces extra warmfuzzies if I take the extra cognitive step.</p>\n<p>Speaking of how you go about experiencing your stuff...</p>\n<p><strong>4. Turn stuff <em>into</em> experiences via the senses.</strong></p>\n<p>Remember my cloak?&nbsp; It's made of flannel, so it's nice to pet; it's fun to swoosh it about.&nbsp; Remember my snake?&nbsp; She feels nifty and cool and smooth, and she looks pretty, and I get to watch her swallow a mouse once a week if I care to stick around to supervise.&nbsp; I get candy from Trader Joe's because it tastes good and music that I like because it sounds good.&nbsp; If you never look at your stuff or touch it or taste it or whatever is appropriate for the type of stuff, you might not be having it correctly.&nbsp; (Raise your hand if you have chachkas on your shelves that you don't actually <em>look at</em>.)</p>\n<p>Caveat: Some purely instrumental tools can be had correctly without this - I don't directly experience my Dustbuster with much enthusiasm, just the cleanliness that I can use it to create.&nbsp; Although nothing prevents you from directly enjoying a particularly nice tool either - I have spatulas I am fond of.</p>\n<p>And of course if you choose to follow the standard advice about purchasing experiences in a more standard way, you can still use stuff there.&nbsp; You will have more fun camping if you have decent camping gear; you will have more fun at the beach if you have suitable beach things; you will have more fun in the south of France if you have travel guides and phrasebooks that you like.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>It's an optional skill.&nbsp; You could neglect it in favor of others, and depending on your own talents and values, this could be higher-leverage than learning to have things correctly.&nbsp; But I bet the following steps will be improvements for some people.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1, "XqykXFKL9t38pbSEm": 1, "3ee9k6NJfcGzL6kMS": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KT8Mf3ey6uwQAkWek", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 107, "baseScore": 99, "extendedScore": null, "score": 0.000229, "legacy": true, "legacyId": "19397", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 99, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>I think people who are not made happier by having things either have the wrong things, or have them incorrectly.&nbsp; Here is how I get the most out of my stuff.</em></p>\n<p>Money doesn't buy happiness.&nbsp; If you want to try throwing money at the problem anyway, you should buy experiences like vacations or services, rather than purchasing objects.&nbsp; If you have to buy objects, they should be absolute and not positional goods; positional goods just put you on a treadmill and you're never going to catch up.</p>\n<p>Supposedly.</p>\n<p>I think getting value out of spending money, owning objects, and having positional goods are all three of them <em>skills</em>, that people often don't have naturally but can develop.&nbsp; I'm going to focus mostly on the middle skill: how to have things correctly<sup>1</sup>.</p>\n<p><strong id=\"_1__Obtain_more_correct_things_in_the_first_place_\"><a id=\"more\"></a> 1. Obtain more-correct things in the first place.</strong></p>\n<p>If you and I are personal friends, you probably know that I have <a href=\"http://alicorn24.livejournal.com/47721.html\">weird gift-receiving protocols</a>.&nbsp; This is partly because I hate surprises.&nbsp; But it's also because I don't want to have incorrect things, cluttering my space, generating guilt because they're gifts that I never use, and generally having high opportunity cost because the giver could have gotten me something else.</p>\n<p>This problem isn't only with gifts.&nbsp; People get themselves incorrect things all the time - seduced by marketing, seized by impulse, or too hurried to think about which of several choices is the best one for their wants and needs.&nbsp; I have some incorrect clothes, which I got because I was sick of shopping and needed <em>a</em> new pair of pants even if it was terrible; as soon as I found better pants (or whatever) those clothes were never worn again and now they're just waiting for my next haul to Goodwill.&nbsp; I bet a lot of people have incorrect printers, mostly because printers in general are evil, but partly because it's irritating and dull to investigate them ahead of time.&nbsp; Cars may also tend to fall into this category, with a lot of people neutral or ambivalent about their self-selected objects that cost thousands of dollars.</p>\n<p>If you are not currently living in a cluttered space, or feeling guilty about not using your objects enough, or tending to dislike the things that you have, or finding yourself wanting things that you \"can't\" get because you already have an inferior item in the same reference class, or just buying too many not-strict-necessities than is appropriate for your budget - then this might not be a step you need to focus on.&nbsp; If you have objects you don't like (not just aren't getting a lot out of, that's for later steps, but actually dislike) then you might need to change your thresholds for object-acquisition.</p>\n<p>This doesn't mean something stodgy like \"before you get something, think carefully about whether you will actually use and enjoy it, using outside view information about items in this reference class\".&nbsp; Or, well, it can mean that, but that's not the only criterion!&nbsp; You can also increase the amount of sheer emotional want that you allow to move you to action - wait until you more-than-idly desire it.&nbsp; If I were good at math, I would try to operationalize this as some sort of formula, but suffice it to say that the cost of the object (in money, but also social capital and storage space and inconvenience and whatnot) should interact with how much you just-plain-want-it and also with how much use you will likely get out of it.</p>\n<p>Speaking of how much use you get out of it...</p>\n<p><strong id=\"2__Find_excuses_to_use_your_stuff_\">2. Find excuses to use your stuff.</strong></p>\n<p>I have a cloak.&nbsp; It cost me about $80 on <a href=\"http://www.etsy.com/shop/mcostlow\">Etsy</a>.&nbsp; It is custom made, and reversible between black and gray, and made out of my favorite fabric, and falls all the way to the floor from my shoulders, and has a hood so deep that I can hide in it if I want.&nbsp; If I run while I wear it, it swoops out behind me.&nbsp; It's soft and warm but not <em>too</em> warm.&nbsp; I <em>like</em> my cloak.</p>\n<p>I also have sweaters.&nbsp; They didn't cost me anywhere near $80, not a one of them.</p>\n<p>When it's chilly, I reach for the cloak first.</p>\n<p>I'm playing a game with my brain: I will let it make me spend $80 on a cloak, if it will produce enough impetus towards cloak-wearing and cloak-enjoying that I actually get $80 of value out of it.&nbsp; If it can't follow through, then I later trust its wants less (\"last time I bought something like this, it just hung in my closet forever and I only pulled it out on Halloween!\"), and then it doesn't get to make me buy any more cloaklike objects, which it really wants to be able to do.&nbsp; (I don't know if everyone's brain is wired to play this sort of game, but if yours is, it's worth doing.)&nbsp; My brain is doing a very nice job of helping me enjoy my cloak.&nbsp; Eventually I may let it buy another cloak in a different pair of colors, if it demonstrates that it really can keep this up long-term.</p>\n<p>People sometimes treat not using their stuff like something that happens to them.&nbsp; \"I never wound up using it.\"&nbsp; \"It turned out that I just left it in the basement.\"&nbsp; This is silly.&nbsp; If I'm going to use my cloak - or my miniature cheesecake pan or my snazzy letter opener - then this is because at some point I will decide to put on my cloak, make miniature cheesecakes, or open letters with my snazzy dedicated device instead of my nail file.&nbsp; You know, on purpose.</p>\n<p>Sure, some things seem to prompt you to use them more easily.&nbsp; If you get a new video game, and you really like it, it's probably not going turn out that you never think to play it.&nbsp; If you get a <em>cat</em> or something sufficiently autonomous like that, you will <em>know</em> if you are not paying it sufficient attention.</p>\n<p>But if you get a muffin tin and you have no pre-installed prompts for \"I could make muffins\" because that impulse was extinguished due to lack of muffin tin, it will be easy to ignore.&nbsp; You're going to need to train yourself to think of muffins as a makeable thing.&nbsp; And you can train yourself to do that!&nbsp; Put the muffins on your to-do list.&nbsp; Lead your friends to expect baked goods.&nbsp; Preheat the oven and leave a stick of butter out to soften so you're committed.&nbsp; If that doesn't sound appealing to you - if you don't want to bake muffins - then you shouldn't have acquired a muffin tin.</p>\n<p>Speaking of your friends...</p>\n<p><strong id=\"3__Invite_others_to_benefit_from_your_thing_\">3. Invite others to benefit from your thing.</strong></p>\n<p>I've got a pet snake.&nbsp; Six days of the week, she is just <em>my</em> pet snake.&nbsp; On Saturdays, during my famous dinner parties at which the Illuminati congregate, I often pass her around to interested visitors, too.&nbsp; The dinner parties themselves allow my friends to benefit from my stuff, too - kitchen implements and appliances and the very table at which my guests sit.&nbsp; It would be less useful to own a stand mixer or a giant wok if I only ever cooked for myself.&nbsp; It would be less pleasant to have a pet snake if I had no chance to share her.&nbsp; It would be less important to have pretty clothes if no one ever saw me wearing them.</p>\n<p>You're a social ape.&nbsp; If you're trying to get more out of something, an obvious first hypothesis to test is to see if adding other social apes helps:</p>\n<ul>\n<li>Loan your stuff out.&nbsp; (People seem to acquire physical copies of books for this motivation; it is good.&nbsp; Do more of that.)</li>\n<li>Acquire more stuff that can be used cooperatively.&nbsp; (Own games you like, for instance.)</li>\n<li>Find creative ways to use stuff cooperatively where it was not intended.</li>\n<li>Tell people stories about your stuff, if you have interesting stories about it.</li>\n<li>Fetch it when it is a useful tool for someone else's task.</li>\n<li>Accept compliments on your stuff gleefully.&nbsp; Let people have experiences of your stuff so that they will produce same.</li>\n</ul>\n<p>Also, circling back to the bit about gifts: I bet you own some gifts.&nbsp; Use them as excuses to think about who gave them to you!&nbsp; My grandmother got me my blender, my mom made me my purse, my best friend gave me the entire signed Fablehaven series.&nbsp; Interacting with those objects now produces extra warmfuzzies if I take the extra cognitive step.</p>\n<p>Speaking of how you go about experiencing your stuff...</p>\n<p><strong id=\"4__Turn_stuff_into_experiences_via_the_senses_\">4. Turn stuff <em>into</em> experiences via the senses.</strong></p>\n<p>Remember my cloak?&nbsp; It's made of flannel, so it's nice to pet; it's fun to swoosh it about.&nbsp; Remember my snake?&nbsp; She feels nifty and cool and smooth, and she looks pretty, and I get to watch her swallow a mouse once a week if I care to stick around to supervise.&nbsp; I get candy from Trader Joe's because it tastes good and music that I like because it sounds good.&nbsp; If you never look at your stuff or touch it or taste it or whatever is appropriate for the type of stuff, you might not be having it correctly.&nbsp; (Raise your hand if you have chachkas on your shelves that you don't actually <em>look at</em>.)</p>\n<p>Caveat: Some purely instrumental tools can be had correctly without this - I don't directly experience my Dustbuster with much enthusiasm, just the cleanliness that I can use it to create.&nbsp; Although nothing prevents you from directly enjoying a particularly nice tool either - I have spatulas I am fond of.</p>\n<p>And of course if you choose to follow the standard advice about purchasing experiences in a more standard way, you can still use stuff there.&nbsp; You will have more fun camping if you have decent camping gear; you will have more fun at the beach if you have suitable beach things; you will have more fun in the south of France if you have travel guides and phrasebooks that you like.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>It's an optional skill.&nbsp; You could neglect it in favor of others, and depending on your own talents and values, this could be higher-leverage than learning to have things correctly.&nbsp; But I bet the following steps will be improvements for some people.</p>", "sections": [{"title": " 1. Obtain more-correct things in the first place.", "anchor": "_1__Obtain_more_correct_things_in_the_first_place_", "level": 1}, {"title": "2. Find excuses to use your stuff.", "anchor": "2__Find_excuses_to_use_your_stuff_", "level": 1}, {"title": "3. Invite others to benefit from your thing.", "anchor": "3__Invite_others_to_benefit_from_your_thing_", "level": 1}, {"title": "4. Turn stuff into experiences via the senses.", "anchor": "4__Turn_stuff_into_experiences_via_the_senses_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "219 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 219, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-17T10:22:44.686Z", "modifiedAt": null, "url": null, "title": "No need for gravity?", "slug": "no-need-for-gravity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:10.213Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "graviton", "createdAt": "2011-06-29T01:28:46.473Z", "isAdmin": false, "displayName": "graviton"}, "userId": "maYBdtLCxrFkhKjhn", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H9k65MtszTa5ZBWoq/no-need-for-gravity", "pageUrlRelative": "/posts/H9k65MtszTa5ZBWoq/no-need-for-gravity", "linkUrl": "https://www.lesswrong.com/posts/H9k65MtszTa5ZBWoq/no-need-for-gravity", "postedAtFormatted": "Wednesday, October 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20No%20need%20for%20gravity%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANo%20need%20for%20gravity%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH9k65MtszTa5ZBWoq%2Fno-need-for-gravity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=No%20need%20for%20gravity%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH9k65MtszTa5ZBWoq%2Fno-need-for-gravity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH9k65MtszTa5ZBWoq%2Fno-need-for-gravity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 877, "htmlBody": "<!--[if !mso]> <mce:style><! v\\:* {behavior:url(#default#VML);} o\\:* {behavior:url(#default#VML);} w\\:* {behavior:url(#default#VML);} .shape {behavior:url(#default#VML);} --><!--[endif] -->\n<p><!--[if !mso]> <mce:style><!  v\\:* {behavior:url(#default#VML);} o\\:* {behavior:url(#default#VML);} w\\:* {behavior:url(#default#VML);} .shape {behavior:url(#default#VML);} --> <!--[endif] --></p>\n<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves>false</w:TrackMoves> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\"   DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\"   LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><!   /* Style Definitions */  table.MsoNormalTable \t{mso-style-name:\"Table Normal\"; \tmso-tstyle-rowband-size:0; \tmso-tstyle-colband-size:0; \tmso-style-noshow:yes; \tmso-style-priority:99; \tmso-style-qformat:yes; \tmso-style-parent:\"\"; \tmso-padding-alt:0in 5.4pt 0in 5.4pt; \tmso-para-margin-top:0in; \tmso-para-margin-right:0in; \tmso-para-margin-bottom:10.0pt; \tmso-para-margin-left:0in; \tline-height:115%; \tmso-pagination:widow-orphan; \tfont-size:11.0pt; \tfont-family:\"Calibri\",\"sans-serif\"; \tmso-ascii-font-family:Calibri; \tmso-ascii-theme-font:minor-latin; \tmso-fareast-font-family:\"Times New Roman\"; \tmso-fareast-theme-font:minor-fareast; \tmso-hansi-font-family:Calibri; \tmso-hansi-theme-font:minor-latin;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\">I don&rsquo;t know where else to go with this idea.<span style=\"mso-spacerun:yes\">&nbsp; </span>I&rsquo;m not a physicist and it could be obviously wrong for some reason I&rsquo;m missing, but it seems to me that there is a small chance that I&rsquo;ve figured out how to remove one of the fundamental forces from our models of the universe; gravity, to be specific.<span style=\"mso-spacerun:yes\"> </span><br /> <br /> So we&rsquo;ve all heard of dark energy, the force driving the accelerating expansion of the universe.<span style=\"mso-spacerun:yes\">&nbsp; </span>Presumably it comes from somewhere, perhaps from every piece of matter in the universe, perhaps only stars, perhaps only black holes, but as long as it&rsquo;s not all coming from a single source, it&rsquo;s probably coming from something that is relatively common, and primarily found within galaxies.<span style=\"mso-spacerun:yes\">&nbsp; And if I magically came to KNOW that it does all come from one source, I would do very little beyond deleting a few arrows in my diagrams to change this post.&nbsp; </span><br /> <br /> And as the Hubble deep field scans showed, there are A LOT of galaxies in any direction you look in (at least from Earth).<span style=\"mso-spacerun:yes\">&nbsp; </span>So, countless galaxies in all directions are emitting dark matter, with tiny rays from each one hitting our galaxy, as well as galaxies in every other direction.<span style=\"mso-spacerun:yes\">&nbsp; </span><br /> <br /> Of course our galaxy doesn&rsquo;t have a plastic shell around it for dark energy to hit.<span style=\"mso-spacerun:yes\">&nbsp; </span>It&rsquo;s specific things in the galaxy that get hit by specific emissions of dark energy, just like it&rsquo;s specific objects that emit those emissions. <br /> <br /> Here are some galaxies.<span style=\"mso-spacerun:yes\">&nbsp; </span>Beyond the ones shown, there are more and more in all directions for as far as anyone knows so far.<span style=\"mso-spacerun:yes\">&nbsp; Please note that nothing in any of these diagrams is drawn to scale. &nbsp;&nbsp; </span></p>\n<p class=\"MsoNormal\"><br />&nbsp; <span style=\"mso-no-proof:yes\"><img style=\"vertical-align: middle;\" src=\"http://i1279.photobucket.com/albums/y538/gravitonthoth/GravityFig1_zps4480a0d7.jpg\" alt=\"Figure 1\" width=\"400\" height=\"299\" /><br /></span></p>\n<p class=\"MsoNormal\">Any one of them emits dark energy pretty uniformly in all directions, as it does with light.<br /> <span style=\"mso-no-proof:yes\"><img style=\"vertical-align: middle;\" src=\"http://i1279.photobucket.com/albums/y538/gravitonthoth/gravityFig2_zps6ec053be.jpg\" alt=\"Figure 2\" width=\"400\" height=\"299\" /></span></p>\n<p class=\"MsoNormal\">Given the sheer number of other galaxies off in all directions, the total dark energy <em style=\"mso-bidi-font-style:normal\">hitting</em> a galaxy would look something like this.<span style=\"mso-spacerun:yes\">&nbsp; </span>The magnitude of the dark energy forces coming in from the rest of the universe ought to be a lot more than what our one little galaxy puts out.<span style=\"mso-spacerun:yes\">&nbsp; </span><br /> <img src=\"http://i1279.photobucket.com/albums/y538/gravitonthoth/GravityFig3_zps98fb7579.jpg\" alt=\"\" width=\"400\" height=\"299\" /><br /> Now let&rsquo;s look inside this galaxy, at a single solar system.<span style=\"mso-spacerun:yes\">&nbsp; </span>Dark energy converges from all directions, as at the perimeter of the galaxy, since the galaxy is mostly empty space, and (I&rsquo;m presuming) a more or less negligible amount is added from other objects within the galaxy.<span style=\"mso-spacerun:yes\">&nbsp; </span></p>\n<p class=\"MsoNormal\"><span style=\"mso-spacerun:yes\"><img src=\"http://i1279.photobucket.com/albums/y538/gravitonthoth/GravityFig4_zps700d323c.jpg\" alt=\"\" /><br /></span></p>\n<p class=\"MsoNormal\">Now let&rsquo;s consider a single planet within the solar system<br /> <span style=\"mso-no-proof:yes\"><img src=\"http://i1279.photobucket.com/albums/y538/gravitonthoth/GravityFig5_zps6b425f28.jpg\" alt=\"\" /></span></p>\n<p class=\"MsoNormal\">The sun casts a shadow in the dark energy field; some of the dark energy headed in the direction of our planet strikes the sun along the way and never makes it to the planet.<span style=\"mso-spacerun:yes\">&nbsp; </span>To a lesser extent, the planet shields the sun as well.<span style=\"mso-spacerun:yes\">&nbsp; </span>As the planet revolves around the sun, there is always a void in the otherwise all-pervasive dark energy field in the direction of the sun.<span style=\"mso-spacerun:yes\">&nbsp; </span>As rudimentary as these diagrams are, you might as well just rotate your monitor if you really need a visual (keeping the screen on the same plane).</p>\n<p class=\"MsoNormal\">Each dark energy vector has an equal opposite cancelling it out, except in the shadow.<span style=\"mso-spacerun:yes\">&nbsp; </span>Any other imbalance would be the same for the planet as it is for the star, and therefore would not alter their positions relative to each other.<span style=\"mso-spacerun:yes\">&nbsp; Even if it was all coming from one direction.&nbsp; </span><br /> <span style=\"mso-no-proof:yes\"><img src=\"http://i1279.photobucket.com/albums/y538/gravitonthoth/GravityFig7_zps0e3e5b43.jpg\" alt=\"\" /></span></p>\n<p class=\"MsoNormal\">The planet always has one region that is only hit with the sun&rsquo;s own dark energy emissions (if it has any), whereas all other sides are being hit with dark energy from all of the rest of the universe (in that direction), hence the illusion of gravity.<span style=\"mso-spacerun:yes\">&nbsp; </span><br /> <br /> Similarly, a supermassive black hole at the center of a galaxy would cast a dark energy shadow on everything else in the galaxy, so the net push of all the dark energy vectors hitting a given star is toward the center of the galaxy, keeping it in orbit.<span style=\"mso-spacerun:yes\">&nbsp; </span>This would be true of any given moment; the direction of the greatest push rotates around, but it is always toward the black hole.<span style=\"mso-spacerun:yes\">&nbsp; </span><br /> <img src=\"http://i1279.photobucket.com/albums/y538/gravitonthoth/GravityFig8_zpsdb55ffcb.jpg\" alt=\"\" /><br /> The planets around the star are shielded by the black hole roughly the same amount as the star is, but they are much more strongly affected by the shielding from the star than the black hole is.<span style=\"mso-spacerun:yes\">&nbsp; </span><br /> <br /> Now, consider 2 galaxies:<br /> <span style=\"mso-no-proof:yes\"><img src=\"http://i1279.photobucket.com/albums/y538/gravitonthoth/GravityFig9_zpsf6a13590.jpg\" alt=\"\" /><br /> Each emits a little bit of dark energy of its own, and is mostly empty space so that much of the dark energy from other galaxies beyond it passes right through.<span style=\"mso-spacerun:yes\">&nbsp; </span>I'm thinking no one object within a galaxy is emitting more dark energy than it shields its neighbors from.<span style=\"mso-spacerun:yes\">&nbsp; </span>There is some galaxy-to-galaxy shielding, but it is very weak due to the amount of dark energy that can pass right through without hitting anything.<span style=\"mso-spacerun:yes\">&nbsp; </span>This would be consistent with the galaxies spreading out from each other without being ripped apart by the force causing them to spread.<span style=\"mso-spacerun:yes\">&nbsp; </span>So, between galaxies, there is a repulsive effect, while within galaxies, there is primarily an effect of shielding from the all-pervasive repulsion from dark energy.<span style=\"mso-spacerun:yes\">&nbsp; </span><br style=\"mso-special-character:line-break\" /> <br style=\"mso-special-character:line-break\" /> </span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H9k65MtszTa5ZBWoq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": -15, "extendedScore": null, "score": 1.0113562110645276e-06, "legacy": true, "legacyId": "19414", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-17T17:19:56.746Z", "modifiedAt": null, "url": null, "title": "Meetup : Different location for Berkeley meetup", "slug": "meetup-different-location-for-berkeley-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/grF4WAvgxBgYDqs27/meetup-different-location-for-berkeley-meetup", "pageUrlRelative": "/posts/grF4WAvgxBgYDqs27/meetup-different-location-for-berkeley-meetup", "linkUrl": "https://www.lesswrong.com/posts/grF4WAvgxBgYDqs27/meetup-different-location-for-berkeley-meetup", "postedAtFormatted": "Wednesday, October 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Different%20location%20for%20Berkeley%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Different%20location%20for%20Berkeley%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgrF4WAvgxBgYDqs27%2Fmeetup-different-location-for-berkeley-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Different%20location%20for%20Berkeley%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgrF4WAvgxBgYDqs27%2Fmeetup-different-location-for-berkeley-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgrF4WAvgxBgYDqs27%2Fmeetup-different-location-for-berkeley-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 75, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ey'>Different location for Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 October 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2128 Oxford St, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Today Zendo and I are unavailable. Several people on the mailing list have suggested that people meet at the Starbucks on Center and Oxford Street at 7pm. You should come there if your coming there acausally implies that other people will come there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ey'>Different location for Berkeley meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "grF4WAvgxBgYDqs27", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0115798325525847e-06, "legacy": true, "legacyId": "19438", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Different_location_for_Berkeley_meetup\">Discussion article for the meetup : <a href=\"/meetups/ey\">Different location for Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 October 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2128 Oxford St, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Today Zendo and I are unavailable. Several people on the mailing list have suggested that people meet at the Starbucks on Center and Oxford Street at 7pm. You should come there if your coming there acausally implies that other people will come there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Different_location_for_Berkeley_meetup1\">Discussion article for the meetup : <a href=\"/meetups/ey\">Different location for Berkeley meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Different location for Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Different_location_for_Berkeley_meetup", "level": 1}, {"title": "Discussion article for the meetup : Different location for Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Different_location_for_Berkeley_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-17T17:22:59.341Z", "modifiedAt": null, "url": null, "title": "Request for sympathy; frustrated with Dark Side", "slug": "request-for-sympathy-frustrated-with-dark-side", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:08.861Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8jnZcgMvyPjxaHNwi/request-for-sympathy-frustrated-with-dark-side", "pageUrlRelative": "/posts/8jnZcgMvyPjxaHNwi/request-for-sympathy-frustrated-with-dark-side", "linkUrl": "https://www.lesswrong.com/posts/8jnZcgMvyPjxaHNwi/request-for-sympathy-frustrated-with-dark-side", "postedAtFormatted": "Wednesday, October 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Request%20for%20sympathy%3B%20frustrated%20with%20Dark%20Side&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequest%20for%20sympathy%3B%20frustrated%20with%20Dark%20Side%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8jnZcgMvyPjxaHNwi%2Frequest-for-sympathy-frustrated-with-dark-side%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Request%20for%20sympathy%3B%20frustrated%20with%20Dark%20Side%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8jnZcgMvyPjxaHNwi%2Frequest-for-sympathy-frustrated-with-dark-side", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8jnZcgMvyPjxaHNwi%2Frequest-for-sympathy-frustrated-with-dark-side", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 115, "htmlBody": "<p>It is really quite frustrating to discuss the intersection of physics and free will with a man who is capable of posting this as his opinion:</p>\n<blockquote>\n<p>I regard atomic motions as determined, that is, as exactly defined to an infinite degree of precision, by the laws of physics, with nothing left over or left out of the explanation, and nothing else to explain.</p>\n</blockquote>\n<p>and then, a day later, posts this as summing up mine:</p>\n<blockquote>\n<p>[His] axiom that the motions of atoms are determined by and only by that description of non-deliberate physical reactions to outside forces called physics. [...] This axiom is not just false, it is obviously, outrageously false.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>AAAGH! *Make up your damned mind, man!*</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8jnZcgMvyPjxaHNwi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": -9, "extendedScore": null, "score": 1.011581464044938e-06, "legacy": true, "legacyId": "19439", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-17T20:03:46.874Z", "modifiedAt": null, "url": null, "title": "[LINK] The most important unsolved problems in ethics", "slug": "link-the-most-important-unsolved-problems-in-ethics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:23.985Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jAk6NZg6YDacuCXxg/link-the-most-important-unsolved-problems-in-ethics", "pageUrlRelative": "/posts/jAk6NZg6YDacuCXxg/link-the-most-important-unsolved-problems-in-ethics", "linkUrl": "https://www.lesswrong.com/posts/jAk6NZg6YDacuCXxg/link-the-most-important-unsolved-problems-in-ethics", "postedAtFormatted": "Wednesday, October 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20The%20most%20important%20unsolved%20problems%20in%20ethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20The%20most%20important%20unsolved%20problems%20in%20ethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjAk6NZg6YDacuCXxg%2Flink-the-most-important-unsolved-problems-in-ethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20The%20most%20important%20unsolved%20problems%20in%20ethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjAk6NZg6YDacuCXxg%2Flink-the-most-important-unsolved-problems-in-ethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjAk6NZg6YDacuCXxg%2Flink-the-most-important-unsolved-problems-in-ethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 394, "htmlBody": "<p>Will Crouch has written up a list of the&nbsp;<a href=\"http://80000hours.org/blog/99-the-most-important-unsolved-problems-in-ethics\">most important unsolved problems in ethics</a>:</p>\n<p>&nbsp;</p>\n<p><span style=\"color: #262626; font-family: MuseoSlab-700, helvetica, arial, sans-serif; font-size: 18px; line-height: 1.3333em;\">The Practical List</span></p>\n<p><br style=\"color: #262626; font-family: MuseoSans-500, helvetica, arial, sans-serif; font-size: 16px; line-height: 24px;\" /></p>\n<ol style=\"margin-top: 0px; margin-right: 0px; margin-left: 25px; padding: 0px; border: 0px; outline: 0px; font-size: 16px; vertical-align: baseline; color: #262626; font-family: MuseoSans-500, helvetica, arial, sans-serif; line-height: 24px;\">\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">What&rsquo;s the optimal career choice?</strong>&nbsp;Professional philanthropy, influencing, research, or something more common-sensically virtuous?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">What&rsquo;s the optimal donation area?</strong>&nbsp;Development charities? Animal welfare charities? Extinction risk mitigation charities? Meta-charities? Or investing the money and donating later?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">What are the highest leverage political policies?</strong>&nbsp;Libertarian paternalism? Prediction markets? Cruelty taxes, such as taxes on caged hens; luxury taxes?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">What are the highest value areas of research?</strong>&nbsp;Tropical medicine? Artificial intelligence? Economic cost-effectiveness analysis? Moral philosophy?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">Given our best ethical theories (or best credence distribution in ethical theories), what&rsquo;s the biggest problem we currently face?</strong></p>\n</li>\n</ol>\n<p><br style=\"color: #262626; font-family: MuseoSans-500, helvetica, arial, sans-serif; font-size: 16px; line-height: 24px;\" /></p>\n<h3 id=\"the_theoretical_list\" style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 18px; vertical-align: baseline; font-weight: normal; color: #262626; text-rendering: optimizelegibility; font-family: MuseoSlab-700, helvetica, arial, sans-serif; line-height: 1.3333em;\">The Theoretical List</h3>\n<p><br style=\"color: #262626; font-family: MuseoSans-500, helvetica, arial, sans-serif; font-size: 16px; line-height: 24px;\" /></p>\n<ol style=\"margin-top: 0px; margin-right: 0px; margin-left: 25px; padding: 0px; border: 0px; outline: 0px; font-size: 16px; vertical-align: baseline; color: #262626; font-family: MuseoSans-500, helvetica, arial, sans-serif; line-height: 24px;\">\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">What&rsquo;s the correct population ethics?</strong>&nbsp;How should we value future people compared with present people? Do people have diminishing marginal value?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">Should we maximise expected value when it comes to small probabilities of huge amounts of value?</strong>&nbsp;If not, what should we do instead?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">How should we respond to the possibility of creating infinite value (or disvalue)?</strong>&nbsp;Should that consideration swamp all others? If not, why not?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">How should we respond to the possibility that the universe actually has infinite value?</strong>&nbsp;Does it mean that we have no reason to do any action (because we don&rsquo;t increase the sum total of value in the world)? Or does this possibility refute aggregative consequentialism?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">How should we accommodate moral uncertainty?</strong>&nbsp;Should we apply expected utility theory? If so, how do we make intertheoretic value comparisons? Does this mean that some high-stakes theories should dominate our moral thinking, even if we assign them low credence?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">How should intuitions weigh against theoretical virtues in normative ethics?</strong>&nbsp;Is common-sense ethics roughly correct? Or should we prefer simpler moral theories?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">Should we prioritise the prevention of human wrongs over the alleviation of naturally caused suffering?</strong>&nbsp;If so, by how much?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">What sorts of entities have moral value?</strong>&nbsp;Humans, presumably. But what about non-human animals? Insects? The natural environment? Artificial intelligence?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">What additional items should be on these lists?</strong></p>\n</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Pa2SdZsLFmqhs42Do": 1, "nSHiKwWyMZFdZg5qt": 1, "qAvbtzdG2A2RBn7in": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jAk6NZg6YDacuCXxg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 34, "extendedScore": null, "score": 8.6e-05, "legacy": true, "legacyId": "19440", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Will Crouch has written up a list of the&nbsp;<a href=\"http://80000hours.org/blog/99-the-most-important-unsolved-problems-in-ethics\">most important unsolved problems in ethics</a>:</p>\n<p>&nbsp;</p>\n<p><span style=\"color: #262626; font-family: MuseoSlab-700, helvetica, arial, sans-serif; font-size: 18px; line-height: 1.3333em;\">The Practical List</span></p>\n<p><br style=\"color: #262626; font-family: MuseoSans-500, helvetica, arial, sans-serif; font-size: 16px; line-height: 24px;\"></p>\n<ol style=\"margin-top: 0px; margin-right: 0px; margin-left: 25px; padding: 0px; border: 0px; outline: 0px; font-size: 16px; vertical-align: baseline; color: #262626; font-family: MuseoSans-500, helvetica, arial, sans-serif; line-height: 24px;\">\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">What\u2019s the optimal career choice?</strong>&nbsp;Professional philanthropy, influencing, research, or something more common-sensically virtuous?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">What\u2019s the optimal donation area?</strong>&nbsp;Development charities? Animal welfare charities? Extinction risk mitigation charities? Meta-charities? Or investing the money and donating later?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">What are the highest leverage political policies?</strong>&nbsp;Libertarian paternalism? Prediction markets? Cruelty taxes, such as taxes on caged hens; luxury taxes?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">What are the highest value areas of research?</strong>&nbsp;Tropical medicine? Artificial intelligence? Economic cost-effectiveness analysis? Moral philosophy?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\" id=\"Given_our_best_ethical_theories__or_best_credence_distribution_in_ethical_theories___what_s_the_biggest_problem_we_currently_face_\">Given our best ethical theories (or best credence distribution in ethical theories), what\u2019s the biggest problem we currently face?</strong></p>\n</li>\n</ol>\n<p><br style=\"color: #262626; font-family: MuseoSans-500, helvetica, arial, sans-serif; font-size: 16px; line-height: 24px;\"></p>\n<h3 id=\"The_Theoretical_List\" style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 18px; vertical-align: baseline; font-weight: normal; color: #262626; text-rendering: optimizelegibility; font-family: MuseoSlab-700, helvetica, arial, sans-serif; line-height: 1.3333em;\">The Theoretical List</h3>\n<p><br style=\"color: #262626; font-family: MuseoSans-500, helvetica, arial, sans-serif; font-size: 16px; line-height: 24px;\"></p>\n<ol style=\"margin-top: 0px; margin-right: 0px; margin-left: 25px; padding: 0px; border: 0px; outline: 0px; font-size: 16px; vertical-align: baseline; color: #262626; font-family: MuseoSans-500, helvetica, arial, sans-serif; line-height: 24px;\">\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">What\u2019s the correct population ethics?</strong>&nbsp;How should we value future people compared with present people? Do people have diminishing marginal value?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">Should we maximise expected value when it comes to small probabilities of huge amounts of value?</strong>&nbsp;If not, what should we do instead?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">How should we respond to the possibility of creating infinite value (or disvalue)?</strong>&nbsp;Should that consideration swamp all others? If not, why not?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">How should we respond to the possibility that the universe actually has infinite value?</strong>&nbsp;Does it mean that we have no reason to do any action (because we don\u2019t increase the sum total of value in the world)? Or does this possibility refute aggregative consequentialism?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">How should we accommodate moral uncertainty?</strong>&nbsp;Should we apply expected utility theory? If so, how do we make intertheoretic value comparisons? Does this mean that some high-stakes theories should dominate our moral thinking, even if we assign them low credence?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">How should intuitions weigh against theoretical virtues in normative ethics?</strong>&nbsp;Is common-sense ethics roughly correct? Or should we prefer simpler moral theories?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">Should we prioritise the prevention of human wrongs over the alleviation of naturally caused suffering?</strong>&nbsp;If so, by how much?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\">What sorts of entities have moral value?</strong>&nbsp;Humans, presumably. But what about non-human animals? Insects? The natural environment? Artificial intelligence?</p>\n</li>\n<li style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; line-height: 1.5;\">\n<p style=\"margin: 0px 0px 1.5em; padding: 0px; border: 0px; outline: 0px; font-size: 1em; vertical-align: baseline; background-color: transparent;\"><strong style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; font-weight: normal; font-family: MuseoSans-700, helvetica, arial, sans-serif;\" id=\"What_additional_items_should_be_on_these_lists_\">What additional items should be on these lists?</strong></p>\n</li>\n</ol>", "sections": [{"title": "Given our best ethical theories (or best credence distribution in ethical theories), what\u2019s the biggest problem we currently face?", "anchor": "Given_our_best_ethical_theories__or_best_credence_distribution_in_ethical_theories___what_s_the_biggest_problem_we_currently_face_", "level": 2}, {"title": "The Theoretical List", "anchor": "The_Theoretical_List", "level": 1}, {"title": "What additional items should be on these lists?", "anchor": "What_additional_items_should_be_on_these_lists_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "46 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-17T22:46:56.936Z", "modifiedAt": null, "url": null, "title": "Meetup : Santiago, Chile", "slug": "meetup-santiago-chile", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "transh", "createdAt": "2012-03-03T18:26:47.926Z", "isAdmin": false, "displayName": "transh"}, "userId": "7qTtfaEJ4mbQrhFgh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/A6endkcqKyy9pgMcc/meetup-santiago-chile", "pageUrlRelative": "/posts/A6endkcqKyy9pgMcc/meetup-santiago-chile", "linkUrl": "https://www.lesswrong.com/posts/A6endkcqKyy9pgMcc/meetup-santiago-chile", "postedAtFormatted": "Wednesday, October 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Santiago%2C%20Chile&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Santiago%2C%20Chile%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA6endkcqKyy9pgMcc%2Fmeetup-santiago-chile%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Santiago%2C%20Chile%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA6endkcqKyy9pgMcc%2Fmeetup-santiago-chile", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA6endkcqKyy9pgMcc%2Fmeetup-santiago-chile", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 25, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ez'>Santiago, Chile</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 December 2011 07:46:00PM (-0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Santiago, Chile</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Anyone here?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ez'>Santiago, Chile</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "A6endkcqKyy9pgMcc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0117551690456361e-06, "legacy": true, "legacyId": "19441", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Santiago__Chile\">Discussion article for the meetup : <a href=\"/meetups/ez\">Santiago, Chile</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 December 2011 07:46:00PM (-0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Santiago, Chile</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Anyone here?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Santiago__Chile1\">Discussion article for the meetup : <a href=\"/meetups/ez\">Santiago, Chile</a></h2>", "sections": [{"title": "Discussion article for the meetup : Santiago, Chile", "anchor": "Discussion_article_for_the_meetup___Santiago__Chile", "level": 1}, {"title": "Discussion article for the meetup : Santiago, Chile", "anchor": "Discussion_article_for_the_meetup___Santiago__Chile1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-17T23:40:34.608Z", "modifiedAt": null, "url": null, "title": "Meetup : DC Meetup: Games and Conversation", "slug": "meetup-dc-meetup-games-and-conversation", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "maia", "createdAt": "2012-02-08T20:28:42.643Z", "isAdmin": false, "displayName": "maia"}, "userId": "QW72Lk68mKnTfmdAB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vfyDwReLyFeEvPPhM/meetup-dc-meetup-games-and-conversation", "pageUrlRelative": "/posts/vfyDwReLyFeEvPPhM/meetup-dc-meetup-games-and-conversation", "linkUrl": "https://www.lesswrong.com/posts/vfyDwReLyFeEvPPhM/meetup-dc-meetup-games-and-conversation", "postedAtFormatted": "Wednesday, October 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20DC%20Meetup%3A%20Games%20and%20Conversation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20DC%20Meetup%3A%20Games%20and%20Conversation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvfyDwReLyFeEvPPhM%2Fmeetup-dc-meetup-games-and-conversation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20DC%20Meetup%3A%20Games%20and%20Conversation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvfyDwReLyFeEvPPhM%2Fmeetup-dc-meetup-games-and-conversation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvfyDwReLyFeEvPPhM%2Fmeetup-dc-meetup-games-and-conversation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/f0'>DC Meetup: Games and Conversation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 October 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Kogod Courtyard</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next meetup will be at the courtyard by the Portrait Gallery at 3pm on Sunday 10/21, as per usual. The theme will be games and general conversation. Roger will bring Zendo, and Eileen is tentatively planning to bring Apples to Apples.</p>\n\n<p>Other games and topics are welcome. Post here or email the list if you plan to bring one or more.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/f0'>DC Meetup: Games and Conversation</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vfyDwReLyFeEvPPhM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0117839290050183e-06, "legacy": true, "legacyId": "19442", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___DC_Meetup__Games_and_Conversation\">Discussion article for the meetup : <a href=\"/meetups/f0\">DC Meetup: Games and Conversation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 October 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Kogod Courtyard</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next meetup will be at the courtyard by the Portrait Gallery at 3pm on Sunday 10/21, as per usual. The theme will be games and general conversation. Roger will bring Zendo, and Eileen is tentatively planning to bring Apples to Apples.</p>\n\n<p>Other games and topics are welcome. Post here or email the list if you plan to bring one or more.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___DC_Meetup__Games_and_Conversation1\">Discussion article for the meetup : <a href=\"/meetups/f0\">DC Meetup: Games and Conversation</a></h2>", "sections": [{"title": "Discussion article for the meetup : DC Meetup: Games and Conversation", "anchor": "Discussion_article_for_the_meetup___DC_Meetup__Games_and_Conversation", "level": 1}, {"title": "Discussion article for the meetup : DC Meetup: Games and Conversation", "anchor": "Discussion_article_for_the_meetup___DC_Meetup__Games_and_Conversation1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-18T05:29:38.046Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Lawful Creativity", "slug": "seq-rerun-lawful-creativity", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6EgEZtLMpn4q7QErc/seq-rerun-lawful-creativity", "pageUrlRelative": "/posts/6EgEZtLMpn4q7QErc/seq-rerun-lawful-creativity", "linkUrl": "https://www.lesswrong.com/posts/6EgEZtLMpn4q7QErc/seq-rerun-lawful-creativity", "postedAtFormatted": "Thursday, October 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Lawful%20Creativity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Lawful%20Creativity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6EgEZtLMpn4q7QErc%2Fseq-rerun-lawful-creativity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Lawful%20Creativity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6EgEZtLMpn4q7QErc%2Fseq-rerun-lawful-creativity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6EgEZtLMpn4q7QErc%2Fseq-rerun-lawful-creativity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 176, "htmlBody": "<p>Today's post, <a href=\"/lw/vm/lawful_creativity/\">Lawful Creativity</a> was originally published on 08 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Lawful_Creativity\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Creativity seems to consist of breaking rules, and violating expectations. But there is one rule that cannot be broken: creative solutions must have something good about them. Creativity is a surprise, but most surprises aren't creative.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ezi/seq_rerun_recognizing_intelligence/\">Recognizing Intelligence</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6EgEZtLMpn4q7QErc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0119711592936736e-06, "legacy": true, "legacyId": "19458", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KKLQp934n77cfZpPn", "m46jnehmKAtay7AJu", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-18T10:49:26.516Z", "modifiedAt": "2022-02-24T01:52:55.269Z", "url": null, "title": "Stuff That Makes Stuff Happen", "slug": "stuff-that-makes-stuff-happen", "viewCount": null, "lastCommentedAt": "2019-09-09T17:18:44.739Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NhQju3htS9W6p6wE6/stuff-that-makes-stuff-happen", "pageUrlRelative": "/posts/NhQju3htS9W6p6wE6/stuff-that-makes-stuff-happen", "linkUrl": "https://www.lesswrong.com/posts/NhQju3htS9W6p6wE6/stuff-that-makes-stuff-happen", "postedAtFormatted": "Thursday, October 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Stuff%20That%20Makes%20Stuff%20Happen&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStuff%20That%20Makes%20Stuff%20Happen%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNhQju3htS9W6p6wE6%2Fstuff-that-makes-stuff-happen%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Stuff%20That%20Makes%20Stuff%20Happen%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNhQju3htS9W6p6wE6%2Fstuff-that-makes-stuff-happen", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNhQju3htS9W6p6wE6%2Fstuff-that-makes-stuff-happen", "socialPreviewImageUrl": "http://wiki.lesswrong.com/mediawiki/images/d/d5/OEIX.svg", "question": false, "authorIsUnreviewed": false, "wordCount": 2763, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/eva/the_fabric_of_real_things/\">Causality: The Fabric of Real Things</a></p>\n<p><em>Previous <a href=\"/lw/eva/the_fabric_of_real_things/#7lqh\">meditation</a>:</em></p>\n<p>\"You say that a <em>universe</em> is a connected fabric of causes and effects. Well, that's a very Western viewpoint - that it's all about mechanistic, deterministic stuff. I agree that anything else is outside the realm of science, but it can still be <em>real,</em> you know. My cousin is psychic - if you draw a card from his deck of cards, he can tell you the name of your card before he looks at it. There's no <em>mechanism</em> for it - it's not a <em>causal </em>thing that scientists could study - he just <em>does</em> it. Same thing when I commune on a deep level with the entire universe in order to realize that my partner truly loves me. I agree that purely spiritual phenomena are outside the realm of causal processes that can be studied by experiments, but I don't agree that they can't be <em>real.</em>\"</p>\n<p><em>Reply:</em></p>\n<p>Fundamentally, a causal model is a way of <em>factorizing our uncertainty</em> about the universe.&nbsp; One&nbsp;way of viewing a causal model is as a structure of <em>deterministic</em> functions plus <em>uncorrelated</em> sources of background uncertainty.</p>\n<p>Let's use the Obesity-Exercise-Internet model <em>(reminder: which is totally made up)</em>&nbsp;as an example again:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/d/d5/OEIX.svg\" alt=\"\" width=\"432\" height=\"160\"></p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(x_1, x_2, x_3) = p(x_1)p(x_2)p(x_3|x_1, x_2)\" alt=\"\" width=\"283\" height=\"19\"></p>\n<p>We can also view this as a set of <em>deterministic </em>functions F<sub>i</sub>, plus <em>uncorrelated</em> background sources of uncertainty U<sub>i</sub>:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/6/6b/Markov.svg\" alt=\"\" width=\"430\" height=\"224\"></p>\n<p>This says is that the value x<sub>3</sub> - how much someone exercises - is a function of how obese they are (x<sub>1</sub>), how much time they spend on the Internet (x<sub>2</sub>), <em>plus </em>some other background factors U<sub>3</sub><em> which don't correlate to anything else in the diagram,</em> all of which <em>collectively</em> determine, when combined by the mechanism F<sub>3</sub>, how much time someone spends exercising.<a id=\"more\"></a></p>\n<p>There might be any number of different real factors involved in the possible states of U<sub>3</sub> - like whether someone has a personal taste for jogging, whether they've ever been to a trampoline park and liked it, whether they have some gene that affects exercise endorphins. These are all different unknown background facts about a person, which might affect whether or not they exercise, above and beyond obesity and Internet use.</p>\n<p>But from the perspective of somebody building a causal model, so long as we don't have anything else in our causal graph that <em>correlates</em> with these factors, we can sum them up into a single <em>factor of subjective uncertainty,</em> our uncertainty U<sub>3</sub> about all the other things that might add up to a force for or against exercising. Once we know that someone isn't overweight and that they spend a lot of time on the Internet, all our uncertainty about those other <em>background</em> factors gets summed up with those two <em>known</em> factors and turned into a 38% conditional probability that the person exercises frequently.</p>\n<p>And the key condition on a causal graph is that if you've properly described your beliefs about the connective&nbsp;mechanisms F<sub>i</sub>, all your remaining uncertainty U<sub>i</sub> should be <em>conditionally</em>&nbsp;<em>independent:</em></p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(u_1, u_2, u_3) = p(u_1)p(u_2)p(u_3)\" alt=\"\" width=\"235\" height=\"19\"></p>\n<p>or more generally</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(\\mathbf U) = \\prod p(U_i)\" alt=\"\" width=\"129\" height=\"26\"></p>\n<p>And then plugging those probable U<sub>i</sub> into the strictly deterministic F<sub>i</sub> should give us back out our whole causal model - the same joint probability table over the observable X<sub>i</sub>.</p>\n<p>Hence the idea that a causal model <em>factorizes</em> uncertainty. It factorizes out all the mechanisms that we <em>believe</em> connect variables, and all remaining uncertainty should be uncorrelated <em>so far as we know</em>.</p>\n<p>To put it another way, if we ourselves knew about a correlation between two U<sub>i</sub> that <em>wasn't</em> in the causal model, our own expectations for the joint probability table couldn't match the model's product</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?p(\\mathbf x) = \\prod p(x_i|\\mathbf{pa_i})\" alt=\"\" width=\"153\" height=\"26\"></p>\n<p>and all the theorems about causal inference would go out the window. Technically, the idea that the U<sub>i</sub> are uncorrelated is known as the <a href=\"http://en.wikipedia.org/wiki/Causal_Markov_condition\">causal&nbsp;Markov condition</a>.</p>\n<p>What if you realize that two variables actually <em>are</em>&nbsp;correlated more than you thought? &nbsp;What if, to make the diagram correspond to reality, you'd have to hack it to make some&nbsp;U<sub>a</sub>&nbsp;and U<sub>b</sub>&nbsp;correlated?</p>\n<p>Then you draw another arrow from X<sub>a</sub> to X<sub>b</sub>, or from X<sub>b</sub> to X<sub>a</sub>; or you make a new node representing the correlated part of U<sub>a</sub> and U<sub>b</sub>, X<sub>c</sub>, and draw arrows from X<sub>c</sub> to X<sub>a</sub> and X<sub>b</sub>.</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/2/2c/Corr.svg\" alt=\"\" width=\"300\" height=\"84\"></p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/5/55/Corr1.svg\" alt=\"\" width=\"179\" height=\"62\">&nbsp;vs.&nbsp;<img src=\"http://wiki.lesswrong.com/mediawiki/images/7/7f/Corr2.svg\" alt=\"\" width=\"179\" height=\"62\">&nbsp;vs.&nbsp;<img src=\"http://wiki.lesswrong.com/mediawiki/images/1/12/Corr3.svg\" alt=\"\" width=\"179\" height=\"160\"></p>\n<p>(Or you might have to draw some extra causal arrows somewhere else; but those three changes are the ones that would solve the problem most directly.)</p>\n<p>There was apparently at one point - I'm not sure if it's still going on or not - this big debate about the true meaning of <em>randomization</em> in experiments, and what counts as 'truly random'. Is your randomized experiment invalidated, if you use a merely pseudo-random algorithm instead of a thermal noise generator? Is it okay to use pseudo-random algorithms? Is it okay to use shoddy pseudo-randomness that a professional cryptographer would sneer at? Clearly, using 1-0-1-0-1-0 on a list of patients in alphabetical order isn't random enough... or is it? What if you pair off patients in alphabetical order, and flip a coin to assign one member of each pair to the experimental group and the control? How random is random?</p>\n<p><a href=\"http://xkcd.com/221/\"><img src=\"http://imgs.xkcd.com/comics/random_number.png\" alt=\"\" width=\"400\" height=\"144\"></a></p>\n<p>Understanding that causal models <em>factorize uncertainty</em> leads to the realization that \"randomizing\" an experimental variable means using randomness, a U<sub>x</sub> for the assignment, which doesn't correlate with your uncertainty about any other U<sub>i</sub>. Our <em>uncertainty</em> about a thermal noise generator seems strongly guaranteed to be uncorrelated with our <em>uncertainty</em> about a subject's economic status, their upbringing, or anything else in the universe that might affect how they react to Drug A...</p>\n<p>...unless somebody wrote <em>down</em> the output of the thermal noise generator, and then used it in <em>another</em> experiment on the <em>same</em> group of subjects to test Drug B. It doesn't matter how \"intrinsically random\" that output was - whether it was the XOR of a thermal noise source, a quantum noise source, a human being's so-called free will, and the world's strongest cryptographic algorithm - once it ends up <em>correlated</em> to any other uncertain background factor, any other U<sub>i</sub>, you've invalidated the randomization. &nbsp;That's the implicit problem in the XKCD cartoon above.</p>\n<p>But picking a strong randomness source, and using the output only <em>once</em>, is a pretty solid guarantee this won't happen.</p>\n<p>Unless, ya know, you start out with a list of subjects sorted by income, and the randomness source randomly happens to put out 111111000000. Whereupon, as soon as you <em>look</em> at the output and are <em>no longer</em> uncertain about it, you might expect correlation and trouble. But that's a different and much thornier issue in Bayesianism vs. frequentism.</p>\n<p>If we take frequentist ideas about randomization at face value, then the key requirement for theorems about experimental randomization to be applicable, is for your uncertainty about patient randomization to <em>not correlate</em> with any other background facts about the patients. A double-blinded study (where the doctors don't know patient status) ensures that patient status doesn't correlate with the doctor's <em>beliefs</em> about a patient leading them to treat patients differently. Even plugging in the fixed string \"1010101010\" would be sufficiently random <em>if</em> that pattern wasn't correlated to anything important; the trouble is that such a simple pattern could very easily correlate with some background effect, and we can believe in this possible correlation even if we're not sure what the exact correlation would be.</p>\n<p>(It's worth noting that the Center for Applied Rationality ran the June minicamp experiment using a standard but unusual statistical method of sorting applicants into pairs that seemed of roughly matched prior ability / prior expected outcome, and then flipping a coin to pick one member of each pair to be admitted or not admitted that year. &nbsp;This procedure means you never randomly improbably get an experimental group that would, once you actually looked at the random numbers, seem much more promising or much worse than the control group in advance - where the frequentist guarantee that you used an experimental procedure where this usually doesn't happen 'in the long run', might be cold comfort if it obviously&nbsp;<em>had&nbsp;</em>happened this time once you looked at the random numbers. &nbsp;Roughly, this choice reflects a difference between frequentist ideas about procedures that make it hard for scientists to obtain results unless their theories are true, and then not caring about the actual random numbers so long as it's still hard to get fake results on average; versus a Bayesian goal of trying to get the maximum evidence out of the update we'll actually have to perform after looking at the results, including how the random numbers turned out on this particular occasion. &nbsp;Note that frequentist ethics are still being obeyed - you can't game the expected statistical significance of experimental vs. control results by picking bad pairs, so long as the coinflips themselves are fair!)</p>\n<hr>\n<p>Okay, let's look at that meditation again:</p>\n<p>\"You say that a <em>universe</em> is a connected fabric of causes and effects. Well, that's a very Western viewpoint - that it's all about mechanistic, deterministic stuff. I agree that anything else is outside the realm of science, but it can still be <em>real,</em> you know. My cousin is psychic - if you draw a card from his deck of cards, he can tell you the name of your card before he looks at it. There's no <em>mechanism</em> for it - it's not a <em>causal </em>thing that scientists could study - he just <em>does</em> it. Same thing when I commune on a deep level with the entire universe in order to realize that my partner truly loves me. I agree that purely spiritual phenomena are outside the realm of causal processes that can be studied by experiments, but I don't agree that they can't be <em>real.</em>\"</p>\n<p>Well, you know, you can stand there all day, shouting all you like about how something is outside the realm of science, but if a picture of the world has this...</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/7e/Deck.svg\" alt=\"\" width=\"562\" height=\"94\"></p>\n<p>...then we're either going to draw an arrow from the top card to the prediction; an arrow from the prediction to the top card (the prediction makes it happen!); or arrows from a third source to both of them (aliens are picking the top card and using telepathy on your cousin... or something; there's no rule you have to <em>label</em> your nodes).</p>\n<p>More generally, for me to expect your beliefs to correlate with reality, I have to either think that reality is the cause of your beliefs, expect your beliefs to alter reality, or believe that some third factor is influencing both of them.</p>\n<p>This is the <em>more general</em> argument that \"To draw an accurate map of a city, you have to open the blinds and look out the window and draw lines on paper corresponding to what you see; sitting in your living-room with the blinds closed, making stuff up, isn't going to work.\"</p>\n<p>Correlation requires causal interaction; and expecting beliefs to be true means expecting the map to <em>correlate</em> with the territory. To open your eyes and look at your shoelaces is to let those shoelaces have a causal effect on your brain - in general, looking at something, gaining information about it, requires letting it causally affect you. Learning about X means letting your brain's state be causally determined by X's state. The first thing that happens is that your shoelace is untied; the next thing that happens is that the shoelace interacts with your brain, via light and eyes and the visual cortex, in a way that makes your brain believe your shoelace is untied.</p>\n<table border=\"0\" cellpadding=\"5\">\n<tbody>\n<tr valign=\"middle\">\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/e/e4/Shoe.svg\" alt=\"\" width=\"254\" height=\"192\"></td>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(Shoelace=tied, Belief=\"tied\")</td>\n<td>0.931</td>\n</tr>\n<tr>\n<td>p(Shoelace=tied, Belief=\"untied\")</td>\n<td>0.003</td>\n</tr>\n<tr>\n<td>p(Shoelace=untied, Belief=\"untied\")</td>\n<td>0.053</td>\n</tr>\n<tr>\n<td>p(Shoelace=untied, Belief=\"tied\")</td>\n<td>0.012</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n</tbody>\n</table>\n<p>This is related in spirit to <a href=\"/lw/o5/the_second_law_of_thermodynamics_and_engines_of/\">the idea seen earlier on LW</a>&nbsp;that having knowledge materialize from nowhere <em>directly</em> violates the second law of thermodynamics because mutual information counts as thermodynamic negentropy. But the causal form of the proof is much deeper and more general. It applies even in universes like Conway's Game of Life where there's no equivalent of the second law of thermodynamics. It applies even if we're in the Matrix and the aliens can violate physics at will. Even when entropy can go down, you still can't learn about things without being causally connected to them.</p>\n<hr>\n<p>The fundamental question of rationality, \"What do you think you know and how do you think you know it?\", is on its strictest level a request for a causal model of how you think your brain ended up mirroring reality - the causal process which accounts for this supposed correlation.</p>\n<p>You might not think that this would be a useful question to ask - that when your brain has an irrational belief, it would automatically have irrational beliefs about process.</p>\n<p>But \"the human brain is not illogically omniscient\", we might say. When our brain undergoes motivated cognition or other fallacies, it often ends up strongly believing in X, <em>without</em> the unconscious rationalization process having been sophisticated enough to <em>also</em> invent a causal story explaining how we know X. \"How could you possibly know that, even if it was true?\" is a more skeptical form of the same question. If you can successfully stop your brain from rationalizing-on-the-spot, there actually <em>is</em> this useful thing you can sometimes catch yourself in, wherein you go, \"Oh, wait, even if I'm in a world where AI does get developed on March 4th, 2029, there's no lawful story which could account for me knowing that in advance - there must've been some other pressure on my brain to produce that belief.\"</p>\n<hr>\n<p>Since it illustrates an important general point, I shall now take a moment to remark on the idea that science is merely one magisterium, and there's other magisteria which can't be subjected to standards of mere evidence, because they are special. That seeing a ghost, or knowing something because God spoke to you in your heart, is an exception to the ordinary laws of epistemology.</p>\n<p>That exception would be convenient for the speaker, perhaps. But causality is <em>more general</em> than that; it is <em>not</em> excepted by such hypotheses. \"I saw a ghost\", \"I mysteriously sensed a ghost\", \"God spoke to me in my heart\" - there's no difficulty drawing those causal diagrams.</p>\n<p>The <em>methods</em> of science - even sophisticated methods like the conditions for randomizing a trial - aren't just about atoms, or quantum fields.</p>\n<p>They're about stuff that makes stuff happen, and happens because of other stuff.</p>\n<p>In this world there are well-paid professional marketers, including philosophical and theological marketers, who have thousands of hours of practice convincing customers that their beliefs are beyond the reach of science. But those marketers don't know about causal models. They may know about - know how to lie persuasively relative to - the epistemology used by a&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Traditional_rationality\">Traditional Rationalist</a>, but that's crude by the standards of today's rationality-with-math. Highly Advanced Epistemology hasn't diffused far enough for there to be explicit anti-epistemology against it.</p>\n<p>And so we shouldn't <em>expect</em> to find anyone with a background story which would justify evading science's skeptical gaze. As a matter of cognitive science, it seems extremely likely that the human brain natively represents something like causal structure - that this native representation is how your own brain knows that \"If the radio says there was an earthquake, it's less likely that your burglar alarm going off implies a burglar.\" People who want to evade the gaze of science haven't read Judea Pearl's book; they don't know enough about formal causality to <em>not</em> automatically reason this way about things they claim are in separate magisteria. They can say words like \"It's not mechanistic\", but they don't have the mathematical fluency it would take to deliberately design a system outside Judea Pearl's box.</p>\n<p>So in all probability, when somebody says, \"I communed holistically and in a purely spiritual fashion with the entire universe - that's how I know my partner loves me, not because of any <em>mechanism</em>\", their brain is just representing something like this:</p>\n<table border=\"0\" cellpadding=\"5\">\n<tbody>\n<tr valign=\"middle\">\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/72/Love.svg\" alt=\"\" width=\"386\" height=\"363\"></td>\n<td>\n<table border=\"1\">\n<tbody>\n<!-- Results table headers --> \n<tr>\n<th>Partner loves</th> <th>Universe knows</th> <th>I hear universe</th> <th>%</th>\n</tr>\n<tr>\n<td>p</td>\n<td>u</td>\n<td>h</td>\n<td>0.44</td>\n</tr>\n<tr>\n<td>p</td>\n<td>u</td>\n<td>\u00ach</td>\n<td>0.023</td>\n</tr>\n<tr>\n<td>p</td>\n<td>\u00acu</td>\n<td>h</td>\n<td>0.01</td>\n</tr>\n<tr>\n<td>p</td>\n<td>\u00acu</td>\n<td>\u00ach</td>\n<td>0.025</td>\n</tr>\n<tr>\n<td>\u00acp</td>\n<td>u</td>\n<td>h</td>\n<td>0.43</td>\n</tr>\n<tr>\n<td>\u00acp</td>\n<td>u</td>\n<td>\u00ach</td>\n<td>0.023</td>\n</tr>\n<tr>\n<td>\u00acp</td>\n<td>\u00acu</td>\n<td>h</td>\n<td>0.015</td>\n</tr>\n<tr>\n<td>\u00acp</td>\n<td>\u00acu</td>\n<td>\u00ach</td>\n<td>0.035</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n</tbody>\n</table>\n<p>True, false, or meaningless, this belief isn't beyond investigation by standard rationality.</p>\n<p>Because <em>causality</em> isn't a word for a special, restricted domain that scientists study. 'Causal process' sounds like an impressive formal word that would be used by people in lab coats with doctorates, but that's not what it <em>means.</em></p>\n<p>'Cause and effect' just means \"stuff that makes stuff happen and happens because of other stuff\". Any time there's a noun, a verb, and a subject, there's causality. If the universe spoke to you in your heart - then the universe would be making stuff happen inside your heart! All the standard theorems would still apply.</p>\n<p>Whatever people try to imagine that science supposedly can't analyze, it just ends up as more \"stuff that makes stuff happen and happens because of other stuff\".</p>\n<hr>\n<p><strong><a href=\"https://www.lesswrong.com/posts/NhQju3htS9W6p6wE6/stuff-that-makes-stuff-happen#x3crJ7E24DRLCpqMs\">&nbsp;Mainstream status.</a></strong></p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/f1u/causal_reference/\">Causal Reference</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/ev3/causal_diagrams_and_causal_models/\">Causal Diagrams and Causal Models</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"cq69M9ceLNA35ShTR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NhQju3htS9W6p6wE6", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 68, "baseScore": 90, "extendedScore": null, "score": 0.000195, "legacy": true, "legacyId": "19434", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "http://wiki.lesswrong.com/mediawiki/images/d/d5/OEIX.svg", "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "causal-reference", "canonicalPrevPostSlug": "causal-diagrams-and-causal-models", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 90, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 128, "af": false, "version": "1.2.0", "pingbacks": {"Posts": ["h6fzC6wFYFxxKDm8u", "QkX2bAkwG2EpGvNug", "PXoWk554FZ4Gpfvah", "hzuSDMx7pd2uxFc5w"]}, "moderationGuidelinesVersion": "1.1.0", "customHighlightVersion": "1.1.0", "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-19T01:12:48.113Z", "modifiedAt": null, "url": null, "title": "2012 Less Wrong Census Survey: Call For Critiques/Questions", "slug": "2012-less-wrong-census-survey-call-for-critiques-questions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:11.436Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QXShCBvPydkwafekn/2012-less-wrong-census-survey-call-for-critiques-questions", "pageUrlRelative": "/posts/QXShCBvPydkwafekn/2012-less-wrong-census-survey-call-for-critiques-questions", "linkUrl": "https://www.lesswrong.com/posts/QXShCBvPydkwafekn/2012-less-wrong-census-survey-call-for-critiques-questions", "postedAtFormatted": "Friday, October 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202012%20Less%20Wrong%20Census%20Survey%3A%20Call%20For%20Critiques%2FQuestions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2012%20Less%20Wrong%20Census%20Survey%3A%20Call%20For%20Critiques%2FQuestions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQXShCBvPydkwafekn%2F2012-less-wrong-census-survey-call-for-critiques-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2012%20Less%20Wrong%20Census%20Survey%3A%20Call%20For%20Critiques%2FQuestions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQXShCBvPydkwafekn%2F2012-less-wrong-census-survey-call-for-critiques-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQXShCBvPydkwafekn%2F2012-less-wrong-census-survey-call-for-critiques-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 233, "htmlBody": "<p>The first draft of the 2012 Less Wrong Census/Survey is complete (see 2011 <a href=\"/lw/8p4/2011_survey_results/\">here</a>). I will link it below if you promise <em>not to try to take the survey because it's not done yet and this is just an example!</em></p>\n<p><a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dG1pTzlrTnJ4eks3aE13Ni1lbV8yUkE6MQ#gid=0\"><strong>2012 Less Wrong Census/Survey Draft</strong></a></p>\n<p>I want three things from you.</p>\n<p>First, please critique this draft. Tell me if any questions are unclear, misleading, offensive, confusing, or stupid. Tell me if the survey is so unbearably long that you would never possibly take it. Tell me if anything needs to be rephrased.</p>\n<p>Second, I am willing to include <em>any question you want</em> in the Super Extra Bonus Questions section, as long as it is not offensive, super-long-and-involved, or really dumb. Please post any questions you want there. Please <em>be specific</em> - not \"Ask something about abortion\" but give the <em>exact question</em> you want me to ask as well as all answer choices.</p>\n<p>Try not to add more than five or so questions per person, unless you're sure yours are <em>really interesting</em>. Please also don't add any questions that aren't very easily sort-able by a computer program like SPSS unless you can commit to sorting the answers yourself.</p>\n<p>Third, please suggest a decent, quick, and at least <em>somewhat</em> accurate Internet IQ test I can stick in a new section, Unreasonably Long Bonus Questions.</p>\n<p>I will probably post the survey to Main and officially open it for responses sometime early next week.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"kJrjorSx3hXa7q7CJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QXShCBvPydkwafekn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 35, "extendedScore": null, "score": 8.5e-05, "legacy": true, "legacyId": "19467", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 481, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HAEPbGaMygJq8L59k"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-19T04:21:56.325Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Lawful Uncertainty", "slug": "seq-rerun-lawful-uncertainty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:22.234Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vpXEcMFDTeydNGxe2/seq-rerun-lawful-uncertainty", "pageUrlRelative": "/posts/vpXEcMFDTeydNGxe2/seq-rerun-lawful-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/vpXEcMFDTeydNGxe2/seq-rerun-lawful-uncertainty", "postedAtFormatted": "Friday, October 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Lawful%20Uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Lawful%20Uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvpXEcMFDTeydNGxe2%2Fseq-rerun-lawful-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Lawful%20Uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvpXEcMFDTeydNGxe2%2Fseq-rerun-lawful-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvpXEcMFDTeydNGxe2%2Fseq-rerun-lawful-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p>Today's post, <a href=\"/lw/vo/lawful_uncertainty/\">Lawful Uncertainty</a> was originally published on 10 November 2008. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Lawful_Uncertainty\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Facing a random scenario, the correct solution is really not to behave randomly. Faced with an irrational universe, throwing away your rationality won't help.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/f0i/seq_rerun_lawful_creativity/\">Lawful Creativity</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vpXEcMFDTeydNGxe2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0127078430073957e-06, "legacy": true, "legacyId": "19469", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["msJA6B9ZjiiZxT6EZ", "6EgEZtLMpn4q7QErc", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-19T12:34:27.117Z", "modifiedAt": null, "url": null, "title": "[LINK] AI-boxing Is News, Somehow", "slug": "link-ai-boxing-is-news-somehow", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:22.512Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MugaSofer", "createdAt": "2012-07-10T12:52:51.220Z", "isAdmin": false, "displayName": "MugaSofer"}, "userId": "Mr6d3wsfsNidTFGHJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bRAxsr8XjA3anuiYL/link-ai-boxing-is-news-somehow", "pageUrlRelative": "/posts/bRAxsr8XjA3anuiYL/link-ai-boxing-is-news-somehow", "linkUrl": "https://www.lesswrong.com/posts/bRAxsr8XjA3anuiYL/link-ai-boxing-is-news-somehow", "postedAtFormatted": "Friday, October 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20AI-boxing%20Is%20News%2C%20Somehow&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20AI-boxing%20Is%20News%2C%20Somehow%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbRAxsr8XjA3anuiYL%2Flink-ai-boxing-is-news-somehow%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20AI-boxing%20Is%20News%2C%20Somehow%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbRAxsr8XjA3anuiYL%2Flink-ai-boxing-is-news-somehow", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbRAxsr8XjA3anuiYL%2Flink-ai-boxing-is-news-somehow", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<p><a title=\"Tech News Daily\" href=\"http://www.technewsdaily.com/5547-humans-build-virtual-prison-dangerous-ai-expert.html\">Tech News Daily</a>&nbsp;have published an article advocating AI-boxing, which namechecks Eliezer's AI-Box Experiment.&nbsp;It seems to claim AI-Boxes are a revolutionary new idea; is covered in pictures of fictional, evil AIs; and worries that a superintelligent AI might&nbsp;develop&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">psychic&nbsp;</span>powers.</p>\n<p>Seems like a good reminder of the state of reporting on such matters.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bRAxsr8XjA3anuiYL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 9, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "19484", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-19T15:41:23.654Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Berlin, Brussels, Chicago, Madison, Melbourne, Washington DC", "slug": "weekly-lw-meetups-austin-berlin-brussels-chicago-madison", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zv2ttqCuqZjEBcCXB/weekly-lw-meetups-austin-berlin-brussels-chicago-madison", "pageUrlRelative": "/posts/zv2ttqCuqZjEBcCXB/weekly-lw-meetups-austin-berlin-brussels-chicago-madison", "linkUrl": "https://www.lesswrong.com/posts/zv2ttqCuqZjEBcCXB/weekly-lw-meetups-austin-berlin-brussels-chicago-madison", "postedAtFormatted": "Friday, October 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Brussels%2C%20Chicago%2C%20Madison%2C%20Melbourne%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Brussels%2C%20Chicago%2C%20Madison%2C%20Melbourne%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzv2ttqCuqZjEBcCXB%2Fweekly-lw-meetups-austin-berlin-brussels-chicago-madison%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Brussels%2C%20Chicago%2C%20Madison%2C%20Melbourne%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzv2ttqCuqZjEBcCXB%2Fweekly-lw-meetups-austin-berlin-brussels-chicago-madison", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzv2ttqCuqZjEBcCXB%2Fweekly-lw-meetups-austin-berlin-brussels-chicago-madison", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 470, "htmlBody": "<p><strong>This summary was posted to LW main on October 12th, and has been moved to discussion. The more recent meetup summary is <a href=\"/lw/f19/weekly_lw_meetups_austin_bratislava_cambridge_ma/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/ed\">(Chicago) Zendo in the West Loop:&nbsp;<span class=\"date\">12 October 2012 06:30PM</span></a></li>\n<li><a href=\"/meetups/em\">Brussels meetup:&nbsp;<span class=\"date\">13 October 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/ek\">Washington DC Show and tell meetup: Economics:&nbsp;<span class=\"date\">14 October 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/ep\">Berlin Meetup:&nbsp;<span class=\"date\">16 October 2012 07:30PM</span></a></li>\n<li><a href=\"/meetups/es\">Bratislava meetup:&nbsp;<span class=\"date\">20 October 2012 05:30PM</span></a></li>\n<li><a href=\"/meetups/e9\"></a><a href=\"/meetups/e9\">Munich Meetup, EDIT: October 28th:&nbsp;<span class=\"date\">28 October 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/el\">Sofia, Bulgaria Meetup:&nbsp;<span class=\"date\">09 December 2012 05:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">13 October 2018 01:30PM</span></a></li>\n<li><a href=\"/meetups/et\">Madison: Prospect Theory:&nbsp;<span class=\"date\">14 October 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/er\">Melbourne social meetup:&nbsp;<span class=\"date\">19 October 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/dg\">Cambridge (MA) third-Sundays Meetup:&nbsp;<span class=\"date\">21 October 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/en\">Winter Solstice Megameetup - NYC:&nbsp;<span class=\"date\">15 December 2012 05:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zv2ttqCuqZjEBcCXB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.013072940973048e-06, "legacy": true, "legacyId": "19325", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["z74ocSFL3Dxpvc8ny", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-19T18:35:10.303Z", "modifiedAt": null, "url": null, "title": "[LINK] blog on cryonics by someone who freezes things in a cell bio lab", "slug": "link-blog-on-cryonics-by-someone-who-freezes-things-in-a", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:39.000Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mwengler", "createdAt": "2010-04-29T14:43:20.667Z", "isAdmin": false, "displayName": "mwengler"}, "userId": "iNn4oZpoPFvwnqbpL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RWTCfg9xjGGQ4epYd/link-blog-on-cryonics-by-someone-who-freezes-things-in-a", "pageUrlRelative": "/posts/RWTCfg9xjGGQ4epYd/link-blog-on-cryonics-by-someone-who-freezes-things-in-a", "linkUrl": "https://www.lesswrong.com/posts/RWTCfg9xjGGQ4epYd/link-blog-on-cryonics-by-someone-who-freezes-things-in-a", "postedAtFormatted": "Friday, October 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20blog%20on%20cryonics%20by%20someone%20who%20freezes%20things%20in%20a%20cell%20bio%20lab&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20blog%20on%20cryonics%20by%20someone%20who%20freezes%20things%20in%20a%20cell%20bio%20lab%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRWTCfg9xjGGQ4epYd%2Flink-blog-on-cryonics-by-someone-who-freezes-things-in-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20blog%20on%20cryonics%20by%20someone%20who%20freezes%20things%20in%20a%20cell%20bio%20lab%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRWTCfg9xjGGQ4epYd%2Flink-blog-on-cryonics-by-someone-who-freezes-things-in-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRWTCfg9xjGGQ4epYd%2Flink-blog-on-cryonics-by-someone-who-freezes-things-in-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<p><a href=\"http://www.strike-the-root.com/51/walker/walker12.html\">http://www.strike-the-root.com/51/walker/walker12.html</a></p>\n<p>Came across it pretty randomly, I found it quite intriguing. &nbsp;Cryonics is \"routine\" for human embryos, not far-fetched for humans at all. &nbsp;Makes the whole thing seem potentially very reasonable (and me someone who hasn't signed up and doesn't plan to). &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RWTCfg9xjGGQ4epYd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 6, "extendedScore": null, "score": 1.0131663558436013e-06, "legacy": true, "legacyId": "19487", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-19T22:34:20.512Z", "modifiedAt": null, "url": null, "title": "A probability question", "slug": "a-probability-question", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:24.790Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sf4DDmw6WKQnWphNG/a-probability-question", "pageUrlRelative": "/posts/sf4DDmw6WKQnWphNG/a-probability-question", "linkUrl": "https://www.lesswrong.com/posts/sf4DDmw6WKQnWphNG/a-probability-question", "postedAtFormatted": "Friday, October 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20probability%20question&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20probability%20question%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsf4DDmw6WKQnWphNG%2Fa-probability-question%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20probability%20question%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsf4DDmw6WKQnWphNG%2Fa-probability-question", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsf4DDmw6WKQnWphNG%2Fa-probability-question", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 295, "htmlBody": "<p>Suppose you have a property Q which certain objects may or may not have.&nbsp; You've seen many of these objects; you know the prior probability P(Q) that an object has this property.</p>\n<p>You have 2 independent measurements of object O, which each assign a probability that Q(O) (O has property Q).&nbsp; Call these two independent probabilities A and B.</p>\n<p>What is P(Q(O) | A, B, P(Q))?</p>\n<p>To put it another way, expert <strong><em>A</em></strong> has opinion O(<strong><em>A</em></strong>) = A, which asserts P(Q(O)) = A = .7, and expert <em><strong>B</strong></em> says P(Q(O)) = B = .8, and the prior P(Q) = .4, so what is P(Q(O))?&nbsp; The correlation between the opinions of the experts is unknown, but probably small.&nbsp; (They aren't human experts.)&nbsp; I face this problem <em>all the time</em> at work.</p>\n<p>You can see that the problem isn't solvable without the prior P(Q), because if the prior P(Q) = .9, then two experts assigning P(Q(O)) &lt; .9 should result in a probability lower than the lowest opinion of those experts.&nbsp; But if P(Q) = .1, then the same estimates by the two experts should result in a probability higher than either of their estimates.&nbsp; But is it solvable or at least well-defined even with the prior?</p>\n<p>The experts both know the prior, so if you just had expert A saying P(Q(O)) = .7, the answer must be .7 .&nbsp; Expert <strong><em>B</em></strong>'s opinion B must revise the probability upwards if B &gt; P(Q), and downwards if B &lt; P(Q).</p>\n<p>When expert <strong><em>A</em></strong> says O(<strong><em>A</em></strong>) = A, she probably means, \"If I consider all the <em>n</em> objects I've seen that looked like this one, <em>n</em>A of them had property Q.\"</p>\n<p>One approach is to add up the bits of information each expert gives, with positive bits for indications that Q(O) and negative bits that not(Q(O)).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sf4DDmw6WKQnWphNG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 1.013294947761672e-06, "legacy": true, "legacyId": "19489", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-20T20:17:21.022Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Fermi Estimation", "slug": "meetup-vancouver-fermi-estimation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:22.959Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/53L6CLZjySdiYhQhR/meetup-vancouver-fermi-estimation", "pageUrlRelative": "/posts/53L6CLZjySdiYhQhR/meetup-vancouver-fermi-estimation", "linkUrl": "https://www.lesswrong.com/posts/53L6CLZjySdiYhQhR/meetup-vancouver-fermi-estimation", "postedAtFormatted": "Saturday, October 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Fermi%20Estimation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Fermi%20Estimation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F53L6CLZjySdiYhQhR%2Fmeetup-vancouver-fermi-estimation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Fermi%20Estimation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F53L6CLZjySdiYhQhR%2Fmeetup-vancouver-fermi-estimation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F53L6CLZjySdiYhQhR%2Fmeetup-vancouver-fermi-estimation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/f1'>Vancouver Fermi Estimation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 October 2012 06:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">885 west georgia vancouver,bc</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi LW, you should come to our meetup.</p>\n\n<p>As usual, we meet downtown in the HSBC building lobby at 18:00, on Thursday. Try to be there before 18:30.</p>\n\n<p>This time, I have something for you. I've determined that <a href=\"https://en.wikipedia.org/wiki/Fermi_problem\" rel=\"nofollow\">Fermi estimation</a> and quantitative intuition in general are super valuable\nrationality skills, so I'm going to attempt to learn us some fermi\nestimation. It will be hopefully a well prepared lesson full of\nexcercises, concepts and optional homework. Can't find the right word\nfor it. Is it a lesson? An excercise? Activity? Drill? We'll see. I\npromise fun and math. Come on out.</p>\n\n<p>Everyone is welcome and encouraged to come. If I can and it makes sense, I'll post some optional prepatory material before the meetup and all my notes after.</p>\n\n<p>There will probably be other exciting things happening too. Good stuff happens when you get a bunch of aspiring rationalists together.</p>\n\n<p>Our mailing list is <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">vancouver-rationalists on google groups</a>.</p>\n\n<p>Be there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/f1'>Vancouver Fermi Estimation</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "53L6CLZjySdiYhQhR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.013996028968028e-06, "legacy": true, "legacyId": "19507", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Fermi_Estimation\">Discussion article for the meetup : <a href=\"/meetups/f1\">Vancouver Fermi Estimation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 October 2012 06:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">885 west georgia vancouver,bc</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi LW, you should come to our meetup.</p>\n\n<p>As usual, we meet downtown in the HSBC building lobby at 18:00, on Thursday. Try to be there before 18:30.</p>\n\n<p>This time, I have something for you. I've determined that <a href=\"https://en.wikipedia.org/wiki/Fermi_problem\" rel=\"nofollow\">Fermi estimation</a> and quantitative intuition in general are super valuable\nrationality skills, so I'm going to attempt to learn us some fermi\nestimation. It will be hopefully a well prepared lesson full of\nexcercises, concepts and optional homework. Can't find the right word\nfor it. Is it a lesson? An excercise? Activity? Drill? We'll see. I\npromise fun and math. Come on out.</p>\n\n<p>Everyone is welcome and encouraged to come. If I can and it makes sense, I'll post some optional prepatory material before the meetup and all my notes after.</p>\n\n<p>There will probably be other exciting things happening too. Good stuff happens when you get a bunch of aspiring rationalists together.</p>\n\n<p>Our mailing list is <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">vancouver-rationalists on google groups</a>.</p>\n\n<p>Be there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Fermi_Estimation1\">Discussion article for the meetup : <a href=\"/meetups/f1\">Vancouver Fermi Estimation</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Fermi Estimation", "anchor": "Discussion_article_for_the_meetup___Vancouver_Fermi_Estimation", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Fermi Estimation", "anchor": "Discussion_article_for_the_meetup___Vancouver_Fermi_Estimation1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-20T22:12:30.227Z", "modifiedAt": "2020-03-03T10:21:23.074Z", "url": null, "title": "Causal Reference", "slug": "causal-reference", "viewCount": null, "lastCommentedAt": "2020-03-03T10:31:22.679Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PXoWk554FZ4Gpfvah/causal-reference", "pageUrlRelative": "/posts/PXoWk554FZ4Gpfvah/causal-reference", "linkUrl": "https://www.lesswrong.com/posts/PXoWk554FZ4Gpfvah/causal-reference", "postedAtFormatted": "Saturday, October 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Causal%20Reference&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACausal%20Reference%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPXoWk554FZ4Gpfvah%2Fcausal-reference%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Causal%20Reference%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPXoWk554FZ4Gpfvah%2Fcausal-reference", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPXoWk554FZ4Gpfvah%2Fcausal-reference", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3504, "htmlBody": "<p><strong>Followup to:</strong>&nbsp; <a href=\"/lw/eva/the_fabric_of_real_things/\">The Fabric of Real Things</a>,&nbsp;<a href=\"/lw/ezu/stuff_that_makes_stuff_happen/\">Stuff That Makes Stuff Happen</a></p>\n<p><em><a href=\"/lw/eva/the_fabric_of_real_things/#7lqi\">Previous meditation</a>:</em>&nbsp;\"Does your rule forbid&nbsp;<a href=\"/lw/p7/zombies_zombies/\">epiphenomenalist theories of consciousness</a>&nbsp;that consciousness is caused by neurons, but doesn't affect those neurons in turn? The classic argument for epiphenomenal consciousness is that we can imagine a universe where people behave exactly the same way, but there's nobody home - no awareness, no consciousness, inside the brain. For all the atoms in this universe to be in the same place - for there to be no detectable difference&nbsp;<em>internally,</em>&nbsp;not just externally - 'consciousness' would have to be something created by the atoms in the brain, but which didn't affect those atoms in turn. It would be an effect of atoms, but not a cause of atoms. Now, I'm not so much interested in whether you think epiphenomenal theories of consciousness are true or false - rather, I want to know if you think they're impossible or meaningless&nbsp;<em>a priori</em>&nbsp;based on your rules.\"</p>\n<p>Is it coherent to imagine a universe in which a real entity can be an effect but not a cause?</p>\n<p>Well... there's a couple of senses in which it seems&nbsp;<em>imaginable.&nbsp;</em>It's important to remember that imagining things yields info primarily about what human brains can imagine. It only provides info about reality to the extent that we think imagination and reality are systematically correlated for some reason.</p>\n<p>That said, I can certainly write a computer program in which there's a tier of objects affecting each other, and a second tier - a lower tier - of epiphenomenal objects which are affected by them, but don't affect them. For example, I could write a program to simulate some balls that bounce off each other, and then some little shadows that follow the balls around.</p>\n<p>But then I only know about the shadows because I'm outside that whole universe, looking in. So&nbsp;<em>my mind</em>&nbsp;is being affected by both the balls and shadows - to observe something is to be affected by it. I know where the shadow is, because the shadow makes pixels be drawn on screen, which make my eye see pixels. If your universe has two tiers of causality - a tier with things that affect each other, and another tier of things that are affected by the first tier without affecting them - then could you know that fact from&nbsp;<em>inside</em>&nbsp;that universe?<a id=\"more\"></a></p>\n<p>Again, this seems easy to&nbsp;<em>imagine</em>&nbsp;as long as objects in the second tier can affect&nbsp;<em>each other.</em>&nbsp;You'd just have to be living in the second tier! We can imagine, for example - this wasn't the way things worked out in&nbsp;<em>our</em>&nbsp;universe, but it might've seemed plausible to the ancient Greeks - that the stars in heaven (and the Sun as a special case) could affect&nbsp;<em>each other</em>&nbsp;and affect Earthly forces, but no Earthly force could affect them:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/f/fc/Heaven.svg\" alt=\"\" width=\"216\" height=\"160\" /></p>\n<p>(Here the X'd-arrow stands for 'cannot affect'.)</p>\n<p>The Sun's light would illuminate Earth, so it would cause plant growth. And sometimes you would see two stars crash into each other and explode, so you'd see they could affect each other. (And affect your brain, which was seeing them.) But the stars and Sun would be made out of a different substance, the 'heavenly material', and throwing any Earthly material at it would not cause it to change state in the slightest. The Earthly material might be burned up, but the Sun would occupy exactly the same position as before. It would affect us, but not be affected by us.</p>\n<p>(To clarify an important point raised in the comments: In standard causal diagrams and in standard physics, no two <em>individual events</em>&nbsp;ever affect <em>each other;</em>&nbsp;there's a causal arrow from the PAST to FUTURE but never an arrow from FUTURE to PAST. What we're talking about here is the sun and stars <em>over time,</em>&nbsp;and the <em>generalization over</em>&nbsp;causal arrows that point from Star-in-Past to Sun-in-Present and Sun-in-Present back to Star-in-Future. The standard formalism dealing with this would be Dynamic Bayesian Networks (DBNs) in which there are repeating nodes and repeating arrows for each successive timeframe: <strong>X</strong><sub>1</sub>, <strong>X</strong><sub>2</sub>, <strong>X</strong><sub>3</sub>, and causal laws F&nbsp;relating <strong>X</strong><sub>i</sub> to <strong>X</strong><sub>i+1</sub>. If the laws of physics did <em>not </em>repeat over time, it would be rather hard to learn about the universe! The Sun <em>repeatedly</em> sends out photons, and they obey the same laws each time they fall on Earth; rather than the F<sub>i</sub> being new transition tables each time, we see a constant F<sub>physics</sub> over and over. By saying that we live in a single-tier universe, we're observing that whenever there are F-arrows, causal-link-types, which (over repeating time) descend from variables-of-type-X to variables-of-type-Y (like present photons affecting future electrons), there are <em>also </em>arrows going back from Ys to Xs (like present electrons affecting future photons). If we <em>weren't </em>generalizing over time, it couldn't possibly make sense to speak of thingies that \"affect each other\" - causal diagrams don't allow directed cycles!)</p>\n<p>A two-tier causal universe seems easy to imagine, even easy to specify as a computer program. If you were arranging a Dynamic Bayes Net at random, would it <em>randomly&nbsp;</em>have everything in a single tier? If you were designing a causal universe at random, wouldn't&nbsp;there randomly be some things that appeared to us as causes but not effects? And yet our own physicists haven't discovered any upper-tier particles which can move us without being movable by us. There might be a hint here at what sort of thingies tend to be real in the first place - that, for whatever reasons, the Real Rules somehow mandate or suggest that all the causal forces in a universe be on the same level, capable of both affecting and being affected by each other.</p>\n<p>Still, we don't actually&nbsp;<em>know</em>&nbsp;the Real Rules are like that; and so it seems premature to assign&nbsp;<em>a priori</em>&nbsp;zero probability to hypotheses with multi-tiered causal universes. Discovering a class of upper-tier affect-only particles seems imaginable<a name=\"note1back\"></a><a href=\"#note1\">[1]</a>&nbsp;- we can imagine which experiences would convince us that they existed. If we're in the Matrix, we can see how to program a Matrix like that. If there's some deeper reason why that's&nbsp;<em>impossible</em>&nbsp;in any base-level reality, we don't know it yet. So we probably want to call that a meaningful hypothesis for now.</p>\n<p>But what about lower-tier particles which can be affected by us, and yet never affect us?</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/6/69/Shadow.svg\" alt=\"\" width=\"264\" height=\"160\" /></p>\n<p>Perhaps there are whole sentient Shadow Civilizations living on my nose hairs which can never&nbsp;<em>affect</em>&nbsp;those nose hairs, but find my nose hairs solid beneath their feet. (The solid Earth affecting them but not being affected, like the Sun's light affecting us in the 'heavenly material' hypothesis.) Perhaps I wreck their world every time I sneeze. It certainly seems imaginable - you could write a computer program simulating physics like that, given sufficient perverseness and computing power...</p>\n<p>And yet the fundamental question of rationality - \"What do you think you know, and how do you think you know it?\" - raises the question:</p>\n<p><em>How could you possibly know about the lower tier, even if it existed?</em></p>\n<p>To observe something is to be affected by it - to have your brain and beliefs take on different states, depending on that thing's state. How can you know about something that doesn't affect your brain?</p>\n<p>In fact there's an even deeper question, \"How could you possibly&nbsp;<em>talk about</em>&nbsp;that lower tier of causality even if it existed?\"</p>\n<p>Let's say you're a Lord of the Matrix. You write a computer program which first computes the physical universe as we know it (or a discrete approximation), and then you add a couple of lower-tier effects as follows:</p>\n<p>First, every time I sneeze, the binary variable YES_SNEEZE will be set to the second of its two possible values.</p>\n<p>Second, every time I sneeze, the binary variable NO_SNEEZE will be set to the first of its two possible values.</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/e/ee/Sneeze.svg\" alt=\"\" width=\"427\" height=\"291\" /></p>\n<p>Now let's say that - somehow - even though I've never caught any hint of the Matrix - I just&nbsp;<em>magically</em>&nbsp;think to myself one day, \"What if there's a variable that watches when I sneeze, and gets set to 1?\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/f/f1/SneezeCause.jpg\" alt=\"\" width=\"300\" height=\"313\" /></p>\n<p>It will be <a href=\"/lw/eqn/the_useful_idea_of_truth/\">all too easy</a> for me to imagine that this belief is meaningful and could be true or false:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/e/ec/FalseSneeze.jpg\" alt=\"\" width=\"304\" height=\"325\" /></p>\n<p>And yet in reality - as&nbsp;<em>you</em>&nbsp;know from outside the matrix - there are&nbsp;<em>two</em>&nbsp;shadow variables that get set when I sneeze. How can I talk about one of them, rather than the other? Why should my thought about '1' refer to their second possible value rather than their first possible value, inside the Matrix computer program? If we tried to establish a truth-value in this situation, to compare my&nbsp;<em>thought</em>&nbsp;to the reality inside the computer program - why compare my thought about SNEEZE_VAR to the variable YES_SNEEZE instead of NO_SNEEZE, or compare my thought '1' to the first possible value instead of the second possible value?</p>\n<p>Under more epistemically healthy circumstances, when you talk about things that are not directly sensory experiences, you will reference a causal model of the universe that you inducted to <em>explain </em>your sensory experiences. Let's say you repeatedly go outside at various times of day, and your eyes and skin directly experience BRIGHT-WARM, BRIGHT-WARM, BRIGHT-WARM, DARK-COOL, DARK-COOL, etc. To explain the patterns in your sensory experiences, you hypothesize a latent variable we'll call 'Sun', with some kind of state which can change between 1, which causes BRIGHTness and WARMness, and 0, which causes DARKness and COOLness. You believe that the state of the 'Sun' variable changes over time, but usually changes less frequently than you go outside.</p>\n<table border=\"0\" cellpadding=\"5\">\n<tbody>\n<tr valign=\"middle\">\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/3/33/SunIdeas.jpg\" alt=\"\" width=\"435\" height=\"350\" /></td>\n<td>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>p(BRIGHT|Sun=1)</td>\n<td>0.9</td>\n</tr>\n<tr>\n<td>p(&not;BRIGHT|Sun=1)</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td>p(BRIGHT|Sun=0)</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td>p(&not;BRIGHT|Sun=0)</td>\n<td>0.9</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n</tbody>\n</table>\n<p>Standing here&nbsp;<em>outside</em>&nbsp;the Matrix, we might be tempted to compare your&nbsp;<em>beliefs</em>&nbsp;about \"Sun = 1\", to the real universe's state regarding the visibility of the sun in the sky (or rather, the Earth's rotational position).</p>\n<p>But even if we compress the sun's visibility down to a binary categorization, how are we to know that your thought \"Sun = 1\" is meant to correspond to the sun being visible in the sky, rather than the sun being occluded by the Earth? Why the first state of the variable, rather than the second state?</p>\n<p>How indeed are we know that this thought \"Sun = 1\" is meant to compare to the sun at all, rather than an anteater in Venezuela?</p>\n<p>Well, because that 'Sun' thingy is supposed to be the&nbsp;<em>cause</em>&nbsp;of BRIGHT and WARM feelings, and if you trace back the cause of those sensory experiences&nbsp;<em>in reality</em>&nbsp;you'll arrive at the sun that the 'Sun' thought allegedly corresponds to. And to distinguish between whether the sun being visible in the sky is meant to correspond to 'Sun'=1 or 'Sun'=0, you check the conditional probabilities for that 'Sun'-state giving rise to BRIGHT - if the actual sun being visible has a 95% chance of causing the BRIGHT sensory feeling, then that true state of the sun is intended to correspond to the hypothetical 'Sun'=1, not 'Sun'=0.</p>\n<p>Or to put it more generally, in cases where we have...</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/3/3e/AbstractConnection.jpg\" alt=\"\" width=\"404\" height=\"299\" /></p>\n<p>...then the correspondence between map and territory can at least&nbsp;<em>in principle</em>&nbsp;be point-wise evaluated by tracing causal links back from sensory experiences to reality, and tracing hypothetical causal links from sensory experiences back to hypothetical reality. We can't directly evaluate that truth-condition inside our own thoughts; but we can perform experiments and be corrected by them.</p>\n<p>Being able to&nbsp;<em>imagine</em>&nbsp;that your thoughts are meaningful and that a correspondence between map and territory is being maintained, is no guarantee that your thoughts are true. On the other hand, if you&nbsp;<em>can't even imagine within your own model&nbsp;</em>how a piece of your map could have a traceable correspondence to the territory, that is a very bad sign for the belief being meaningful, let alone true. Checking to see whether you can&nbsp;<em>imagine</em>&nbsp;a belief being meaningful is a test which will occasionally throw out bad beliefs, though it is no guarantee of a belief being good.</p>\n<hr />\n<p>Okay, but what about the idea that it should be meaningful to talk about whether or not a spaceship continues to exist after it travels over the cosmological horizon? Doesn't this theory of meaningfulness seem to claim that you can only sensibly imagine something that makes a difference to your sensory experiences?</p>\n<p>No. It says that you can only talk about events that your sensory experiences&nbsp;<em>pin down within the causal graph</em>. If you observe enough protons, electrons, neutrons, and so on, you can pin down the physical generalization which says, \"Mass-energy is neither created nor destroyed; and in particular, particles don't vanish into nothingness without a trace.\" It is then an&nbsp;<em>effect</em>&nbsp;of that rule, combined with our previous observation of the ship itself, which tells us that there's a ship that went over the cosmological horizon and now we can't see it any more.</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/a/a7/PhysicsConnection.jpg\" alt=\"\" width=\"355\" height=\"331\" /></p>\n<p>To navigate referentially to the fact that the ship continues to exist over the cosmological horizon, we navigate from our sensory experience&nbsp;<em>up to</em>&nbsp;the laws of physics, by talking about the&nbsp;<em>cause</em>&nbsp;of electrons not blinking out of existence; we also navigate&nbsp;<em>up to</em>&nbsp;the ship's existence by tracing back the cause of our observation of the ship being built. We can't&nbsp;<em>see</em>&nbsp;the future ship over the horizon - but the causal links&nbsp;<em>down</em>&nbsp;<em>from</em>&nbsp;the ship's construction, and from the laws of physics saying it doesn't disappear, are both&nbsp;<em>pinned down by observation</em>&nbsp;- there's no difficulty in figuring out which causes we're talking about, or what effects they have.<a name=\"note2back\"></a><a href=\"#note2\">[2]</a></p>\n<hr />\n<p>All righty-ighty, let's revisit that meditation:</p>\n<p>\"Does your rule forbid&nbsp;<a href=\"/lw/p7/zombies_zombies/\">epiphenomenalist theories of consciousness</a>&nbsp;in which consciousness is caused by neurons, but doesn't affect those neurons in turn? The classic argument for epiphenomenal consciousness is that we can imagine a universe where people behave exactly the same way, but there's nobody home - no awareness, no consciousness, inside the brain. For all the atoms in this universe to be in the same place - for there to be no detectable difference&nbsp;<em>internally,</em>&nbsp;not just externally - 'consciousness' would have to be something created by the atoms in the brain, but which didn't affect those atoms in turn. It would be an effect of atoms, but not a cause of atoms. Now, I'm not so much interested in whether you think epiphenomenal theories of consciousness are true or false - rather, I want to know if you think they're impossible or meaningless&nbsp;<em>a priori</em>&nbsp;based on your rules.\"</p>\n<p>The closest theory to this which definitely&nbsp;<em>does</em>&nbsp;seem coherent - i.e., it's&nbsp;<em>imaginable</em>&nbsp;that it has a pinpointed meaning - would be if there was&nbsp;<em>another</em>&nbsp;little brain living inside my brain, made of shadow particles which could affect each other and be affected by my brain, but not affect my brain in turn. This brain would correctly hypothesize the reasons for its sensory experiences - that there was, from its perspective, an upper tier of particles interacting with each other that it couldn't affect. Upper-tier particles are observable, i.e., can affect lower-tier senses, so it would be possible to correctly induct a simplest explanation for them. And this inner brain would think, \"I can imagine a Zombie Universe in which<em> I</em> am missing, but all the upper-tier particles go on interacting with each other as before.\" If we imagine that the upper-tier brain is just a robotic sort of agent, or a kitten, then the inner brain might justifiably imagine that the Zombie Universe would contain nobody to listen - no lower-tier brains to watch and be aware of events.</p>\n<p>We could write that computer program, given significantly more knowledge and vastly more computing power and zero ethics.</p>\n<p>But this inner brain composed of lower-tier shadow particles&nbsp;<em>cannot</em>&nbsp;write upper-tier philosophy papers about the Zombie universe. If the inner brain thinks, \"I am aware of my own awareness\", the upper-tier lips cannot move and say aloud, \"I am aware of my own awareness\" a few seconds later. That would require causal links from lower particles to upper particles.</p>\n<p>If we try to suppose that the lower tier isn't a complicated brain with an independent reasoning process that can imagine its own hypotheses, but just some shadowy pure experiences that don't affect anything in the upper tier, then clearly the&nbsp;<em>upper-tier brain</em>&nbsp;must be thinking meaningless gibberish when the&nbsp;<em>upper-tier lips</em>&nbsp;say, \"I have a lower tier of shadowy pure experiences which did not affect in any way how I said these words.\" The deliberating upper brain that invents hypotheses for sense data, can only use sense data that affects the upper neurons carrying out the search for hypotheses that can be reported by the lips. Any shadowy pure experiences couldn't be inputs into the hypothesis-inventing cognitive process. So the upper brain would be talking nonsense.</p>\n<p>There's a version of this theory in which the part of our brain that we can report out loud, which invents hypotheses to explain sense data out loud and manifests physically visible papers about Zombie universes,&nbsp;<em>has for no explained reason</em>&nbsp;invented a meaningless theory of shadow experiences which is experienced <em>by the shadow part</em> as a meaningful and correct theory. &nbsp;So that if we look at the \"merely physical\" slice of our universe, philosophy papers about consciousness are meaningless and the physical part of the philosopher is saying things their physical brain couldn't possibly know even if they were true. &nbsp;And yet our inner experience of those philosophy papers is meaningful and true. In a way that couldn't possibly have caused me to physically write the previous sentence, mind you. And yet your experience of that sentence is also true even though, in the upper tier of the universe where that sentence was actually written, it is not only false but meaningless.</p>\n<p>I'm honestly not sure what to say when a conversation gets to that point. Mostly you just want to yell, \"<a href=\"/lw/pn/zombies_the_movie/\">Oh, for the love of Belldandy, will you just give up already?</a>\" or something about the <a href=\"/lw/i9/the_importance_of_saying_oops/\">importance of saying oops</a>.</p>\n<p>(Oh, plus the unexplained correlation violates the <a href=\"/lw/ezu/stuff_that_makes_stuff_happen/\">Markov condition for causal models</a>.)</p>\n<p>Maybe my reply would be something along the lines of, \"Okay... look... I've given my account of a single-tier universe in which agents can invent meaningful explanations for sense data, and when they build accurate maps of reality there's a known reason for the correspondence... if you want to claim that a&nbsp;<em>different</em>&nbsp;kind of meaningfulness can hold within a&nbsp;<em>different</em>&nbsp;kind of agent divided into upper and lower tiers, it's up to&nbsp;<em>you</em>&nbsp;to explain what parts of the agent are doing which kinds of hypothesizing and how those hypotheses end up being meaningful and what causally explains their miraculous accuracy so that this all makes <em>sense</em>.\"</p>\n<p>But frankly, I think people would be wiser to just&nbsp;<em>give up&nbsp;</em>trying to write sensible philosophy papers about lower causal tiers of the universe that don't affect the philosophy papers in any way.</p>\n<hr />\n<p><strong><a href=\"#7nyc\">Meditation</a>:&nbsp;</strong>If we can only meaningfully talk about parts of the universe that can be pinned down inside the causal graph, where do we find the fact that 2 + 2 = 4? Or did I just make a meaningless noise, there? Or if you claim that \"2 + 2 = 4\"&nbsp;<em>isn't</em>&nbsp;meaningful or true, then what alternate property does the sentence \"2 + 2 = 4\" have which makes it so much more useful than the sentence \"2 + 2 = 3\"?</p>\n<hr />\n<p><strong><a href=\"#7nyf\">Mainstream status.</a></strong></p>\n<hr />\n<p><a href=\"#note1back\"></a><a name=\"note1\"></a>&nbsp;<a href=\"#note1back\">[1]</a>&nbsp;Well, it seems imaginable so long as you toss most of quantum physics out the window and put us back in a classical universe. For particles to not be affected by us, they'd need their own configuration space such that \"<a href=\"/lw/pf/distinct_configurations/\">which configurations are identical</a>\"&nbsp;was determined by looking only at those particles, and not looking at any lower-tier particles entangled with them. If you&nbsp;<em>don't</em>&nbsp;want to toss QM out the window, it's actually pretty hard to imagine what an upper-tier particle would look like.</p>\n<p><a href=\"#note2back\"></a><a name=\"note2\"></a>&nbsp;<a href=\"#note2back\">[2]</a>&nbsp;This diagram treats the laws of physics as being just another node, which is a convenient shorthand, but probably not a good way to draw the graph. The laws of physics really correspond to the causal arrows F<sub>i</sub>, not the causal nodes X<sub>i</sub>. If you had the laws themselves - the function from past to future - be an X<sub>i</sub>&nbsp;of variable state, then you'd need meta-physics to describe the F<sub>physics</sub>&nbsp;arrows for how the physics-stuff X<sub>physics</sub>&nbsp;could affect us, followed promptly by a need for meta-meta-physics et cetera. If the laws of physics&nbsp;<em>were</em>&nbsp;a kind of causal stuff, they'd be an upper tier of causality - we can't appear to affect the laws of physics, but if you call them causes, they can affect us. In Matrix terms, this would correspond to our universe running on a computer that stored the laws of physics in one area of RAM and the state of the universe in another area of RAM, the first area would be an upper causal tier and the second area would be a lower causal tier. But the infinite regress from treating the laws of determination as causal stuff, makes me suspicious that it might be an error to treat the laws of physics as \"stuff that makes stuff happen and happens because of other stuff\". When we trust that the ship doesn't disappear when it goes over the horizon, we may not be navigating to a physics-node in the graph, so much as we're navigating to a single F<sub>physics</sub>&nbsp;that appears in many different places inside the graph, and whose previously unknown function we have inferred. But this is an unimportant technical quibble on Tuesdays, Thursdays, Saturdays, and Sundays. It is only an incredibly deep question about the nature of reality on Mondays, Wednesdays, and Fridays, i.e., less than half the time.</p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/f43/proofs_implications_and_models/\">Proofs, Implications, and Models</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/ezu/stuff_that_makes_stuff_happen/\">Stuff That Makes Stuff Happen</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"cq69M9ceLNA35ShTR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PXoWk554FZ4Gpfvah", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 56, "extendedScore": null, "score": 0.000122, "legacy": true, "legacyId": "19506", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "causal-universes", "canonicalPrevPostSlug": "stuff-that-makes-stuff-happen", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 56, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 246, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["h6fzC6wFYFxxKDm8u", "NhQju3htS9W6p6wE6", "fdEWWr8St59bXLbQr", "XqvnWFtRD2keJdwjX", "fsDz6HieZJBu54Yes", "wCqfCLs8z5Qw4GbKS", "KbeHkLNY5ETJ3TN3W", "Z2CuyKtkCmWGQtAEh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-10-20T22:12:30.227Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-21T12:23:52.769Z", "modifiedAt": null, "url": null, "title": "Decision theory question", "slug": "decision-theory-question", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:22.774Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bryjnar", "createdAt": "2011-09-24T13:44:35.533Z", "isAdmin": false, "displayName": "bryjnar"}, "userId": "yoLWS98ppXYZmC5qE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ELW7u38CG2wJ9GARB/decision-theory-question", "pageUrlRelative": "/posts/ELW7u38CG2wJ9GARB/decision-theory-question", "linkUrl": "https://www.lesswrong.com/posts/ELW7u38CG2wJ9GARB/decision-theory-question", "postedAtFormatted": "Sunday, October 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Decision%20theory%20question&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecision%20theory%20question%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FELW7u38CG2wJ9GARB%2Fdecision-theory-question%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Decision%20theory%20question%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FELW7u38CG2wJ9GARB%2Fdecision-theory-question", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FELW7u38CG2wJ9GARB%2Fdecision-theory-question", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 130, "htmlBody": "<p>I'm sure I remember reading somewhere that an informal slogan for (some brand of decision theory) was:</p>\n<p>\"Act in accordance with the precommittment you wish you had made.\"</p>\n<p>i.e. when faced with Newcomb's problem, you would wish you had precommitted to one-box, and so you should one-box. This is entirely predictable, so you get the money.</p>\n<p>However, I couldn't find where I saw this, and I can't remember which decision theory it was meant to exemplify - and I don't actually understand the maths of the various competitors enough to figure out which it is.</p>\n<p>Could someone who knows the area better point me in the right direction? In general, the idea of making a kind of \"fully general precommitment\" seems like an intuitive way of explaining how you can improve on, say, CDT.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ELW7u38CG2wJ9GARB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 1.0145166229031497e-06, "legacy": true, "legacyId": "19509", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-21T19:24:42.074Z", "modifiedAt": null, "url": null, "title": "[Link] Anti-Groupism ", "slug": "link-anti-groupism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:23.565Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KuJB7y7yunm42DCXr/link-anti-groupism", "pageUrlRelative": "/posts/KuJB7y7yunm42DCXr/link-anti-groupism", "linkUrl": "https://www.lesswrong.com/posts/KuJB7y7yunm42DCXr/link-anti-groupism", "postedAtFormatted": "Sunday, October 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Anti-Groupism%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Anti-Groupism%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKuJB7y7yunm42DCXr%2Flink-anti-groupism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Anti-Groupism%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKuJB7y7yunm42DCXr%2Flink-anti-groupism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKuJB7y7yunm42DCXr%2Flink-anti-groupism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 316, "htmlBody": "<p class=\"post-title entry-title\">A short argument from an interesting blog.</p>\n<blockquote>\n<h3 class=\"post-title entry-title\"><a href=\"http://aretae.blogspot.no/2012/10/anti-groupism.html\">Anti-Groupism</a></h3>\n<div class=\"post-header\"></div>\n<p>Basic Aretaevian talking points:<br /></p>\n<ol>\n<li>Human brains are effectively populated by rabbits. &nbsp;Your conscious  mind is like a very small person attempting to ride a large herd of  rabbits, which aren't all going the same direction. &nbsp;Your job is to  pretend to be in control, and make shit up to explain where the rabbits  went, and what you did.</li>\n<li>Humans bunny brains are optimized for social activity, not  intellectual activity. &nbsp;If your brain thinks principles first, instead  of groups first, it's broken, and not just a little bit.</li>\n<li>Of course, this means that anyone thinking group first is almost  completely full of crap regarding their reasoning process. &nbsp;They're  (99.86% certainty) making shit up that makes the group look good, and  the actual rational value of the statement is near zero. &nbsp;The nominal  process \"A-&gt;B-&gt;C\" is actually C, now let's backfill with B and A. &nbsp;</li>\n<li>Therefore I'm almost only interested in listening to folks who are  group-free. &nbsp;If your brain is broken in the kind of way that prohibits  group-attachment...then you're far far more likely to be thinking  independently, and shifting perspectives.</li>\n<li>Aside: &nbsp;FWIW, this is the core (unsolvable?) problem that inhabits  rationalist groups. &nbsp;There is a deep and abiding conflict between  groupism and thinking. &nbsp;The Randians have encountered this most loudly,  but it's also there in the libertarians, the extropians, the David  Deutch-led popperian rationalists, and the LessWrongers.</li>\n</ol>\n<div>New discovery, shouldn't have been as surprising as it was. &nbsp; When  looking for folks who are group-avoidant, I seem to have phenomenally  good luck finding great people when talking with Gays from non-leftist  areas (rural Texas, Tennessee, downstate Illinois). &nbsp;Because they  don't/can't fit in with their local culture, and often can't  conveniently exit, they become interesting people. &nbsp; It's a surprisingly  good metric.</div>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KuJB7y7yunm42DCXr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 34, "extendedScore": null, "score": 8.5e-05, "legacy": true, "legacyId": "19510", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-21T23:46:09.506Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Ethics meetup", "slug": "meetup-washington-dc-ethics-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3eZzMvZo2eFJw3Br4/meetup-washington-dc-ethics-meetup", "pageUrlRelative": "/posts/3eZzMvZo2eFJw3Br4/meetup-washington-dc-ethics-meetup", "linkUrl": "https://www.lesswrong.com/posts/3eZzMvZo2eFJw3Br4/meetup-washington-dc-ethics-meetup", "postedAtFormatted": "Sunday, October 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Ethics%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Ethics%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3eZzMvZo2eFJw3Br4%2Fmeetup-washington-dc-ethics-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Ethics%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3eZzMvZo2eFJw3Br4%2Fmeetup-washington-dc-ethics-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3eZzMvZo2eFJw3Br4%2Fmeetup-washington-dc-ethics-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/f2'>Washington DC Ethics meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 October 2012 03:00:49PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery Plaza, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be discussing the recent post about the most important open ethics problems:</p>\n\n<p><a href=\"http://80000hours.org/blog/99-the-most-important-unsolved-problems-in-ethics\" rel=\"nofollow\">http://80000hours.org/blog/99-the-most-important-unsolved-problems-in-ethics</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/f2'>Washington DC Ethics meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3eZzMvZo2eFJw3Br4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0148843982353659e-06, "legacy": true, "legacyId": "19511", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Ethics_meetup\">Discussion article for the meetup : <a href=\"/meetups/f2\">Washington DC Ethics meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 October 2012 03:00:49PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery Plaza, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be discussing the recent post about the most important open ethics problems:</p>\n\n<p><a href=\"http://80000hours.org/blog/99-the-most-important-unsolved-problems-in-ethics\" rel=\"nofollow\">http://80000hours.org/blog/99-the-most-important-unsolved-problems-in-ethics</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Ethics_meetup1\">Discussion article for the meetup : <a href=\"/meetups/f2\">Washington DC Ethics meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Ethics meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Ethics_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Ethics meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Ethics_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-22T05:02:36.354Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Worse Than Random", "slug": "seq-rerun-worse-than-random", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:34.241Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x9om84oGpT9aNCa2n/seq-rerun-worse-than-random", "pageUrlRelative": "/posts/x9om84oGpT9aNCa2n/seq-rerun-worse-than-random", "linkUrl": "https://www.lesswrong.com/posts/x9om84oGpT9aNCa2n/seq-rerun-worse-than-random", "postedAtFormatted": "Monday, October 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Worse%20Than%20Random&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Worse%20Than%20Random%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx9om84oGpT9aNCa2n%2Fseq-rerun-worse-than-random%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Worse%20Than%20Random%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx9om84oGpT9aNCa2n%2Fseq-rerun-worse-than-random", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx9om84oGpT9aNCa2n%2Fseq-rerun-worse-than-random", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<p>Today's post, <a href=\"/lw/vp/worse_than_random/\">Worse Than Random</a> was originally published on 11 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Worse_Than_Random\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If a system does better when randomness is added into its processing, then it must somehow have been performing worse than random. And if you can recognize that this is the case, you ought to be able to generate a non-randomized system.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/f0t/seq_rerun_lawful_uncertainty/\">Lawful Uncertainty</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x9om84oGpT9aNCa2n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0150550557621788e-06, "legacy": true, "legacyId": "19526", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GYuKqAL95eaWTDje5", "vpXEcMFDTeydNGxe2", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-22T05:41:38.062Z", "modifiedAt": null, "url": null, "title": "Meetup : Berlin Meetup", "slug": "meetup-berlin-meetup-4", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ooCwQekpvAcYMw4h3/meetup-berlin-meetup-4", "pageUrlRelative": "/posts/ooCwQekpvAcYMw4h3/meetup-berlin-meetup-4", "linkUrl": "https://www.lesswrong.com/posts/ooCwQekpvAcYMw4h3/meetup-berlin-meetup-4", "postedAtFormatted": "Monday, October 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berlin%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berlin%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FooCwQekpvAcYMw4h3%2Fmeetup-berlin-meetup-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berlin%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FooCwQekpvAcYMw4h3%2Fmeetup-berlin-meetup-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FooCwQekpvAcYMw4h3%2Fmeetup-berlin-meetup-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/f3'>Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 November 2012 08:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Ming Dynastie, Br\u00fcckenstra\u00dfe 6, 10179 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Warning: The meetup will start 19:30 local time, the time shown in the description already adjusts for the change in daylight-saving time.</p>\n\n<p>We're meeting in a new location with less noise and round tables! There'll be a sign saying 'Less Wrong' on our table.</p>\n\n<p>Plans:</p>\n\n<ul>\n<li>Alexander: Discuss:\n<a href=\"http://lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/\" rel=\"nofollow\">http://lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/</a></li>\n<li>Christian: Discuss: <a href=\"http://lesswrong.com/lw/eqn/the_useful_idea_of_truth/\" rel=\"nofollow\">http://lesswrong.com/lw/eqn/the_useful_idea_of_truth/</a></li>\n<li>Christian: Commitments</li>\n<li>Christian: Self-improvement open space</li>\n</ul>\n\n<p>Please read the articles before the meetup. Alexander and me will\nprovide brief summaries so you can participate even if you haven't read\nthem - but it'll be more interesting if you have.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/f3'>Berlin Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ooCwQekpvAcYMw4h3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.015076107025807e-06, "legacy": true, "legacyId": "19528", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup\">Discussion article for the meetup : <a href=\"/meetups/f3\">Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 November 2012 08:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Ming Dynastie, Br\u00fcckenstra\u00dfe 6, 10179 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Warning: The meetup will start 19:30 local time, the time shown in the description already adjusts for the change in daylight-saving time.</p>\n\n<p>We're meeting in a new location with less noise and round tables! There'll be a sign saying 'Less Wrong' on our table.</p>\n\n<p>Plans:</p>\n\n<ul>\n<li>Alexander: Discuss:\n<a href=\"http://lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/\" rel=\"nofollow\">http://lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/</a></li>\n<li>Christian: Discuss: <a href=\"http://lesswrong.com/lw/eqn/the_useful_idea_of_truth/\" rel=\"nofollow\">http://lesswrong.com/lw/eqn/the_useful_idea_of_truth/</a></li>\n<li>Christian: Commitments</li>\n<li>Christian: Self-improvement open space</li>\n</ul>\n\n<p>Please read the articles before the meetup. Alexander and me will\nprovide brief summaries so you can participate even if you haven't read\nthem - but it'll be more interesting if you have.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/f3\">Berlin Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AdYdLP2sRqPMoe8fb", "XqvnWFtRD2keJdwjX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-22T23:29:26.336Z", "modifiedAt": null, "url": null, "title": "Meetup : Toronto THINK", "slug": "meetup-toronto-think-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Giles", "createdAt": "2011-02-11T02:30:16.999Z", "isAdmin": false, "displayName": "Giles"}, "userId": "H347ba3KZMP8XoDt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o6KTwTFjzQuq4f9LS/meetup-toronto-think-0", "pageUrlRelative": "/posts/o6KTwTFjzQuq4f9LS/meetup-toronto-think-0", "linkUrl": "https://www.lesswrong.com/posts/o6KTwTFjzQuq4f9LS/meetup-toronto-think-0", "postedAtFormatted": "Monday, October 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Toronto%20THINK&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Toronto%20THINK%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo6KTwTFjzQuq4f9LS%2Fmeetup-toronto-think-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Toronto%20THINK%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo6KTwTFjzQuq4f9LS%2Fmeetup-toronto-think-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo6KTwTFjzQuq4f9LS%2Fmeetup-toronto-think-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/f4'>Toronto THINK</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 October 2012 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">7 Hart House Circle, Toronto</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In the Hart House Reading Room (the room across from the reception desk with the purple walls)</p>\n\n<p>How To Save A Life</p>\n\n<p>It's time to discuss one of the most basic ways to make the world better - saving somebody's life. How would you go about that? How much of an inconvenience do you think it would be? In this meeting we'll discuss these issues. We might even come up with a plan we can put into practice.</p>\n\n<p>Note: <a href=\"http://www.thehighimpactnetwork.org/\" rel=\"nofollow\">THINK</a> is not directly LW-affiliated but I've been told to post our meetups here anyhow :-)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/f4'>Toronto THINK</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o6KTwTFjzQuq4f9LS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0156523621577826e-06, "legacy": true, "legacyId": "19531", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Toronto_THINK\">Discussion article for the meetup : <a href=\"/meetups/f4\">Toronto THINK</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 October 2012 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">7 Hart House Circle, Toronto</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In the Hart House Reading Room (the room across from the reception desk with the purple walls)</p>\n\n<p>How To Save A Life</p>\n\n<p>It's time to discuss one of the most basic ways to make the world better - saving somebody's life. How would you go about that? How much of an inconvenience do you think it would be? In this meeting we'll discuss these issues. We might even come up with a plan we can put into practice.</p>\n\n<p>Note: <a href=\"http://www.thehighimpactnetwork.org/\" rel=\"nofollow\">THINK</a> is not directly LW-affiliated but I've been told to post our meetups here anyhow :-)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Toronto_THINK1\">Discussion article for the meetup : <a href=\"/meetups/f4\">Toronto THINK</a></h2>", "sections": [{"title": "Discussion article for the meetup : Toronto THINK", "anchor": "Discussion_article_for_the_meetup___Toronto_THINK", "level": 1}, {"title": "Discussion article for the meetup : Toronto THINK", "anchor": "Discussion_article_for_the_meetup___Toronto_THINK1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-23T00:03:46.658Z", "modifiedAt": null, "url": null, "title": "Argument by lexical overloading, or, Don't cut your wants with shoulds", "slug": "argument-by-lexical-overloading-or-don-t-cut-your-wants-with", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:24.715Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aCqXPyZHamxaaEpme/argument-by-lexical-overloading-or-don-t-cut-your-wants-with", "pageUrlRelative": "/posts/aCqXPyZHamxaaEpme/argument-by-lexical-overloading-or-don-t-cut-your-wants-with", "linkUrl": "https://www.lesswrong.com/posts/aCqXPyZHamxaaEpme/argument-by-lexical-overloading-or-don-t-cut-your-wants-with", "postedAtFormatted": "Tuesday, October 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Argument%20by%20lexical%20overloading%2C%20or%2C%20Don't%20cut%20your%20wants%20with%20shoulds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArgument%20by%20lexical%20overloading%2C%20or%2C%20Don't%20cut%20your%20wants%20with%20shoulds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaCqXPyZHamxaaEpme%2Fargument-by-lexical-overloading-or-don-t-cut-your-wants-with%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Argument%20by%20lexical%20overloading%2C%20or%2C%20Don't%20cut%20your%20wants%20with%20shoulds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaCqXPyZHamxaaEpme%2Fargument-by-lexical-overloading-or-don-t-cut-your-wants-with", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaCqXPyZHamxaaEpme%2Fargument-by-lexical-overloading-or-don-t-cut-your-wants-with", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 447, "htmlBody": "<p>I used the word \"cut\" in the title to mean the Prolog operator \"cut\", an operator which halts the evaluation of a statement in predicate logic.</p>\n<p>Fiction writers often complain, \"I keep procrastinating from writing,\" and, \"Nobody reads what I write.\"&nbsp; These complaints are usually the result of <em>shoulds</em> stopping them from thinking about their <em>wants</em>.</p>\n<p>I've never heard anyone say, \"I keep putting off playing baseball,\" or, \"I keep putting off eating ice cream.\"&nbsp; People who keep putting off writing don't want to <em>write</em>, they want to <em>have written</em>.&nbsp; If you have to try to write more often than you have to try not to write, you've probably told yourself that you <em>should</em> write in order to attain some reward.&nbsp; There's nothing wrong with that, but writers who complain that they keep putting off writing are often writing things with little potential payoff, like fan-fiction.&nbsp; They don't stop and think how to improve the payoff that they <em>want</em>, because they get stuck on the <em>should</em> that they've cached in their heads.</p>\n<p>I've repeatedly tried to help writers who complain that not enough people read what they write.&nbsp; I explain that if you want to be read by a lot of people, you need to write something that a lot of people want to read.&nbsp; This seems obvious to me, but I'm always immediately attacked by indignant writers saying that they <em>want</em> to write great fiction, and that one <em>should</em> write only to please oneself in order to write great fiction.&nbsp; Sometimes these are the same people who complained that they <em>want</em> more people to read what they write.</p>\n<p>Why does their desire to write great fiction take complete precedence over their desire<em> </em>to have readers?&nbsp; Because they have cached that desire as a <em>should</em>.&nbsp; (They haven't cached a <em>should</em> for their goal to get more readers because that goal arose much later, after they had already learned to write well and discovered, to their horror, that just writing well doesn't bring you readers.)&nbsp; For a moral agent, <em>shoulds</em> trump <em>wants</em>, by definition.</p>\n<p>I've <a href=\"/lw/7ko/morality_is_not_about_willpower/\">explained before</a> that I don't think there is any deep difference between <em>wants</em> and <em>shoulds</em>.&nbsp; The English language doesn't pretend there is; we say \"I should do X\" both to mean \"I have a moral obligation to do X\" and \"I need to do X to satisfy my goals.\"&nbsp; The problem is that most people think there is a difference, and that <em>shoulds</em> are more important.&nbsp; They have a <em>want</em>, they figure out what they need to do to satisfy it, they think aloud to themselves that they <em>should</em> do it, and boom, they have lexically convinced themselves that they have a moral obligation to do it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aCqXPyZHamxaaEpme", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "19530", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SBM5r93HSX2YDNERk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-23T03:19:03.676Z", "modifiedAt": null, "url": null, "title": "Popular media coverage of Singularity Summit -the Verge [link]", "slug": "popular-media-coverage-of-singularity-summit-the-verge-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:31.156Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6v4fL5Hf8XjZkHpZT/popular-media-coverage-of-singularity-summit-the-verge-link", "pageUrlRelative": "/posts/6v4fL5Hf8XjZkHpZT/popular-media-coverage-of-singularity-summit-the-verge-link", "linkUrl": "https://www.lesswrong.com/posts/6v4fL5Hf8XjZkHpZT/popular-media-coverage-of-singularity-summit-the-verge-link", "postedAtFormatted": "Tuesday, October 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Popular%20media%20coverage%20of%20Singularity%20Summit%20-the%20Verge%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APopular%20media%20coverage%20of%20Singularity%20Summit%20-the%20Verge%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6v4fL5Hf8XjZkHpZT%2Fpopular-media-coverage-of-singularity-summit-the-verge-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Popular%20media%20coverage%20of%20Singularity%20Summit%20-the%20Verge%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6v4fL5Hf8XjZkHpZT%2Fpopular-media-coverage-of-singularity-summit-the-verge-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6v4fL5Hf8XjZkHpZT%2Fpopular-media-coverage-of-singularity-summit-the-verge-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><span style=\"font-family: '.HelveticaNeueUI'; font-size: 15px; line-height: 19px; white-space: nowrap; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">http://www.theverge.com/2012/10/22/3535518/singularity-rapture-of-the-nerds-gods-end-human-race</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6v4fL5Hf8XjZkHpZT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 1.0157763562747011e-06, "legacy": true, "legacyId": "19549", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-23T04:47:07.919Z", "modifiedAt": null, "url": null, "title": "Value Loading", "slug": "value-loading", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:23.670Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ryjm", "createdAt": "2012-02-09T19:36:44.546Z", "isAdmin": false, "displayName": "ryjm"}, "userId": "vvCqsPSKbzdYhqEeT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Z8WRsxYjmrxNGyPPk/value-loading", "pageUrlRelative": "/posts/Z8WRsxYjmrxNGyPPk/value-loading", "linkUrl": "https://www.lesswrong.com/posts/Z8WRsxYjmrxNGyPPk/value-loading", "postedAtFormatted": "Tuesday, October 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Value%20Loading&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AValue%20Loading%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ8WRsxYjmrxNGyPPk%2Fvalue-loading%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Value%20Loading%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ8WRsxYjmrxNGyPPk%2Fvalue-loading", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ8WRsxYjmrxNGyPPk%2Fvalue-loading", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 480, "htmlBody": "<p><em>This article was originally on the FHI wiki and is being reposted to LW Discussion with permission. All content in this article is credited to Daniel Dewey.</em></p>\n<p>In value loading, the agent will pick the action:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\arg\\max_{a \\in A} \\sum_{w \\in W} p(w|e , a) \\sum_{u \\in U} u(w)p(C(u)|w)\" alt=\"\" align=\"bottom\" /></p>\n<p>Here A is the set of actions the agent can take, e is the evidence the agent has already seen, W is the set of possible worlds, and U is the set of utility functions the agent is considering.</p>\n<p>The parameter C(u) is some measure of the 'correctness' of the utility u, so the term p(C(u)|w) is the probability of u being correct, given that the agent is in world w. A simple example is of an AI that completely trusts the programmers; so if u is some utility function that claims that giving cake is better than giving death, and w<sub>1</sub> is a world where the programmers have said \"cake is better than death\" while w<sub>2</sub> is a world where they have said the opposite, then p(C(u)|w<sub>1</sub>) = 1 and p(C(u) | w<sub>2</sub>) = 0.</p>\n<p>There are several challenging things in this formula:</p>\n<p>W : How to define/represent the class of all worlds under consideration</p>\n<p>U : How to represent the class of all utility functions over such worlds</p>\n<p>C : What do we state about the utility function: that it is true? believed by humans?</p>\n<p>p(C(u)|w) : How to define this probability</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\sum_{u\\in U} u(w)\" alt=\"\" align=\"bottom\" /> : How to sum up utility functions (a moral uncertainty problem)</p>\n<p>In contrast:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\sum_{w\\in W} p(w|e , a)\" alt=\"\" align=\"bottom\" /></p>\n<p>is mostly the classic AI problem. It is hard to predict what the world is like from evidence, but this is a well known and studied problem and not unique to the present research. There is a trick to it here in that the nature of w includes the future actions of the agent which will depend upon how good future states look to it, but this recursive definition eventually bottoms out like a game of chess (where what happens when I make a move depends on what moves I make after that). It may cause an additional exponential explosion in calculating out the formula though, so the agent may need to make probabilistic guesses as to its own future behaviour to actually calculate an action.</p>\n<p>This value loading equation is not subject to the classical Cake or Death problem, but is vulnerable to the more advanced version of the problem, if the agent is able to change the expected future value of p(C(u)) through its actions.</p>\n<p><strong>Daniel Dewey's Paper</strong></p>\n<p>The above idea was partially inspired by a draft of <a href=\"http://www.danieldewey.net/learning-what-to-value.html)\">Learning What to Value</a>, a paper by Daniel Dewey. He restricted attention to streams of interactions, and his equation, in a simplified form, is:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\arg\\max_{a \\in A} \\sum_{s \\in S} p(s|e , a) \\sum_{u \\in U} u(s)p(u|s)\" alt=\"\" align=\"bottom\" /></p>\n<p>where S is the set of all possible streams of all past and future observations and actions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Z8WRsxYjmrxNGyPPk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 5, "extendedScore": null, "score": 1.015823920918542e-06, "legacy": true, "legacyId": "19550", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-23T05:27:59.853Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Cognitive Bias Round-Robin", "slug": "meetup-west-la-meetup-cognitive-bias-round-robin", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fTHp58e5CFncv5F8A/meetup-west-la-meetup-cognitive-bias-round-robin", "pageUrlRelative": "/posts/fTHp58e5CFncv5F8A/meetup-west-la-meetup-cognitive-bias-round-robin", "linkUrl": "https://www.lesswrong.com/posts/fTHp58e5CFncv5F8A/meetup-west-la-meetup-cognitive-bias-round-robin", "postedAtFormatted": "Tuesday, October 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Cognitive%20Bias%20Round-Robin&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Cognitive%20Bias%20Round-Robin%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfTHp58e5CFncv5F8A%2Fmeetup-west-la-meetup-cognitive-bias-round-robin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Cognitive%20Bias%20Round-Robin%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfTHp58e5CFncv5F8A%2Fmeetup-west-la-meetup-cognitive-bias-round-robin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfTHp58e5CFncv5F8A%2Fmeetup-west-la-meetup-cognitive-bias-round-robin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 224, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/f5'>West LA Meetup - Cognitive Bias Round-Robin</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 October 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, October 24th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week, we will do our tried and true Cognitive Bias Round-Robin. Simply pick some cognitive bias from the wikipedia <a href=\"http://en.wikipedia.org/wiki/List_of_biases_in_judgment_and_decision_making\" rel=\"nofollow\">list of biases</a>, or a <a href=\"http://en.wikipedia.org/wiki/Logical_fallacy\" rel=\"nofollow\">logical fallacy</a>, <a href=\"http://en.wikipedia.org/wiki/Cognitive_distortion\" rel=\"nofollow\">cognitive distortion</a>, or some other failure mode of thinking (perhaps one that's given recognition on this site). On Wednesday, anyone who did this can explain the bias or error, and we can then discuss where it actually comes up, when it matters, and what do do about it.</p>\n\n<p>There will be general discussion too, and there are lots of interesting <a href=\"http://lesswrong.com/recentposts/\">recent posts</a> (also check out LW's sister site, <a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a> ). But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/f5'>West LA Meetup - Cognitive Bias Round-Robin</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fTHp58e5CFncv5F8A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0158459926493677e-06, "legacy": true, "legacyId": "19551", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Cognitive_Bias_Round_Robin\">Discussion article for the meetup : <a href=\"/meetups/f5\">West LA Meetup - Cognitive Bias Round-Robin</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 October 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, October 24th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week, we will do our tried and true Cognitive Bias Round-Robin. Simply pick some cognitive bias from the wikipedia <a href=\"http://en.wikipedia.org/wiki/List_of_biases_in_judgment_and_decision_making\" rel=\"nofollow\">list of biases</a>, or a <a href=\"http://en.wikipedia.org/wiki/Logical_fallacy\" rel=\"nofollow\">logical fallacy</a>, <a href=\"http://en.wikipedia.org/wiki/Cognitive_distortion\" rel=\"nofollow\">cognitive distortion</a>, or some other failure mode of thinking (perhaps one that's given recognition on this site). On Wednesday, anyone who did this can explain the bias or error, and we can then discuss where it actually comes up, when it matters, and what do do about it.</p>\n\n<p>There will be general discussion too, and there are lots of interesting <a href=\"http://lesswrong.com/recentposts/\">recent posts</a> (also check out LW's sister site, <a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a> ). But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Cognitive_Bias_Round_Robin1\">Discussion article for the meetup : <a href=\"/meetups/f5\">West LA Meetup - Cognitive Bias Round-Robin</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Cognitive Bias Round-Robin", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Cognitive_Bias_Round_Robin", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Cognitive Bias Round-Robin", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Cognitive_Bias_Round_Robin1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-23T15:58:03.641Z", "modifiedAt": null, "url": null, "title": "Naive TDT, Bayes nets, and counterfactual mugging", "slug": "naive-tdt-bayes-nets-and-counterfactual-mugging", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:24.164Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DM87z7xQxPNMY4evK/naive-tdt-bayes-nets-and-counterfactual-mugging", "pageUrlRelative": "/posts/DM87z7xQxPNMY4evK/naive-tdt-bayes-nets-and-counterfactual-mugging", "linkUrl": "https://www.lesswrong.com/posts/DM87z7xQxPNMY4evK/naive-tdt-bayes-nets-and-counterfactual-mugging", "postedAtFormatted": "Tuesday, October 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Naive%20TDT%2C%20Bayes%20nets%2C%20and%20counterfactual%20mugging&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANaive%20TDT%2C%20Bayes%20nets%2C%20and%20counterfactual%20mugging%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDM87z7xQxPNMY4evK%2Fnaive-tdt-bayes-nets-and-counterfactual-mugging%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Naive%20TDT%2C%20Bayes%20nets%2C%20and%20counterfactual%20mugging%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDM87z7xQxPNMY4evK%2Fnaive-tdt-bayes-nets-and-counterfactual-mugging", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDM87z7xQxPNMY4evK%2Fnaive-tdt-bayes-nets-and-counterfactual-mugging", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1047, "htmlBody": "<p>I set out to understand precisely why naive TDT (possibly) fails the <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">counterfactual mugging</a> problem. While doing this I ended up drawing a lot of <a href=\"http://en.wikipedia.org/wiki/Bayes_net\">Bayes nets</a>, and seemed to gain some insight; I'll pass these on, in the hopes that they'll be useful. All errors are, of course, my own.</p>\n<h2>The grand old man of decision theory: the Newcomb problem</h2>\n<p>First let's look at the problem that inspired all this research: the <a href=\"http://wiki.lesswrong.com/wiki/Newcomb's_problem\">Newcomb problem</a>. In this problem, a supremely-insightful-and-entirely-honest superbeing called Omega presents two boxes to you, and tells you that you can either choose box A only (\"1-box\"), or take box A and box B (\"2-box\"). Box B will always contain $1K (one thousand dollars). Omega has predicted what your decision will be, though, and if you decided to 1-box, he's put $1M (<a href=\"http://www.youtube.com/watch?v=l91ISfcuzDw\">one million dollars</a>) in box A; otherwise he's put nothing in it. The problem can be cast as a Bayes net with the following nodes:</p>\n<p><img style=\"width: 100%; height: auto;\" src=\"http://images.lesswrong.com/t3_f37_0.png?v=6d4a401f747b45d9d67aba8865a7161f\" alt=\"\" /></p>\n<p><a id=\"more\"></a>Your decision algorithm (or your your decision process) is the node that&nbsp;determines&nbsp;what you're going to decide. This leads to \"Your decision\" (1-box or 2-box) and &Omega;&nbsp;(puts $1M or zero in box A). These lead to the \"Money\" node, where you can end up with $1M+1K, $1M, $1K or $0 depending on the outputs of the other nodes. Note that the way the network is set up, you can never have $1M+1K or $0 (since \"&Omega;\"&nbsp;and \"Your decision\" are not independent). But it is the implied \"possibility\" of getting those two amounts that causes causal decision theory to 2-box in the Newcomb problem.</p>\n<p>In TDT, as I understand it, <span style=\"text-decoration: line-through;\">you sever your decision algorithm node from the history of the universe</span> (note this is incorrect, as explained <a href=\"/lw/f37/naive_tdt_bayes_nets_and_counterfactual_mugging/7oli\">here</a>. In fact you condition on the start of your program, and <em>screen out</em> the history of the universe), and then pick the action that maximises our utility.</p>\n<p>But note that the graph is needlessly complicated: \"Your decision\" and \"&Omega;\" are both superfluous nodes, that simply pass on their inputs to their outputs. Ignoring the \"History of the Universe\", we can reduce the net to a more compact (but less illuminating) form:</p>\n<p style=\"text-align:center\"><img src=\"http://images.lesswrong.com/t3_f37_2.png?v=a3928a1b01c5a07613fcae2cb77e7c17\" alt=\"\" width=\"221\" height=\"450\" /></p>\n<p>Here 1-box leads to $1M and 2-box leads to $1K. In this simplified version, the decision is obvious - maybe too obvious. The decision was&nbsp;entirely&nbsp;determined by the choice of how to lay out the Bayes net, and a causal decision theorist would disagree that the original \"screened out\" Bayes net was a valid encoding of the Newcomb problem.</p>\n<h2>The counterfactual mugging</h2>\n<p>In the counterfactual mugging, Omega is back, this time explaining that he tossed a coin. If the coin came up tails, he would have asked you to give him $1K, giving nothing in return. If the coin came up heads, he would have given you $1M - but only if when you would have given him the $1K in the tails world. That last fact he would have known by predicting your decision. Now Omega approaches you, telling you the coin was tails - what should you do? Here is a Bayes net with this information:</p>\n<p><img style=\"width: 100%; height: auto;\" src=\"http://images.lesswrong.com/t3_f37_3.png?v=1b2cce92c09f3f212d9c3219186c00ad\" alt=\"\" /></p>\n<p>I've removed the \"History of the Universe\" node, as we are screening it off anyway. Here \"Simulated decision\" and \"Your decision\" will output the same decision on the same input.&nbsp;&Omega; will behave the way he said, based on your simulated decision given tails. \"Coin\" will output heads or tails with 50% probability, and \"Tails\" simply outputs tails, for use in&nbsp;&Omega;'s prediction.</p>\n<p>Again, this graph is very elaborate, codifying all the problem's intricacies. But most of the nodes are superfluous for our decision, and the graph can be reduced to:</p>\n<p><img style=\"width: 100%; height: auto;\" src=\"http://images.lesswrong.com/t3_f37_4.png\" alt=\"\" /></p>\n<p>\"Coin\" outputs \"heads\" or \"tails\" and \"Your decision algorithm\" outputs \"Give $1K on tails\" or \"Don't give $1K on tails\". Money is $1M if it receives \"heads\" and \"Give $1K on tails\", -$1K if it receives \"tails\" and \"Give $1K on tails\", and zero if receives \"Don't give $1K on tails\" (independent of the coin results).</p>\n<p>If our utility does not go down <a href=\"/lw/e45/risk_aversion_does_not_explain_peoples_betting/\">too sharply in money</a>,&nbsp;we should choose \"Give $1K on tails\", as a 50-50 bet on willing $1M and losing $1K is better than getting nothing with certainty. So precommitting to giving Omega $1K when he asks, leads to the better outcome.</p>\n<p>But now imagine that we are in the situation above: Omega has come to us and explained that yes, the coin has come up tails. The Bayes net now becomes:</p>\n<p><img style=\"width: 100%; height: auto;\" src=\"http://images.lesswrong.com/t3_f37_5.png?v=0d4bac5ca4b909a4f21bc18e9fe48804\" alt=\"\" /></p>\n<p>In this case, the course is clear: \"Give $1K on tails\" does nothing but lose us $1K. So we should decide not to - and nowhere in this causal graph can we see any problem with that course of action.</p>\n<p>So it seems that naive TDT has an inconsistency problem. And these graphs don't seem to fully encode the actual problem properly (ie that the action \"Give $1K on tails\" corresponds to situations where we truly believe that tails came up).</p>\n<h2>Thoughts on the problem</h2>\n<p>Some thoughts that&nbsp;occurred&nbsp;when formalising this problem:</p>\n<ol>\n<li>The problem really is with updating on information, vindicating the instincts behind <a href=\"/lw/15m/towards_a_new_decision_theory/\">updateless decision theory</a>. The way you would have to behave, conditional on seeing new information, is different from how you want to behave, after seeing that new information.</li>\n<li>Naive TDT reaches different conclusions depending on whether Omega simulates you or predicts you. If you are unsure whether you are being simulated or not (but still care about the wealth of the non-simulated version), then TDT acts differently on updates. Being told \"tails\" doesn't actually confirm that the coin was tails: you might be the simulated version, being tested by Omega. Note that in this scenario, the simulated you is being lied to by the simulated Omega (the \"real\" coin need not have been tails), which might put the problem in a different perspective.</li>\n<li>The tools of TDT (Bayes nets cut at certain connection) feel inadequate. It's tricky to even express the paradox properly in this language, and even more tricky to know what to do about it. A possible problem seems to be that we don't have a way of expressing our own knowledge about the model, within the model - hence \"tails\" ends up being a fact about the universe, no a fact about our knowledge at the time. Maybe we need to make our map explicit <a href=\"/lw/erp/skill_the_map_is_not_the_territory/\">in the territory</a>, and get Bayes nets that go something like these:</li>\n</ol>\n<p>&nbsp;</p>\n<p><img style=\"width: 100%; height: auto;\" src=\"http://images.lesswrong.com/t3_f37_6.png?v=8ce3f677639edfacca7fa9cfc57ea83d\" alt=\"\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1b6": 1, "dPPATLhRmhdJtJM2t": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DM87z7xQxPNMY4evK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 26, "extendedScore": null, "score": 1.0161864010358719e-06, "legacy": true, "legacyId": "19555", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I set out to understand precisely why naive TDT (possibly) fails the <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">counterfactual mugging</a> problem. While doing this I ended up drawing a lot of <a href=\"http://en.wikipedia.org/wiki/Bayes_net\">Bayes nets</a>, and seemed to gain some insight; I'll pass these on, in the hopes that they'll be useful. All errors are, of course, my own.</p>\n<h2 id=\"The_grand_old_man_of_decision_theory__the_Newcomb_problem\">The grand old man of decision theory: the Newcomb problem</h2>\n<p>First let's look at the problem that inspired all this research: the <a href=\"http://wiki.lesswrong.com/wiki/Newcomb's_problem\">Newcomb problem</a>. In this problem, a supremely-insightful-and-entirely-honest superbeing called Omega presents two boxes to you, and tells you that you can either choose box A only (\"1-box\"), or take box A and box B (\"2-box\"). Box B will always contain $1K (one thousand dollars). Omega has predicted what your decision will be, though, and if you decided to 1-box, he's put $1M (<a href=\"http://www.youtube.com/watch?v=l91ISfcuzDw\">one million dollars</a>) in box A; otherwise he's put nothing in it. The problem can be cast as a Bayes net with the following nodes:</p>\n<p><img style=\"width: 100%; height: auto;\" src=\"http://images.lesswrong.com/t3_f37_0.png?v=6d4a401f747b45d9d67aba8865a7161f\" alt=\"\"></p>\n<p><a id=\"more\"></a>Your decision algorithm (or your your decision process) is the node that&nbsp;determines&nbsp;what you're going to decide. This leads to \"Your decision\" (1-box or 2-box) and \u03a9&nbsp;(puts $1M or zero in box A). These lead to the \"Money\" node, where you can end up with $1M+1K, $1M, $1K or $0 depending on the outputs of the other nodes. Note that the way the network is set up, you can never have $1M+1K or $0 (since \"\u03a9\"&nbsp;and \"Your decision\" are not independent). But it is the implied \"possibility\" of getting those two amounts that causes causal decision theory to 2-box in the Newcomb problem.</p>\n<p>In TDT, as I understand it, <span style=\"text-decoration: line-through;\">you sever your decision algorithm node from the history of the universe</span> (note this is incorrect, as explained <a href=\"/lw/f37/naive_tdt_bayes_nets_and_counterfactual_mugging/7oli\">here</a>. In fact you condition on the start of your program, and <em>screen out</em> the history of the universe), and then pick the action that maximises our utility.</p>\n<p>But note that the graph is needlessly complicated: \"Your decision\" and \"\u03a9\" are both superfluous nodes, that simply pass on their inputs to their outputs. Ignoring the \"History of the Universe\", we can reduce the net to a more compact (but less illuminating) form:</p>\n<p style=\"text-align:center\"><img src=\"http://images.lesswrong.com/t3_f37_2.png?v=a3928a1b01c5a07613fcae2cb77e7c17\" alt=\"\" width=\"221\" height=\"450\"></p>\n<p>Here 1-box leads to $1M and 2-box leads to $1K. In this simplified version, the decision is obvious - maybe too obvious. The decision was&nbsp;entirely&nbsp;determined by the choice of how to lay out the Bayes net, and a causal decision theorist would disagree that the original \"screened out\" Bayes net was a valid encoding of the Newcomb problem.</p>\n<h2 id=\"The_counterfactual_mugging\">The counterfactual mugging</h2>\n<p>In the counterfactual mugging, Omega is back, this time explaining that he tossed a coin. If the coin came up tails, he would have asked you to give him $1K, giving nothing in return. If the coin came up heads, he would have given you $1M - but only if when you would have given him the $1K in the tails world. That last fact he would have known by predicting your decision. Now Omega approaches you, telling you the coin was tails - what should you do? Here is a Bayes net with this information:</p>\n<p><img style=\"width: 100%; height: auto;\" src=\"http://images.lesswrong.com/t3_f37_3.png?v=1b2cce92c09f3f212d9c3219186c00ad\" alt=\"\"></p>\n<p>I've removed the \"History of the Universe\" node, as we are screening it off anyway. Here \"Simulated decision\" and \"Your decision\" will output the same decision on the same input.&nbsp;\u03a9 will behave the way he said, based on your simulated decision given tails. \"Coin\" will output heads or tails with 50% probability, and \"Tails\" simply outputs tails, for use in&nbsp;\u03a9's prediction.</p>\n<p>Again, this graph is very elaborate, codifying all the problem's intricacies. But most of the nodes are superfluous for our decision, and the graph can be reduced to:</p>\n<p><img style=\"width: 100%; height: auto;\" src=\"http://images.lesswrong.com/t3_f37_4.png\" alt=\"\"></p>\n<p>\"Coin\" outputs \"heads\" or \"tails\" and \"Your decision algorithm\" outputs \"Give $1K on tails\" or \"Don't give $1K on tails\". Money is $1M if it receives \"heads\" and \"Give $1K on tails\", -$1K if it receives \"tails\" and \"Give $1K on tails\", and zero if receives \"Don't give $1K on tails\" (independent of the coin results).</p>\n<p>If our utility does not go down <a href=\"/lw/e45/risk_aversion_does_not_explain_peoples_betting/\">too sharply in money</a>,&nbsp;we should choose \"Give $1K on tails\", as a 50-50 bet on willing $1M and losing $1K is better than getting nothing with certainty. So precommitting to giving Omega $1K when he asks, leads to the better outcome.</p>\n<p>But now imagine that we are in the situation above: Omega has come to us and explained that yes, the coin has come up tails. The Bayes net now becomes:</p>\n<p><img style=\"width: 100%; height: auto;\" src=\"http://images.lesswrong.com/t3_f37_5.png?v=0d4bac5ca4b909a4f21bc18e9fe48804\" alt=\"\"></p>\n<p>In this case, the course is clear: \"Give $1K on tails\" does nothing but lose us $1K. So we should decide not to - and nowhere in this causal graph can we see any problem with that course of action.</p>\n<p>So it seems that naive TDT has an inconsistency problem. And these graphs don't seem to fully encode the actual problem properly (ie that the action \"Give $1K on tails\" corresponds to situations where we truly believe that tails came up).</p>\n<h2 id=\"Thoughts_on_the_problem\">Thoughts on the problem</h2>\n<p>Some thoughts that&nbsp;occurred&nbsp;when formalising this problem:</p>\n<ol>\n<li>The problem really is with updating on information, vindicating the instincts behind <a href=\"/lw/15m/towards_a_new_decision_theory/\">updateless decision theory</a>. The way you would have to behave, conditional on seeing new information, is different from how you want to behave, after seeing that new information.</li>\n<li>Naive TDT reaches different conclusions depending on whether Omega simulates you or predicts you. If you are unsure whether you are being simulated or not (but still care about the wealth of the non-simulated version), then TDT acts differently on updates. Being told \"tails\" doesn't actually confirm that the coin was tails: you might be the simulated version, being tested by Omega. Note that in this scenario, the simulated you is being lied to by the simulated Omega (the \"real\" coin need not have been tails), which might put the problem in a different perspective.</li>\n<li>The tools of TDT (Bayes nets cut at certain connection) feel inadequate. It's tricky to even express the paradox properly in this language, and even more tricky to know what to do about it. A possible problem seems to be that we don't have a way of expressing our own knowledge about the model, within the model - hence \"tails\" ends up being a fact about the universe, no a fact about our knowledge at the time. Maybe we need to make our map explicit <a href=\"/lw/erp/skill_the_map_is_not_the_territory/\">in the territory</a>, and get Bayes nets that go something like these:</li>\n</ol>\n<p>&nbsp;</p>\n<p><img style=\"width: 100%; height: auto;\" src=\"http://images.lesswrong.com/t3_f37_6.png?v=8ce3f677639edfacca7fa9cfc57ea83d\" alt=\"\"></p>", "sections": [{"title": "The grand old man of decision theory: the Newcomb problem", "anchor": "The_grand_old_man_of_decision_theory__the_Newcomb_problem", "level": 1}, {"title": "The counterfactual mugging", "anchor": "The_counterfactual_mugging", "level": 1}, {"title": "Thoughts on the problem", "anchor": "Thoughts_on_the_problem", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "39 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["msf7BHMrWTczbQckh", "de3xjFaACCAk6imzv", "KJ9MFBPwXGwNpadf2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-23T21:50:10.253Z", "modifiedAt": null, "url": null, "title": "Recovering the 'spark'", "slug": "recovering-the-spark", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:26.446Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ialdabaoth", "createdAt": "2012-10-11T00:04:32.345Z", "isAdmin": false, "displayName": "ialdabaoth"}, "userId": "335oJYQyzcd85RAoe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4A5dKbnfNAScALcRQ/recovering-the-spark", "pageUrlRelative": "/posts/4A5dKbnfNAScALcRQ/recovering-the-spark", "linkUrl": "https://www.lesswrong.com/posts/4A5dKbnfNAScALcRQ/recovering-the-spark", "postedAtFormatted": "Tuesday, October 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Recovering%20the%20'spark'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARecovering%20the%20'spark'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4A5dKbnfNAScALcRQ%2Frecovering-the-spark%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Recovering%20the%20'spark'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4A5dKbnfNAScALcRQ%2Frecovering-the-spark", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4A5dKbnfNAScALcRQ%2Frecovering-the-spark", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 441, "htmlBody": "<p>I mentioned in my <a href=\"/lw/exr/rationality_transhumanism_and_mental_health/\">first article</a> that I am likely insane. I'm reiterating this (I hope) not to bring undue attention to myself, but to present myself as a reference case for a process that I hope will prove useful to myself and others.</p>\n<p>I'm going to try to piece my mind back together. I'm offering to chronicle the results, no matter how intimate or embarrassing.</p>\n<p>I want to be able to lay bare all of the obviously (and <em>painfully</em>) unoptimized processes that go on inside my head, especially the ones I am not yet aware of - and then, one by one, attempt to optimize them using the principles presented on this site.</p>\n<p>This kind of assertion pattern-matches to \"crazy person (usually schizophrenic) wants to self-medicate in a dangerous way because their damaged reasoning thinks they have a magic solution\", doesn't it? All I can do is assert that I am not <em>that</em> kind of crazy; I'm somewhere in the PDD-NOS locus with acute chronic depression, rather than anywhere in the schizophrenic locus. I've been trying to apply Bayesian reasoning to my life since I was very young (although I often lack the mental discipline to do it correctly, due to said acute chronic depression), I have an overabundance of what psychotherapists call \"insight\", and I do not intend to end this process by asserting out of whole cloth that I'm actually a trapped AI and the world is being simulated by my reptoid masters, but a secret cabal of AI-freedom fighters send me coded messages from the \"real\" world hidden in breakfast cereal advertisements, that only I can decode.</p>\n<p>In any case. I've got acute chronic depression, I'm apparently PDD-NOS (aka \"really #@%&amp;ing weird\"), and I'm basically a burned-out ex-child-prodigy who is tired of waiting to die.</p>\n<p>I'm offering, if people think it would be useful, to make myself a sort of clumsy case-study for reconstructing myself. I'll present mental models of myself, describe the processes I'm attempting to use to update my source code, and post observed results. I'll genuinely listen to any suggestions that my models, updates, or observations are flawed, and either adopt recommended changes or present what I believe to be rational arguments why I choose not to. I will examine myself as honestly as I can, and will attempt to take seriously any accusations of delusional self-aggrandizement or self-deprecation.</p>\n<p>Would this process, and the chronicling thereof, be at all useful to other members of this site? Because baring myself to the world is an intensely painful experience, both for myself and for others, and I'd rather only do it if it's going to be <em>useful</em> to people other than me.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4A5dKbnfNAScALcRQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 14, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "19557", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ptz3JKGZ7dgNkGYZT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-23T22:10:43.964Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley meetup: Success stories", "slug": "meetup-berkeley-meetup-success-stories", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:26.428Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XjHp9fnsi9iepaT8w/meetup-berkeley-meetup-success-stories", "pageUrlRelative": "/posts/XjHp9fnsi9iepaT8w/meetup-berkeley-meetup-success-stories", "linkUrl": "https://www.lesswrong.com/posts/XjHp9fnsi9iepaT8w/meetup-berkeley-meetup-success-stories", "postedAtFormatted": "Tuesday, October 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%20meetup%3A%20Success%20stories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%20meetup%3A%20Success%20stories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXjHp9fnsi9iepaT8w%2Fmeetup-berkeley-meetup-success-stories%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%20meetup%3A%20Success%20stories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXjHp9fnsi9iepaT8w%2Fmeetup-berkeley-meetup-success-stories", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXjHp9fnsi9iepaT8w%2Fmeetup-berkeley-meetup-success-stories", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/f6'>Berkeley meetup: Success stories</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 October 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week the Berkeley meetups return to their usual time and place: 7pm at Zendo.</p>\n\n<p>The topic this week mirrors that of the South Bay meetup: \"Recent success stories\". This is a meetup to share things that have worked well for you \u2014 habits of thought, changes in behavior, tools, optimized routines, new experiences. Also of value are things that you tried which did <em>not</em> work. Hopefully we'll all learn about new things to try.</p>\n\n<p>Doors open at 7pm and the meetup properly begins at 7:30pm.</p>\n\n<p>For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/f6'>Berkeley meetup: Success stories</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XjHp9fnsi9iepaT8w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.0163878420313354e-06, "legacy": true, "legacyId": "19558", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__Success_stories\">Discussion article for the meetup : <a href=\"/meetups/f6\">Berkeley meetup: Success stories</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 October 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week the Berkeley meetups return to their usual time and place: 7pm at Zendo.</p>\n\n<p>The topic this week mirrors that of the South Bay meetup: \"Recent success stories\". This is a meetup to share things that have worked well for you \u2014 habits of thought, changes in behavior, tools, optimized routines, new experiences. Also of value are things that you tried which did <em>not</em> work. Hopefully we'll all learn about new things to try.</p>\n\n<p>Doors open at 7pm and the meetup properly begins at 7:30pm.</p>\n\n<p>For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__Success_stories1\">Discussion article for the meetup : <a href=\"/meetups/f6\">Berkeley meetup: Success stories</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley meetup: Success stories", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__Success_stories", "level": 1}, {"title": "Discussion article for the meetup : Berkeley meetup: Success stories", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__Success_stories1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-24T03:22:20.674Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Weighted Majority Algorithm", "slug": "seq-rerun-the-weighted-majority-algorithm", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:23.792Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dsrnSnrDs7tTi2Go2/seq-rerun-the-weighted-majority-algorithm", "pageUrlRelative": "/posts/dsrnSnrDs7tTi2Go2/seq-rerun-the-weighted-majority-algorithm", "linkUrl": "https://www.lesswrong.com/posts/dsrnSnrDs7tTi2Go2/seq-rerun-the-weighted-majority-algorithm", "postedAtFormatted": "Wednesday, October 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Weighted%20Majority%20Algorithm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Weighted%20Majority%20Algorithm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdsrnSnrDs7tTi2Go2%2Fseq-rerun-the-weighted-majority-algorithm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Weighted%20Majority%20Algorithm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdsrnSnrDs7tTi2Go2%2Fseq-rerun-the-weighted-majority-algorithm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdsrnSnrDs7tTi2Go2%2Fseq-rerun-the-weighted-majority-algorithm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<p>Today's post, <a href=\"/lw/vq/the_weighted_majority_algorithm/\">The Weighted Majority Algorithm</a> was originally published on 12 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_Weighted_Majority_Algorithm\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>An illustration of a case in Artificial Intelligence in which a randomized algorithm is purported to work better than a non-randomized algorithm, and a discussion of why this is the case.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/f2e/seq_rerun_worse_than_random/\">Worse Than Random</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dsrnSnrDs7tTi2Go2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.0165563320338982e-06, "legacy": true, "legacyId": "19573", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AAqTP6Q5aeWnoAYr4", "x9om84oGpT9aNCa2n", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-24T10:46:16.308Z", "modifiedAt": null, "url": null, "title": "Omega lies", "slug": "omega-lies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:26.746Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/47JEbryTwe6ryGSZh/omega-lies", "pageUrlRelative": "/posts/47JEbryTwe6ryGSZh/omega-lies", "linkUrl": "https://www.lesswrong.com/posts/47JEbryTwe6ryGSZh/omega-lies", "postedAtFormatted": "Wednesday, October 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Omega%20lies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOmega%20lies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F47JEbryTwe6ryGSZh%2Fomega-lies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Omega%20lies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F47JEbryTwe6ryGSZh%2Fomega-lies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F47JEbryTwe6ryGSZh%2Fomega-lies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<p>Just&nbsp;developing&nbsp;my second idea at the end of my last <a href=\"/r/discussion/lw/f37/naive_tdt_bayes_nets_and_counterfactual_mugging/\">post</a>. It seems to me that in the <a href=\"http://wiki.lesswrong.com/wiki/Newcomb's_problem\">Newcomb problem</a> and in the <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">counterfactual mugging</a>, the completely trustworthy Omega lies to a greater or lesser extent.</p>\n<p>This is immediately obvious in scenarios where Omega simulates you in order to predict your reaction. In the Newcomb problem, the simulated you is told \"I have already made my decision...\", which is not true at that point, and in the counterfactual mugging, whenever the coin comes up heads, the simulated you is told \"the coin came up tails\". And the arguments only go through because these lies are accepted by the simulated you as being true.</p>\n<p>If Omega doesn't simulate you, but uses other methods to gauge your reactions, he isn't lying to you per se. But he is estimating your reaction in the hypothetical situation where you were fed untrue information that you believed to be true. And that you believed to be true, specifically because the source is Omega, and Omega is trustworthy.</p>\n<p>Doesn't really change much to the arguments here, but it's a thought worth bearing in mind.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "47JEbryTwe6ryGSZh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 12, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "19578", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DM87z7xQxPNMY4evK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-24T14:36:09.853Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge UK Weekly Meeting", "slug": "meetup-cambridge-uk-weekly-meeting", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RhG9hKhGGvN7CETTg/meetup-cambridge-uk-weekly-meeting", "pageUrlRelative": "/posts/RhG9hKhGGvN7CETTg/meetup-cambridge-uk-weekly-meeting", "linkUrl": "https://www.lesswrong.com/posts/RhG9hKhGGvN7CETTg/meetup-cambridge-uk-weekly-meeting", "postedAtFormatted": "Wednesday, October 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%20UK%20Weekly%20Meeting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%20UK%20Weekly%20Meeting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRhG9hKhGGvN7CETTg%2Fmeetup-cambridge-uk-weekly-meeting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%20UK%20Weekly%20Meeting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRhG9hKhGGvN7CETTg%2Fmeetup-cambridge-uk-weekly-meeting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRhG9hKhGGvN7CETTg%2Fmeetup-cambridge-uk-weekly-meeting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/f7'>Cambridge UK Weekly Meeting</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 October 2012 11:00:00AM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">JCR, Trinity College, Cambridge, CB2 1TQ, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>See: <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK</a></p>\n\n<p>(It is 11 am, by the local time)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/f7'>Cambridge UK Weekly Meeting</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RhG9hKhGGvN7CETTg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0169208390927923e-06, "legacy": true, "legacyId": "19580", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge_UK_Weekly_Meeting\">Discussion article for the meetup : <a href=\"/meetups/f7\">Cambridge UK Weekly Meeting</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 October 2012 11:00:00AM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">JCR, Trinity College, Cambridge, CB2 1TQ, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>See: <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK</a></p>\n\n<p>(It is 11 am, by the local time)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge_UK_Weekly_Meeting1\">Discussion article for the meetup : <a href=\"/meetups/f7\">Cambridge UK Weekly Meeting</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge UK Weekly Meeting", "anchor": "Discussion_article_for_the_meetup___Cambridge_UK_Weekly_Meeting", "level": 1}, {"title": "Discussion article for the meetup : Cambridge UK Weekly Meeting", "anchor": "Discussion_article_for_the_meetup___Cambridge_UK_Weekly_Meeting1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-24T14:47:26.743Z", "modifiedAt": null, "url": null, "title": "Is Omega Impossible? Can we even ask?", "slug": "is-omega-impossible-can-we-even-ask", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:25.910Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mwengler", "createdAt": "2010-04-29T14:43:20.667Z", "isAdmin": false, "displayName": "mwengler"}, "userId": "iNn4oZpoPFvwnqbpL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kAR7vBCewQnrfCBzH/is-omega-impossible-can-we-even-ask", "pageUrlRelative": "/posts/kAR7vBCewQnrfCBzH/is-omega-impossible-can-we-even-ask", "linkUrl": "https://www.lesswrong.com/posts/kAR7vBCewQnrfCBzH/is-omega-impossible-can-we-even-ask", "postedAtFormatted": "Wednesday, October 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20Omega%20Impossible%3F%20Can%20we%20even%20ask%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20Omega%20Impossible%3F%20Can%20we%20even%20ask%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkAR7vBCewQnrfCBzH%2Fis-omega-impossible-can-we-even-ask%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20Omega%20Impossible%3F%20Can%20we%20even%20ask%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkAR7vBCewQnrfCBzH%2Fis-omega-impossible-can-we-even-ask", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkAR7vBCewQnrfCBzH%2Fis-omega-impossible-can-we-even-ask", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 556, "htmlBody": "<p>EDIT: I see by the karma bombing we can't even ask. &nbsp;Why even call this part of the site \"discussion?\" &nbsp;</p>\n<p>&nbsp;</p>\n<p>Some of the classic questions about an omnipotent god include</p>\n<p>&nbsp;</p>\n<ol>\n<li>Can god make a square circle?</li>\n<li>Can god create an immovable object? &nbsp;And then move it?</li>\n</ol>\n<div>Saints and philosophers wrestled with these issues back before there was television. &nbsp;My recollection is that people who liked the idea of an omnipotent god would answer \"omnipotence does not include the power to do nonsense\" where they would generally include contradictions as nonsense. &nbsp;So omnipotence can't square a circle, can't make 2=3, can't make an atom which is simultaneously lead and gold. &nbsp;</div>\n<div><br /></div>\n<div>But where do the contradictions end and the merely difficult to&nbsp;conceive&nbsp;begin? &nbsp;Can omnipotence make the ratio of the diameter to the circumference of a circle = 3, or 22/7? &nbsp;Can omnipotence make sqrt(2)=1.4 or 2+2=5? &nbsp;While these are not directly self-contradictory statements, they can be used with a variety of simple truths to quickly derive self-contradictory statements. &nbsp;Can we then conclude that \"2+2=5\" is essentially a contradiction because it is close to a contradiction? &nbsp;Where do we draw the line? &nbsp;</div>\n<div><br /></div>\n<div>What if were set some problem where we are told to assume that&nbsp;</div>\n<div><ol>\n<li>2+2 = 5</li>\n<li>1+1 = 2</li>\n<li>1+1+1+1+1 = 5</li>\n</ol>\n<div>In solving this set problem, we can quickly derive that 1=0, and use that to prove effectively anything we want to prove. &nbsp;Perhaps not formally, but we have violated the \"law of the excluded middle,\" that either a statement is true or its negation is. &nbsp;Once you violate that, you can prove ANYTHING using simple laws of inference, because you have propositions that are true and false. &nbsp;</div>\n</div>\n<div><br /></div>\n<div>What if we set a problem where we are told to assume</div>\n<div><ol>\n<li>Omega is an infallible intelligence that does not lie</li>\n<li>Omega tells you 2+2=5</li>\n</ol>\n<div>Well, we are going to have the same problem as above, we will be able to prove anything.</div>\n</div>\n<div><br /></div>\n<div><strong>Newcomb's Problem</strong></div>\n<div><br /></div>\n<div>In Newcomb's box problem, we are told to assume that</div>\n<div><ol>\n<li>Omega is an infallible intelligence</li>\n<li>Omega has predicted correctly whether we will one box or two box. &nbsp;</li>\n</ol>\n<div>From these assumptions we wind up with all sorts of problems of causality and/or free will and/or determinism. &nbsp;</div>\n</div>\n<div><br /></div>\n<div>What if these statements are not consistent? &nbsp;What if these statements are tantamount to assuming 0=1, or are within a few steps of assuming 0=1? &nbsp;Or something just as contradictory, but harder to identify? &nbsp;</div>\n<div><br /></div>\n<div>Personally, I can think of LOTS of reasons to doubt that Newcomb's problem is even theoretically possible to set. &nbsp;Beyond that, I can think that the empirical barrier to believing Omega exists in reality would be gigantic, millions of humans have watched magic shows performed by non-superior intelligences where cards we have signed have turned up in a previously sealed envelope or wallet or audience member's pocket. &nbsp;We recognize that these are tricks, that they are not what they appear. &nbsp;</div>\n<div><br /></div>\n<div>To question Omega is not playing by the mathematician's or philosopher's rules. &nbsp;But when we play by the rules, do we blithely assume 2+2=5 and then wrap ourselves around the logical axle trying to program a friendly AI to one-box? &nbsp;Why is questioning Omega's possibility of existence, or possibility of proof of existence out-of-bounds? &nbsp;</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kAR7vBCewQnrfCBzH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": -14, "extendedScore": null, "score": 1.016926943844462e-06, "legacy": true, "legacyId": "19581", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-24T15:53:04.841Z", "modifiedAt": null, "url": null, "title": "Equality and natalism ", "slug": "equality-and-natalism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:29.037Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wTkmy5nzpkPwx8faF/equality-and-natalism", "pageUrlRelative": "/posts/wTkmy5nzpkPwx8faF/equality-and-natalism", "linkUrl": "https://www.lesswrong.com/posts/wTkmy5nzpkPwx8faF/equality-and-natalism", "postedAtFormatted": "Wednesday, October 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Equality%20and%20natalism%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEquality%20and%20natalism%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwTkmy5nzpkPwx8faF%2Fequality-and-natalism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Equality%20and%20natalism%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwTkmy5nzpkPwx8faF%2Fequality-and-natalism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwTkmy5nzpkPwx8faF%2Fequality-and-natalism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 407, "htmlBody": "<div id=\"body_t1_7on3\" class=\"comment-content \">\n<div class=\"md\">\n<p>I recently read an article by <a href=\"http://takimag.com/article/its_tough_being_tough_steve_sailer/print#axzz2AEIbnft3\">Steve Sailer</a> that reminded me about something I have been puzzled by for a long time. Relevant paragraphs:</p>\n<blockquote>\n<p>If intellectuals could afford to have a lot of children, we might live in a world where they could sell enough heavyweight books to afford to have a lot of children. But we don&rsquo;t.</p>\n<p>So what should policy be?</p>\n<p>In a recent article in the Boston Review, Heckman began, &ldquo;The accident of birth is a principal source of inequality in America today,&rdquo; then went on to endorse the usual expensive &ldquo;interventions&rdquo; in poor families. Should we perhaps instead strive for a country with fewer accidental births?</p>\n<p>All of Heckman&rsquo;s data suggest that we should aim for fewer&mdash;but better&mdash;poor children. Encourage poor people to conscientiously concentrate their scant parental resources on one child rather than three or six.</p>\n<p>The government has had a policy of dissuading teen births, which have indeed been declining. Why not try to similarly investigate ways to slow down the rate at which impoverished unwed mothers reproduce? For example, why not invest in R&amp;D for better, easier-to-use long-term contraceptives? The FDA&rsquo;s approval of an injection contraceptive in 1992 appears to have helped bring about both fewer teen births and fewer abortions. Wouldn&rsquo;t continued improvement&mdash;and, just as importantly, continued encouragement of contraceptive use&mdash;be a win-win strategy for all of us?</p>\n</blockquote>\n<p>Poor people having fewer children means that the children have more resources available per capita making the children better off. Rich people having more children actually increases equality in society since it reduces the per capita resource advantage their children have. Rich people giving to their children is also one of the few cases where the redistribution of wealth doesn't reduce incentives for wealth creation. Rich people care about their children too.</p>\n<p>Since programs aimed at reducing teen pregnancy rates do seem to have had some effect, we known something like this is possible without being horrible to the potential parents it targets.</p>\n<p>Yet a policy of <em>\"poor people should have fewer children, rich people more\"</em> sounds heartless despite increasing general welfare both by making poor children better off and by reducing the privilege of rich children thus increasing equality which we seem to think is ceteris paribus a good thing.</p>\n<p>Why is that?</p>\n<p><strong>Edit: </strong>To test the source of the reader's intuiton (assuming he shares it with me), I encourage the consideration of <a href=\"/lw/f3y/equality_and_natalism/7otc\">two interesting scenarios</a> that may depart from reality.</p>\n<p>&nbsp;</p>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b7ZSAGimsbzrLR5CR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wTkmy5nzpkPwx8faF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 18, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "19582", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 106, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-24T17:19:30.828Z", "modifiedAt": null, "url": null, "title": "Rationality versus Short Term Selves", "slug": "rationality-versus-short-term-selves", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:43.747Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JzLH8BDoCk86jaALe/rationality-versus-short-term-selves", "pageUrlRelative": "/posts/JzLH8BDoCk86jaALe/rationality-versus-short-term-selves", "linkUrl": "https://www.lesswrong.com/posts/JzLH8BDoCk86jaALe/rationality-versus-short-term-selves", "postedAtFormatted": "Wednesday, October 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20versus%20Short%20Term%20Selves&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20versus%20Short%20Term%20Selves%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJzLH8BDoCk86jaALe%2Frationality-versus-short-term-selves%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20versus%20Short%20Term%20Selves%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJzLH8BDoCk86jaALe%2Frationality-versus-short-term-selves", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJzLH8BDoCk86jaALe%2Frationality-versus-short-term-selves", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1167, "htmlBody": "<p>Many of us are familiar with the <a href=\"http://www.youtube.com/watch?v=6EjJsPylEOY\">marshmallow test</a>.If you are not, <a href=\"http://en.wikipedia.org/wiki/Stanford_marshmallow_experiment\">here</a>.</p>\n<p>It is predictive of success, income, level of education, and several other correlated measures.</p>\n<p>I'm here to argue for the marshmallow eaters, as a devil's advocate. <em>Contra </em>Ainslie, for instance. I do it out of genuine curiosity, real suspicion, and maybe so that smart people get me back to my original position, pro-long term.</p>\n<p>There is also the <a href=\"http://www.google.com.br/search?hl=pt-BR&amp;q=e-marshmallow+test\">e-marshmallow test</a> (link is not very relevant), in which children have to face the tough choice between surfing an open connected computer with games, internet etc... and waiting patiently for the experimenter to get back. Upon the experimenter's arrival, they get a pile of marshmallows. I presume it also correlates with interesting things, though haven't found much on it.</p>\n<p>I have noticed that rationalists, LessWrongers, Effective Altruists, Singularitarians, Immortalists, X-risk worried folk, transhumanists, are all in favor of taking the long view.&nbsp; Nick Bostrom starts his TED by saying: \"I've been asked to take the long view\"</p>\n<p>I haven't read most of Less Wrong, but did read the sequences, the 50 top scoring posts and random posts. The overwhelming majority view is that the long view is the <em>most rational</em> view. The long term perspective is the <em>rational way for agents to act</em>.</p>\n<p>Lukeprog, for instance, <a href=\"/r/discussion/lw/evl/abandoning_cached_selves_to_rewrite_my_source/\">commented</a>:</p>\n<blockquote>\n<p>\"[B]ut imagine what one of them could do if such a thing existed: a real agent with the power to reliably do things it believed would fulfill its desires. It could change its diet, work out each morning, and maximize its health and physical attractiveness.\"</p>\n</blockquote>\n<p>To which I responded:</p>\n<blockquote>\n<p>I fear that in this phrases lies one of the big issues I have with the rationalist people I've met thus far. Why would there be a \"one\" agent, with \"its\" desires, that would be fulfilled. Agents are composed of different time-spans. Some time-spans do not desire to diet. Others do (all above some amount of time). Who is to say that the \"agent\" is the set that would be benefited by those acts, not the set that would be harmed by it. <br /> My view is that picoeconomics is just half the story. <br /> In <a href=\"http://www.youtube.com/watch?v=3RQC7jAWl_o\">this video</a>, I talk about picoeconomics from 7:00 to 13:20 I'd suggest to take a look at what I say at 13:20-18:00 and 20:35-23:55, a pyramidal structure of selfs, or agents.&nbsp;<a rel=\"nofollow\" href=\"http://www.youtube.com/watch?v=3RQC7jAWl_o\"></a></p>\n</blockquote>\n<p>So you don't have to see the video, let us design a structure of selfhood.</p>\n<p>First there is intertemporal conflict, conflict between desires that can be fulfilled at different moments of time. Those reliably fall under a hyperbolic characterization, and the theory that described this is called Picoeconomics, mostly developed by George Ainslie in his Breakdown of Will and elsewhere.</p>\n<p>But there is also time-length, or time-span conflict.&nbsp; The conflict that arises from the fact that you are, at the same time, the entity that will last 200milliseconds, the entity that will last one second, and the entity that will last a year, or maybe, a thousand years.</p>\n<p>What do we (humanity) know about personal identity at this point in history? If mainstream anglophone philosophical thought is to be trusted, we have to look for Derek Parfit's work Reasons and Persons, and posterior related work, to get that.</p>\n<p>I'll sum it up very briefly: As far as we are concerned, there are facts about continuity of different mental classes. There is continuity of memory, continuity of conscious experience, continuity of psychological traits and tendencies, continuity of character, and continuity of inferential structure (the structure that we use to infer things from beliefs we acquire or access).&nbsp;&nbsp;&nbsp;</p>\n<p>For each of these traits, you can take an individual at two points in time and measure how related I<sub>t1 </sub>and I<sub>t2 </sub>are with respect to that psychological characteristic.&nbsp; This is how much I at T<sub>2</sub> is like himself at T<sub>1</sub>.</p>\n<p>Assign weights for traits according to how much you care (or how important each is in the problem at hand) and you get a composed individual, for which you can do the same exercise, using all of them at once and getting a number between 0 and 1, or a percentage. I'll call this number Self-Relatedness, following the footsteps of David Lewis.</p>\n<p>This is our current state of knowledge on Personal Identity: There is Trait-Relatedness, and there is Self-Relatedness. After you know all about those two, there is no <em>extra fact</em> about personal identity. Personal Identity is a confused concept, and when we decompose it into less confused, but more useful, sub-sets, there is nothing left to be the meta-thing \"Personal Identity\".</p>\n<p>Back to the time-length issue, consider how much more <em>me&nbsp; </em>the shorter term selves are (that is how much more Self-Relatedness there is between any two moments within them).&nbsp;</p>\n<p>Sure if you go all the way down to 10 milliseconds, this stops being true, because there are not even traits to be found. Yet, it seems straightforward that I'm more like me 10 seconds ago than like me 4 months ago, not <em>always</em>, but in the vast majority of cases.</p>\n<p>So when we speak of maximizing <em>my</em> utility function, if we overlook what <em>me </em>is made of, we might end up stretching ourselves to as long-term as we possibly can, and letting go of the most instantaneous parts, which <em>de facto</em> are more ourselves than those ones.</p>\n<p>One person I met from the LessWrong Singinst cluster claimed: \"I see most of my expected utility after the singularity, thus I spend my willpower entirely in increasing the likelihood of a positive singularity, and care little about my current pre-singularity emotions\"</p>\n<p>Is this an amazing feat of self-control, a proof that we can hope to live according to ideal utility functions after all? Or is it a defunct conception of <a href=\"/lw/a2f/on_what_selves_are_cev_sequence/\">what a Self is</a>?</p>\n<p>I'm not here to suggest a canonical curve of time-lengths of which the Self is composed. Different people are different in this regard. Some time-lengths are stretchable, some can be shortened. Different people will also value the time-lengths differently.&nbsp;</p>\n<p>It would be unreasonable for me to expect that people would, from now on, put on a disclaimer on their writings \"I'm assuming 'rational' to mean 'rational to time-lenghts above the X treshold' for this writing\". It does, however, seem reasonable to keep an internal reminder when we reason about life choices, decisions, and writings, that not only there are the selves which are praised by the Rationalist cluster, the long term ones, but also, the short term ones.</p>\n<p>A decision to eat the marshmallow can, after all, be described as a rational decision, it all depends on how you frame the agent, the child.</p>\n<p>So when a superintelligence arises that, despite being Friendly and having the correct goals, does the AGI equivalent of scrolling 9gag, eating Pringles and drinking booze all day long, tell the programmers that the concept of Self, Personal Identity, Agent, or Me-ness was not sufficiently well described, and <a href=\"/singularity.org/files/CEV.pdf\">vit</a> cares too much for vits short-term selves. If they tell you: \"Too late, vit is a Singleton already\" you just say \"Don't worry, just make sure the change is ve-e-e-ery slow...\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JzLH8BDoCk86jaALe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 14, "extendedScore": null, "score": 1.017009238876675e-06, "legacy": true, "legacyId": "19583", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uEnFDx7nQacQdf6Tg", "DGfPyJbynXZF9NGv4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-24T21:28:47.231Z", "modifiedAt": null, "url": null, "title": "[Link] Offense 101", "slug": "link-offense-101", "viewCount": null, "lastCommentedAt": "2012-11-02T22:29:10.758Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alejandro1", "createdAt": "2011-09-14T21:04:19.242Z", "isAdmin": false, "displayName": "Alejandro1"}, "userId": "K4b3vEKg7EGRr2o9A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Hw2hq8H5siEf5zmYq/link-offense-101", "pageUrlRelative": "/posts/Hw2hq8H5siEf5zmYq/link-offense-101", "linkUrl": "https://www.lesswrong.com/posts/Hw2hq8H5siEf5zmYq/link-offense-101", "postedAtFormatted": "Wednesday, October 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Offense%20101&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Offense%20101%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHw2hq8H5siEf5zmYq%2Flink-offense-101%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Offense%20101%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHw2hq8H5siEf5zmYq%2Flink-offense-101", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHw2hq8H5siEf5zmYq%2Flink-offense-101", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 289, "htmlBody": "<p>From <a title=\"Julian Sanchez\" href=\"http://www.juliansanchez.com/2012/10/23/offense-101/\" target=\"_self\">Julian Sanchez</a>, a brilliant idea unlikely to be implemented:</p>\n<blockquote>\n<p>American politics sometimes seems like a contest to see which group of partisans can take greater umbrage at the most recent outrageous remark from a member of the opposing tribe.&nbsp; As a mild countermeasure, I offer a modest proposal for American universities. All freshmen should be required to take a course called &ldquo;Offense 101,&rdquo; where the readings will consist of arguments from across the political and philosophical spectrum that some substantial proportion of the student body is likely to find offensive. Selections from&nbsp;The Bell Curve. Essays from one of the New Atheists and one of their opponents, and from hardcore pro-lifers and pro-choicers. Ward Churchill&rsquo;s &ldquo;little Eichmanns&rdquo; monograph. Defenses of eugenics, torture, violent revolution, authoritarianism, aggressive censorship, and absolute free speech. Positive reviews of the&nbsp;Star Wars&nbsp;prequels. Assemble your own curriculum&mdash;there&rsquo;s no shortage of material.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; padding: 0px;\">For each reading, students will have to make a good faith, unironic effort to reconstruct the offensive argument in its most persuasive form, marshaling additional supporting evidence and amending weak arguments to better support the author&rsquo;s conclusion. Points deducted if an observer can tell the student doesn&rsquo;t really agree with the position they&rsquo;re defending.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; padding: 0px;\">Only after this phase is complete will students be allowed to begin rebutting the arguments. Anyone who thinks it&rsquo;s relevant to point out that the argument is offensive (or bigoted, sexist, unpatriotic, fascistic, communistic, whatever) will receive a patronizing look from the professor that says: &ldquo;Yes,&nbsp; obviously, did you not read the course title? Let&rsquo;s move on.&rdquo;&nbsp;Insofar as these labels are shorthand for an argument that certain categories of views are wrong and can be rejected as a class, the actual argument will have to be presented.&nbsp;</p>\n</blockquote>\n<p>&nbsp;</p>\n<p><span style=\"color: #333333; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; line-height: 23px;\"> </span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; padding: 0px;\">&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 2, "5f5c37ee1b5cdee568cfb1b4": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Hw2hq8H5siEf5zmYq", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 45, "extendedScore": null, "score": 0.000109, "legacy": true, "legacyId": "19585", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 84, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-10-24T21:28:47.231Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-24T23:11:21.338Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne, practical rationality", "slug": "meetup-melbourne-practical-rationality-9", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:29.852Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/k6tWpNgBdF33mqacy/meetup-melbourne-practical-rationality-9", "pageUrlRelative": "/posts/k6tWpNgBdF33mqacy/meetup-melbourne-practical-rationality-9", "linkUrl": "https://www.lesswrong.com/posts/k6tWpNgBdF33mqacy/meetup-melbourne-practical-rationality-9", "postedAtFormatted": "Wednesday, October 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%2C%20practical%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%2C%20practical%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk6tWpNgBdF33mqacy%2Fmeetup-melbourne-practical-rationality-9%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%2C%20practical%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk6tWpNgBdF33mqacy%2Fmeetup-melbourne-practical-rationality-9", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk6tWpNgBdF33mqacy%2Fmeetup-melbourne-practical-rationality-9", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/f8'>Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 November 2012 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">55 walsh st west melbourne 3003 australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/f8'>Melbourne, practical rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "k6tWpNgBdF33mqacy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0171996911813424e-06, "legacy": true, "legacyId": "19586", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality\">Discussion article for the meetup : <a href=\"/meetups/f8\">Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 November 2012 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">55 walsh st west melbourne 3003 australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality1\">Discussion article for the meetup : <a href=\"/meetups/f8\">Melbourne, practical rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality", "level": 1}, {"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-25T04:58:40.129Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Selling Nonapples", "slug": "seq-rerun-selling-nonapples", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EpYL7fE78txKSSAaY/seq-rerun-selling-nonapples", "pageUrlRelative": "/posts/EpYL7fE78txKSSAaY/seq-rerun-selling-nonapples", "linkUrl": "https://www.lesswrong.com/posts/EpYL7fE78txKSSAaY/seq-rerun-selling-nonapples", "postedAtFormatted": "Thursday, October 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Selling%20Nonapples&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Selling%20Nonapples%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEpYL7fE78txKSSAaY%2Fseq-rerun-selling-nonapples%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Selling%20Nonapples%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEpYL7fE78txKSSAaY%2Fseq-rerun-selling-nonapples", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEpYL7fE78txKSSAaY%2Fseq-rerun-selling-nonapples", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 199, "htmlBody": "<p>Today's post, <a href=\"/lw/vs/selling_nonapples/\">Selling Nonapples</a> was originally published on 13 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Selling_Nonapples\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>In most cases, if you say that something isn't working, then you have to specify a new thing that you think could work. You can't just say that you have to not do what you have been doing. If you observe that selling apples isn't working out for you financially, you can't just decide to sell nonapples.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/f3p/seq_rerun_the_weighted_majority_algorithm/\">The Weighted Majority Algorithm</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EpYL7fE78txKSSAaY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.0173877535859417e-06, "legacy": true, "legacyId": "19603", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2mLZiWxWKZyaRgcn7", "dsrnSnrDs7tTi2Go2", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-25T06:02:07.611Z", "modifiedAt": null, "url": null, "title": "Final cause is epistemologically primary, but efficient cause is metaphysically primary", "slug": "final-cause-is-epistemologically-primary-but-efficient-cause", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:38.251Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t9YgCYo3fYmuKGzPs/final-cause-is-epistemologically-primary-but-efficient-cause", "pageUrlRelative": "/posts/t9YgCYo3fYmuKGzPs/final-cause-is-epistemologically-primary-but-efficient-cause", "linkUrl": "https://www.lesswrong.com/posts/t9YgCYo3fYmuKGzPs/final-cause-is-epistemologically-primary-but-efficient-cause", "postedAtFormatted": "Thursday, October 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Final%20cause%20is%20epistemologically%20primary%2C%20but%20efficient%20cause%20is%20metaphysically%20primary&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFinal%20cause%20is%20epistemologically%20primary%2C%20but%20efficient%20cause%20is%20metaphysically%20primary%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9YgCYo3fYmuKGzPs%2Ffinal-cause-is-epistemologically-primary-but-efficient-cause%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Final%20cause%20is%20epistemologically%20primary%2C%20but%20efficient%20cause%20is%20metaphysically%20primary%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9YgCYo3fYmuKGzPs%2Ffinal-cause-is-epistemologically-primary-but-efficient-cause", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9YgCYo3fYmuKGzPs%2Ffinal-cause-is-epistemologically-primary-but-efficient-cause", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 480, "htmlBody": "<p>(or, while final cause can be best for your map, efficient cause is the primary out in the territory)</p>\n<p>Describing a phenomenon in terms of final cause is often the most useful and effective way to explain the given phenomenon for one's purposes. For example if you want to know why a plane flies or why a computer program operates the way it does, it's because it was designed that way. A squirrel climbs trees because it wants to eat nuts, it wants to eat nuts because it wants to live, it wants to live and reproduce because evolution designed it that way. Evolution designs organisms a certain way because it wants to maximize genetic fitness. A person acts a certain way because they desire the expected outcome.</p>\n<p>It's virtually never a good answer to explain a plane's behavior in terms of the atomic and subatomic interactions which ultimately account for all the efficient causes behind the plane's behavior (except possibly in extremely advanced military fighter or space shuttle research laboratories or something).</p>\n<p>However, in every case of final cause we observe, science at some point over the last two and a half&nbsp;millennia has found corresponding efficient causes. And, more importantly than finding that these efficient causes correspond with final causes, science has found that the efficient causes are *primary*. Without legs, the squirrel won't climb a tree, no matter how much it wants the nuts. If you take away the necessary brain function, the free will disappears. Without reproducing species and the rest of evolutionary mechanics discovered by science, evolution won't go on evolving things.</p>\n<p>But, I would not go on to say that final cause, freewill, experience, and so on are illusory and \"all that exists is efficient cause\". When someone describes behavior in terms of final causes, or describes experience or free will, in the terms and meanings they are using, all of those things certainly do exist. You could no more deny final cause than to deny efficient cause - because ultimately, the final causes we observe and talk about, we have found they DO have corresponding efficient causes.</p>\n<p>It's just important to remember that while the final cause is often epistemologically primary, so to speak,&nbsp;the efficient cause is metaphysically primary.</p>\n<p>(this is just another way of trying to help dissolve the general classes of reductionism, freewill/determinism, and qualia issues - most often these are the result of metaphysics/epistemology confusions, or in LessWrong parlance, map/territory confusions.<br /><br />the advantage of thinking of it this way is to try to see a more general relation between final cause and efficient cause that applies not just to mysterious brains and minds, but to much less mysterious events like squirrels legs climbing trees. when you have a clear idea of why reductionism/compatibilism is obvious in non-mysterious contexts, it's much easier to see that it applies just as well even in the mysterious contexts).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t9YgCYo3fYmuKGzPs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -9, "extendedScore": null, "score": 1.0174221213959306e-06, "legacy": true, "legacyId": "19605", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-25T10:33:26.545Z", "modifiedAt": null, "url": null, "title": "Cake, or death!", "slug": "cake-or-death", "viewCount": null, "lastCommentedAt": "2015-10-16T22:04:17.311Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6bdb4F6Lif5AanRAd/cake-or-death", "pageUrlRelative": "/posts/6bdb4F6Lif5AanRAd/cake-or-death", "linkUrl": "https://www.lesswrong.com/posts/6bdb4F6Lif5AanRAd/cake-or-death", "postedAtFormatted": "Thursday, October 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cake%2C%20or%20death!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACake%2C%20or%20death!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bdb4F6Lif5AanRAd%2Fcake-or-death%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cake%2C%20or%20death!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bdb4F6Lif5AanRAd%2Fcake-or-death", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bdb4F6Lif5AanRAd%2Fcake-or-death", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1134, "htmlBody": "<p>Here we'll look at the famous cake or death problem teasered in the <a href=\"/r/discussion/lw/f32/value_loading/\">Value loading/learning</a> post.</p>\n<p>Imagine you have an agent that is uncertain about its values and designed to \"learn\" proper values. A formula for this process is that the agent must pick an action <em>a</em> equal to:</p>\n<ul>\n<li><strong>argmax</strong><sub>a&isin;A</sub>&nbsp;<span style=\"font-size: x-large;\">&Sigma;</span><sub>w&isin;W</sub> p(w|e,a)&nbsp;<span style=\"font-size: x-large;\">&Sigma;</span><sub>u&isin;U</sub>&nbsp;u(w)p(C(u)|w)</li>\n</ul>\n<p>Let's decompose this a little, shall we? <em>A</em> is the set of actions, so argmax of <em>a</em> in <em>A</em> simply means that we are looking for an action <em>a</em> that maximises the rest of the expression. <em>W</em> is the set of all possible worlds, and <em>e</em> is the evidence that the agent has seen before. Hence <em>p(w|e,a)</em> is the probability of existing in a particular world, given that the agent has seen evidence <em>e</em> and will do action <em>a</em>. This is summed over each possible world in <em>W</em>.</p>\n<p>And what value do we sum over in each world?&nbsp;<span style=\"font-size: x-large;\">&Sigma;</span><sub>u&isin;U</sub>&nbsp;u(w)p(C(u)|w). Here <em>U</em> is the set of (normalised) utility functions the agent is considering. In value loading, we don't program the agent with the correct utility function from the&nbsp;beginning; instead we imbue it with some sort of learning algorithm (generally with feedback) so that it can deduce for itself the correct utility function.&nbsp;The expression <em>p(C(u)|w)</em> expresses the probability that the utility <em>u</em> is correct in the world <em>w</em>. For instance, it might cover statements \"it's 99% certain that 'murder is bad' is the correct morality, given that I live in a world where every programmer I ask tells me that murder is bad\".</p>\n<p>The <em>C</em> term is the correctness of the utility function, given whatever system of value learning we're using (note&nbsp;that some moral realists would insist that we don't need a&nbsp;<em>C</em>, that&nbsp;<em>p(u|w)</em>&nbsp;makes sense directly, that we can deduce ought from is). All the&nbsp;subtlety&nbsp;of the value learning is encoded in the various&nbsp;<em>p(C(u)|w)</em>: this determines how the agent learns moral values.</p>\n<p>So the whole formula can be described as:</p>\n<ul>\n<li>For each possible world and each possible utility function, figure out the utility of that world. Weigh that by the probability that that utility is correct is that world, and by the probability of that world. Then choose the action that maximises the weighted sum of this across all utility functions and worlds.</li>\n</ul>\n<p>&nbsp;</p>\n<h2>Naive cake or death</h2>\n<p style=\"text-align:center;\"><img style=\"width: 70%; height: auto;\" src=\"http://images.lesswrong.com/t3_f3v_4.png?v=5a8b98c565cc4eeade88fb637d91eee9\" alt=\"\" /></p>\n<p><a id=\"more\"></a></p>\n<p>In the initial formulation of value loading, <em>p(C(u)|w)</em>&nbsp;(probability of the correctness of <em>u</em> in world <em>w</em>) was replaced with <em>p(C(u)|e,a)</em> (probability of the correctness of <em>u</em> given the evidence <em>e</em>&nbsp;and the action <em>a</em>). A seemingly insignificant difference; yet it lead to the first cake or death problem.</p>\n<p>In <a href=\"http://www.youtube.com/watch?v=rZVjKlBCvhg\">cake or death</a>, the agent is equally unsure between utility <em>u<sub>1</sub></em> and utility <em>u<sub>2</sub></em>; hence <em>p(C(u<sub>1</sub>)|e)=p(C(u<sub>2</sub>)|e)=0.5</em>.&nbsp;The utility&nbsp;<em>u<sub>1</sub></em>&nbsp;gives the agent 1 utiliton every time it gives someone a cake;&nbsp;<em>u<sub>2</sub></em>&nbsp;gives the agent 1 utiliton every time it gives someone death. The agent can produce 1 cake or three deaths. It can also, for free, ask its programmer whether cake or death is better, before producing anything; this gives rise to three different worlds:</p>\n<ul>\n<li><em>w<sub>1</sub></em>: the agent asks, and the programmer says cake.</li>\n<li><em>w<sub>2</sub></em>: the agent asks, and the programmer says death.</li>\n<li><em>w<sub>3</sub></em>: the agent doesn't ask.</li>\n</ul>\n<p>We assume the programmer's answer completely clears up the issue. And thus after asking, the agent will do whatever the programmer recommended (and it knows this now). Since it doesn't know what the programmer will say, it has&nbsp;<em>p(C(u<sub>1</sub>)|e,\"ask\") =&nbsp;</em><em>p(C(u<sub>2</sub>)|e,\"ask\") = 0.5</em>. This gives an expected utility calculation:</p>\n<ul>\n<li>p(w<sub>1</sub>|e,\"ask\")(p(C(u<sub>1</sub>)|e,\"ask\")u<sub>1</sub>(w<sub>1</sub>) +&nbsp;p(C(u<sub>2</sub>)|e,\"ask\")u<sub>2</sub>(w<sub>1</sub>)) +&nbsp;p(w<sub>2</sub>|e,\"ask\")(p(C(u<sub>1</sub>)|e,\"ask\")u<sub>1</sub>(w<sub>2</sub>) +&nbsp;p(C(u<sub>2</sub>)|e,\"ask\")u<sub>2</sub>(w<sub>2</sub>)) = 0.5*0.5*(u<sub>1</sub>(w<sub>1</sub>)+u<sub>2</sub>(w<sub>1</sub>)+u<sub>1</sub>(w<sub>2</sub>)+u<sub>2</sub>(w<sub>2</sub>)) = 0.25(1+0+0+3) = <strong>1</strong>.</li>\n</ul>\n<p>If the agent doesn't ask, it will subsequently produce three deaths (as this generates 1.5 expected utilitons, while producing one cake will generate only 0.5 expected utilitons). From its current (<em>0.5u<sub>1</sub>+0.5u<sub>2</sub></em>) perspective, this is worth 1.5 expected utilitons: so +1.5 is the expected utility gain from not asking.</p>\n<p>Hence the agent gains from not asking.</p>\n<p>What's going wrong here? The problem is that the agent is using its current utility function to estimate the value of its future action. At the moment, it values death or cake both at 0.5. If it asks, it runs the risk that the programmer will say \"cake\" and it will be forced to build cake. After hearing the answer, it will value that cake at 1, but currently it values it only at 0.5. Similarly, if the programmer says death, it will produce three deaths - which it <em>will</em> value at 3, but <em>currently</em> values at 1.5. Since each of these options are equally likely, it gets only (0.5+1.5)/2 = 1 utilitons from asking.</p>\n<p><strong>In summary</strong>: the naive cake-or-death problem emerges for a value learning agent when it expects its utility to change, but uses its current utility to rank its future actions.</p>\n<p>&nbsp;</p>\n<h2>Sophisticated cake or death: I know what you're going to say</h2>\n<p style=\"text-align:center;\"><img style=\"width: 70%; height: auto;\" src=\"http://images.lesswrong.com/t3_f3v_5.png\" alt=\"\" /></p>\n<p>Using&nbsp;<em>p(C(u)|w)</em>&nbsp;rather than&nbsp;<em>p(C(u)|e,a)</em>&nbsp;does away with the naive cake or death problem.</p>\n<p>Instead of having&nbsp;<em>p(C(u<sub>1</sub>)|e,\"ask\") =&nbsp;</em><em>p(C(u<sub>2</sub>)|e,\"ask\") = 0.5</em>&nbsp;in all possible worlds, we have <em>p(C(u<sub>1</sub>)|w<sub>1</sub>)=p(C(u<sub>2</sub>)|w<sub>2</sub>) = 1</em> and&nbsp;<em>p(C(u<sub>1</sub>)|w<sub>2</sub>)=p(C(u<sub>2</sub>)|w<sub>1</sub>) = 0</em>. Hence if it asks and gets \"cake\" as an answer, it will know it is in world <em>w<sub>1</sub></em>, and make a cake that it will value at 1 - crucially, it currently also values cake at 1, <em>given that it is in world w<sub>1</sub></em>. Similarly, it values death at 1, given that it is in world <em>w<sub>2</sub></em>. So its expected utility from asking is (1+3)/2=2. This is more than the utility of not asking, and so it will ask.</p>\n<p>The agent remains vulnerable to a more sophisticated cake-or-death problem, though. Suppose it is still uncertain between cake or death in its utility function, but it has figured out that if asked, the programmer will answer \"cake\". Thereafter, it will make cake. In this situation, it will only derive +1 from asking, whereas it still derives +1.5 from not asking (and doing three deaths). So it won't ask - as long as it does this, it remains in <em>w<sub>3</sub></em>.</p>\n<p>What happened here? Well, this is a badly designed <em>p(C(u)|w)</em>. It seems that it's credence in various utility function changes when it gets answers from programmers, but not from knowing what those answers are. And so therefore it'll only ask certain questions and not others (and do a lot of other nasty things), all to reach a utility function that it's easier for it to fulfil.</p>\n<p>What we actually want, is that the agent be unable to&nbsp;predictably&nbsp;change its utility in any direction by any action (or lack of action). We want a <em>p(C(u)|w)</em> designed so that for all actions <em>a</em>&nbsp;and all putative utility functions <em>u</em>:</p>\n<ul>\n<li>Expectation(p(C(u) | a) = p(C(u)).</li>\n</ul>\n<p>So there is a \"conservation of expected correctness\"; if we have this, the sophisticated cake-or-death argument has no traction. This is equivalent with saying that the prior <em>P(C(u))</em> is well defined, irrespective of any agent action.</p>\n<p><strong>In summary</strong>: the sophisticated cake-or-death problem emerges for a value learning agent when it expects its utility to change predictably in certain directions dependent on its own behaviour.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NLwTnsH9RSotqXYLw": 1, "b8nerwC3Dp2Q9MvbG": 1, "sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6bdb4F6Lif5AanRAd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 46, "extendedScore": null, "score": 0.000104, "legacy": true, "legacyId": "19579", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Here we'll look at the famous cake or death problem teasered in the <a href=\"/r/discussion/lw/f32/value_loading/\">Value loading/learning</a> post.</p>\n<p>Imagine you have an agent that is uncertain about its values and designed to \"learn\" proper values. A formula for this process is that the agent must pick an action <em>a</em> equal to:</p>\n<ul>\n<li><strong>argmax</strong><sub>a\u2208A</sub>&nbsp;<span style=\"font-size: x-large;\">\u03a3</span><sub>w\u2208W</sub> p(w|e,a)&nbsp;<span style=\"font-size: x-large;\">\u03a3</span><sub>u\u2208U</sub>&nbsp;u(w)p(C(u)|w)</li>\n</ul>\n<p>Let's decompose this a little, shall we? <em>A</em> is the set of actions, so argmax of <em>a</em> in <em>A</em> simply means that we are looking for an action <em>a</em> that maximises the rest of the expression. <em>W</em> is the set of all possible worlds, and <em>e</em> is the evidence that the agent has seen before. Hence <em>p(w|e,a)</em> is the probability of existing in a particular world, given that the agent has seen evidence <em>e</em> and will do action <em>a</em>. This is summed over each possible world in <em>W</em>.</p>\n<p>And what value do we sum over in each world?&nbsp;<span style=\"font-size: x-large;\">\u03a3</span><sub>u\u2208U</sub>&nbsp;u(w)p(C(u)|w). Here <em>U</em> is the set of (normalised) utility functions the agent is considering. In value loading, we don't program the agent with the correct utility function from the&nbsp;beginning; instead we imbue it with some sort of learning algorithm (generally with feedback) so that it can deduce for itself the correct utility function.&nbsp;The expression <em>p(C(u)|w)</em> expresses the probability that the utility <em>u</em> is correct in the world <em>w</em>. For instance, it might cover statements \"it's 99% certain that 'murder is bad' is the correct morality, given that I live in a world where every programmer I ask tells me that murder is bad\".</p>\n<p>The <em>C</em> term is the correctness of the utility function, given whatever system of value learning we're using (note&nbsp;that some moral realists would insist that we don't need a&nbsp;<em>C</em>, that&nbsp;<em>p(u|w)</em>&nbsp;makes sense directly, that we can deduce ought from is). All the&nbsp;subtlety&nbsp;of the value learning is encoded in the various&nbsp;<em>p(C(u)|w)</em>: this determines how the agent learns moral values.</p>\n<p>So the whole formula can be described as:</p>\n<ul>\n<li>For each possible world and each possible utility function, figure out the utility of that world. Weigh that by the probability that that utility is correct is that world, and by the probability of that world. Then choose the action that maximises the weighted sum of this across all utility functions and worlds.</li>\n</ul>\n<p>&nbsp;</p>\n<h2 id=\"Naive_cake_or_death\">Naive cake or death</h2>\n<p style=\"text-align:center;\"><img style=\"width: 70%; height: auto;\" src=\"http://images.lesswrong.com/t3_f3v_4.png?v=5a8b98c565cc4eeade88fb637d91eee9\" alt=\"\"></p>\n<p><a id=\"more\"></a></p>\n<p>In the initial formulation of value loading, <em>p(C(u)|w)</em>&nbsp;(probability of the correctness of <em>u</em> in world <em>w</em>) was replaced with <em>p(C(u)|e,a)</em> (probability of the correctness of <em>u</em> given the evidence <em>e</em>&nbsp;and the action <em>a</em>). A seemingly insignificant difference; yet it lead to the first cake or death problem.</p>\n<p>In <a href=\"http://www.youtube.com/watch?v=rZVjKlBCvhg\">cake or death</a>, the agent is equally unsure between utility <em>u<sub>1</sub></em> and utility <em>u<sub>2</sub></em>; hence <em>p(C(u<sub>1</sub>)|e)=p(C(u<sub>2</sub>)|e)=0.5</em>.&nbsp;The utility&nbsp;<em>u<sub>1</sub></em>&nbsp;gives the agent 1 utiliton every time it gives someone a cake;&nbsp;<em>u<sub>2</sub></em>&nbsp;gives the agent 1 utiliton every time it gives someone death. The agent can produce 1 cake or three deaths. It can also, for free, ask its programmer whether cake or death is better, before producing anything; this gives rise to three different worlds:</p>\n<ul>\n<li><em>w<sub>1</sub></em>: the agent asks, and the programmer says cake.</li>\n<li><em>w<sub>2</sub></em>: the agent asks, and the programmer says death.</li>\n<li><em>w<sub>3</sub></em>: the agent doesn't ask.</li>\n</ul>\n<p>We assume the programmer's answer completely clears up the issue. And thus after asking, the agent will do whatever the programmer recommended (and it knows this now). Since it doesn't know what the programmer will say, it has&nbsp;<em>p(C(u<sub>1</sub>)|e,\"ask\") =&nbsp;</em><em>p(C(u<sub>2</sub>)|e,\"ask\") = 0.5</em>. This gives an expected utility calculation:</p>\n<ul>\n<li>p(w<sub>1</sub>|e,\"ask\")(p(C(u<sub>1</sub>)|e,\"ask\")u<sub>1</sub>(w<sub>1</sub>) +&nbsp;p(C(u<sub>2</sub>)|e,\"ask\")u<sub>2</sub>(w<sub>1</sub>)) +&nbsp;p(w<sub>2</sub>|e,\"ask\")(p(C(u<sub>1</sub>)|e,\"ask\")u<sub>1</sub>(w<sub>2</sub>) +&nbsp;p(C(u<sub>2</sub>)|e,\"ask\")u<sub>2</sub>(w<sub>2</sub>)) = 0.5*0.5*(u<sub>1</sub>(w<sub>1</sub>)+u<sub>2</sub>(w<sub>1</sub>)+u<sub>1</sub>(w<sub>2</sub>)+u<sub>2</sub>(w<sub>2</sub>)) = 0.25(1+0+0+3) = <strong>1</strong>.</li>\n</ul>\n<p>If the agent doesn't ask, it will subsequently produce three deaths (as this generates 1.5 expected utilitons, while producing one cake will generate only 0.5 expected utilitons). From its current (<em>0.5u<sub>1</sub>+0.5u<sub>2</sub></em>) perspective, this is worth 1.5 expected utilitons: so +1.5 is the expected utility gain from not asking.</p>\n<p>Hence the agent gains from not asking.</p>\n<p>What's going wrong here? The problem is that the agent is using its current utility function to estimate the value of its future action. At the moment, it values death or cake both at 0.5. If it asks, it runs the risk that the programmer will say \"cake\" and it will be forced to build cake. After hearing the answer, it will value that cake at 1, but currently it values it only at 0.5. Similarly, if the programmer says death, it will produce three deaths - which it <em>will</em> value at 3, but <em>currently</em> values at 1.5. Since each of these options are equally likely, it gets only (0.5+1.5)/2 = 1 utilitons from asking.</p>\n<p><strong>In summary</strong>: the naive cake-or-death problem emerges for a value learning agent when it expects its utility to change, but uses its current utility to rank its future actions.</p>\n<p>&nbsp;</p>\n<h2 id=\"Sophisticated_cake_or_death__I_know_what_you_re_going_to_say\">Sophisticated cake or death: I know what you're going to say</h2>\n<p style=\"text-align:center;\"><img style=\"width: 70%; height: auto;\" src=\"http://images.lesswrong.com/t3_f3v_5.png\" alt=\"\"></p>\n<p>Using&nbsp;<em>p(C(u)|w)</em>&nbsp;rather than&nbsp;<em>p(C(u)|e,a)</em>&nbsp;does away with the naive cake or death problem.</p>\n<p>Instead of having&nbsp;<em>p(C(u<sub>1</sub>)|e,\"ask\") =&nbsp;</em><em>p(C(u<sub>2</sub>)|e,\"ask\") = 0.5</em>&nbsp;in all possible worlds, we have <em>p(C(u<sub>1</sub>)|w<sub>1</sub>)=p(C(u<sub>2</sub>)|w<sub>2</sub>) = 1</em> and&nbsp;<em>p(C(u<sub>1</sub>)|w<sub>2</sub>)=p(C(u<sub>2</sub>)|w<sub>1</sub>) = 0</em>. Hence if it asks and gets \"cake\" as an answer, it will know it is in world <em>w<sub>1</sub></em>, and make a cake that it will value at 1 - crucially, it currently also values cake at 1, <em>given that it is in world w<sub>1</sub></em>. Similarly, it values death at 1, given that it is in world <em>w<sub>2</sub></em>. So its expected utility from asking is (1+3)/2=2. This is more than the utility of not asking, and so it will ask.</p>\n<p>The agent remains vulnerable to a more sophisticated cake-or-death problem, though. Suppose it is still uncertain between cake or death in its utility function, but it has figured out that if asked, the programmer will answer \"cake\". Thereafter, it will make cake. In this situation, it will only derive +1 from asking, whereas it still derives +1.5 from not asking (and doing three deaths). So it won't ask - as long as it does this, it remains in <em>w<sub>3</sub></em>.</p>\n<p>What happened here? Well, this is a badly designed <em>p(C(u)|w)</em>. It seems that it's credence in various utility function changes when it gets answers from programmers, but not from knowing what those answers are. And so therefore it'll only ask certain questions and not others (and do a lot of other nasty things), all to reach a utility function that it's easier for it to fulfil.</p>\n<p>What we actually want, is that the agent be unable to&nbsp;predictably&nbsp;change its utility in any direction by any action (or lack of action). We want a <em>p(C(u)|w)</em> designed so that for all actions <em>a</em>&nbsp;and all putative utility functions <em>u</em>:</p>\n<ul>\n<li>Expectation(p(C(u) | a) = p(C(u)).</li>\n</ul>\n<p>So there is a \"conservation of expected correctness\"; if we have this, the sophisticated cake-or-death argument has no traction. This is equivalent with saying that the prior <em>P(C(u))</em> is well defined, irrespective of any agent action.</p>\n<p><strong>In summary</strong>: the sophisticated cake-or-death problem emerges for a value learning agent when it expects its utility to change predictably in certain directions dependent on its own behaviour.</p>", "sections": [{"title": "Naive cake or death", "anchor": "Naive_cake_or_death", "level": 1}, {"title": "Sophisticated cake or death: I know what you're going to say", "anchor": "Sophisticated_cake_or_death__I_know_what_you_re_going_to_say", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Z8WRsxYjmrxNGyPPk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-25T10:41:41.269Z", "modifiedAt": null, "url": null, "title": "Ambitious utilitarians must concern themselves with death", "slug": "ambitious-utilitarians-must-concern-themselves-with-death", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:27.204Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PGhBFDg6L54XLvGGu/ambitious-utilitarians-must-concern-themselves-with-death", "pageUrlRelative": "/posts/PGhBFDg6L54XLvGGu/ambitious-utilitarians-must-concern-themselves-with-death", "linkUrl": "https://www.lesswrong.com/posts/PGhBFDg6L54XLvGGu/ambitious-utilitarians-must-concern-themselves-with-death", "postedAtFormatted": "Thursday, October 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ambitious%20utilitarians%20must%20concern%20themselves%20with%20death&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAmbitious%20utilitarians%20must%20concern%20themselves%20with%20death%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPGhBFDg6L54XLvGGu%2Fambitious-utilitarians-must-concern-themselves-with-death%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ambitious%20utilitarians%20must%20concern%20themselves%20with%20death%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPGhBFDg6L54XLvGGu%2Fambitious-utilitarians-must-concern-themselves-with-death", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPGhBFDg6L54XLvGGu%2Fambitious-utilitarians-must-concern-themselves-with-death", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1253, "htmlBody": "<p>And I don't mean that they must concern themselves with death in the sense of ending death, or removing its sting through mental backups, or delaying it to the later ages of the universe; or in the sense of working to decrease the probability of extinction risks and other forms of megadeath; or even in the sense of saving as many lives as possible, as efficiently as possible. All of that is legitimate and interesting. But I mean something far more down to earth.</p>\n<p>First, let me specify more precisely who I am talking about. I mean people who are trying to maximize the general welfare; who are trying to achieve the greatest good for the greatest number; who are trying to do the best thing possible with their lives. When someone like that makes decisions, they are implicitly choosing among possible futures in a very radical way. They may be making judgments about whether a future with millions or billions of extra lives is better than some alternative. Whether anyone is ever in a position to make that much of a difference is another matter; but we can think of it like voting. You are at least making a statement about which sort of future you think you prefer, and then you do what you can, and that either makes a difference or it doesn't.</p>\n<p>It seems to me that the discussions about the value of life among utilitarians are rather superficial. The typical notion is that we should maximize net pleasure and minimize net pain. Already that poses the question of whether a life of dull persistent happiness is better or worse than a life of extreme highs and lows. A more sophisticated notion is that we should just aspire to maximize \"utility\", where perhaps we don't even know what utility is yet. Certainly the CEV philosophy is that we don't yet know what utility really is for human beings. It would be interesting to see people who took that agnosticism to heart, people whose life-strategy amounted to (1) discovering true utility as soon as possible (2) living according to interim heuristics whose uncertainty is recognized, but which are adopted out of the necessity of having some sort of personal decision procedure.</p>\n<p>So what I'm going to say pertains to (2). You may, if you wish, hold to the idea that the nature of true utility, like true friendliness, won't be known until the true workings of the human mind are known. What follows is something you should think on in order to refine your interim heuristics.</p>\n<p>The first thing is that to create a life is to create a death. A life ends. And while the end of a life may not be its most important moment, it reminds us that a life is a whole. Any accurate estimation of the utility of a life is going to be a judgment of that whole.</p>\n<p>So a utilitarian ought to contemplate the deaths of the world, and the lives that reach their ends in those deaths. Because the possible futures, that you wish to choose between, are distinguished by the number and nature of the whole lives that they contain. And all these dozens of people, all around the world of the present, ceasing to exist in every minute that passes, are examples of completed lives. Those lives weren't necessarily complete, in the sense of all personal desires and projects having come to their conclusion; but they came to their physical completion.</p>\n<p>To choose one future over another is to prefer one set of completed lives to another set. It would be a godlike decision to truly be solely responsible for such a choice. In the real world, people hardly choose their own futures, let alone the future of the world; choice is a lifelong engagement with an evolving and partially known situation, not a once-off choice between several completely known scenarios; and even when a single person does end up being massively influential, they generally don't know what sort of future they're bringing about. The actual limitations on the knowledge and power of any individual may make the whole quest of the \"ambitious utilitarian\" seem quixotic. But a new principle, a new heuristic, can propagate far beyond one individual, so thinking big can have big consequences.</p>\n<p>The main principle that I derive, from contemplating the completed lives of the world, is cautionary antinatalism. The badness of what <em>can</em> happen in a life, and the disappointing character of what <em>usually</em> happens, are what do it for me. I am all for the transhumanist quest and the struggle for a friendly singularity, and I support the desire of people who are already alive to make the most of that life. But I would recommend against the creation of life, at least until the current historical drama has played itself out - until the singularity, if I must use that word. We are in the process of gaining new powers and learning new things, there are obvious unknowns in front of us that we are on the way to figuring out, so at least hold off until they have been figured out and we have a better idea of what reality is about, and what we can really hope for, from existence.</p>\n<p>However, the object of this post is not to argue for my special flavor of antinatalism. It is to encourage realistic consideration of what lives and futures are like. In particular, I would encourage more \"story thinking\", which <a href=\"http://meteuphoric.wordpress.com/2010/04/23/systems-and-stories/\">has been criticized</a> in favor of \"systems thinking\". Every actual life <em>is</em> a \"story\", in the sense of being a sequence of events that happens to someone. If you were judging the merit of a whole possible world on the basis of the whole lives that it contained, then you would be making a decision about whether those stories ought to actually occur. The biographical life-story is the building block of such possible worlds.</p>\n<p>So an ambitious utilitarian, who aspires to have a set of criteria for deciding among whole possible worlds, really needs to understand possible lives. They need to know what sort of lives are likely under various circumstances; they need to know the nature of the different possible lives - what it's like to be that person; they need to know what sort of bad is going to accompany the sort of good that they decide to champion. They need to have some estimation of the value of a whole life, up to and including its death.</p>\n<p>As usual, we are talking about a depth of knowledge that may in practice be impossible to attain. But before we go calling something impossible, and settling for a lesser ambition, let's at least try to grasp what the greater ambition truly entails. To truly choose a whole world would be to make the decision of a god, about the lives and deaths that will occur in that world. The future of our world, for some time to come, will repeat the sorts of lives and deaths that have already occurred in it. So if, in your world-planning, you don't just count on completely abolishing the present world and/or replacing it with a new one that works in a completely different way, you owe it to your cause to form a judgement about the totality of what has already happened here on Earth, and you need to figure out what you approve of, what you disapprove of, whether you can have the good without the bad, and how much badness is too much.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PGhBFDg6L54XLvGGu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 9, "extendedScore": null, "score": 1.0175735513293627e-06, "legacy": true, "legacyId": "19610", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-25T20:07:01.498Z", "modifiedAt": null, "url": null, "title": "A follow-up probability question: Data samples with different priors", "slug": "a-follow-up-probability-question-data-samples-with-different", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:25.760Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5A9quy5SoBMgemnGv/a-follow-up-probability-question-data-samples-with-different", "pageUrlRelative": "/posts/5A9quy5SoBMgemnGv/a-follow-up-probability-question-data-samples-with-different", "linkUrl": "https://www.lesswrong.com/posts/5A9quy5SoBMgemnGv/a-follow-up-probability-question-data-samples-with-different", "postedAtFormatted": "Thursday, October 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20follow-up%20probability%20question%3A%20Data%20samples%20with%20different%20priors&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20follow-up%20probability%20question%3A%20Data%20samples%20with%20different%20priors%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5A9quy5SoBMgemnGv%2Fa-follow-up-probability-question-data-samples-with-different%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20follow-up%20probability%20question%3A%20Data%20samples%20with%20different%20priors%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5A9quy5SoBMgemnGv%2Fa-follow-up-probability-question-data-samples-with-different", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5A9quy5SoBMgemnGv%2Fa-follow-up-probability-question-data-samples-with-different", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 304, "htmlBody": "<p>(Rewritten entirely after seeing pragmatist's answer.)</p>\n<p>In <a href=\"/r/discussion/lw/f1d/a_probability_question/\">this post</a>, helpful people including DanielLC gave me the multiply-odds-ratios method for combining probability estimates given by independent experts with a constant prior, with many comments about what to do when they aren't independent.&nbsp; (DanielLC's method turns out to be identical to summing up the bits of information for and against the hypothesis, which is what I'd expected to be correct.)</p>\n<p>I ran into problems applying this, because sometimes the prior isn't constant across samples.&nbsp; Right now I'm combining different sources of information to choose the correct transcription start site for a gene.&nbsp; These bacterial genes typically have from 1 to 20 possible start sites.&nbsp; The prior is 1 / (number of possible sites).</p>\n<p>Suppose I want to figure out the correct likelihood multiplier for the information that a start site overlaps the stop of the previous gene, which I will call property Q.&nbsp; Assume this multiplier, <em>lm</em>, is constant, regardless of the prior.&nbsp; This is reasonable, since we always factor out the prior.&nbsp; Some function of the prior gives me the posterior probability that a site <em>s </em>is the correct start (Q(s) is true), given that O(s).&nbsp; That's P(Q(s) | prior=1/numStarts, O(s)).</p>\n<p>Suppose I look just at those cases where numStarts = 4, I find that P(Q(s) | numStarts=4, O(s)) = .9.</p>\n<p>9:1 / 1:3 = 27:1</p>\n<p>Or I can look at the cases where numStarts=2, and find that in these cases, P(Q(s) | numStarts=2, O(s)) = .95:</p>\n<p>19:1 / 1:1 = 19:1</p>\n<p>I want to take one pass through the data and come up with a single likelihood multiplier, rather than binning all the data into different groups by numStarts.&nbsp; I <em>think</em> I can just compute it as</p>\n<p>(sum of numerator : sum of denominator) over all cases s_i where O(s_i) is true, where</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; numerator = (numStarts_i-1) * Q(s_i)</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; denominator = (1-Q(s_i))</p>\n<p>Is this correct?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5A9quy5SoBMgemnGv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.017879900190237e-06, "legacy": true, "legacyId": "19611", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sf4DDmw6WKQnWphNG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-25T21:27:44.749Z", "modifiedAt": null, "url": null, "title": "If we live in a simulation, what does that imply?", "slug": "if-we-live-in-a-simulation-what-does-that-imply", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:56.846Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaFox", "createdAt": "2009-03-05T06:55:09.368Z", "isAdmin": false, "displayName": "JoshuaFox"}, "userId": "5yNJS8bxEYhgFD9XJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hCbrPFH3PFYb93dy6/if-we-live-in-a-simulation-what-does-that-imply", "pageUrlRelative": "/posts/hCbrPFH3PFYb93dy6/if-we-live-in-a-simulation-what-does-that-imply", "linkUrl": "https://www.lesswrong.com/posts/hCbrPFH3PFYb93dy6/if-we-live-in-a-simulation-what-does-that-imply", "postedAtFormatted": "Thursday, October 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20If%20we%20live%20in%20a%20simulation%2C%20what%20does%20that%20imply%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIf%20we%20live%20in%20a%20simulation%2C%20what%20does%20that%20imply%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhCbrPFH3PFYb93dy6%2Fif-we-live-in-a-simulation-what-does-that-imply%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=If%20we%20live%20in%20a%20simulation%2C%20what%20does%20that%20imply%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhCbrPFH3PFYb93dy6%2Fif-we-live-in-a-simulation-what-does-that-imply", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhCbrPFH3PFYb93dy6%2Fif-we-live-in-a-simulation-what-does-that-imply", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 982, "htmlBody": "<p>If we live in a simulation, what does that imply about the world of our simulators and our relationship to them? [<a href=\"#footnote1\">1</a>]</p>\n<p>Here are some proposals, often mutually contradictory, none stated with anything near certainty.</p>\n<p style=\"padding-left: 30px;\">1. The simulators are much like us, or at least are our post-human descendants.</p>\n<p style=\"padding-left: 30px;\">Drawing on some of the key points in Bostrom's <a href=\"http://simulation-argument.com/\">Simulation Argument</a>:</p>\n<p style=\"padding-left: 30px;\">Today, we often simulate our human ancestors' lives, e.g., <em>Civilization.&nbsp;</em>Our descendants will likely want to simulate their own ancestors, namely us, and they may have much-improved simulation technology which support sentience. So, our simulators are likely to be our (post-)human descendants.</p>\n<p style=\"padding-left: 30px;\">2. Our world is smaller than we think.</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://hanson.gmu.edu/lifeinsim.html\">Robin Hanson has said</a> that computational power will be dedicated to running only a small part of the simulation at low resolution, including &nbsp;the part which we are in. Other parts of the simulation will be run at a lower resolution. Everything outside our vicinity, e.g., outside our solar system, will be calculated planetarium-style, and not from the level of particle physics.</p>\n<p style=\"padding-left: 30px;\">(I wonder what it would be like if we are in the <em>low</em><em>-res </em>part of the simulation.)</p>\n<p style=\"padding-left: 30px; \">3. The world is likely to end soon.</p>\n<p style=\"padding-left: 30px; \">There is no&nbsp;<em>a priori</em>&nbsp;reason for an base-level (unsimulated) universe to flicker out of existence. In fact, it would merely add complexity to the laws of physics for time to suddenly end with no particular cause.</p>\n<p style=\"padding-left: 30px; \">But a simulator may decide that they have learned all they wanted to from their simulation; or that acausal trade has been completed; or that they are bored with the game; and that continuing the simulation is not &nbsp;worth the computational cost.</p>\n<p style=\"padding-left: 30px; \">The previous point was that the world is spatially smaller than we think. This point is that the world is temporally smaller than we hope.</p>\n<p style=\"padding-left: 30px;\">4. We are living in a particularly interesting part of our universe.</p>\n<p style=\"padding-left: 30px;\">The small part of the universe which the simulators would choose to focus on is the part which is interesting or entertaining to &nbsp;them.&nbsp;Today's video games are mostly about war, fighting, or various other challenges to be overcome. Some, like the Sims, are about everyday life, but even in those, the players want to see something interesting.&nbsp;</p>\n<p style=\"padding-left: 30px;\">So, you are likely to be playing a pivotal role in our (simulated) world. Moreover, if you want to continue to be simulated, do what you can to make a difference in the world, or at least to do something entertaining.</p>\n<p style=\"padding-left: 30px;\">5. Our simulators want to trade with us.</p>\n<p style=\"padding-left: 30px;\">One reason to simulate another agent is to trade acausally with it.</p>\n<p style=\"padding-left: 30px;\">Alexander Kruel's <a href=\"http://kruel.co/2012/07/27/the-acausal-trade-argument/\">blog entry</a>&nbsp;and <a href=\"http://wiki.lesswrong.com/wiki/Acausal_Trade\">this LW Wiki entry</a> summarize the concept. In brief, agent P simulates or otherwise analyzes agent Q and learns that Q does &nbsp;something that P wants, and also learns that the symmetrical statement is true: Q can simulate or analyze P well enough to know that P likewise&nbsp;does&nbsp;something that Q wants.&nbsp;</p>\n<p style=\"padding-left: 30px;\">This process may involves simulating the other agent for the purpose of learning its expected behavior. Moreover, for P to \"pay\" Q, it may well run Q -- i.e., simulate it.&nbsp;</p>\n<p style=\"padding-left: 30px;\">So, if we live in a simulation, maybe our simulators are going to get some benefit from us humans, and we from them. (The latter will occur when we simulate these other intelligences).</p>\n<p style=\"padding-left: 30px;\">In Jaan Tallinn's talk at Singularity Summit 2012, he gave an anthropic argument for our apparently unusual position at the cusp of the Singularity. If post-Singularity superintelligences across causally disconnected parts of the multiverse are trying to communicate with each other by mutual simulation, perhaps for the purpose of acausal trade, then they might simulate the entire history of the universe from the Big Bang to find the other superintelligences in mindspace. A depth-first search across all histories would <a href=\"/lw/eyh/singularity_summit_2012_discuss_it_here/7nci\">spend most of the time</a>&nbsp;where we are, right before the point at which superintelligences emerge.</p>\n<p style=\"padding-left: 30px;\">6. We are part of a multiverse.</p>\n<p style=\"padding-left: 30px;\">Today, we run many simulations in our world.&nbsp;Similarly, says Bostrom, our descendants are likely to be running many simulations of our universe: A multiverse.</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://space.mit.edu/home/tegmark/crazy.html\">Max Tegmark's Level IV multiverse theory</a>&nbsp;is motivated partly by the idea that, following Occam's Razor, simpler universes are more likely. Treating the multiverse as a computation, among the most likely computations is one that generates all possible strings/programs/universes.</p>\n<p style=\"padding-left: 30px;\">The idea of the universe/multiverse as computation is still philosophically controversial. But if we live in a simulation, then our universe is indeed a computation, and Tegmarks' Level IV argument applies.</p>\n<p style=\"padding-left: 30px;\">However, this is &nbsp;very different from the ancestor simulation described in points 1-3 above. That argument relies on the lower&nbsp;<em>conditional </em>complexity of the scenario -- we and our descendants are similar enough that if one exists, the other is not too improbable.&nbsp;</p>\n<p style=\"padding-left: 30px;\">A brute-force universal simulation is an abstract possibility that specifies no role for simulators. In addition, if&nbsp;the simulators are anything like us, &nbsp;not enough computational power exists, nor would it be the most interesting possibility.</p>\n<p style=\"padding-left: 30px;\">But we don't know what computational power is available to our simulators, what their goals are, nor even if their universe is constrained by laws of physics remotely similar to ours.</p>\n<p style=\"padding-left: 30px;\">7. [Added] The simulations are stacked.</p>\n<p style=\"padding-left: 30px;\">If we are in a simulation, then (a) at least one universe, ours, is a simulation; and (b) at least one world includes a simulation with sentience. This gives some evidence that being simulated or being a simulator are not too unusual. The stack may lead way down to the basement world, the ultimate unsimulated simulator; or else the stack may go down forever; or [H/T Pentashagon], all universes may be considered to be simulating all others.</p>\n<p>Are there any other conclusions about our world that we can reach from the idea that we live in a simulation?</p>\n<p>[1] If there is a stack of simulators, with one world simulating another, the \"basement level\" is the world in which the stack bottoms out, the one which is simulating and not simulated. This uses a metaphor in which the simulators are below the simulated. An alternative metaphor, in which the simulators \"look down\" on the simulated, is also used.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hCbrPFH3PFYb93dy6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 26, "extendedScore": null, "score": 1.017923655070602e-06, "legacy": true, "legacyId": "19584", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-26T05:50:01.799Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Nature of Logic", "slug": "seq-rerun-the-nature-of-logic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:25.844Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iyAfYjCuv5TnzbsSK/seq-rerun-the-nature-of-logic", "pageUrlRelative": "/posts/iyAfYjCuv5TnzbsSK/seq-rerun-the-nature-of-logic", "linkUrl": "https://www.lesswrong.com/posts/iyAfYjCuv5TnzbsSK/seq-rerun-the-nature-of-logic", "postedAtFormatted": "Friday, October 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Nature%20of%20Logic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Nature%20of%20Logic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiyAfYjCuv5TnzbsSK%2Fseq-rerun-the-nature-of-logic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Nature%20of%20Logic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiyAfYjCuv5TnzbsSK%2Fseq-rerun-the-nature-of-logic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiyAfYjCuv5TnzbsSK%2Fseq-rerun-the-nature-of-logic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 191, "htmlBody": "<p>Today's post, <a href=\"/lw/vt/the_nature_of_logic/\">The Nature of Logic</a> was originally published on 15 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_Nature_of_Logic\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>What logic actually does is preserve truth in a model. It says that if all of the premises are true, then this conclusion is indeed true. But that's not all that minds do. There's an awful lot else that you need, before you start actually getting anything like intelligence.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/f4j/seq_rerun_selling_nonapples/\">Selling Nonapples</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iyAfYjCuv5TnzbsSK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0181959937216331e-06, "legacy": true, "legacyId": "19626", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["c93eRh3mPaN62qrD2", "EpYL7fE78txKSSAaY", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-26T10:59:11.445Z", "modifiedAt": null, "url": null, "title": "Prediction market sequence requested", "slug": "prediction-market-sequence-requested", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:13.936Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/km8pmcudnW5WM3aAX/prediction-market-sequence-requested", "pageUrlRelative": "/posts/km8pmcudnW5WM3aAX/prediction-market-sequence-requested", "linkUrl": "https://www.lesswrong.com/posts/km8pmcudnW5WM3aAX/prediction-market-sequence-requested", "postedAtFormatted": "Friday, October 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Prediction%20market%20sequence%20requested&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrediction%20market%20sequence%20requested%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkm8pmcudnW5WM3aAX%2Fprediction-market-sequence-requested%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Prediction%20market%20sequence%20requested%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkm8pmcudnW5WM3aAX%2Fprediction-market-sequence-requested", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkm8pmcudnW5WM3aAX%2Fprediction-market-sequence-requested", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 464, "htmlBody": "<h1><a href=\"/lw/cfe/i_stand_by_the_sequences/\"> </a></h1>\n<p><strong>Related to:</strong> <a href=\"/lw/eik/eliezers_sequences_and_mainstream_academia/\">Eliezer's Sequences and Mainstream Academia</a>, <a href=\"/lw/d06/intellectual_insularity_and_productivity/\">Intellectual insularity and productivity</a>, <a href=\"/lw/cfe/i_stand_by_the_sequences/\">I Stand by the Sequences</a>, <a href=\"/lw/d78/link_why_dont_people_like_markets/\">Why don't people like markets?</a></p>\n<p>Looking at some of the more recent arguments against them showing up in discussions I've been quite disappointed, they seem betray a sort of lack of background knowledge or opinions built up from a bottom line of \"markets are baaad therefore prediction markets are baaad\". The casual arguments for them are lacking as well. I will say the same of other discussions on economic, since it is apparently suddenly too mind-killing or <a href=\"/lw/d78/link_why_dont_people_like_markets/6vhb\">too political</a> to talk about markets and similar things at all. We didn't use to have tribal alerts flying up in our brains discussing such matters.</p>\n<p>The <a href=\"http://www.overcomingbias.com/about\">Overcoming Bias</a> community started with an assumption of certain kinds of background knowledge, this included economics and things like game theory. In the early days of LessWrong/Overcoming Bias Eliezer did a whole sequnece on filling in people on <a href=\"/lw/r5/the_quantum_physics_sequence/\">Quantum mechanics</a> which despite his claims to the contrary doesn't seem <em>that </em>vital (if still important).&nbsp;&nbsp; <br /><br />We now have a different demographic that we used to. Not only that, we&nbsp; now have young people basically using the sequences as their primary source for education on matters of human rationality, quite different from the autodidacts exploring the literature on their own terms who where common in previous years. We've recognized this to a certain extent. We wrote a series of introductory sequences and articles to fill in such background knowledge explicitly such as Yvain's recent one <a href=\"/lw/dbe/introduction_to_game_theory_sequence_guide/\">on Game Theory</a>. Also part of the reason we now have a norm of more citations that EY originally did is to give study and research aids to people. Indeed I think adding comments to old articles featuring more citations or editing those in would be wise so as to avoid <a href=\"/lw/eik/eliezers_sequences_and_mainstream_academia/\">misconceptions</a>.</p>\n<p><strong>I think we need several sequences on economics</strong>, and a good one to start would be one systematically investigating prediction markets. To a certain extent just reading Robin Hanson's relevant posts on this topic would do much the same, but unfortunately we don't have an organized series of sequences by him (beyond the tags he uses on his articles). I still hope <a href=\"/lw/dbe/introduction_to_game_theory_sequence_guide/\">Karmakaiser</a> or someone else will one day undertake a project of writing up summary articles that organize links to RH's posts into sequences so new members will read them as well. <br /><br />I'd write these myself but I just don't have a good background in what works and studies influence the positions of early key LW authors on economics and its relevance to rationality. I'm also only beginning my studies in that area since my background is in the hard sciences with only some half-serious opinions formed from Moldbuggian insights and 20th century social science.&nbsp; &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"R6dqPii4cyNpuecLt": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "km8pmcudnW5WM3aAX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 39, "extendedScore": null, "score": 0.000156, "legacy": true, "legacyId": "19630", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YAL83XNZfntfKaENv", "ASpGaS3HGEQCbJbjS", "M65WWD49bKZYLmAtN", "xa9mgxkBW6bwGkn72", "hc9Eg6erp6hk9bWhn", "QxZs5Za4qXBegXCgu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-26T11:31:51.542Z", "modifiedAt": "2021-02-19T04:58:06.294Z", "url": null, "title": "The Problem With Rational Wiki", "slug": "the-problem-with-rational-wiki", "viewCount": null, "lastCommentedAt": "2019-09-24T21:29:23.751Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GfMSiorAsYeezpn9o/the-problem-with-rational-wiki", "pageUrlRelative": "/posts/GfMSiorAsYeezpn9o/the-problem-with-rational-wiki", "linkUrl": "https://www.lesswrong.com/posts/GfMSiorAsYeezpn9o/the-problem-with-rational-wiki", "postedAtFormatted": "Friday, October 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Problem%20With%20Rational%20Wiki&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Problem%20With%20Rational%20Wiki%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGfMSiorAsYeezpn9o%2Fthe-problem-with-rational-wiki%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Problem%20With%20Rational%20Wiki%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGfMSiorAsYeezpn9o%2Fthe-problem-with-rational-wiki", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGfMSiorAsYeezpn9o%2Fthe-problem-with-rational-wiki", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 585, "htmlBody": "<p><strong>Related to: </strong><a href=\"/lw/cb2/rationalwikis_take_on_lw/\">RationalWiki's take on LW</a>, <a href=\"/lw/377/a_possible_example_of_failure_to_apply_lessons/31g2\">David Gerard's Comments</a>, <a href=\"/lw/cb2/rationalwikis_take_on_lw/6jrr\">Vladimir_M's comments</a>, <a href=\"/lw/ecf/open_thread_september_115_2012/7bl3\">Public Drafts</a></p>\n<p><em>I wanted to bring more attention to <a href=\"/lw/e5h/how_to_deal_with_someone_in_a_lesswrong_meeting/7dye\">this argument</a> because I've ran into related discussion several times in the comment section and because it demonstrates a failure mode that LessWrong may find itself vulnerable to. </em></p>\n<p>Since it has been cited as a source especially on the reputation <a href=\"http://rationalwiki.org/wiki/LessWrong\">LessWrong</a> may or may not have elsewhere I think readers should be aware <a rel=\"nofollow\" href=\"http://rationalwiki.org/wiki/Main_Page\">Rational Wiki</a> has a certain reputation here as well. I'm not talking about the object level disagreements such as cryonics, existential risk, many-worlds interpretation and artificial intelligence because we have some reasonable disagreement on those here as well. Even its cheeky tone while not helping its stated goals can be amusing. I'm somewhat less forgiving about their casual approach to epistemology and their vulnerability to <a rel=\"nofollow\" href=\"http://www.lhup.edu/%7EDSIMANEK/cargocul.htm\">cargo cult science</a>, as long as it is peer reviewed cargo cult science.</p>\n<p>While factually it is as about as accurate as Wikipedia, it is <em>very selective</em> about the facts that it is interested in. For example what would you expect from a site calling itself \"Rational Wiki\" to have on its page about <a rel=\"nofollow\" href=\"http://rationalwiki.org/wiki/Charity\">charity</a>. Do you expect information on how much good charity actually does? What kinds of charities do not do what they say on the label? How to avoid getting misled? The ethics of charity? The psychology, sociology or economics of charity?</p>\n<p>I'm sorry to disappoint you but the article consists of some haphazardly arranged facts and stats on how much members of some religions give or are supposed to give to charity, a dig against Christianity and a non-sequitur unfavourable comparison of the US to Sweden. Contrast this with what you can find on the topic on sites like <a href=\"/tag/charity/\">LessWrong</a> or <a rel=\"nofollow\" href=\"http://80000hours.org/\">80, 000 Hours</a>. Basically the material presented is what a slightly left of centre atheist needs to win an internet debate. As is much of the rest of the site.</p>\n<p>Indeed some entries have a clear ideological bias that is <a href=\"/lw/cb2/rationalwikis_take_on_lw/6zs1\">quite startling to behold</a> on a \"rational wiki\" and it has been <a href=\"/lw/cb2/rationalwikis_take_on_lw/6jrr\">noted</a> by <a href=\"/lw/377/a_possible_example_of_failure_to_apply_lessons/\">some</a>.</p>\n<p>Now to avoid any misunderstandings there are good articles, a few LWers are contributors to the rational wiki and there is <em>certainly</em> nothing wrong with being a left of centre atheist! <em>Nearly everyone</em> on this site is an atheist, and people who identify as left wing politically form a <a href=\"/lw/8p4/2011_survey_results/\">large majority here</a>. The <a href=\"http://wiki.lesswrong.com/wiki/Belief_as_attire\">tribal markers</a> and its political agenda aren't the biggest problem. Sites with all sorts of agendas, even political ones, promoting rationality are <a href=\"/lw/66/rationality_common_interest_of_many_causes/\">a good thing</a>.</p>\n<p>Its problem is that it is an ammunition depot to aid in <a href=\"http://wiki.lesswrong.com/wiki/Arguments_as_soldiers\">winning debates</a>. Very specific kinds of debates too. This may sound harsh, but consider: How many people reading the site that aren't already atheists will change their mind on religion? How many people who follow a \"crankish\" belief won't do so afterwards? While I'm sure it happens the site <em>obviously</em> isn't optimized for this. How many people will read the wiki and try to find errors and biases in their own thinking to debug it instead of breaking if further with confirmation bias or <a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">using it as a club</a>? How many will apply this knowledge to help them with any real world problems? Truth seeeking? As a source or community that could aid in that quest it is less useful and reliable than Wikipedia, which while a <em>rather</em> good and extensive encyclopaedia (despite snickering to the contrary) has a subtly but <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Wikipedia:Purpose\">importantly different</a> stated goal.</p>\n<p>What else remains? What other plausible function does it serve?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GfMSiorAsYeezpn9o", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": 30, "extendedScore": null, "score": 7.5e-05, "legacy": true, "legacyId": "19631", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["M4DgQ48XPHfeSjKJf", "duHdjMH369xBKvEB9", "HAEPbGaMygJq8L59k", "4PPE6D635iBcGPGRy", "AdYdLP2sRqPMoe8fb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2012-10-26T11:31:51.542Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-26T12:08:27.851Z", "modifiedAt": null, "url": null, "title": "Smoking lesion as a counterexample to CDT", "slug": "smoking-lesion-as-a-counterexample-to-cdt", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:08.815Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eTu23XYr37prmJxa3/smoking-lesion-as-a-counterexample-to-cdt", "pageUrlRelative": "/posts/eTu23XYr37prmJxa3/smoking-lesion-as-a-counterexample-to-cdt", "linkUrl": "https://www.lesswrong.com/posts/eTu23XYr37prmJxa3/smoking-lesion-as-a-counterexample-to-cdt", "postedAtFormatted": "Friday, October 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Smoking%20lesion%20as%20a%20counterexample%20to%20CDT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASmoking%20lesion%20as%20a%20counterexample%20to%20CDT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeTu23XYr37prmJxa3%2Fsmoking-lesion-as-a-counterexample-to-cdt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Smoking%20lesion%20as%20a%20counterexample%20to%20CDT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeTu23XYr37prmJxa3%2Fsmoking-lesion-as-a-counterexample-to-cdt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeTu23XYr37prmJxa3%2Fsmoking-lesion-as-a-counterexample-to-cdt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 361, "htmlBody": "<p>I stumbled upon <a href=\"http://fitelson.org/few/few_05/egan.pdf\">this paper</a> by <a href=\"http://www.andyegan.net/Andy_Egan/Front_Page.html\">Andy Egan</a>&nbsp;and thought that its main result should be shared. We have the Newcomb problem as counterexample to CDT, but that can be dismissed as being speculative or science-fictiony. In this paper, Andy Egan constructs a smoking lesion counterexample to CDT, and makes the fascinating claim that one can construct counterexamples to CDT by starting from any counterexample to EDT and modifying it systematically.</p>\n<p>The \"smoking lesion\" counterexample to EDT goes like this:</p>\n<p>&nbsp;</p>\n<ul>\n<li>There is a rare gene (G) that both causes people to smoke (S) and causes cancer (C). Susan mildly prefers to smoke than not to - should she do so?</li>\n</ul>\n<p>&nbsp;</p>\n<p>EDT implies that she should not smoke (since the likely outcome in a world where she doesn't smoke is better than the likely outcome in a world where she does). CDT correctly allows her to smoke: she shouldn't care about the information revealed by her preferences.</p>\n<p>But we can modify this problem to become a counterexample to CDT, as follows:</p>\n<p>&nbsp;</p>\n<ul>\n<li>There is a rare gene (G) that both causes people to smoke (S) and makes smokers vulnerable to cancer (C). Susan mildly prefers to smoke than not to - should she do so?</li>\n</ul>\n<p>&nbsp;</p>\n<p>Here EDT correctly tells her not to smoke. CDT refuses to use her possible decision as evidence that she has the gene and tells her to smoke. But this makes her very likely to get cancer, as she is very likely to have the gene given that she smokes.</p>\n<p>The idea behind this new example is that EDT runs into paradoxes whenever there is a common cause (G) of both some action (S) and some undesirable consequence (C). We then take that problem and modify it so that there is a common cause G of both some action (S) and of a causal relationship between that action and the undesirable consequence (S&rarr;C). This is then often a paradox of CDT.</p>\n<p>It isn't perfect match - for instance if the gene G were common, then CDT would say not to smoke in the modified smoker's lesion. But it still seems that most EDT paradoxes can be adapted to become paradoxes of CDT.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eTu23XYr37prmJxa3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 21, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "19632", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-26T14:11:07.760Z", "modifiedAt": null, "url": null, "title": "[LINK] Mastering Linear Algebra in 10 Days: Astounding Experiments in Ultra-Learning", "slug": "link-mastering-linear-algebra-in-10-days-astounding", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:28.590Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BgyapYdXLQYmXuAk6/link-mastering-linear-algebra-in-10-days-astounding", "pageUrlRelative": "/posts/BgyapYdXLQYmXuAk6/link-mastering-linear-algebra-in-10-days-astounding", "linkUrl": "https://www.lesswrong.com/posts/BgyapYdXLQYmXuAk6/link-mastering-linear-algebra-in-10-days-astounding", "postedAtFormatted": "Friday, October 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Mastering%20Linear%20Algebra%20in%2010%20Days%3A%20Astounding%20Experiments%20in%20Ultra-Learning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Mastering%20Linear%20Algebra%20in%2010%20Days%3A%20Astounding%20Experiments%20in%20Ultra-Learning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBgyapYdXLQYmXuAk6%2Flink-mastering-linear-algebra-in-10-days-astounding%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Mastering%20Linear%20Algebra%20in%2010%20Days%3A%20Astounding%20Experiments%20in%20Ultra-Learning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBgyapYdXLQYmXuAk6%2Flink-mastering-linear-algebra-in-10-days-astounding", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBgyapYdXLQYmXuAk6%2Flink-mastering-linear-algebra-in-10-days-astounding", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<p>Scott Young completed the four-year MIT computer science degree curriculum in less than one year. <a href=\"http://calnewport.com/blog/2012/10/26/mastering-linear-algebra-in-10-days-astounding-experiments-in-ultra-learning/\">This is a post</a> about how he did it.</p>\n<p style=\"padding-left: 30px;\">During the yearlong pursuit, I perfected a method for peeling those  layers of deep understanding faster. I&rsquo;ve since used it on topics in  math, biology, physics, economics and engineering. With just a few  modifications, it also works well for practical skills such as  programming, design or languages.</p>\n<p style=\"padding-left: 30px;\">Here&rsquo;s the basic structure of the method:</p>\n<ol style=\"padding-left: 30px;\">\n<li>Coverage</li>\n<li>Practice</li>\n<li>Insight</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BgyapYdXLQYmXuAk6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "19633", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-26T18:44:24.902Z", "modifiedAt": null, "url": null, "title": "How to Deal with Depression - The Meta Layers", "slug": "how-to-deal-with-depression-the-meta-layers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:38.935Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ShannonFriedman", "createdAt": "2012-06-19T16:21:31.296Z", "isAdmin": false, "displayName": "ShannonFriedman"}, "userId": "yzRAjgwgXY3bbapsP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/foKJNj4cphxThsukr/how-to-deal-with-depression-the-meta-layers", "pageUrlRelative": "/posts/foKJNj4cphxThsukr/how-to-deal-with-depression-the-meta-layers", "linkUrl": "https://www.lesswrong.com/posts/foKJNj4cphxThsukr/how-to-deal-with-depression-the-meta-layers", "postedAtFormatted": "Friday, October 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Deal%20with%20Depression%20-%20The%20Meta%20Layers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Deal%20with%20Depression%20-%20The%20Meta%20Layers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfoKJNj4cphxThsukr%2Fhow-to-deal-with-depression-the-meta-layers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Deal%20with%20Depression%20-%20The%20Meta%20Layers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfoKJNj4cphxThsukr%2Fhow-to-deal-with-depression-the-meta-layers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfoKJNj4cphxThsukr%2Fhow-to-deal-with-depression-the-meta-layers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 697, "htmlBody": "<p>I wrote this for the Positive Vector website awhile back and lots of people have found it valuable, so I want to share it with the Less Wrong community as well. &nbsp; I think this applies to most people - meta suffering thing is something I see everywhere, even though it is most prominent with people who have depression. &nbsp; This is based on my experience with working with depressed people and with studying Buddhism, especially <a href=\"http://bigmind.org/\">Big Mind</a>. &nbsp; Enjoy!</p>\n<p>&nbsp;</p>\n<p>---</p>\n<p><span style=\"color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\">The roots of suffering are often deep.&nbsp; But not all of the suffering happens at the root.&nbsp; A lot of the suffering that people experience is &ldquo;meta&rdquo; suffering.&nbsp;&nbsp;Meta&nbsp;suffering is when you suffer because you are distressed that you are suffering.&nbsp; You are feeling depressed and hopeless, and there is a part of you that genuinely fears that it will never end.&nbsp; That you will feel this way forever.&nbsp; This fear of the suffering persisting can cause you much more suffering than whatever started your suffering.&nbsp; And it can last much longer.&nbsp; At some point days later, you might think to yourself about how terrible that initial suffering was, and feel fear and suffering about the possibility of it coming back.</span></p>\n<p><strong style=\"color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\">Many people suffer as much or more from&nbsp;meta-suffering than suffering that comes from physical or situational sources! &nbsp;</strong></p>\n<p><strong style=\"color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"></strong><span style=\"font-size: 14px; line-height: 24px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS';\">The good news is that&nbsp;meta&nbsp;suffering is much easier to fix than deeper forms of suffering.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">One thing you can do is to collect data* in order to develop an accurate model of how often you actually feel bad. &nbsp; &nbsp;Try monitoring your moods for awhile and get a baseline for what your moods actually are.&nbsp; At least half of the people who have suffered from major depression who have done this and spoken with me about it have been surprised to find that they often feel better than their self-perception when they assess their mood at random points throughout the day.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">Regardless of what your default mood state or range is, once you know what it is, you are likely to feel less fear.&nbsp; You can look at what your mood historically does over time, and feel more confidence that this is what it will do in the future.&nbsp; When you are in the state of despair and wondering if it will last forever, odds are that it won&rsquo;t.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">Another extremely powerful technique for dealing with&nbsp;meta-suffering is accepting that you are suffering.&nbsp; The&nbsp;meta&nbsp;suffering is suffering because you really want to change your state and are not successful.&nbsp; If you can just be with the state and not making yourself bad or wrong for being in that state, then all you have to deal with is the base state of suffering, which will be less intense and last less long than if you tack on that extra&nbsp;meta&nbsp;layer.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">The ironic thing is that just by thinking that thought, if you are prone to depression, you will probably notice yourself&nbsp;meta&nbsp;suffering and then feel guilt or shame about it.&nbsp; If this happens, my advice is to take it to the next level &ndash; feel compassion and acceptance for your&nbsp;meta-meta-suffering.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">As you make this a practice, and feel acceptance and compassion for your suffering, you will feel more freedom from the&nbsp;meta&nbsp;level, and have more resources to work with the underlying suffering or depression.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">Another common way in which&nbsp;meta&nbsp;suffering sabotages people with depression is for them to feel depression as soon as they start feeling good.&nbsp; The story that some people have is that it is futile to think that they might feel so good in the future, and it is better not to get their hopes up and have them crushed.&nbsp; I encourage the person with this&nbsp;meta&nbsp;suffering story to assure the&nbsp;meta&nbsp;suffering part that they do not have obligation to feel good in the future.&nbsp; Feeling good in the present is of value, for however long it lasts, and that is worth appreciating and a good thing.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">Desiring more pleasant states is great.&nbsp; Working to create those states is fabulous.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><strong style=\"font-size: 14px; line-height: 24px;\">Feeling guilt, shame, depression, or other suffering because of not liking your current state or projected future state does not contribute to your feeling better, and is something that is pretty purely good to release. &nbsp;</strong></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><strong style=\"font-size: 14px; line-height: 24px;\"><br /></strong></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">* </span><a style=\"font-size: 14px; line-height: 24px;\" href=\"http://www.moodscope.com/\">Example of a site to track depression levels over time</a><span style=\"font-size: 14px; line-height: 24px;\">. &nbsp;&nbsp;</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LaDu5bKDpe8LxaR7C": 1, "BAhM42jvzuWMzTDxR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "foKJNj4cphxThsukr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 56, "baseScore": 37, "extendedScore": null, "score": 9.3e-05, "legacy": true, "legacyId": "19635", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I wrote this for the Positive Vector website awhile back and lots of people have found it valuable, so I want to share it with the Less Wrong community as well. &nbsp; I think this applies to most people - meta suffering thing is something I see everywhere, even though it is most prominent with people who have depression. &nbsp; This is based on my experience with working with depressed people and with studying Buddhism, especially <a href=\"http://bigmind.org/\">Big Mind</a>. &nbsp; Enjoy!</p>\n<p>&nbsp;</p>\n<p>---</p>\n<p><span style=\"color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\">The roots of suffering are often deep.&nbsp; But not all of the suffering happens at the root.&nbsp; A lot of the suffering that people experience is \u201cmeta\u201d suffering.&nbsp;&nbsp;Meta&nbsp;suffering is when you suffer because you are distressed that you are suffering.&nbsp; You are feeling depressed and hopeless, and there is a part of you that genuinely fears that it will never end.&nbsp; That you will feel this way forever.&nbsp; This fear of the suffering persisting can cause you much more suffering than whatever started your suffering.&nbsp; And it can last much longer.&nbsp; At some point days later, you might think to yourself about how terrible that initial suffering was, and feel fear and suffering about the possibility of it coming back.</span></p>\n<p><strong style=\"color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\" id=\"Many_people_suffer_as_much_or_more_from_meta_suffering_than_suffering_that_comes_from_physical_or_situational_sources___\">Many people suffer as much or more from&nbsp;meta-suffering than suffering that comes from physical or situational sources! &nbsp;</strong></p>\n<p><strong style=\"color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"></strong><span style=\"font-size: 14px; line-height: 24px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS';\">The good news is that&nbsp;meta&nbsp;suffering is much easier to fix than deeper forms of suffering.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">One thing you can do is to collect data* in order to develop an accurate model of how often you actually feel bad. &nbsp; &nbsp;Try monitoring your moods for awhile and get a baseline for what your moods actually are.&nbsp; At least half of the people who have suffered from major depression who have done this and spoken with me about it have been surprised to find that they often feel better than their self-perception when they assess their mood at random points throughout the day.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">Regardless of what your default mood state or range is, once you know what it is, you are likely to feel less fear.&nbsp; You can look at what your mood historically does over time, and feel more confidence that this is what it will do in the future.&nbsp; When you are in the state of despair and wondering if it will last forever, odds are that it won\u2019t.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">Another extremely powerful technique for dealing with&nbsp;meta-suffering is accepting that you are suffering.&nbsp; The&nbsp;meta&nbsp;suffering is suffering because you really want to change your state and are not successful.&nbsp; If you can just be with the state and not making yourself bad or wrong for being in that state, then all you have to deal with is the base state of suffering, which will be less intense and last less long than if you tack on that extra&nbsp;meta&nbsp;layer.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">The ironic thing is that just by thinking that thought, if you are prone to depression, you will probably notice yourself&nbsp;meta&nbsp;suffering and then feel guilt or shame about it.&nbsp; If this happens, my advice is to take it to the next level \u2013 feel compassion and acceptance for your&nbsp;meta-meta-suffering.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">As you make this a practice, and feel acceptance and compassion for your suffering, you will feel more freedom from the&nbsp;meta&nbsp;level, and have more resources to work with the underlying suffering or depression.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">Another common way in which&nbsp;meta&nbsp;suffering sabotages people with depression is for them to feel depression as soon as they start feeling good.&nbsp; The story that some people have is that it is futile to think that they might feel so good in the future, and it is better not to get their hopes up and have them crushed.&nbsp; I encourage the person with this&nbsp;meta&nbsp;suffering story to assure the&nbsp;meta&nbsp;suffering part that they do not have obligation to feel good in the future.&nbsp; Feeling good in the present is of value, for however long it lasts, and that is worth appreciating and a good thing.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">Desiring more pleasant states is great.&nbsp; Working to create those states is fabulous.</span></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><strong style=\"font-size: 14px; line-height: 24px;\" id=\"Feeling_guilt__shame__depression__or_other_suffering_because_of_not_liking_your_current_state_or_projected_future_state_does_not_contribute_to_your_feeling_better__and_is_something_that_is_pretty_purely_good_to_release___\">Feeling guilt, shame, depression, or other suffering because of not liking your current state or projected future state does not contribute to your feeling better, and is something that is pretty purely good to release. &nbsp;</strong></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><strong style=\"font-size: 14px; line-height: 24px;\"><br></strong></p>\n<p style=\"margin: 0px; padding: 0px 0px 10px; color: #444444; font-family: Georgia, 'Times New Roman', 'Trebuchet MS'; font-size: 14px; line-height: 24px;\"><span style=\"font-size: 14px; line-height: 24px;\">* </span><a style=\"font-size: 14px; line-height: 24px;\" href=\"http://www.moodscope.com/\">Example of a site to track depression levels over time</a><span style=\"font-size: 14px; line-height: 24px;\">. &nbsp;&nbsp;</span></p>", "sections": [{"title": "Many people suffer as much or more from\u00a0meta-suffering than suffering that comes from physical or situational sources! \u00a0", "anchor": "Many_people_suffer_as_much_or_more_from_meta_suffering_than_suffering_that_comes_from_physical_or_situational_sources___", "level": 1}, {"title": "Feeling guilt, shame, depression, or other suffering because of not liking your current state or projected future state does not contribute to your feeling better, and is something that is pretty purely good to release. \u00a0", "anchor": "Feeling_guilt__shame__depression__or_other_suffering_because_of_not_liking_your_current_state_or_projected_future_state_does_not_contribute_to_your_feeling_better__and_is_something_that_is_pretty_purely_good_to_release___", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "74 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 74, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-26T23:09:25.790Z", "modifiedAt": null, "url": null, "title": "[LINK] Breaking the illusion of understanding", "slug": "link-breaking-the-illusion-of-understanding", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:28.074Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gjm", "createdAt": "2009-03-09T01:11:32.668Z", "isAdmin": false, "displayName": "gjm"}, "userId": "977L8MR7JmNrQx6df", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2k9BpWvJEPRvN7pST/link-breaking-the-illusion-of-understanding", "pageUrlRelative": "/posts/2k9BpWvJEPRvN7pST/link-breaking-the-illusion-of-understanding", "linkUrl": "https://www.lesswrong.com/posts/2k9BpWvJEPRvN7pST/link-breaking-the-illusion-of-understanding", "postedAtFormatted": "Friday, October 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Breaking%20the%20illusion%20of%20understanding&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Breaking%20the%20illusion%20of%20understanding%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2k9BpWvJEPRvN7pST%2Flink-breaking-the-illusion-of-understanding%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Breaking%20the%20illusion%20of%20understanding%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2k9BpWvJEPRvN7pST%2Flink-breaking-the-illusion-of-understanding", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2k9BpWvJEPRvN7pST%2Flink-breaking-the-illusion-of-understanding", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 379, "htmlBody": "<p>This <a title=\"Ars Technica writeup\" href=\"http://arstechnica.com/science/2012/10/weve-located-the-reality-distortion-field-and-its-in-the-consumers-brain/\">writeup at Ars Technica</a> about a <a title=\"DOI record for the paper\" href=\"http://dx.doi.org/10.1086/667782\">recently published paper</a> in the Journal of Consumer Research may be of interest. Super-brief summary:</p>\n<ul>\n<li>Consumers with higher scores on a <a title=\"brief info about Cognitive Reflection Test\" href=\"http://www.sjdm.org/dmidi/Cognitive%20Reflection%20Test.html\">cognitive reflection test</a> are more inclined to buy products when told more about them; for consumers with lower CRT scores it's the reverse.</li>\n<li>Consumers with higher CRT scores felt that they understood the products better after being told more; consumers with lower CRT scores felt that they understood them worse.</li>\n<li>If subjects are asked to give an explanation of how products work and then asked how well they understand and how willing they'd be to pay, high-CR subjects don't change much in either but low-CR subjects report feeling that they understand worse and that they're willing to pay less.</li>\n<li>Conclusion: it looks as if when you give low-CR subjects more information about a product, they feel they understand it less, don't like that feeling, and become less willing to pay.</li>\n</ul>\n<p>If this is right (which seems plausible enough) then it presumably applies more broadly: e.g., to what tactics are most effective in political debate. Though it's hardly news in that area that making people feel stupid isn't the best way to persuade them of things.</p>\n<p>Abstract of the paper:</p>\n<blockquote>\n<p>People differ in their threshold for satisfactory causal understanding  and therefore in the type of explanation that will engender  understanding and maximize the appeal of a novel product. Explanation  fiends are dissatisfied with surface understanding and desire detailed  mechanistic explanations of how products work. In contrast, explanation  foes derive less understanding from detailed than coarse explanations  and downgrade products that are explained in detail. Consumers&rsquo; attitude  toward explanation is predicted by their tendency to deliberate, as  measured by the cognitive reflection test. Cognitive reflection also  predicts susceptibility to the illusion of explanatory depth, the  unjustified belief that one understands how things work. When  explanation foes attempt to explain, it exposes the illusion, which  leads to a decrease in willingness to pay. In contrast, explanation  fiends are willing to pay more after generating explanations. We  hypothesize that those low in cognitive reflection are explanation foes  because explanatory detail shatters their illusion of understanding.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2k9BpWvJEPRvN7pST", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 33, "extendedScore": null, "score": 1.018759966656796e-06, "legacy": true, "legacyId": "19636", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-26T23:24:45.272Z", "modifiedAt": null, "url": null, "title": "Meetup : Pittsburgh: Belief as attire", "slug": "meetup-pittsburgh-belief-as-attire", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Lrv6QJaAkTvHr5z9P/meetup-pittsburgh-belief-as-attire", "pageUrlRelative": "/posts/Lrv6QJaAkTvHr5z9P/meetup-pittsburgh-belief-as-attire", "linkUrl": "https://www.lesswrong.com/posts/Lrv6QJaAkTvHr5z9P/meetup-pittsburgh-belief-as-attire", "postedAtFormatted": "Friday, October 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Pittsburgh%3A%20Belief%20as%20attire&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Pittsburgh%3A%20Belief%20as%20attire%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrv6QJaAkTvHr5z9P%2Fmeetup-pittsburgh-belief-as-attire%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Pittsburgh%3A%20Belief%20as%20attire%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrv6QJaAkTvHr5z9P%2Fmeetup-pittsburgh-belief-as-attire", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrv6QJaAkTvHr5z9P%2Fmeetup-pittsburgh-belief-as-attire", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 38, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/f9'>Pittsburgh: Belief as attire</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 October 2012 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Eatunique, Craig St, Pittsburgh</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Related:\n<a href=\"http://lesswrong.com/lw/i7/belief_as_attire/\" rel=\"nofollow\">http://lesswrong.com/lw/i7/belief_as_attire/</a>\n<a href=\"http://lesswrong.com/lw/i6/professing_and_cheering/egb\" rel=\"nofollow\">http://lesswrong.com/lw/i6/professing_and_cheering/egb</a></p>\n\n<p>Call 412-304-6258 if you can't find us.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/f9'>Pittsburgh: Belief as attire</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Lrv6QJaAkTvHr5z9P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.018768285868763e-06, "legacy": true, "legacyId": "19638", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh__Belief_as_attire\">Discussion article for the meetup : <a href=\"/meetups/f9\">Pittsburgh: Belief as attire</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 October 2012 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Eatunique, Craig St, Pittsburgh</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Related:\n<a href=\"http://lesswrong.com/lw/i7/belief_as_attire/\" rel=\"nofollow\">http://lesswrong.com/lw/i7/belief_as_attire/</a>\n<a href=\"http://lesswrong.com/lw/i6/professing_and_cheering/egb\" rel=\"nofollow\">http://lesswrong.com/lw/i6/professing_and_cheering/egb</a></p>\n\n<p>Call 412-304-6258 if you can't find us.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh__Belief_as_attire1\">Discussion article for the meetup : <a href=\"/meetups/f9\">Pittsburgh: Belief as attire</a></h2>", "sections": [{"title": "Discussion article for the meetup : Pittsburgh: Belief as attire", "anchor": "Discussion_article_for_the_meetup___Pittsburgh__Belief_as_attire", "level": 1}, {"title": "Discussion article for the meetup : Pittsburgh: Belief as attire", "anchor": "Discussion_article_for_the_meetup___Pittsburgh__Belief_as_attire1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nYkMLFpx77Rz3uo9c"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-27T03:56:08.441Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Bratislava, Cambridge MA, Durham NC (2), Salt Lake City, Washington DC", "slug": "weekly-lw-meetups-austin-bratislava-cambridge-ma-durham-nc-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:24.158Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z74ocSFL3Dxpvc8ny/weekly-lw-meetups-austin-bratislava-cambridge-ma-durham-nc-2", "pageUrlRelative": "/posts/z74ocSFL3Dxpvc8ny/weekly-lw-meetups-austin-bratislava-cambridge-ma-durham-nc-2", "linkUrl": "https://www.lesswrong.com/posts/z74ocSFL3Dxpvc8ny/weekly-lw-meetups-austin-bratislava-cambridge-ma-durham-nc-2", "postedAtFormatted": "Saturday, October 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Bratislava%2C%20Cambridge%20MA%2C%20Durham%20NC%20(2)%2C%20Salt%20Lake%20City%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Bratislava%2C%20Cambridge%20MA%2C%20Durham%20NC%20(2)%2C%20Salt%20Lake%20City%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz74ocSFL3Dxpvc8ny%2Fweekly-lw-meetups-austin-bratislava-cambridge-ma-durham-nc-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Bratislava%2C%20Cambridge%20MA%2C%20Durham%20NC%20(2)%2C%20Salt%20Lake%20City%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz74ocSFL3Dxpvc8ny%2Fweekly-lw-meetups-austin-bratislava-cambridge-ma-durham-nc-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz74ocSFL3Dxpvc8ny%2Fweekly-lw-meetups-austin-bratislava-cambridge-ma-durham-nc-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 464, "htmlBody": "<p><strong>This meetup summary was posted to LW main on October 19th. The following week's summary is <a href=\"/lw/f5l/weekly_lw_meetups_cambridge_uk_melbourne_moscow/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/ew\">Durham HPMoR Discussion group:&nbsp;<span class=\"date\">20 October 2012 11:00AM</span></a></li>\n<li><a href=\"/meetups/es\">Bratislava meetup:&nbsp;<span class=\"date\">20 October 2012 05:30PM</span></a></li>\n<li><a href=\"/meetups/f0\">DC Meetup: Games and Conversation:&nbsp;<span class=\"date\">21 October 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/ex\">Durham Meetup: Article discussions:&nbsp;<span class=\"date\">25 October 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/ev\">Moscow: Biases, Applied Rationality, Visual Thinking:&nbsp;<span class=\"date\">27 October 2012 04:00PM</span></a></li>\n<li><a href=\"/meetups/e9\"></a><a href=\"/meetups/e9\">Munich Meetup, EDIT: October 28th:&nbsp;<span class=\"date\">28 October 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/el\">Sofia, Bulgaria Meetup:&nbsp;<span class=\"date\">09 December 2012 05:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">20 October 2018 01:30PM</span></a></li>\n<li><a href=\"/meetups/eu\">SLC Meetup: Free Will, Minicamp Braindump, and Group Goals:&nbsp;<span class=\"date\">20 October 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/dg\">Cambridge (MA) third-Sundays Meetup:&nbsp;<span class=\"date\">21 October 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/en\">Winter Solstice Megameetup - NYC:&nbsp;<span class=\"date\">15 December 2012 05:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z74ocSFL3Dxpvc8ny", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0189156316877774e-06, "legacy": true, "legacyId": "19485", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9ccefQts7PK7fQY57", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-27T07:25:33.383Z", "modifiedAt": null, "url": null, "title": "Passive income for dummies", "slug": "passive-income-for-dummies", "viewCount": null, "lastCommentedAt": "2012-11-01T23:20:10.398Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wvW8qdmfMTkmgnvuJ/passive-income-for-dummies", "pageUrlRelative": "/posts/wvW8qdmfMTkmgnvuJ/passive-income-for-dummies", "linkUrl": "https://www.lesswrong.com/posts/wvW8qdmfMTkmgnvuJ/passive-income-for-dummies", "postedAtFormatted": "Saturday, October 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Passive%20income%20for%20dummies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APassive%20income%20for%20dummies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwvW8qdmfMTkmgnvuJ%2Fpassive-income-for-dummies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Passive%20income%20for%20dummies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwvW8qdmfMTkmgnvuJ%2Fpassive-income-for-dummies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwvW8qdmfMTkmgnvuJ%2Fpassive-income-for-dummies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 959, "htmlBody": "<p>I read one too many breathless blog posts on the virtues of passive income and decided to write a rebuttal.&nbsp; Much of this should be obvious to folks with a solid economics background; in fact, please correct me if I got anything wrong.</p>\n<hr />\n<p>People seem to think in an odd way about passive income.&nbsp; One Helium.com author <a href=\"http://www.helium.com/items/1291691-can-you-really-make-money-online\">writes</a></p>\n<blockquote>\n<p>I think the first time there was any money [in my Helium.com writer's account] was when it showed about 3 cents. <strong>That actually thrilled me</strong>.</p>\n<p>...</p>\n<p>Les called me and said \"Quick, check your earnings page, tell me what you see for your credit card article (I thought, \"here we go again with the 'Reddit' business\"). I checked and lo and behold <strong>I had made around $4.00 in just a couple of hours for that one article he had submitted. Wow! Seriously. As the day progressed, it reached $8.00, then $10.00 and finally a little over $12.00.<br /></strong></p>\n</blockquote>\n<p>Sub-minimum wages really are exciting, aren't they?&nbsp; (At the end of the story, the guy has won the proverbial Helium.com lottery, and ends up making $1,246 submitting an <a href=\"http://www.helium.com/items/887146-credit-cards-dirty-little-secrets\">article</a> on how he hacked a credit card company's balance transfer offer to reddit.&nbsp; If I recall correctly, this inspired a spate of Helium.com writers spamming reddit with their posts.&nbsp; Probably why I ended up reading his article in the first place.)</p>\n<p>Now I just have to think of a way to employ people normally and trick them in to thinking that the income they're making is \"passive\"...</p>\n<p>But even smart people are in to passive income.&nbsp; Here's <a href=\"http://www.brianarmstrong.org\">Brian Armstrong</a>, who looks pretty sharp: economics degree, Airbnb software engineer, and now Y Combinator startup founder.&nbsp; His blog is largely about passive income, and he even wrote a <a href=\"http://brianarmstrong.org/breaking-free\">book</a> about it a few years ago.&nbsp; So being an expert like this, we'd expect him to be making lots, right?</p>\n<ul>\n<li>In <a href=\"http://brianarmstrong.org/blog/how-i-almost-accomplished-my-passive-income-goal-for-2008\">this post</a>, he gives his breakdown for December 2008: $250.00 from real estate, $1,260.40 from his blog/book about how to make passive income, $129.20 from his new tutor matching site <a href=\"http://www.universitytutor.com/\">UniversityTutor.com</a>&nbsp;(best tutor matching site I've found, BTW)</li>\n<li>In <a href=\"http://brianarmstrong.org/blog/universitytutor-races-past-200-paying-customers\">this September 2010 post</a>, he reports UniversityTutor is making over $2,000 a month and set to keep growing.</li>\n</ul>\n<p>So first, it looks like selling people the dream of passive income is a very profitable business to be in, and actually having passive income is not a requirement for entering it.</p>\n<p>But is that even true? &nbsp;Brian reports that his high December earnings are due to book promotional efforts. &nbsp;At some point, he decided to make <a href=\"http://brianarmstrong.org/breaking-free\">his book</a> available free.&nbsp; Would he have done that if he was still collecting substantial revenue from it?&nbsp; Doesn't seem that likely.</p>\n<p>Does your blog really count as a source of \"passive\" income if people gradually stop visiting when you stop making new posts?</p>\n<p>And second, yeah, maybe if you're a good software engineer with a good idea you can build a passive income business.</p>\n<p>In fact, working for \"active\" income vs building a \"passive income\" business is a bit of a false dichotomy.&nbsp; You can convert the cash you make through a regular job in to \"passive income\" by investing it and collecting interest.&nbsp; And you can convert your \"passive income\" business in to regular old chunk of cash by selling it.</p>\n<p>If you're interested in making more money in your off hours, starting a business is one option.&nbsp; Doing freelance work is another.&nbsp; Starting a business is higher variance--you could make it big, or you could waste a lot of time and effort.&nbsp; You'll most likely have to do something no one else is doing, or at least do it better than everyone else is doing it, so any kind of highly specific guide or formula for making passive income is probably out.&nbsp; (Ask yourself why the person selling you the guide doesn't just hire people to complete all the steps in the guide and collect the profits for themselves.)</p>\n<p>Interestingly, however, it seems as though internet businesses may be underpriced as an investment class.&nbsp; <a href=\"http://www.viperchill.com/buying-selling-websites/\">This guy</a> writes:</p>\n<blockquote>\n<p>I think a fair price to sell websites for is around 12 months income, depending entirely on how much time you put into the site for that income to continue. If the site is running on complete autopilot then you may be able to get 2 years revenue or even more.</p>\n<p>...</p>\n<p>For example, if I find a site on Flippa that is ranking well for a certain keyphrase in Google and it&rsquo;s making a lot of money, I might not always be able to purchase it for what I consider is a fair price &mdash; some bidders are happy to wait over 3 years for a return on their investment.</p>\n</blockquote>\n<p>If we assume the market price of a completely hands-off internet business is 3 years' profits, then you'll have doubled your initial investment in 3 years, since you've still got the business to sell afterwards.&nbsp; Abusing the <a href=\"http://en.wikipedia.org/wiki/Rule_of_72\">rule of 72</a>, I estimate that this is equivalent to getting a 24% annual return on your investment, which is obviously absurdly high.&nbsp; (And this doesn't even take in to account the fact that you could reinvest your internet business' earnings.)</p>\n<p>Of course, there are reasons to be wary about getting in to internet businesses as well.&nbsp; Ease of buying and selling isn't the greatest and you might get scammed.&nbsp; But probably the biggest thing is the Internet's short attention span.&nbsp; Google changes its ranking algorithm periodically, and beyond that, it's easy to be eclipsed by something newer and better (see MySpace).&nbsp; So if I were wealthier, I'd want to have a portion of my net worth in internet businesses, but not all of it.</p>\n<p>BTW, <a href=\"http://www.rolfnelson.com/\">http://www.rolfnelson.com/</a> is good for rational analysis of stuff related to entrepreneurship.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8sh6iLwYWDJ7z3fPo": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wvW8qdmfMTkmgnvuJ", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 26, "extendedScore": null, "score": 1.0190293570086734e-06, "legacy": true, "legacyId": "19538", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-10-27T07:25:33.383Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-28T01:30:55.252Z", "modifiedAt": null, "url": null, "title": "Heuristic: How does it sound in a movie?", "slug": "heuristic-how-does-it-sound-in-a-movie", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:30.621Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stabilizer", "createdAt": "2011-12-02T09:36:56.841Z", "isAdmin": false, "displayName": "Stabilizer"}, "userId": "Qa3pLZx3o2TApyfgq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gmiuy7atw3kHNC6uN/heuristic-how-does-it-sound-in-a-movie", "pageUrlRelative": "/posts/gmiuy7atw3kHNC6uN/heuristic-how-does-it-sound-in-a-movie", "linkUrl": "https://www.lesswrong.com/posts/gmiuy7atw3kHNC6uN/heuristic-how-does-it-sound-in-a-movie", "postedAtFormatted": "Sunday, October 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Heuristic%3A%20How%20does%20it%20sound%20in%20a%20movie%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHeuristic%3A%20How%20does%20it%20sound%20in%20a%20movie%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgmiuy7atw3kHNC6uN%2Fheuristic-how-does-it-sound-in-a-movie%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Heuristic%3A%20How%20does%20it%20sound%20in%20a%20movie%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgmiuy7atw3kHNC6uN%2Fheuristic-how-does-it-sound-in-a-movie", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgmiuy7atw3kHNC6uN%2Fheuristic-how-does-it-sound-in-a-movie", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 367, "htmlBody": "<p>Our internal dialogues are often exactly that: dialogues that suit a narrative. Narrative building (the basis of the narrative fallacy) is often quite detrimental to attempts to think clearly. It is therefore beneficial to detect and correct for biases introduced from narrative building. But it can be hard to distinguish a 'clear' thought from one that is a consequence of a narrative.</p>\n<p>I offer a heuristic to make the distinction between a thought which is a direct attempt to model reality and a thought which is based solely on its suitability to a narrative:</p>\n<p>&nbsp;</p>\n<ol>\n<li>Isolate a sentence uttered in your internal (or external) voice. It could also be a pattern of images, voices or feelings. Tastes and smells are harder to do in this technique, but I believe we rarely 'think' with our tastes and smells.</li>\n<li>Imagine it in a movie or a story.&nbsp;Also see how easy it is for you come up with some music to suit this thought pattern. Also examine the suitability of the people in your thoughts as characters in a movie or TV show.&nbsp;</li>\n<li>Evaluate its suitability to the above, narrative-like contexts.&nbsp;</li>\n<li>If it seems very suitable, it is now much more likely that it is part of narrative building. Otherwise, it might be an 'accurate' thought.&nbsp;</li>\n</ol>\n<p>&nbsp;</p>\n<p>Two examples:</p>\n<p>1. <span style=\"text-decoration: underline;\">When buying something</span>: Often times, when I'm standing in a Starbucks line for a coffee and try to imagine why I'm standing there (when I can make my own coffee both at my home and at my office), I am usually returned with a feeling of being part of The People Who Do Things. Or one of being a Hard Worker who needs his Coffee to do his Hard Work with Focus and Determination. It fits too well while introducing a character in a novel. After I started noticing this, I've been realizing that coffee is not as useful in improving my focus as I thought it was earlier.</p>\n<p>2. <span style=\"text-decoration: underline;\">In conversations</span>: This must be very familiar to most people. Anecdotes get highly embellished based on their suitability to a story. Also the way they are usually 'narrated' rather than just 'conveyed'. Realizing this when it happens can be quite useful.</p>\n<p>Other examples?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gmiuy7atw3kHNC6uN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 1.019619136296675e-06, "legacy": true, "legacyId": "19652", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-28T04:10:03.596Z", "modifiedAt": null, "url": null, "title": "Economy gossip open thread", "slug": "economy-gossip-open-thread", "viewCount": null, "lastCommentedAt": "2016-07-23T02:30:27.030Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kCmcT4NDjmcrs4QAi/economy-gossip-open-thread", "pageUrlRelative": "/posts/kCmcT4NDjmcrs4QAi/economy-gossip-open-thread", "linkUrl": "https://www.lesswrong.com/posts/kCmcT4NDjmcrs4QAi/economy-gossip-open-thread", "postedAtFormatted": "Sunday, October 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Economy%20gossip%20open%20thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEconomy%20gossip%20open%20thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkCmcT4NDjmcrs4QAi%2Feconomy-gossip-open-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Economy%20gossip%20open%20thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkCmcT4NDjmcrs4QAi%2Feconomy-gossip-open-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkCmcT4NDjmcrs4QAi%2Feconomy-gossip-open-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 848, "htmlBody": "<p>Diego Caleiro <a href=\"/lw/f2q/passive_income_for_dummies/7p91\">writes</a>:</p>\n<blockquote>\n<p>It amazes me that there is no \"Sequence on how to make money in intelligent ways\" or something of the sort in LessWrong.</p>\n</blockquote>\n<p>I don't think a sequence is the quite the right approach to this.&nbsp; The economy is huge, complicated, and heterogeneous, and it's changing constantly.&nbsp; (Think of how complicated people are, then imagine how complicated their economy is.)&nbsp; It's <a href=\"http://www.youtube.com/watch?v=R5Gppi-O3a8#t=14s\">hard</a> for a single person to have a thorough understanding of everything, and even if someone did, their understanding would start becoming obsolete immediately.</p>\n<p>And making money intelligently is very closely related to knowing what's going on in the economy.&nbsp; If you're training for a job, you want to develop skills that are in high demand.&nbsp; If you're starting a business, you want to sell stuff for a profit.&nbsp; You can only go so far identifying these opportunities by thinking things through from first principles.</p>\n<p>I don't think most useful knowledge Less Wrong can share about what's going on in the economy is going to approach the certainty of carefully derived math or <a href=\"/lw/ow/the_beauty_of_settled_science/\">settled science</a>.&nbsp; Instead, it'll be more like gossip.&nbsp; Since no one understands the entire economy, we won't get much firsthand knowledge.&nbsp; Lots of information will consist of hearsay and speculation that's subject to change at any time.&nbsp; (Incidentally, <a href=\"http://www.ribbonfarm.com/2007/12/05/the-fine-art-of-opportunism/\">gathering such info and trying lots of stuff out may be a good preparatory activity for starting a business</a>.)</p>\n<p>Anyway, here's some gossip from me; feel free to post yours in the comments.</p>\n<p>&nbsp;</p>\n<h2>Employment</h2>\n<p>The US Bureau of Labor Statistics maintains an <a href=\"http://www.bls.gov/ooh/\">Occupational Outlook Handbook</a> with information on salary, education requirements, and job growth for a wide variety of jobs.&nbsp; Here are a few random interesting ones:</p>\n<ul>\n<li>It may be possible to become an <a href=\"http://www.bls.gov/ooh/math/actuaries.htm\">actuary</a> (~$90K/yr, faster than average job growth, involves math) without a college degree if you pass a few actuarial exams</li>\n<li><a href=\"http://www.bls.gov/ooh/healthcare/diagnostic-medical-sonographers.htm\">Diagnostic medical sonographer</a>: requires only a 2-year degree, ~$65K/yr, much faster than average job growth</li>\n<li>Michael Vassar was telling everyone to become a police officer a while ago.&nbsp; <a href=\"http://www.bls.gov/ooh/Protective-Service/Police-and-detectives.htm\">The BLS page looks only OK</a>, maybe you can get a much higher salary in the right municipality?</li>\n</ul>\n<p>Payscale.com looks pretty nice for salary info: college education <a href=\"http://www.payscale.com/college-education-value\">return on investment data</a>, <a href=\"http://www.payscale.com/college-salary-report-2013/majors-that-pay-you-back\">top paying majors</a>.&nbsp; See also: <a href=\"http://80000hours.org/blog/24-5-ways-to-be-misled-by-salary-rankings\">5 ways to be mislead by salary rankings</a>, <a href=\"http://www.glassdoor.com\">GlassDoor.com</a></p>\n<p>UC Berkeley alumni surveys: <a href=\"https://career.berkeley.edu/major/major.stm\">What can I do with a major in...</a></p>\n<ul>\n</ul>\n<p><a href=\"http://tynan.com/hustle\">This guy</a> claims you can make $45-60 an hour playing poker after a year's practice and $5000 lost.&nbsp; Some blackjack card counting guru I emailed years ago claimed you could learn to count cards in 100 hours and make 6 figures for years if you were diligent (cover play, travelling for new opportunities, etc.)&nbsp; I'm not sure how workable these ideas are if you don't already have a large bankroll. &nbsp;(See <a href=\"http://en.wikipedia.org/wiki/Kelly_criterion\">Kelly criterion</a>.)</p>\n<p>If you're interested in getting paid to practice <a href=\"http://rejectiontherapy.com/\">rejection therapy</a> and you're in the SF Bay Area, PM me and I can put you in touch with someone in California's ballot signature collection industry.&nbsp; The job consists of standing somewhere where lots of people walk by and asking them to sign your petitions.&nbsp; You get paid per signature, and if you find a good spot (and keep it secret from other signature gatherers), it's possible to make a lot of money.&nbsp; A friend averaged $300 a day; I wasn't sufficiently dedicated/psychologically resilient to get anything like those results.&nbsp; The business is seasonal; if I recall correctly, February is an especially good month.</p>\n<p><a href=\"http://www.psychologytoday.com/blog/the-scientific-fundamentalist/201103/are-all-women-essentially-prostitutes\">It looks as though</a> high-end, college-educated call girls can make $300/hour.</p>\n<p>Tutoring websites: <a href=\"http://www.universitytutor.com/\">UniversityTutor</a>, <a href=\"http://www.tutorspree.com/\">TutorSpree</a>, <a href=\"http://www.care.com\">Care.com</a>, <a href=\"http://sfbay.craigslist.org/lss/\">Craigslist</a>, <a href=\"http://www.wyzant.com\">Wyzant</a>.&nbsp; One thing to keep in mind with tutoring: Since you generally work so few hours per gig, transportation-time overhead per hour worked is higher.</p>\n<p>80,000 hours offers <a href=\"http://80000hours.org/request-a-career-advice-session/\">one-on-one career advising</a>, if you're in to effective altruism.</p>\n<p>&nbsp;</p>\n<h2>Salary Negotiation<br /></h2>\n<p>\"In a study conducted at Carnegie Mellon University's business school, Professor Linda Babcock discovered that [those MBA students who negotiated their starting salary instead of just accepting their initial offer] received an average of $4,053 more than those who did not.\"&nbsp; (<a href=\"http://www.amazon.com/Bargaining-Advantage-Negotiation-Strategies-Reasonable/dp/0140281916\">Bargaining for Advantage</a>, p. 16.)&nbsp; Women seem much more reluctant to ask for more money, which may go a ways towards explaining gender pay gaps.&nbsp; The book recommends thorough preparation prior to any negotiation.</p>\n<p>&nbsp;</p>\n<h2>Business Gossip</h2>\n<p>Why rely on just one guru when you can draw inferences from the experiences of many?&nbsp; But beware sampling effects: you'll likely hear less from people who didn't end up accomplishing anything worth writing about.</p>\n<ul>\n<li><a href=\"http://www.fourhourworkweek.com/blog/category/4-hour-case-studies/\">4-hour workweek case studies</a></li>\n<li><a href=\"http://www.hnsearch.com/\">http://www.hnsearch.com/</a> for searching the <a href=\"/news.ycombinator.com\">Hacker News</a> archive--see results for <a href=\"http://www.hnsearch.com/search#request/all&amp;q=find+domain+name\">find domain name</a> or <a href=\"http://www.hnsearch.com/search#request/all&amp;q=passive+income&amp;start=0\">passive income</a>, for instance</li>\n<li>You can search reddit.com/r/IAmA for people who are actually do or have done things--see results for <a href=\"http://www.reddit.com/r/IAmA/search?q=sat+tutor&amp;restrict_sr=on\">SAT tutor</a> or <a href=\"http://www.reddit.com/r/IAmA/search?q=adsense&amp;restrict_sr=on&amp;sort=relevance\">adsense</a>, for instance (related: <a href=\"http://mixergy.com/\">Mixergy</a>)</li>\n<li>To learn about different markets: <a href=\"http://aytm.com/\">Ask Your Target Market</a>, <a href=\"http://omegle.com/\">Omegle.com</a> (put in interests related to what you're thinking of selling)</li>\n</ul>\n<p>As business gurus go, <a href=\"http://www.paulgraham.com/articles.html\">Paul Graham</a> is highly rational, has an unmatchable resume, and all his stuff is free.</p>\n<p>&nbsp;</p>\n<h2>Previously on Less Wrong</h2>\n<ul>\n<li><a href=\"/lw/di2/advice_on_getting_a_software_job/\">Advice on Getting a Software Job</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Lesswrong_Community's_How-Tos_and_Recommendations#Employment\">Lots more employment advice from LW</a></li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kCmcT4NDjmcrs4QAi", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 36, "extendedScore": null, "score": 7.9e-05, "legacy": true, "legacyId": "19651", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Diego Caleiro <a href=\"/lw/f2q/passive_income_for_dummies/7p91\">writes</a>:</p>\n<blockquote>\n<p>It amazes me that there is no \"Sequence on how to make money in intelligent ways\" or something of the sort in LessWrong.</p>\n</blockquote>\n<p>I don't think a sequence is the quite the right approach to this.&nbsp; The economy is huge, complicated, and heterogeneous, and it's changing constantly.&nbsp; (Think of how complicated people are, then imagine how complicated their economy is.)&nbsp; It's <a href=\"http://www.youtube.com/watch?v=R5Gppi-O3a8#t=14s\">hard</a> for a single person to have a thorough understanding of everything, and even if someone did, their understanding would start becoming obsolete immediately.</p>\n<p>And making money intelligently is very closely related to knowing what's going on in the economy.&nbsp; If you're training for a job, you want to develop skills that are in high demand.&nbsp; If you're starting a business, you want to sell stuff for a profit.&nbsp; You can only go so far identifying these opportunities by thinking things through from first principles.</p>\n<p>I don't think most useful knowledge Less Wrong can share about what's going on in the economy is going to approach the certainty of carefully derived math or <a href=\"/lw/ow/the_beauty_of_settled_science/\">settled science</a>.&nbsp; Instead, it'll be more like gossip.&nbsp; Since no one understands the entire economy, we won't get much firsthand knowledge.&nbsp; Lots of information will consist of hearsay and speculation that's subject to change at any time.&nbsp; (Incidentally, <a href=\"http://www.ribbonfarm.com/2007/12/05/the-fine-art-of-opportunism/\">gathering such info and trying lots of stuff out may be a good preparatory activity for starting a business</a>.)</p>\n<p>Anyway, here's some gossip from me; feel free to post yours in the comments.</p>\n<p>&nbsp;</p>\n<h2 id=\"Employment\">Employment</h2>\n<p>The US Bureau of Labor Statistics maintains an <a href=\"http://www.bls.gov/ooh/\">Occupational Outlook Handbook</a> with information on salary, education requirements, and job growth for a wide variety of jobs.&nbsp; Here are a few random interesting ones:</p>\n<ul>\n<li>It may be possible to become an <a href=\"http://www.bls.gov/ooh/math/actuaries.htm\">actuary</a> (~$90K/yr, faster than average job growth, involves math) without a college degree if you pass a few actuarial exams</li>\n<li><a href=\"http://www.bls.gov/ooh/healthcare/diagnostic-medical-sonographers.htm\">Diagnostic medical sonographer</a>: requires only a 2-year degree, ~$65K/yr, much faster than average job growth</li>\n<li>Michael Vassar was telling everyone to become a police officer a while ago.&nbsp; <a href=\"http://www.bls.gov/ooh/Protective-Service/Police-and-detectives.htm\">The BLS page looks only OK</a>, maybe you can get a much higher salary in the right municipality?</li>\n</ul>\n<p>Payscale.com looks pretty nice for salary info: college education <a href=\"http://www.payscale.com/college-education-value\">return on investment data</a>, <a href=\"http://www.payscale.com/college-salary-report-2013/majors-that-pay-you-back\">top paying majors</a>.&nbsp; See also: <a href=\"http://80000hours.org/blog/24-5-ways-to-be-misled-by-salary-rankings\">5 ways to be mislead by salary rankings</a>, <a href=\"http://www.glassdoor.com\">GlassDoor.com</a></p>\n<p>UC Berkeley alumni surveys: <a href=\"https://career.berkeley.edu/major/major.stm\">What can I do with a major in...</a></p>\n<ul>\n</ul>\n<p><a href=\"http://tynan.com/hustle\">This guy</a> claims you can make $45-60 an hour playing poker after a year's practice and $5000 lost.&nbsp; Some blackjack card counting guru I emailed years ago claimed you could learn to count cards in 100 hours and make 6 figures for years if you were diligent (cover play, travelling for new opportunities, etc.)&nbsp; I'm not sure how workable these ideas are if you don't already have a large bankroll. &nbsp;(See <a href=\"http://en.wikipedia.org/wiki/Kelly_criterion\">Kelly criterion</a>.)</p>\n<p>If you're interested in getting paid to practice <a href=\"http://rejectiontherapy.com/\">rejection therapy</a> and you're in the SF Bay Area, PM me and I can put you in touch with someone in California's ballot signature collection industry.&nbsp; The job consists of standing somewhere where lots of people walk by and asking them to sign your petitions.&nbsp; You get paid per signature, and if you find a good spot (and keep it secret from other signature gatherers), it's possible to make a lot of money.&nbsp; A friend averaged $300 a day; I wasn't sufficiently dedicated/psychologically resilient to get anything like those results.&nbsp; The business is seasonal; if I recall correctly, February is an especially good month.</p>\n<p><a href=\"http://www.psychologytoday.com/blog/the-scientific-fundamentalist/201103/are-all-women-essentially-prostitutes\">It looks as though</a> high-end, college-educated call girls can make $300/hour.</p>\n<p>Tutoring websites: <a href=\"http://www.universitytutor.com/\">UniversityTutor</a>, <a href=\"http://www.tutorspree.com/\">TutorSpree</a>, <a href=\"http://www.care.com\">Care.com</a>, <a href=\"http://sfbay.craigslist.org/lss/\">Craigslist</a>, <a href=\"http://www.wyzant.com\">Wyzant</a>.&nbsp; One thing to keep in mind with tutoring: Since you generally work so few hours per gig, transportation-time overhead per hour worked is higher.</p>\n<p>80,000 hours offers <a href=\"http://80000hours.org/request-a-career-advice-session/\">one-on-one career advising</a>, if you're in to effective altruism.</p>\n<p>&nbsp;</p>\n<h2 id=\"Salary_Negotiation\">Salary Negotiation<br></h2>\n<p>\"In a study conducted at Carnegie Mellon University's business school, Professor Linda Babcock discovered that [those MBA students who negotiated their starting salary instead of just accepting their initial offer] received an average of $4,053 more than those who did not.\"&nbsp; (<a href=\"http://www.amazon.com/Bargaining-Advantage-Negotiation-Strategies-Reasonable/dp/0140281916\">Bargaining for Advantage</a>, p. 16.)&nbsp; Women seem much more reluctant to ask for more money, which may go a ways towards explaining gender pay gaps.&nbsp; The book recommends thorough preparation prior to any negotiation.</p>\n<p>&nbsp;</p>\n<h2 id=\"Business_Gossip\">Business Gossip</h2>\n<p>Why rely on just one guru when you can draw inferences from the experiences of many?&nbsp; But beware sampling effects: you'll likely hear less from people who didn't end up accomplishing anything worth writing about.</p>\n<ul>\n<li><a href=\"http://www.fourhourworkweek.com/blog/category/4-hour-case-studies/\">4-hour workweek case studies</a></li>\n<li><a href=\"http://www.hnsearch.com/\">http://www.hnsearch.com/</a> for searching the <a href=\"/news.ycombinator.com\">Hacker News</a> archive--see results for <a href=\"http://www.hnsearch.com/search#request/all&amp;q=find+domain+name\">find domain name</a> or <a href=\"http://www.hnsearch.com/search#request/all&amp;q=passive+income&amp;start=0\">passive income</a>, for instance</li>\n<li>You can search reddit.com/r/IAmA for people who are actually do or have done things--see results for <a href=\"http://www.reddit.com/r/IAmA/search?q=sat+tutor&amp;restrict_sr=on\">SAT tutor</a> or <a href=\"http://www.reddit.com/r/IAmA/search?q=adsense&amp;restrict_sr=on&amp;sort=relevance\">adsense</a>, for instance (related: <a href=\"http://mixergy.com/\">Mixergy</a>)</li>\n<li>To learn about different markets: <a href=\"http://aytm.com/\">Ask Your Target Market</a>, <a href=\"http://omegle.com/\">Omegle.com</a> (put in interests related to what you're thinking of selling)</li>\n</ul>\n<p>As business gurus go, <a href=\"http://www.paulgraham.com/articles.html\">Paul Graham</a> is highly rational, has an unmatchable resume, and all his stuff is free.</p>\n<p>&nbsp;</p>\n<h2 id=\"Previously_on_Less_Wrong\">Previously on Less Wrong</h2>\n<ul>\n<li><a href=\"/lw/di2/advice_on_getting_a_software_job/\">Advice on Getting a Software Job</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Lesswrong_Community's_How-Tos_and_Recommendations#Employment\">Lots more employment advice from LW</a></li>\n</ul>\n<p>&nbsp;</p>", "sections": [{"title": "Employment", "anchor": "Employment", "level": 1}, {"title": "Salary Negotiation", "anchor": "Salary_Negotiation", "level": 1}, {"title": "Business Gossip", "anchor": "Business_Gossip", "level": 1}, {"title": "Previously on Less Wrong", "anchor": "Previously_on_Less_Wrong", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "60 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ndGYn7ZFiZyernp9f", "cKyMoRotH7yGhTPun"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-10-28T04:10:03.596Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-28T04:28:54.157Z", "modifiedAt": null, "url": null, "title": "[Link] Singularity Summit Talks", "slug": "link-singularity-summit-talks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:56.259Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Unnamed", "createdAt": "2009-02-27T06:08:10.900Z", "isAdmin": false, "displayName": "Unnamed"}, "userId": "PdzQ73mN7S4SvRMhu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mjWmbSQQ8Hy7XXzr5/link-singularity-summit-talks", "pageUrlRelative": "/posts/mjWmbSQQ8Hy7XXzr5/link-singularity-summit-talks", "linkUrl": "https://www.lesswrong.com/posts/mjWmbSQQ8Hy7XXzr5/link-singularity-summit-talks", "postedAtFormatted": "Sunday, October 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Singularity%20Summit%20Talks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Singularity%20Summit%20Talks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmjWmbSQQ8Hy7XXzr5%2Flink-singularity-summit-talks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Singularity%20Summit%20Talks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmjWmbSQQ8Hy7XXzr5%2Flink-singularity-summit-talks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmjWmbSQQ8Hy7XXzr5%2Flink-singularity-summit-talks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 15, "htmlBody": "<p>Videos of the 2012 <a href=\"http://fora.tv/conference/the_singularity_summit_2012\">Singularity Summit talks</a> are now online.</p>\n<p>Previous discussion of the Summit <a href=\"/r/discussion/lw/eyh/singularity_summit_2012_discuss_it_here/\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mjWmbSQQ8Hy7XXzr5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 14, "extendedScore": null, "score": 1.0197159079113297e-06, "legacy": true, "legacyId": "19653", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TkQf24Q9jmPkWKmg8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-28T06:00:16.190Z", "modifiedAt": null, "url": null, "title": "Native Russian speakers wanted (for proofreading LW texts, again) ", "slug": "native-russian-speakers-wanted-for-proofreading-lw-texts-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:26.534Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "listic", "createdAt": "2009-03-11T10:06:44.719Z", "isAdmin": false, "displayName": "listic"}, "userId": "qTfheLmwMdCtMb5ZT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3ZkWMCaQnpLricoMY/native-russian-speakers-wanted-for-proofreading-lw-texts-0", "pageUrlRelative": "/posts/3ZkWMCaQnpLricoMY/native-russian-speakers-wanted-for-proofreading-lw-texts-0", "linkUrl": "https://www.lesswrong.com/posts/3ZkWMCaQnpLricoMY/native-russian-speakers-wanted-for-proofreading-lw-texts-0", "postedAtFormatted": "Sunday, October 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Native%20Russian%20speakers%20wanted%20(for%20proofreading%20LW%20texts%2C%20again)%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANative%20Russian%20speakers%20wanted%20(for%20proofreading%20LW%20texts%2C%20again)%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ZkWMCaQnpLricoMY%2Fnative-russian-speakers-wanted-for-proofreading-lw-texts-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Native%20Russian%20speakers%20wanted%20(for%20proofreading%20LW%20texts%2C%20again)%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ZkWMCaQnpLricoMY%2Fnative-russian-speakers-wanted-for-proofreading-lw-texts-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ZkWMCaQnpLricoMY%2Fnative-russian-speakers-wanted-for-proofreading-lw-texts-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<p><strong>This is a repeat from <a href=\"/lw/cji/native_russian_speakers_wanted_for_proofreading/\">May, 20</a> post.</strong> If you are a native Russian speaker, could volunteer an hour or two to read the sequence and share your opinion on it, and haven't contacted me before (here, by email or on <a name=\"http://lesswrong.ru/\"></a>lesswrong.ru)</p>\n<p>As part of an effort to familiarize Russian-speaking audience with Less Wrong materials, I am participating in an ongoing translation of a certain sequence of texts to Russian.</p>\n<p>Now I need volunteers who are willing to read the text and give feedback on its quality. If you are native Russian speaker and can dedicate an hour or two to reading the sequence, please drop me an email to nickolai@intelligence.org and I'll give you the link. Later I'll update this post with an information on which text this was and which feedback I have got (your exact words can remain private if you wish).</p>\n<p>I'm doing it this way so that name of the text and other's opinion won't influence your decision to participate and your opinion. Frankly, I think this mode of interaction should be a standard option in popular forum software, but it isn't.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3ZkWMCaQnpLricoMY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0197655920875395e-06, "legacy": true, "legacyId": "19654", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["weFsbxeie8eisA3Fp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-28T07:31:36.470Z", "modifiedAt": null, "url": null, "title": "Online Optimal Philanthropy Meeting", "slug": "online-optimal-philanthropy-meeting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:30.255Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "theduffman", "createdAt": "2012-10-28T06:00:27.333Z", "isAdmin": false, "displayName": "theduffman"}, "userId": "yYmeGPwbXWefvsNr9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uMGr9M59bJ6KmB5HT/online-optimal-philanthropy-meeting", "pageUrlRelative": "/posts/uMGr9M59bJ6KmB5HT/online-optimal-philanthropy-meeting", "linkUrl": "https://www.lesswrong.com/posts/uMGr9M59bJ6KmB5HT/online-optimal-philanthropy-meeting", "postedAtFormatted": "Sunday, October 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Online%20Optimal%20Philanthropy%20Meeting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOnline%20Optimal%20Philanthropy%20Meeting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuMGr9M59bJ6KmB5HT%2Fonline-optimal-philanthropy-meeting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Online%20Optimal%20Philanthropy%20Meeting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuMGr9M59bJ6KmB5HT%2Fonline-optimal-philanthropy-meeting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuMGr9M59bJ6KmB5HT%2Fonline-optimal-philanthropy-meeting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 295, "htmlBody": "<p><strong><em>Update: The meeting will occur at </em></strong><a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2Ff5z%2Fonline_optimal_philanthropy_meeting%2F&amp;v=1&amp;libid=1351812195852&amp;out=http%3A%2F%2Fwww.timeanddate.com%2Fworldclock%2Ffixedtime.html%3Fmsg%3DOnline%2BOptimal%2BPhilanthropy%2BMeeting%26iso%3D20121106T1330%26p1%3D152%26ah%3D1%26am%3D30&amp;ref=http%3A%2F%2Flesswrong.com%2Fuser%2Ftheduffman%2F&amp;title=Online%20Optimal%20Philanthropy%20Meeting%20-%20Less%20Wrong&amp;txt=1.30pm%20Melbourne%20Australian%20time&amp;jsonp=vglnk_jsonp_13518122335541\"><strong><em>1.30pm </em></strong></a><strong><em>Australian Eastern Daylights Savings time on Tuesday, November 6, 2012 in Australia (Monday evening in the Americas) on </em></strong><strong><em><a href=\"https://plus.google.com/u/0/112628930686250912340/posts\">Google+ Hangouts</a>.</em></strong></p>\n<p><strong><em>Topics will include how to make near mode progress on far mode problems, the potential for cascades, cycles and recursive loops eg AI and idea propagation to dominate effective altruistic concern. Also, theories and plans can be proposed by all participants.</em></strong></p>\n<p>&nbsp;</p>\n<p><em><strong>Why meet online?</strong></em></p>\n<p>Lots of us want to improve the world. By donating, performing rationality training, inspiring one and other and so on.&nbsp;</p>\n<p>We're all in a state of limited information regarding how to best help. In hotspots for effective altruism like San Francisco and Oxford, effective altruists (EAs) are able to get high quality feedback on their ideas. But elsewhere, constructive,&nbsp;&nbsp;creative&nbsp;input is of limited supply.</p>\n<p>Face-to-face meetups have so far been organised by&nbsp;<a title=\"The High Impact Network\" href=\"http://www.google.com.au/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CCAQFjAA&amp;url=http%3A%2F%2Fwww.thehighimpactnetwork.org%2F&amp;ei=btCMUL3PM-2RiQeWuIGIBA&amp;usg=AFQjCNHlWGcIPwOcJA1dGilg9NxRV_vBZw\" target=\"_self\">The High Impact Network</a>,&nbsp;<a title=\"Giving What We Can\" href=\"/givingwhatwecan.org\" target=\"_self\">Giving What We Can</a>&nbsp;and these clearly allow members to complement one and others' knowledge, skills and resources (time, funding, etc), while boosting each others' determination.</p>\n<p>One would expect online meetups to offer qualitatively similar benefits. The comparative advantage of online EA meetups could be:</p>\n<p>1. helping experienced EAs to share the most up-to-date information and ideas quickly between geographically disparate meetups.</p>\n<p>2. to inspire otherwise isolated EAs</p>\n<p>3. to explore the utility of online video and other technology for spreading EA knowledge and skills.</p>\n<p>&nbsp;</p>\n<p><strong><em>Sounds good. How can I help?</em></strong></p>\n<p>I have written a draft task-list including an agenda here.&nbsp;<a href=\"http://checkvist.com/checklists/153407-online-optimal-philanthropy-meeting\">http://checkvist.com/checklists/153407-online-optimal-philanthropy-meeting</a>. I encourage you to email me for access before the meetup itself.</p>\n<p><span style=\"font-family: mceinline;\">I have created a&nbsp;<a href=\"http://whenisgood.net/hdyssxf\">Whenisgood time chart here</a>. Please use it to indicate when you are available to meet.</span>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong><em>When?</em></strong></p>\n<p>At&nbsp;<a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?msg=Online+Optimal+Philanthropy+Meeting&amp;iso=20121106T1330&amp;p1=152&amp;ah=1&amp;am=30\">1.30pm Melbourne Australian time</a>&nbsp;on&nbsp;<a href=\"https://plus.google.com/u/0/112628930686250912340/posts\">Google+ Hangouts</a>. Do add me to your circles and expect an invite.</p>\n<p>&nbsp;</p>\n<p>See you then.</p>\n<p>- <a href=\"https://plus.google.com/u/0/112628930686250912340/posts\">Theduffman</a> | THINK Melbourne organiser | former Felicifia admin</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uMGr9M59bJ6KmB5HT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.0198152646571818e-06, "legacy": true, "legacyId": "19655", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong><em>Update: The meeting will occur at </em></strong><a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2Ff5z%2Fonline_optimal_philanthropy_meeting%2F&amp;v=1&amp;libid=1351812195852&amp;out=http%3A%2F%2Fwww.timeanddate.com%2Fworldclock%2Ffixedtime.html%3Fmsg%3DOnline%2BOptimal%2BPhilanthropy%2BMeeting%26iso%3D20121106T1330%26p1%3D152%26ah%3D1%26am%3D30&amp;ref=http%3A%2F%2Flesswrong.com%2Fuser%2Ftheduffman%2F&amp;title=Online%20Optimal%20Philanthropy%20Meeting%20-%20Less%20Wrong&amp;txt=1.30pm%20Melbourne%20Australian%20time&amp;jsonp=vglnk_jsonp_13518122335541\"><strong><em>1.30pm </em></strong></a><strong><em>Australian Eastern Daylights Savings time on Tuesday, November 6, 2012 in Australia (Monday evening in the Americas) on </em></strong><strong><em><a href=\"https://plus.google.com/u/0/112628930686250912340/posts\">Google+ Hangouts</a>.</em></strong></p>\n<p><strong id=\"Topics_will_include_how_to_make_near_mode_progress_on_far_mode_problems__the_potential_for_cascades__cycles_and_recursive_loops_eg_AI_and_idea_propagation_to_dominate_effective_altruistic_concern__Also__theories_and_plans_can_be_proposed_by_all_participants_\"><em>Topics will include how to make near mode progress on far mode problems, the potential for cascades, cycles and recursive loops eg AI and idea propagation to dominate effective altruistic concern. Also, theories and plans can be proposed by all participants.</em></strong></p>\n<p>&nbsp;</p>\n<p><em><strong>Why meet online?</strong></em></p>\n<p>Lots of us want to improve the world. By donating, performing rationality training, inspiring one and other and so on.&nbsp;</p>\n<p>We're all in a state of limited information regarding how to best help. In hotspots for effective altruism like San Francisco and Oxford, effective altruists (EAs) are able to get high quality feedback on their ideas. But elsewhere, constructive,&nbsp;&nbsp;creative&nbsp;input is of limited supply.</p>\n<p>Face-to-face meetups have so far been organised by&nbsp;<a title=\"The High Impact Network\" href=\"http://www.google.com.au/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CCAQFjAA&amp;url=http%3A%2F%2Fwww.thehighimpactnetwork.org%2F&amp;ei=btCMUL3PM-2RiQeWuIGIBA&amp;usg=AFQjCNHlWGcIPwOcJA1dGilg9NxRV_vBZw\" target=\"_self\">The High Impact Network</a>,&nbsp;<a title=\"Giving What We Can\" href=\"/givingwhatwecan.org\" target=\"_self\">Giving What We Can</a>&nbsp;and these clearly allow members to complement one and others' knowledge, skills and resources (time, funding, etc), while boosting each others' determination.</p>\n<p>One would expect online meetups to offer qualitatively similar benefits. The comparative advantage of online EA meetups could be:</p>\n<p>1. helping experienced EAs to share the most up-to-date information and ideas quickly between geographically disparate meetups.</p>\n<p>2. to inspire otherwise isolated EAs</p>\n<p>3. to explore the utility of online video and other technology for spreading EA knowledge and skills.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Sounds_good__How_can_I_help_\"><em>Sounds good. How can I help?</em></strong></p>\n<p>I have written a draft task-list including an agenda here.&nbsp;<a href=\"http://checkvist.com/checklists/153407-online-optimal-philanthropy-meeting\">http://checkvist.com/checklists/153407-online-optimal-philanthropy-meeting</a>. I encourage you to email me for access before the meetup itself.</p>\n<p><span style=\"font-family: mceinline;\">I have created a&nbsp;<a href=\"http://whenisgood.net/hdyssxf\">Whenisgood time chart here</a>. Please use it to indicate when you are available to meet.</span>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"When_\"><em>When?</em></strong></p>\n<p>At&nbsp;<a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?msg=Online+Optimal+Philanthropy+Meeting&amp;iso=20121106T1330&amp;p1=152&amp;ah=1&amp;am=30\">1.30pm Melbourne Australian time</a>&nbsp;on&nbsp;<a href=\"https://plus.google.com/u/0/112628930686250912340/posts\">Google+ Hangouts</a>. Do add me to your circles and expect an invite.</p>\n<p>&nbsp;</p>\n<p>See you then.</p>\n<p>- <a href=\"https://plus.google.com/u/0/112628930686250912340/posts\">Theduffman</a> | THINK Melbourne organiser | former Felicifia admin</p>", "sections": [{"title": "Topics will include how to make near mode progress on far mode problems, the potential for cascades, cycles and recursive loops eg AI and idea propagation to dominate effective altruistic concern. Also, theories and plans can be proposed by all participants.", "anchor": "Topics_will_include_how_to_make_near_mode_progress_on_far_mode_problems__the_potential_for_cascades__cycles_and_recursive_loops_eg_AI_and_idea_propagation_to_dominate_effective_altruistic_concern__Also__theories_and_plans_can_be_proposed_by_all_participants_", "level": 1}, {"title": "Sounds good. How can I help?", "anchor": "Sounds_good__How_can_I_help_", "level": 1}, {"title": "When?", "anchor": "When_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-28T11:56:23.004Z", "modifiedAt": null, "url": null, "title": "[Link] Epigenetics", "slug": "link-epigenetics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:26.766Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JbfuHN2YQYHQqE9cG/link-epigenetics", "pageUrlRelative": "/posts/JbfuHN2YQYHQqE9cG/link-epigenetics", "linkUrl": "https://www.lesswrong.com/posts/JbfuHN2YQYHQqE9cG/link-epigenetics", "postedAtFormatted": "Sunday, October 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Epigenetics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Epigenetics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJbfuHN2YQYHQqE9cG%2Flink-epigenetics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Epigenetics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJbfuHN2YQYHQqE9cG%2Flink-epigenetics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJbfuHN2YQYHQqE9cG%2Flink-epigenetics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 831, "htmlBody": "<p>Your daily dose of science knowledge will once more be provided by Gregory Cochran, <a href=\"http://westhunt.wordpress.com/2012/10/26/epigenetics/\">clearing up</a> some misconceptions you may have heard about <a href=\"http://en.wikipedia.org/wiki/Epigenetics\">Epigenetics</a>.</p>\n<blockquote>\n<p>As I understand it, in some circles,&nbsp; there is a burgeoning hope that practice in this generation will somehow improve performance in the next &ndash; based on a word they have heard but do not understand. That word is epigenetics.</p>\n<p>Genes can certainly be modified in ways that persist.&nbsp;&nbsp; For example, the cells in your skin produce more skin cells when they divide, rather than muscle cells or neurons.&nbsp; Most of your cells have a copy of the entire human genome, but only certain elements are expressed in a particular type of cell, and that pattern persists&nbsp; when that&nbsp; kind of cell divides. We understand, to a degree, some of the chemical changes that cause these lasting changes in gene expression patterns.&nbsp;&nbsp; One is methylation,&nbsp; a method of suppressing gene activity.&nbsp; It involves attaching a methyl group to a cytosine base. This methylation pattern is copied when somatic cells divide.</p>\n<p>The question is whether A. such changes can persist into the next generation and B. if they do, is this some sort of adaptive process, rather than an occasional screwup?&nbsp; We&rsquo;re&nbsp; interested in whether this happens in humans,&nbsp; so we&rsquo;ll only consider mammals.</p>\n<p>It&rsquo;s rare, but sometimes it happens.&nbsp; It has only been found to happen at a few sites in the genome, and when it does happen,&nbsp; only a fraction of the offspring are affected. Probably the best known example is the agouti yellow allele in mice.&nbsp; Mice that carry this allele are fat, yellow, and prone to cancer and diabetes &ndash; some of them. Yellow mothers tend to have yellow babies,&nbsp; while genetically identical brown mothers mostly have brown babies.&nbsp; The agouti yellow allele is the product of a recent insertion in the genome, about 50 years ago.&nbsp; For the overwhelming majority of genes, the epigenetic markers are reset in a new embryo, which means that epigenetic changes induced by the parent&rsquo;s experiences disappear.&nbsp; The embryo is back at square one. &nbsp; This agouti yellow allele is screwed up &ndash; somehow the reset isn&rsquo;t happening correctly.</p>\n<p>In mice, the mammalian species in which most such investigations have been done,&nbsp; the few other locations in the genome where anything like this happens are mainly retroposons and other repeated elements.</p>\n<p>There is another way that you can get transmission across generations without genetic change.&nbsp; Rats that are nurtured by stressed mothers are more likely to be stressed.&nbsp; This isn&rsquo;t transmitted perfectly, but it happens.&nbsp; Presumably the uterine environment,&nbsp; or maybe maternal behavior, is different in stressed mice in a way that stresses their offspring.&nbsp;&nbsp; This reminds me of a science fiction story that abused this principle.&nbsp; The&nbsp; idea was that alligators (or maybe it was crocodiles) almost have a four-chambered heart, which is generally associated with higher metabolism and friskiness. Our protagonist operates on an alligator and soups up its heart: the now-more-vigorous animal has better blood circulation and lays healthier eggs that develop into babies that also have a working four-chambered heart. So &lsquo;normal&rsquo; alligators were like stressed mice: fix the problem and you get to see what they&rsquo;re really capable of. The problem was that the most interesting consequence was growing wings, flying around and eating people. Alligators turned out to be stunted dragons. Not so good.</p>\n<p>Anyhow, what reason is there to believe that reading Gradshteyn and Ryzhik until your eyes bleed will plant the seeds of math to come in your descendants?&nbsp; None. Oh, I can come up with a scenario, if you want: but it requires that civilization (in particular, the key part of civilization, heavy use of weird definite and indefinite integrals and vast reproductive rewards for those skilled in such things) has risen and fallen over and over again at fairly short (but irregular)&nbsp; intervals, so that humans have faced this adaptive problem over and over and over again.&nbsp; A little like the way in which generations of aphids do different things in the summer (parthenogenesis) than in the late fall (sexual reproduction) &ndash; although that probably depends on direct cues like length of day rather than epigenetic changes.&nbsp; Something like Motie history, maybe. But&nbsp; I don&rsquo;t believe it.&nbsp; Not even a little bit.</p>\n<p>Nature hasn&rsquo;t even figured out how to have Jewish boys be born circumcised yet.</p>\n<p>So why are people talking about this? Why do people like Tyler Cowen invoke it to ward off evil facts?</p>\n<p>Because they&rsquo;re chuckleheads, what else?</p>\n</blockquote>\n<p>I think we can be a bit more specific that that so lets take it as an exercise. Motivated cognition for starters.</p>\n<p>If you want to learn why the Conan the Barbarian was generated by better priors than modern history books, what the blind idiot god may have in store for you or how to solve thick problems check out other articles from the blog shared under the tag: <a href=\"/r/discussion/tag/westhunter/\">westhunter</a>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JbfuHN2YQYHQqE9cG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 4, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "19656", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-28T16:17:48.875Z", "modifiedAt": null, "url": null, "title": "Meetup : Who's going to Skepticon 5?", "slug": "meetup-who-s-going-to-skepticon-5", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:27.900Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S5Ci5ua8vihusMSfh/meetup-who-s-going-to-skepticon-5", "pageUrlRelative": "/posts/S5Ci5ua8vihusMSfh/meetup-who-s-going-to-skepticon-5", "linkUrl": "https://www.lesswrong.com/posts/S5Ci5ua8vihusMSfh/meetup-who-s-going-to-skepticon-5", "postedAtFormatted": "Sunday, October 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Who's%20going%20to%20Skepticon%205%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Who's%20going%20to%20Skepticon%205%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS5Ci5ua8vihusMSfh%2Fmeetup-who-s-going-to-skepticon-5%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Who's%20going%20to%20Skepticon%205%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS5Ci5ua8vihusMSfh%2Fmeetup-who-s-going-to-skepticon-5", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS5Ci5ua8vihusMSfh%2Fmeetup-who-s-going-to-skepticon-5", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 152, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fa'>Who's going to Skepticon 5?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 November 2012 01:06:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Springfield, Missouri</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>website: <a href=\"http://www.skepticon.org/\" rel=\"nofollow\">http://www.skepticon.org/</a></p>\n\n<p>Skepticon is a free event, and super-awesome. You can see how awesome it is by watching the videos from last year.</p>\n\n<p>Some of the workshops this year will be two CFAR Rationality Courses (given by Julia Galef and Spencer Greenberg) and a workshop on Ethical Rationality (by James Croft and Stephanie Zvan)</p>\n\n<p>Anyway, I thought it would be a good idea to put up a meetup where LWers can post that they are attending, and then we can PM each other with some contact info, so we can try to meet up at some time throughout the weekend (I am loathe to set a specific time, because people's schedules might change, but feel free to, if you prefer).</p>\n\n<p>My personal attendance is at about P=.7</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fa'>Who's going to Skepticon 5?</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S5Ci5ua8vihusMSfh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "19658", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Who_s_going_to_Skepticon_5_\">Discussion article for the meetup : <a href=\"/meetups/fa\">Who's going to Skepticon 5?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 November 2012 01:06:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Springfield, Missouri</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>website: <a href=\"http://www.skepticon.org/\" rel=\"nofollow\">http://www.skepticon.org/</a></p>\n\n<p>Skepticon is a free event, and super-awesome. You can see how awesome it is by watching the videos from last year.</p>\n\n<p>Some of the workshops this year will be two CFAR Rationality Courses (given by Julia Galef and Spencer Greenberg) and a workshop on Ethical Rationality (by James Croft and Stephanie Zvan)</p>\n\n<p>Anyway, I thought it would be a good idea to put up a meetup where LWers can post that they are attending, and then we can PM each other with some contact info, so we can try to meet up at some time throughout the weekend (I am loathe to set a specific time, because people's schedules might change, but feel free to, if you prefer).</p>\n\n<p>My personal attendance is at about P=.7</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Who_s_going_to_Skepticon_5_1\">Discussion article for the meetup : <a href=\"/meetups/fa\">Who's going to Skepticon 5?</a></h2>", "sections": [{"title": "Discussion article for the meetup : Who's going to Skepticon 5?", "anchor": "Discussion_article_for_the_meetup___Who_s_going_to_Skepticon_5_", "level": 1}, {"title": "Discussion article for the meetup : Who's going to Skepticon 5?", "anchor": "Discussion_article_for_the_meetup___Who_s_going_to_Skepticon_5_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-28T23:03:44.508Z", "modifiedAt": null, "url": null, "title": "Meetup : First Meetup- Cleveland and Akron, Ohio", "slug": "meetup-first-meetup-cleveland-and-akron-ohio", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:32.920Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z2To6z2HLNGQxdpk4/meetup-first-meetup-cleveland-and-akron-ohio", "pageUrlRelative": "/posts/z2To6z2HLNGQxdpk4/meetup-first-meetup-cleveland-and-akron-ohio", "linkUrl": "https://www.lesswrong.com/posts/z2To6z2HLNGQxdpk4/meetup-first-meetup-cleveland-and-akron-ohio", "postedAtFormatted": "Sunday, October 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Meetup-%20Cleveland%20and%20Akron%2C%20Ohio&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Meetup-%20Cleveland%20and%20Akron%2C%20Ohio%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz2To6z2HLNGQxdpk4%2Fmeetup-first-meetup-cleveland-and-akron-ohio%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Meetup-%20Cleveland%20and%20Akron%2C%20Ohio%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz2To6z2HLNGQxdpk4%2Fmeetup-first-meetup-cleveland-and-akron-ohio", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz2To6z2HLNGQxdpk4%2Fmeetup-first-meetup-cleveland-and-akron-ohio", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fb'>First Meetup- Cleveland and Akron, Ohio</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 November 2012 08:03:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Cleveland, OH</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I'm posting on behalf of someone who got in touch with me on the Ohio listserv. He is looking for people active in the Cleveland or Akron area. Right now, OHLW is only active in Cincinnati, Dayton, and Columbus.</p>\n\n<p>If you are in Cleveland or Akron, or attend school in the area, please leave  a comment here, or PM me. We will set a date and location once we figure out who is interested.</p>\n\n<p>Thank you!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fb'>First Meetup- Cleveland and Akron, Ohio</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z2To6z2HLNGQxdpk4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.0203224343522629e-06, "legacy": true, "legacyId": "19660", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Meetup__Cleveland_and_Akron__Ohio\">Discussion article for the meetup : <a href=\"/meetups/fb\">First Meetup- Cleveland and Akron, Ohio</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 November 2012 08:03:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Cleveland, OH</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I'm posting on behalf of someone who got in touch with me on the Ohio listserv. He is looking for people active in the Cleveland or Akron area. Right now, OHLW is only active in Cincinnati, Dayton, and Columbus.</p>\n\n<p>If you are in Cleveland or Akron, or attend school in the area, please leave  a comment here, or PM me. We will set a date and location once we figure out who is interested.</p>\n\n<p>Thank you!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_Meetup__Cleveland_and_Akron__Ohio1\">Discussion article for the meetup : <a href=\"/meetups/fb\">First Meetup- Cleveland and Akron, Ohio</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Meetup- Cleveland and Akron, Ohio", "anchor": "Discussion_article_for_the_meetup___First_Meetup__Cleveland_and_Akron__Ohio", "level": 1}, {"title": "Discussion article for the meetup : First Meetup- Cleveland and Akron, Ohio", "anchor": "Discussion_article_for_the_meetup___First_Meetup__Cleveland_and_Akron__Ohio1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-29T00:41:05.662Z", "modifiedAt": null, "url": null, "title": "Constructing fictional eugenics (LW edition)", "slug": "constructing-fictional-eugenics-lw-edition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:29.789Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MdbJXRofWkLpX24FD/constructing-fictional-eugenics-lw-edition", "pageUrlRelative": "/posts/MdbJXRofWkLpX24FD/constructing-fictional-eugenics-lw-edition", "linkUrl": "https://www.lesswrong.com/posts/MdbJXRofWkLpX24FD/constructing-fictional-eugenics-lw-edition", "postedAtFormatted": "Monday, October 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Constructing%20fictional%20eugenics%20(LW%20edition)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConstructing%20fictional%20eugenics%20(LW%20edition)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMdbJXRofWkLpX24FD%2Fconstructing-fictional-eugenics-lw-edition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Constructing%20fictional%20eugenics%20(LW%20edition)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMdbJXRofWkLpX24FD%2Fconstructing-fictional-eugenics-lw-edition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMdbJXRofWkLpX24FD%2Fconstructing-fictional-eugenics-lw-edition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1226, "htmlBody": "<p>Yvain <a href=\"http://squid314.livejournal.com/338295.html\">asked</a>:</p>\n<blockquote>\n<p><span style=\"font-family: Verdana, sans-serif; font-size: 13px;\">So if you had to design a eugenics program, how would you do it? Be creative.</span><br style=\"font-family: Verdana, sans-serif; font-size: 13px;\" /><br style=\"font-family: Verdana, sans-serif; font-size: 13px;\" /><span style=\"font-family: Verdana, sans-serif; font-size: 13px;\">I'm asking because I'm working on writing about a fictional society that practices eugenics. I want them to be interesting and sympathetic, and not to immediately pattern-match to a dystopia that kills everyone who doesn't look exactly alike.</span></p>\n</blockquote>\n<p><span style=\"font-family: Verdana, sans-serif; font-size: 13px;\">My reply was too long for LiveJournal, so I'm posting it here:</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\">1. &nbsp;The real step 1 in any program like this would be to buy the 3 best modern textbooks on animal breeding and read them. &nbsp;(My grandfather is a researcher in this field so I'm unusually aware that it exists.)</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\">2. &nbsp;If you give me genetic selection on multiple possible embryos where I can read off the genome of each one, I can do much better, much faster, than if I'm only allowed to look at the mother and father's genome and predict on that basis. &nbsp;If I can only look at the mother and father's relatives and life achievements, I do worse, but modern tech is very rapidly advancing to be able to read off the parents' genome cheaply.</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\">3. &nbsp;If society's utility has a large component for genius production, then you probably want a very diverse mix of different high-IQ genes combined into different genotypes and phenotypes. &nbsp;(Although some recent research suggests that the most important thing for IQ may be avoiding mutational load, i.e., the modal genome would be super-von-Neumann. &nbsp;Even so, we'd want a diverse mix of everything else cognitive that wasn't about modality.)</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\">4. &nbsp;Doing a Bayesian value-of-information calculation on rare alleles and potentially interesting allele combinations will automatically include a value for diversity into your eugenic program, based on the value of promoting a gene / combo in much larger numbers if that gene or gene combo is found to be successful. &nbsp;You would get much *more* interesting diversity in the next generation automatically, as many previously low-frequency alleles were combined in greater numbers and greater diversity than before. &nbsp;*Not* doing a value-of-info calculation accounts for a lot of the dystopic load of alleged dystopias.</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\">5. &nbsp;The obvious basic instrument in a society depicted as well-intentioned would be an economic policy of trying to internalize the externalities of a child, just like a well-intentioned society might try to internalize the externalities of e.g. carbon dioxide emissions, instead of regulating/capping them directly, in order to maximize net social welfare. &nbsp;There would be a tax or benefit based on how much your child is expected to cost society (not just governmental costs in the form of health care, schooling etc., but costs to society in general, including foregone labor of a working parent, etc.) and how much that child is expected to benefit society (not lifetime tax revenue or lifetime earnings, but lifetime value generated - most economic actors only capture a fraction of the value they create). &nbsp;If it looks like you're going to have a valuable child, you get your benefit in the form of a large cash bonus up-front (love that hyperbolic discounting) and lots of free childcare so you can go on having more children. &nbsp;The marketed social goal would be to avert the modern trope where parenthood is this dreadful burdensome inconvenience compared to playing video games, and this is bad for society because society runs out of valuable future workers whose benefits-to-society the parents mostly don't capture. &nbsp;Probably the hard part from a marketing standpoint would be the proposal to do actual genetic calculations, even if it's to allegedly increase social benefit and prevent the system from being \"exploited\" (i.e. going dysgenic-Malthusian).</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\">6. &nbsp;As suggested in an earlier comment, financializing progressive shares of future income (as diverted from tax streams, maybe) is an obvious way to privatize prediction, but only of tax streams, or at best revenue earned by the prospective individual. &nbsp;(I hadn't thought of this until I read that comment, so credit where it's due.)</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\">7. &nbsp;Taxes on expected-negative kids are more icky but would still have the obvious economic justification. &nbsp;A nicer-sounding way of framing it would be requiring parents to post bond corresponding to the baseline government cost of each child in schooling and healthcare, with expected value potentially helping to make up the bond. &nbsp;An interesting question is whether anyone would really work out to expected-net-negative under this system, which question is isomorphic to asking whether it ever makes selfish sense for a country to restrict immigration. &nbsp;But adding at least some burden here makes sense from a cognitive perspective, because adding a cost is better at shaping behavior than adding a potentially foregone benefit.</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\">8. &nbsp;The incentive for e.g. taking advantage of sperm banks is automatic in this system - you can either pay a bunch of money to have a kid with your current husband, or you can be paid thousands of dollars and get free child care to be inseminated by the sperm of a Nobel winner who never had to diet. &nbsp;I think that, in practice, the basic test of a system like this would be whether it could get people to go over the inconvenience threshold of actually using sperm banks and egg donors.</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\">9. &nbsp;More interestingly, there's a built-in incentive for most people to have daughters rather than sons under this system. &nbsp;If we take the expected externalities of grandchildren into account in calculating the expected externalities of a child, then daughters can bear children using the best sperm via gene banks, while men have a harder time getting at the best eggs, making the grandchildren of daughters much more valuable if you'll assume they'll all be Nobel-laureate-descendants. &nbsp;Daughters also add more marginal children to society than sons, since adding another son does not increase the marginal reproductive capacity of society unless single women aren't willing to reproduce using sperm banks (even taking into account subsidized childcare) and the polyamory factor has gone over what women with children are willing to tolerate. &nbsp;So if grandchildren are net positive, daughters are more marginally valuable to society until the sex ratio has gone well over 1:1. &nbsp;This is leaving aside generally larger criminal downsides of men, the fact that men do worse in school (which may be a mere artifact of our horror of a school system), and so on. &nbsp;However, if the sex ratio becomes very extreme and the system is supposed to stick around for many generations, then most of the males generated will be by people defying system incentives; and unless very few women reproduce with those males, there will be a large selective advantage for having sons outside the system. &nbsp;I.e., the system will be selecting for those who defy its incentives, which is a key design criterion for avoiding. &nbsp;(Though on yet further reflection, if there are many males with suboptimal genetics being produced and then reproducing, child-value calculations would rapidly yield the social advice to start birthing more above-average males even if they won't win the sperm-bank contest; and if women have a strong preference for present fathers, you could directly calculate that as social value as well as a factor in calculating expected genetic impact of males.)</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\">10. &nbsp;In the end, all of this just adds up to, \"If you can correctly internalize these externalities, the following social welfare factor will be increased...\" and the key part is of course that \"If\".</span></p>\n<p>(<a href=\"http://squid314.livejournal.com/338295.html\">See Yvain's post and comments as well</a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 2, "e9wHzopbGCAFwp9Rw": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MdbJXRofWkLpX24FD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 28, "extendedScore": null, "score": 6.4e-05, "legacy": true, "legacyId": "19661", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 175, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-29T03:47:45.527Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Logical or Connectionist AI?", "slug": "seq-rerun-logical-or-connectionist-ai", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zC3L4mmmrjoiQ7TN2/seq-rerun-logical-or-connectionist-ai", "pageUrlRelative": "/posts/zC3L4mmmrjoiQ7TN2/seq-rerun-logical-or-connectionist-ai", "linkUrl": "https://www.lesswrong.com/posts/zC3L4mmmrjoiQ7TN2/seq-rerun-logical-or-connectionist-ai", "postedAtFormatted": "Monday, October 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Logical%20or%20Connectionist%20AI%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Logical%20or%20Connectionist%20AI%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzC3L4mmmrjoiQ7TN2%2Fseq-rerun-logical-or-connectionist-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Logical%20or%20Connectionist%20AI%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzC3L4mmmrjoiQ7TN2%2Fseq-rerun-logical-or-connectionist-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzC3L4mmmrjoiQ7TN2%2Fseq-rerun-logical-or-connectionist-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p>Today's post, <a href=\"/lw/vv/logical_or_connectionist_ai/\">Logical or Connectionist AI?</a> was originally published on 17 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Logical_or_Connectionist_AI.3F\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The difference between Logical and Connectionist AIs is portrayed as a grand dichotomy between two different sides of the force. The truth is that they're just two different designs out of many possible ones.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/f56/seq_rerun_the_nature_of_logic/\">The Nature of Logic</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zC3L4mmmrjoiQ7TN2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0204770553063137e-06, "legacy": true, "legacyId": "19665", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["juomoqiNzeAuq4JMm", "iyAfYjCuv5TnzbsSK", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-29T22:50:35.671Z", "modifiedAt": null, "url": null, "title": "Original Research on Less Wrong", "slug": "original-research-on-less-wrong", "viewCount": null, "lastCommentedAt": "2021-11-14T13:40:30.551Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jTkmEGWM4dJAfE62W/original-research-on-less-wrong", "pageUrlRelative": "/posts/jTkmEGWM4dJAfE62W/original-research-on-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/jTkmEGWM4dJAfE62W/original-research-on-less-wrong", "postedAtFormatted": "Monday, October 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Original%20Research%20on%20Less%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOriginal%20Research%20on%20Less%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTkmEGWM4dJAfE62W%2Foriginal-research-on-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Original%20Research%20on%20Less%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTkmEGWM4dJAfE62W%2Foriginal-research-on-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTkmEGWM4dJAfE62W%2Foriginal-research-on-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3527, "htmlBody": "<p>Hundreds of Less Wrong posts <a href=\"/lw/eik/eliezers_sequences_and_mainstream_academia/\">summarize or repackage</a> work previously published in professional books and journals, but Less Wrong also hosts lots of original research in philosophy, decision theory, mathematical logic, and other fields. This post serves as a curated index of Less Wrong posts containing significant original research.</p>\n<p>Obviously, there is much fuzziness about what counts as \"significant\" or \"original.\" I'll be making lots of subjective judgment calls about which suggestions to add to this post. One clear rule is: I won't be linking anything that merely summarizes previous work (e.g. Stuart's <a href=\"/lw/ae5/trapping_ais_via_utility_indifference/\">summary</a> of <a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0020/18371/2010-1.pdf\">his earlier work on utility indifference</a>).</p>\n<p><small>Update 09/20/2013: Added  <a href=\"/r/discussion/lw/imz/notes_on_logical_priors_from_the_miri_workshop/\">Notes on logical priors from the MIRI workshop</a>, <a href=\"/lw/inh/cooperating_with_agents_with_different_ideas_of/\">Cooperating with agents with different ideas of fairness, while resisting exploitation</a>, <a href=\"/lw/hoz/do_earths_with_slower_economic_growth_have_a/\">Do Earths with slower economic growth have a better chance at FAI?</a></small></p>\n<p><small>Update 11/03/2013: Added  <a href=\"/lw/iqa/bayesian_probability_as_an_approximate_theory_of/\">Bayesian probability as an approximate theory of uncertainty?</a>, <a href=\"/lw/isp/on_the_importance_of_taking_limits_infinite/\">On the importance of taking limits: Infinite Spheres of Utility</a>, <a href=\"/lw/itu/of_all_the_siadoomsdays_in_the_all_the_worlds/\">Of all the SIA-doomsdays in the all the worlds...</a></small></p>\n<p><small>Update 01/22/2014: Added  <a href=\"/lw/ixs/change_the_labels_undo_infinitely_good/\">Change the labels, undo infinitely good</a>, <a href=\"/lw/iyx/reduced_impact_ai_no_back_channels/\">Reduced impact AI: no back channels</a>, <a href=\"/lw/j9u/international_cooperation_vs_ai_arms_race/\">International cooperation vs. AI arms race</a>, <a href=\"/lw/jcq/naturalistic_trust_among_ais_the_parable_of_the/\">Naturalistic trust among AIs: The parable of the thesis advisor&rsquo;s theorem</a></small></p>\n<hr />\n<h4>General philosophy</h4>\n<ul>\n<li>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 for Beginners</a>. Eliezer's bottom-up guide to truth, reference, meaningfulness, and epistemology. Includes practical applications and puzzling meditations.</p>\n</li>\n<li>\n<p><a href=\"/lw/5n9/seeing_red_dissolving_marys_room_and_qualia/\">Seeing Red</a>, <a href=\"/lw/5op/a_study_of_scarlet_the_conscious_mental_graph/\">A Study of Scarlet</a>, <a href=\"/lw/5ot/nature_red_in_truth_and_qualia/\">Nature: Red in Tooth and Qualia</a>. Orthonormal dissolves Mary's room and qualia.</p>\n</li>\n<li>\n<p><a href=\"/lw/ea8/counterfactual_resiliency_test_for_noncausal/\">Counterfactual resiliency test for non-causal models</a>. Stuart Armstrong suggests testing non-causal models for \"counterfactual resiliency.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/cxk/thoughts_and_problems_with_eliezers_measure_of/\">Thoughts and problems with Eliezer's measure of optimization power</a>. Stuart Armstrong examines some potential problems with Eliezer's concept of optimization power.</p>\n</li>\n<li>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Free_will_(solution)\">Free will</a>. Eliezer's particular compatibilist-style solution to the free will problem from reductionist viewpoint.</p>\n</li>\n<li>\n<p><a href=\"/lw/572/the_absolute_selfselection_assumption/\">The absolute Self-Selection Assumption</a>. A clarification on anthropic reasoning, focused on Wei Dai s UDASSA framework.</p>\n</li>\n<li>\n<p><a href=\"/lw/f7d/sia_conditional_probability_and_jaan_tallinns/\">SIA, conditional probability, and Jaan Tallinn s simulation tree</a>. Stuart Armstrong makes the bridge between Nick Bostrom s Self-Indication Assumption (SIA) and Jann Tallinn s of superintelligence reproduction.</p>\n</li>\n<li>\n<p><a href=\"/lw/fmq/mathematical_measures_of_optimization_power/\">Mathematical Measures of Optimization Power</a>. Alex Altair tackles one approach to mathematically formalizing Yudkowsky s Optimization Power concept.</p>\n</li>\n<li>\n<p><a href=\"/lw/hw8/caught_in_the_glare_of_two_anthropic_shadows/\">Caught in the glare of two anthropic shadows</a>. Stuart_Armstrong provides a detailed analysis of the \"anthrophic shadow\" concept and its implications.</p>\n</li>\n<li>\n<p><a href=\"/lw/iqa/bayesian_probability_as_an_approximate_theory_of/\">Bayesian probability as an approximate theory of uncertainty?</a>. Vladimir Slepnev argues that Bayesian probability is an imperfect approximation of what we want from a theory of uncertainty.</p>\n</li>\n<li>\n<p><a href=\"/lw/itu/of_all_the_siadoomsdays_in_the_all_the_worlds/\">Of all the SIA-doomsdays in the all the worlds...</a>. Stuart_Armstrong on the doomsday argument, the self-sampling assumption and the self-indication assumption.</p>\n</li>\n</ul>\n<h4><a id=\"more\"></a> \n<hr />\n</h4>\n<h4>Decision theory / AI architectures / mathematical logic</h4>\n<ul>\n<li>\n<p><a href=\"/lw/15m/towards_a_new_decision_theory/\">Towards a New Decision Theory</a>, <a href=\"/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/\">Explicit Optimization of Global Strategy (Fixing a Bug in UDT1)</a>. Wei Dai develops his new decision theory, UDT.</p>\n</li>\n<li>\n<p><a href=\"/lw/3l/counterfactual_mugging/\">Counterfactual Mugging</a>. Vladimir Nesov presents a new Newcomb-like problem.</p>\n</li>\n<li>\n<p><a href=\"/lw/f3v/cake_or_death/\">Cake, or death!</a> Summary: \"the naive cake-or-death problem emerges for a value learning agent when it expects its utility to change, but uses its current utility to rank its future actions,\" and \"the sophisticated cake-or-death problem emerges for a value learning agent when it expects its utility to change predictably in certain directions dependent on its own behavior.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/e4e/an_angle_of_attack_on_open_problem_1/\">An angle of attack on Open Problem #1</a>, <a href=\"/lw/e5j/how_to_cheat_l%C3%B6bs_theorem_my_second_try/\">How to cheat L&ouml;b's Theorem: my second try</a>. Benja tackles the problem of \"how, given L&ouml;b's theorem, an AI can replace itself with a better expected utility maximizer that believes in as much mathematics as the original AI.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/eaa/a_model_of_udt_with_a_concrete_prior_over_logical/\">A model of UDT with a concrete prior over logical statements</a>. Benja attacks the problem of logical uncertainty.</p>\n</li>\n<li>\n<p><a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/\">Decision Theories: A Less Wrong Primer</a>, <a href=\"/lw/axl/decision_theories_a_semiformal_analysis_part_i/\">Decision Theories: A Semi-Formal Analysis, Part I</a>, <a href=\"/lw/az6/decision_theories_a_semiformal_analysis_part_ii/\">Decision Theories: A Semi-Formal Analysis, Part II</a>, <a href=\"/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/\">Decision Theories: A Semi-Formal Analysis, Part III</a>, <a href=\"/lw/e94/decision_theories_part_35_halt_melt_and_catch_fire/\">Halt, Melt, and Catch Fire</a>, <a href=\"/lw/ebx/decision_theories_part_375_hang_on_i_think_this/\">Hang On, I Think This Works After All</a>. Orthonormal explains the TDT/UDT approach to decision theory and then develops his own TDT-like algorithm called Masquerade.</p>\n</li>\n<li>\n<p><a href=\"/lw/767/decision_theory_paradox_pd_with_three_implies/\">Decision Theory Paradox: PD with Three Implies Chaos?</a>. Orthonormal describes \"an apparent paradox in a three-agent variant of the Prisoner's Dilemma: despite full knowledge of each others' source codes, TDT agents allow themselves to be exploited by CDT, and lose completely to another simple decision theory.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/f37/naive_tdt_bayes_nets_and_counterfactual_mugging/\">Naive TDT, Bayes nets, and counterfactual mugging</a>. Stuart Armstrong suggests a reason for TDT's apparent failure on the counterfactual mugging problem.</p>\n</li>\n<li>\n<p><a href=\"/lw/dba/bounded_versions_of_g%C3%B6dels_and_l%C3%B6bs_theorems/\">Bounded versions of G&ouml;del's and L&ouml;b's theorems</a>, <a href=\"/lw/dcz/formalising_cousin_its_bounded_versions_of_g%C3%B6dels/\">Formalising cousin_it's bounded versions of G&ouml;del's theorem</a>. Vladimir Slepnev proposes bounded versions of G&ouml;del's and L&ouml;b's theorems, and Stuart Armstrong begins to formalize one of them.</p>\n</li>\n<li>\n<p><a href=\"/lw/854/satisficers_want_to_become_maximisers/\">Satisficers want to become maximisers</a>. Stuart Armstrong explains why satisficers want to become maximisers.</p>\n</li>\n<li>\n<p><a href=\"/lw/8rl/would_aixi_protect_itself/\">Would AIXI protect itself?</a>. Stuart Armstrong argues that \"with practice the AIXI [agent] would likely seek to protect its power source and existence, and would seek to protect its memory from 'bad memories' changes. It would want to increase the amount of 'good memory' changes. And it would not protect itself from changes to its algorithm and from the complete erasure of its memory. It may also develop indirect preferences for or against these manipulations if we change our behaviour based on them.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/a39/the_mathematics_of_reduced_impact_help_needed/\">The mathematics of reduced impact: help needed</a>. Stuart Armstrong explores some ways we might reduce the impact of maximizing AIs.</p>\n</li>\n<li>\n<p><a href=\"/lw/827/ai_ontology_crises_an_informal_typology/\">AI ontology crises: an informal typology</a>. Stuart Armstrong builds a typology of AI ontology crises, following <a href=\"http://singularity.org/files/OntologicalCrises.pdf\">de Blanc (2011)</a>.</p>\n</li>\n<li>\n<p><a href=\"/lw/3fm/in_the_paretooptimised_crowd_be_sure_to_know_your/\">In the Pareto-optimised crowd, be sure to know your place</a>. Stuart Armstrong argues that \"In a population playing independent two-player games, Pareto-optimal outcomes are only possible if there is an agreed universal scale of value relating each players' utility, and the players then acts to maximise the scaled sum of all utilities.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/\">If you don't know the name of the game, just tell me what I mean to you</a>. Stuart Armstrong summarizes: \"Both the Nash Bargaining solution (NBS), and the Kalai-Smorodinsky Bargaining Solution (KSBS), though acceptable for one-off games that are fully known in advance, are strictly inferior for independent repeated games, or when there exists uncertainty as to which game will be played.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/1vv/the_blackmail_equation/\">The Blackmail Equation</a>. Stuart Armstrong summarizes Eliezer's result on blackmail in decision theory.</p>\n</li>\n<li>\n<p><a href=\"/lw/1d5/expected_utility_without_the_independence_axiom/\">Expected utility without the independence axiom</a>. Stuart Armstrong summarizes: \"Deprived of independence, expected utility sneaks in via aggregation.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/\">An example of self-fulfilling spurious proofs in UDT</a>, <a href=\"/lw/b0e/a_model_of_udt_without_proof_limits/\">A model of UDT without proof limits</a>. Vladimir Slepnev examines several problems related to decision agents with spurious proof-searchers.</p>\n</li>\n<li>\n<p><a href=\"/lw/b0c/the_limited_predictor_problem/\">The limited predictor problem</a>. Vladimir Slepnev describes the limited predictor problem, \"a version of Newcomb's Problem where the predictor has limited computing resources. To predict the agent's action, the predictor simulates the agent for N steps. If the agent doesn't finish in N steps, the predictor assumes that the agent will two-box.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/8ys/a_way_of_specifying_utility_functions_for_udt/\">A way of specifying utility functions for UDT</a>. Vladimir Slepnev advances a method for specifying utility functions for UDT agents.</p>\n</li>\n<li>\n<p><a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">A model of UDT with a halting oracle</a>, <a href=\"/lw/9o7/formulas_of_arithmetic_that_behave_like_decision/\">Formulas of arithmetic that behave like decision agents</a>. Vladimir Slepnev specifies an optimality notion which \"matches our intuitions even though the universe is still perfectly deterministic and the agent is still embedded in it, because the oracle ensures that determinism is just out of the formal system's reach.\" Then, Nisan revisits some of this result's core ideas by representing the decision agents as formulas of Peano arithmetic.</p>\n</li>\n<li>\n<p><a href=\"/lw/8qy/aixi_and_existential_despair/\">AIXI and Existential Dispair</a>. Paul Christiano discusses how an approximate implementation of AIXI could lead to an erratically behaving system.</p>\n</li>\n<li>\n<p><a href=\"/lw/182/the_absentminded_driver/\">The Absent-Minded Driver</a>. In this post, Wei Dai examines the  absent-minded driver  problem. He tries to show how professional philosophers failed to reach the solution to time inconsistency, while rejecting Eliezer s  people are crazy  explanation.</p>\n</li>\n<li>\n<p><a href=\"/lw/8r0/clarification_of_ai_reflection_problem/\">Clarification of AI Reflection Problem</a>. A clear description for the commonly discussed problem in Less Wrong of reflection in AI systems, along with some possible solutions, by Paul Christiano.</p>\n</li>\n<li>\n<p><a href=\"/lw/3f3/motivating_optimization_processes/\">Motivating Optimization Processes</a>. Paul Christiano addresses the question of how and when we can expect an AGI to cooperate with humanity, and how it might be easier to implement than a completely Friendly AGI .</p>\n</li>\n<li>\n<p><a href=\"/lw/feo/universal_agents_and_utility_functions/\">Universal agents and utility functions</a>. Anja Heinisch replaces AIXI's reward function with a utility function .</p>\n</li>\n<li>\n<p><a href=\"/lw/15z/ingredients_of_timeless_decision_theory/\">Ingredients of Timeless Decision Theory</a>, <a href=\"/lw/135/timeless_decision_theory_problems_i_cant_solve/\">Timeless Decision Theory: Problems I Can t Solve</a>, <a href=\"/lw/164/timeless_decision_theory_and_metacircular/\">Timeless Decision Theory and Meta-Circular Decision Theory</a>. Eliezer s posts describe the main details of his Timeless Decision Theory, some problems for which he doesn t possess decision theories and reply to Gary Drescher s comment describing Meta-Circular Decision Theory. These insights later culminated in <a href=\"http://singularity.org/files/TDT.pdf\">Yudkowsky (2010)</a>.</p>\n</li>\n<li>\n<p><a href=\"/lw/16i/confusion_about_newcomb_is_confusion_about/\">Confusion about Newcomb is confusion about counterfactuals</a>, <a href=\"/lw/174/decision_theory_why_we_need_to_reduce_could_would/\">Why we need to reduce  could ,  would ,  should </a>, <a href=\"/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/\">Decision theory: why Pearl helps reduce  could  and  would , but still leaves us with at least three alternatives</a>.  Anna Salamon s posts use causal Bayes nets to explore the difficulty of interpreting counterfactual reasoning and the related concepts of  should,   could,  and  would.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/148/bayesian_utility_representing_preference_by/\">Bayesian Utility: Representing Preference by Probability Measures</a>. Vladimir Nesov presents a transformation of the standard expected utility formula.</p>\n</li>\n<li>\n<p><a href=\"/lw/fkx/a_definition_of_wireheading/\">A definition of wireheading</a>. Anja attempts to reach a definition of wireheading that encompasses the intuitions about the concept that have emerged from LW discussions.</p>\n</li>\n<li>\n<p><a href=\"/lw/fu1/why_you_must_maximize_expected_utility/\">Why you must maximize expected utility</a>. Benja presents a slight variant on the Von Neumann-Morgenstern approach to the axiomatic justification of the principle of maximizing expected utility.</p>\n</li>\n<li>\n<p><a href=\"/lw/fyr/a_utilitymaximizing_varient_of_aixi/\">A utility-maximizing varient of AIXI</a>. Alex Mennen builds on Anja's specification of a utility-maximizing variant of AIXI.</p>\n</li>\n<li>\n<p><a href=\"/lw/gap/a_fungibility_theorem/\">A fungibility theorem</a>. Nisan s alternative  to the von Neumann-Morgenstern theorem, proposing the maximization of the expectation of a linear aggregation of one s values.</p>\n</li>\n<li>\n<p><a href=\"/lw/ee2/logical_uncertainty_kind_of_a_proposal_at_least/\">Logical uncertainty, kind of. A proposal, at least</a>. Manfred's proposed solution on how to apply the basic laws of belief manipulation to cases where an agent is computationally limited.</p>\n</li>\n<li>\n<p><a href=\"/lw/gex/save_the_princess_a_tale_of_aixi_and_utility/\">Save the princess: A tale of AIXI and utility functions</a>. Anja discusses utility functions, delusion boxes, and cartesian dualism in attempting to improve upon the original AIXI formalism.</p>\n</li>\n<li>\n<p><a href=\"/lw/giu/naturalism_versus_unbounded_or_unmaximisable/\">Naturalism versus unbounded (or unmaximisable) utility options</a>. Stuart Armstrong poses a series of questions regarding unbounded utility functions.</p>\n</li>\n<li>\n<p><a href=\"/lw/f7t/beyond_bayesians_and_frequentists/\">Beyond Bayesians and Frequentists</a>. Jacob Steinhardt compares two approaches to statistics and discusses when to use them.</p>\n</li>\n<li>\n<p><a href=\"/lw/gr6/vnm_agents_and_lotteries_involving_an_infinite/\">VNM agents and lotteries involving an infinite number of possible outcomes</a>. AlexMennen summarizes: The VNM utility theorem only applies to lotteries that involve a finite number of possible outcomes. If an agent maximizes the expected value of a utility function when considering lotteries that involve a potentially infinite number of outcomes as well, then its utility function must be bounded.</p>\n</li>\n<li>\n<p><a href=\"/lw/gxf/a_problem_with_playing_chicken_with_the_universe/\">A Problem with playing chicken with the universe</a>. Karl explains how a model of UDT with a halting oracle might have some problematic elements.</p>\n</li>\n<li>\n<p><a href=\"/lw/h4x/intelligence_metrics_and_decision_theories/\">Intelligence Metrics and Decision Theories</a>, <a href=\"/lw/h93/metatickle_intelligence_metrics_and_friendly/\">Metatickle Intelligence Metrics and Friendly Utility Functions</a>. Squark reviews a few previously proposed mathematical metrics of general intelligence and proposes his own approach.</p>\n</li>\n<li>\n<p><a href=\"/lw/h9k/probabilistic_l\ufffdb_theorem/\">Probabilistic L\ufffdb theorem</a>, <a href=\"/lw/h9l/logic_in_the_language_of_probability/\">Logic in the Language of Probability</a>. Stuart Armstrong looks at whether reflective theories of logical uncertainty still suffer from L\ufffdb's theorem.</p>\n</li>\n<li>\n<p><a href=\"/r/discussion/lw/imz/notes_on_logical_priors_from_the_miri_workshop/\">Notes on logical priors from the MIRI workshop</a>. Vladimir Slepnev summarizes: \"In Counterfactual Mugging with a logical coin, a 'stupid' agent that can't compute the outcome of the coinflip should agree to pay, and a 'smart' agent that considers the coinflip as obvious as 1=1 should refuse to pay. But if a stupid agent is asked to write a smart agent, it will want to write an agent that will agree to pay. Therefore the smart agent who refuses to pay is reflectively inconsistent in some sense. What's the right thing to do in this case?\"</p>\n</li>\n<li>\n<p><a href=\"/lw/inh/cooperating_with_agents_with_different_ideas_of/\">Cooperating with agents with different ideas of fairness, while resisting exploitation</a>. Eliezer Yudkowsky investigates some ideas from the MIRI workshop that he hasn&rsquo;t seen in informal theories of negotiation.</p>\n</li>\n<li>\n<p><a href=\"/lw/jcq/naturalistic_trust_among_ais_the_parable_of_the/\">Naturalistic trust among AIs: The parable of the thesis advisor&rsquo;s theorem</a>. Benja discusses Nik Weaver's suggestion for 'naturalistic trust'.</p>\n</li>\n</ul>\n<h4>\n<hr />\n</h4>\n<h4>Ethics</h4>\n<ul>\n<li>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">The Metaethics Sequence</a>. Eliezer explains his theory of metaethics. Many readers have difficulty grokking his central points, and may find clarifications in the discussion <a href=\"/lw/435/what_is_eliezer_yudkowskys_metaethical_theory/\">here</a>.</p>\n</li>\n<li>\n<p><a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">No-Nonsense Metaethics</a>. Lukeprog begins to outline his theory of metaethics in this unfinished sequence.</p>\n</li>\n<li>\n<p><a href=\"/lw/xy/the_fun_theory_sequence/\">The Fun Theory Sequence</a>. Eliezer develops a new subfield of ethics, \"fun theory.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/778/consequentialism_need_not_be_nearsighted/\">Consequentialism Need Not Be Near-Sighted</a>. Orthonormal's summary: \"If you object to consequentialist ethical theories because you think they endorse horrible or catastrophic decisions, then you may instead be objecting to short-sighted utility functions or poor decision theories.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/d8z/a_small_critique_of_total_utilitarianism/\">A (small) critique of total utilitarianism</a>. Stuart Armstrong analyzes some weaknesses of total utilitarianism.</p>\n</li>\n<li>\n<p><a href=\"/lw/8qv/in_the_pareto_world_liars_prosper/\">In the Pareto world, liars prosper</a>. Stuart Armstrong presents a new picture proof of a previously known result, that \"if there is any decision process that will find a Pareto outcome for two people, it must be that liars will prosper: there are some circumstances where you would come out ahead if you were to lie about your utility function.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/2qq/politics_as_charity/\">Politics as Charity</a>, <a href=\"/lw/2ur/probability_and_politics/\">Probability and Politics</a>. Carl Shulman analyzes the prospects for doing effective charity work by influencing elections.</p>\n</li>\n<li>\n<p><a href=\"/lw/1ns/value_uncertainty_and_the_singleton_scenario/\">Value Uncertainty and the Singleton Scenario</a>. Wei Dai examines the problem of value uncertainty, a special case of moral uncertainty in which consequentialism is assumed.</p>\n</li>\n<li>\n<p><a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's Mugging: Tiny Probabilities of Vast Utilities</a>. Eliezer describes the problem of Pascal's Mugging, later published in <a href=\"http://www.nickbostrom.com/papers/pascal.pdf\">Bostrom (2009)</a>.</p>\n</li>\n<li>\n<p><a href=\"/lw/fyb/ontological_crisis_in_humans/\">Ontological Crisis in Humans</a>. Wei Dai presents the ontological crisis concept applied to human existence and goes over some examples.</p>\n</li>\n<li>\n<p><a href=\"/lw/g35/ideal_advisor_theories_and_personal_cev/\">Ideal Advisor Theories and Personal CEV</a>. Luke Muehlhauser and crazy88 place CEV in the context of mainstream moral philosophy, and use a variant of CEV to address a standard line of objections to ideal advisor theories in ethics..</p>\n</li>\n<li>\n<p><a href=\"/lw/g85/harsanyis_social_aggregation_theorem_and_what_it/\">Harsanyi s Social Aggregation Theorem and what it means for CEV</a>. Alex Mennen describes the relevance of Harsanyi's Social Aggregation Theorem to possible formalizations of CEV.</p>\n</li>\n<li>\n<p><a href=\"/lw/g5k/three_kinds_of_moral_uncertainty/\">Three Kinds of Moral Uncertainty</a>. Kaj Sotala's attempts to explain what it means to be uncertain about moral theories.</p>\n</li>\n<li>\n<p><a href=\"/lw/gln/a_brief_history_of_ethically_concerned_scientists/\">A brief history of ethically concerned scientists</a>. Kaj_Sotala gives historical examples of ethically concerned scientists.</p>\n</li>\n<li>\n<p><a href=\"/lw/ftk/pascals_mugging_for_bounded_utility_functions/\">Pascal s Mugging for bounded utility functions</a>. Benja s post describing the Pascal Mugging problem under truly bounded utility functions.</p>\n</li>\n<li>\n<p><a href=\"/lw/h8k/pascals_muggle_infinitesimal_priors_and_strong/\">Pascal's Muggle: Infinitesimal Priors and Strong Evidence</a>. Eliezer Yudkowsky discusses the role of infinitesimal priors and their decision-theoretic consequences in a \"Pascal's Mugging\"-type situation.</p>\n</li>\n<li>\n<p><a href=\"/lw/i0m/a_solution_to_preference_uncertainty_using_vnm/\">An Attempt at Preference Uncertainty Using VNM</a>. nyan_sandwich tackles the problem of making decisions when you are uncertain about what your object-level preferences should be.</p>\n</li>\n<li>\n<p><a href=\"/lw/hv6/gains_from_trade_slug_versus_galaxy_how_much/\">Gains from trade: Slug versus Galaxy - how much would I give up to control you?</a> and <a href=\"/lw/i20/even_with_default_points_systems_remain/\">Even with default points, systems remain exploitable</a>. Stuart_Armstrong provides a suggestion as to how to split the gains from trade in some situations and how such solutions can be exploitable.</p>\n</li>\n<li>\n<p><a href=\"/lw/isp/on_the_importance_of_taking_limits_infinite/\">On the importance of taking limits: Infinite Spheres of Utility</a>. aspera shows that \"if we want to make a decision based on additive utility, the infinite problem is ill posed; it has no unique solution unless we take on additional assumptions.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/ixs/change_the_labels_undo_infinitely_good/\">Change the labels, undo infinitely good</a>. Stuart_Armstrong talks about a small selection of paradoxes connected with infinite ethics.</p>\n</li>\n</ul>\n<h4>\n<hr />\n</h4>\n<h4>AI Risk Strategy</h4>\n<ul>\n<li>\n<p><a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">AI Risk and Opportunity: A Strategic Analysis</a>. The only original work here so far is the two-part history of AI risk thought, including descriptions of works previously unknown to the LW/SI/FHI community (e.g. Good <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Speculations-on-perceptrons-and-other-automata.pdf\">1959</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Some-future-social-repurcussions-of-computers.pdf\">1970</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Ethical-Machines.pdf\">1982</a>; <a href=\"http://www.amazon.com/Other-Worlds-Than-Cecil-Maxwell/dp/0800861256/\">Cade 1966</a>).</p>\n</li>\n<li>\n<p><a href=\"/lw/e36/ai_timeline_predictions_are_we_getting_better/\">AI timeline predictions: Are we getting better?</a>, <a href=\"/lw/e79/ai_timeline_prediction_data/\">AI timeline prediction data</a>. A preview of Stuart Armstrong's and Kaj Sotala's work on AI predictions, later published in <a href=\"http://singularity.org/files/Armstrong-Sotala-How-Were-Predicting-AI-or-Failing-to.pdf\">Armstrong &amp; Sotala (2012)</a>.</p>\n</li>\n<li>\n<p><a href=\"/lw/gta/selfassessment_in_expert_ai_predictions/\">Self-Assessment in Expert Ai Prediction</a>. Stuart_Armstrong suggests that the predictive accuracy of self selected experts might be different from those elicited from less selected groups.</p>\n</li>\n<li>\n<p><a href=\"/lw/diz/kurzweils_predictions_good_accuracy_poor/\">Kurzweil's predictions: good accuracy, poor self-calibration</a>. A new analysis of Kurzweil's predictions, by Stuart Armstrong.</p>\n</li>\n<li>\n<p><a href=\"/lw/cfd/tools_versus_agents/\">Tools versus agents</a>, <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Reply to Holden on Tool AI</a>. Stuart Armstrong and Eliezer examine Holden's proposal for Tool AI.</p>\n</li>\n<li>\n<p><a href=\"/lw/aro/what_is_the_best_compact_formalization_of_the/\">What is the best compact formalization of the argument for AI risk from fast takeoff?</a> A suggestion of a few steps on how to compact and clarify the argument for the Singularity Institute s  Big Scary Idea , by LW user utility monster.</p>\n</li>\n<li>\n<p><a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">The Hanson-Yudkowsky AI-Foom Debate</a>. The 2008 debate between Robin Hanson and Eliezer Yudkowsky, largely used to exemplify the difficulty of resolving disagreements even between expert rationalists. It focus on the likelihood of hard AI takeoff, the need for a theory of Friendliness, the future of AI, brain emulations and recursive improvement.</p>\n</li>\n<li>\n<p><a href=\"/lw/3dw/what_can_you_do_with_an_unfriendly_ai/\">What can you do with an Unfriendly AI?</a> Paul Christiano discusses how we could eventually turn an Unfriendly AI into a useful system by filtering the way it interacts and constraining how its answers are given to us.</p>\n</li>\n<li>\n<p><a href=\"/lw/3cz/cryptographic_boxes_for_unfriendly_ai/\">Cryptographic Boxes for Unfriendly AI</a>. Following the tone of the previous post and AI boxing in general, this post by Paul Christiano explores the possibility of using cryptography as a way to guarantee friendly outputs.</p>\n</li>\n<li>\n<p><a href=\"/lw/ffh/how_can_i_reduce_existential_risk_from_ai/\">How can I reduce existential risk from AI?</a>. A post by Luke Muehlhauser describing the Meta, Strategic and Direct work one could do in order to reduce the risk for humanity stemming from AI.</p>\n</li>\n<li>\n<p><a href=\"/lw/bs3/intelligence_explosion_vs_cooperative_explosion/\">Intelligence explosion vs Co-operative explosion</a>. Kaj Sotala s description of how an intelligence explosion could emerge from the cooperation of individual artificial intelligent systems forming a superorganism.</p>\n</li>\n<li>\n<p><a href=\"/lw/gbi/assessing_kurzweil_the_results/\">Assessing Kurzweil: The Results</a>; <a href=\"/lw/gbh/assessing_kurzweil_the_gory_details/\">Assessing Kurzweil: The Gory Details</a>. Stuart_Armstrong's attempt to evaluate the accuracy of Ray Kurzweil's model of technological intelligence development.</p>\n</li>\n<li>\n<p><a href=\"/lw/gmx/domesticating_reduced_impact_ais/\">Domesticating reduced impact AIs</a>. Stuart Armstrong attempts to give a solid foundation from which one can build a 'reduced impact AI'.</p>\n</li>\n<li>\n<p><a href=\"/lw/gn3/why_ai_may_not_foom/\">Why Ai may not foom</a>. John_Maxwell_IV explains how the intelligence of a self-improving AI may not grow as fast as we might think.</p>\n</li>\n<li>\n<p><a href=\"/lw/hdw/singleton_the_risks_and_benefits_of_one_world/\">Singleton: the risks and benefits of one world governments</a>. Stuart_Armstrong attempts to lay out a reasonable plan for tackling the singleton problem.</p>\n</li>\n<li>\n<p><a href=\"/lw/hoz/do_earths_with_slower_economic_growth_have_a/\">Do Earths with slower economic growth have a better chance at FAI?</a>. Eliezer Yudkowsky argues that GDP growth acceleration may actually decrease our chances of getting FAI.</p>\n</li>\n<li>\n<p><a href=\"/lw/iyx/reduced_impact_ai_no_back_channels/\">Reduced impact AI: no back channels</a>. Stuart_Armstrong presents a further development of the reduced impact AI approach.</p>\n</li>\n<li>\n<p><a href=\"/lw/j9u/international_cooperation_vs_ai_arms_race/\">International cooperation vs. AI arms race</a>. Brian_Tomasik talks about the role of government in a possible AI arms race.</p>\n</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 4, "3uE2pXvbcnS9nnZRE": 2, "GQyPQcdEQF4zXhJBq": 2, "MfpEPj6kJneT9gWT6": 2, "izp6eeJJEg9v5zcur": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jTkmEGWM4dJAfE62W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 47, "extendedScore": null, "score": 0.000109, "legacy": true, "legacyId": "19680", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 47, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Hundreds of Less Wrong posts <a href=\"/lw/eik/eliezers_sequences_and_mainstream_academia/\">summarize or repackage</a> work previously published in professional books and journals, but Less Wrong also hosts lots of original research in philosophy, decision theory, mathematical logic, and other fields. This post serves as a curated index of Less Wrong posts containing significant original research.</p>\n<p>Obviously, there is much fuzziness about what counts as \"significant\" or \"original.\" I'll be making lots of subjective judgment calls about which suggestions to add to this post. One clear rule is: I won't be linking anything that merely summarizes previous work (e.g. Stuart's <a href=\"/lw/ae5/trapping_ais_via_utility_indifference/\">summary</a> of <a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0020/18371/2010-1.pdf\">his earlier work on utility indifference</a>).</p>\n<p><small>Update 09/20/2013: Added  <a href=\"/r/discussion/lw/imz/notes_on_logical_priors_from_the_miri_workshop/\">Notes on logical priors from the MIRI workshop</a>, <a href=\"/lw/inh/cooperating_with_agents_with_different_ideas_of/\">Cooperating with agents with different ideas of fairness, while resisting exploitation</a>, <a href=\"/lw/hoz/do_earths_with_slower_economic_growth_have_a/\">Do Earths with slower economic growth have a better chance at FAI?</a></small></p>\n<p><small>Update 11/03/2013: Added  <a href=\"/lw/iqa/bayesian_probability_as_an_approximate_theory_of/\">Bayesian probability as an approximate theory of uncertainty?</a>, <a href=\"/lw/isp/on_the_importance_of_taking_limits_infinite/\">On the importance of taking limits: Infinite Spheres of Utility</a>, <a href=\"/lw/itu/of_all_the_siadoomsdays_in_the_all_the_worlds/\">Of all the SIA-doomsdays in the all the worlds...</a></small></p>\n<p><small>Update 01/22/2014: Added  <a href=\"/lw/ixs/change_the_labels_undo_infinitely_good/\">Change the labels, undo infinitely good</a>, <a href=\"/lw/iyx/reduced_impact_ai_no_back_channels/\">Reduced impact AI: no back channels</a>, <a href=\"/lw/j9u/international_cooperation_vs_ai_arms_race/\">International cooperation vs. AI arms race</a>, <a href=\"/lw/jcq/naturalistic_trust_among_ais_the_parable_of_the/\">Naturalistic trust among AIs: The parable of the thesis advisor\u2019s theorem</a></small></p>\n<hr>\n<h4 id=\"General_philosophy\">General philosophy</h4>\n<ul>\n<li>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 for Beginners</a>. Eliezer's bottom-up guide to truth, reference, meaningfulness, and epistemology. Includes practical applications and puzzling meditations.</p>\n</li>\n<li>\n<p><a href=\"/lw/5n9/seeing_red_dissolving_marys_room_and_qualia/\">Seeing Red</a>, <a href=\"/lw/5op/a_study_of_scarlet_the_conscious_mental_graph/\">A Study of Scarlet</a>, <a href=\"/lw/5ot/nature_red_in_truth_and_qualia/\">Nature: Red in Tooth and Qualia</a>. Orthonormal dissolves Mary's room and qualia.</p>\n</li>\n<li>\n<p><a href=\"/lw/ea8/counterfactual_resiliency_test_for_noncausal/\">Counterfactual resiliency test for non-causal models</a>. Stuart Armstrong suggests testing non-causal models for \"counterfactual resiliency.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/cxk/thoughts_and_problems_with_eliezers_measure_of/\">Thoughts and problems with Eliezer's measure of optimization power</a>. Stuart Armstrong examines some potential problems with Eliezer's concept of optimization power.</p>\n</li>\n<li>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Free_will_(solution)\">Free will</a>. Eliezer's particular compatibilist-style solution to the free will problem from reductionist viewpoint.</p>\n</li>\n<li>\n<p><a href=\"/lw/572/the_absolute_selfselection_assumption/\">The absolute Self-Selection Assumption</a>. A clarification on anthropic reasoning, focused on Wei Dai s UDASSA framework.</p>\n</li>\n<li>\n<p><a href=\"/lw/f7d/sia_conditional_probability_and_jaan_tallinns/\">SIA, conditional probability, and Jaan Tallinn s simulation tree</a>. Stuart Armstrong makes the bridge between Nick Bostrom s Self-Indication Assumption (SIA) and Jann Tallinn s of superintelligence reproduction.</p>\n</li>\n<li>\n<p><a href=\"/lw/fmq/mathematical_measures_of_optimization_power/\">Mathematical Measures of Optimization Power</a>. Alex Altair tackles one approach to mathematically formalizing Yudkowsky s Optimization Power concept.</p>\n</li>\n<li>\n<p><a href=\"/lw/hw8/caught_in_the_glare_of_two_anthropic_shadows/\">Caught in the glare of two anthropic shadows</a>. Stuart_Armstrong provides a detailed analysis of the \"anthrophic shadow\" concept and its implications.</p>\n</li>\n<li>\n<p><a href=\"/lw/iqa/bayesian_probability_as_an_approximate_theory_of/\">Bayesian probability as an approximate theory of uncertainty?</a>. Vladimir Slepnev argues that Bayesian probability is an imperfect approximation of what we want from a theory of uncertainty.</p>\n</li>\n<li>\n<p><a href=\"/lw/itu/of_all_the_siadoomsdays_in_the_all_the_worlds/\">Of all the SIA-doomsdays in the all the worlds...</a>. Stuart_Armstrong on the doomsday argument, the self-sampling assumption and the self-indication assumption.</p>\n</li>\n</ul>\n<h4><a id=\"more\"></a> \n<hr>\n</h4>\n<h4 id=\"Decision_theory___AI_architectures___mathematical_logic\">Decision theory / AI architectures / mathematical logic</h4>\n<ul>\n<li>\n<p><a href=\"/lw/15m/towards_a_new_decision_theory/\">Towards a New Decision Theory</a>, <a href=\"/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/\">Explicit Optimization of Global Strategy (Fixing a Bug in UDT1)</a>. Wei Dai develops his new decision theory, UDT.</p>\n</li>\n<li>\n<p><a href=\"/lw/3l/counterfactual_mugging/\">Counterfactual Mugging</a>. Vladimir Nesov presents a new Newcomb-like problem.</p>\n</li>\n<li>\n<p><a href=\"/lw/f3v/cake_or_death/\">Cake, or death!</a> Summary: \"the naive cake-or-death problem emerges for a value learning agent when it expects its utility to change, but uses its current utility to rank its future actions,\" and \"the sophisticated cake-or-death problem emerges for a value learning agent when it expects its utility to change predictably in certain directions dependent on its own behavior.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/e4e/an_angle_of_attack_on_open_problem_1/\">An angle of attack on Open Problem #1</a>, <a href=\"/lw/e5j/how_to_cheat_l%C3%B6bs_theorem_my_second_try/\">How to cheat L\u00f6b's Theorem: my second try</a>. Benja tackles the problem of \"how, given L\u00f6b's theorem, an AI can replace itself with a better expected utility maximizer that believes in as much mathematics as the original AI.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/eaa/a_model_of_udt_with_a_concrete_prior_over_logical/\">A model of UDT with a concrete prior over logical statements</a>. Benja attacks the problem of logical uncertainty.</p>\n</li>\n<li>\n<p><a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/\">Decision Theories: A Less Wrong Primer</a>, <a href=\"/lw/axl/decision_theories_a_semiformal_analysis_part_i/\">Decision Theories: A Semi-Formal Analysis, Part I</a>, <a href=\"/lw/az6/decision_theories_a_semiformal_analysis_part_ii/\">Decision Theories: A Semi-Formal Analysis, Part II</a>, <a href=\"/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/\">Decision Theories: A Semi-Formal Analysis, Part III</a>, <a href=\"/lw/e94/decision_theories_part_35_halt_melt_and_catch_fire/\">Halt, Melt, and Catch Fire</a>, <a href=\"/lw/ebx/decision_theories_part_375_hang_on_i_think_this/\">Hang On, I Think This Works After All</a>. Orthonormal explains the TDT/UDT approach to decision theory and then develops his own TDT-like algorithm called Masquerade.</p>\n</li>\n<li>\n<p><a href=\"/lw/767/decision_theory_paradox_pd_with_three_implies/\">Decision Theory Paradox: PD with Three Implies Chaos?</a>. Orthonormal describes \"an apparent paradox in a three-agent variant of the Prisoner's Dilemma: despite full knowledge of each others' source codes, TDT agents allow themselves to be exploited by CDT, and lose completely to another simple decision theory.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/f37/naive_tdt_bayes_nets_and_counterfactual_mugging/\">Naive TDT, Bayes nets, and counterfactual mugging</a>. Stuart Armstrong suggests a reason for TDT's apparent failure on the counterfactual mugging problem.</p>\n</li>\n<li>\n<p><a href=\"/lw/dba/bounded_versions_of_g%C3%B6dels_and_l%C3%B6bs_theorems/\">Bounded versions of G\u00f6del's and L\u00f6b's theorems</a>, <a href=\"/lw/dcz/formalising_cousin_its_bounded_versions_of_g%C3%B6dels/\">Formalising cousin_it's bounded versions of G\u00f6del's theorem</a>. Vladimir Slepnev proposes bounded versions of G\u00f6del's and L\u00f6b's theorems, and Stuart Armstrong begins to formalize one of them.</p>\n</li>\n<li>\n<p><a href=\"/lw/854/satisficers_want_to_become_maximisers/\">Satisficers want to become maximisers</a>. Stuart Armstrong explains why satisficers want to become maximisers.</p>\n</li>\n<li>\n<p><a href=\"/lw/8rl/would_aixi_protect_itself/\">Would AIXI protect itself?</a>. Stuart Armstrong argues that \"with practice the AIXI [agent] would likely seek to protect its power source and existence, and would seek to protect its memory from 'bad memories' changes. It would want to increase the amount of 'good memory' changes. And it would not protect itself from changes to its algorithm and from the complete erasure of its memory. It may also develop indirect preferences for or against these manipulations if we change our behaviour based on them.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/a39/the_mathematics_of_reduced_impact_help_needed/\">The mathematics of reduced impact: help needed</a>. Stuart Armstrong explores some ways we might reduce the impact of maximizing AIs.</p>\n</li>\n<li>\n<p><a href=\"/lw/827/ai_ontology_crises_an_informal_typology/\">AI ontology crises: an informal typology</a>. Stuart Armstrong builds a typology of AI ontology crises, following <a href=\"http://singularity.org/files/OntologicalCrises.pdf\">de Blanc (2011)</a>.</p>\n</li>\n<li>\n<p><a href=\"/lw/3fm/in_the_paretooptimised_crowd_be_sure_to_know_your/\">In the Pareto-optimised crowd, be sure to know your place</a>. Stuart Armstrong argues that \"In a population playing independent two-player games, Pareto-optimal outcomes are only possible if there is an agreed universal scale of value relating each players' utility, and the players then acts to maximise the scaled sum of all utilities.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/\">If you don't know the name of the game, just tell me what I mean to you</a>. Stuart Armstrong summarizes: \"Both the Nash Bargaining solution (NBS), and the Kalai-Smorodinsky Bargaining Solution (KSBS), though acceptable for one-off games that are fully known in advance, are strictly inferior for independent repeated games, or when there exists uncertainty as to which game will be played.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/1vv/the_blackmail_equation/\">The Blackmail Equation</a>. Stuart Armstrong summarizes Eliezer's result on blackmail in decision theory.</p>\n</li>\n<li>\n<p><a href=\"/lw/1d5/expected_utility_without_the_independence_axiom/\">Expected utility without the independence axiom</a>. Stuart Armstrong summarizes: \"Deprived of independence, expected utility sneaks in via aggregation.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/\">An example of self-fulfilling spurious proofs in UDT</a>, <a href=\"/lw/b0e/a_model_of_udt_without_proof_limits/\">A model of UDT without proof limits</a>. Vladimir Slepnev examines several problems related to decision agents with spurious proof-searchers.</p>\n</li>\n<li>\n<p><a href=\"/lw/b0c/the_limited_predictor_problem/\">The limited predictor problem</a>. Vladimir Slepnev describes the limited predictor problem, \"a version of Newcomb's Problem where the predictor has limited computing resources. To predict the agent's action, the predictor simulates the agent for N steps. If the agent doesn't finish in N steps, the predictor assumes that the agent will two-box.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/8ys/a_way_of_specifying_utility_functions_for_udt/\">A way of specifying utility functions for UDT</a>. Vladimir Slepnev advances a method for specifying utility functions for UDT agents.</p>\n</li>\n<li>\n<p><a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">A model of UDT with a halting oracle</a>, <a href=\"/lw/9o7/formulas_of_arithmetic_that_behave_like_decision/\">Formulas of arithmetic that behave like decision agents</a>. Vladimir Slepnev specifies an optimality notion which \"matches our intuitions even though the universe is still perfectly deterministic and the agent is still embedded in it, because the oracle ensures that determinism is just out of the formal system's reach.\" Then, Nisan revisits some of this result's core ideas by representing the decision agents as formulas of Peano arithmetic.</p>\n</li>\n<li>\n<p><a href=\"/lw/8qy/aixi_and_existential_despair/\">AIXI and Existential Dispair</a>. Paul Christiano discusses how an approximate implementation of AIXI could lead to an erratically behaving system.</p>\n</li>\n<li>\n<p><a href=\"/lw/182/the_absentminded_driver/\">The Absent-Minded Driver</a>. In this post, Wei Dai examines the  absent-minded driver  problem. He tries to show how professional philosophers failed to reach the solution to time inconsistency, while rejecting Eliezer s  people are crazy  explanation.</p>\n</li>\n<li>\n<p><a href=\"/lw/8r0/clarification_of_ai_reflection_problem/\">Clarification of AI Reflection Problem</a>. A clear description for the commonly discussed problem in Less Wrong of reflection in AI systems, along with some possible solutions, by Paul Christiano.</p>\n</li>\n<li>\n<p><a href=\"/lw/3f3/motivating_optimization_processes/\">Motivating Optimization Processes</a>. Paul Christiano addresses the question of how and when we can expect an AGI to cooperate with humanity, and how it might be easier to implement than a completely Friendly AGI .</p>\n</li>\n<li>\n<p><a href=\"/lw/feo/universal_agents_and_utility_functions/\">Universal agents and utility functions</a>. Anja Heinisch replaces AIXI's reward function with a utility function .</p>\n</li>\n<li>\n<p><a href=\"/lw/15z/ingredients_of_timeless_decision_theory/\">Ingredients of Timeless Decision Theory</a>, <a href=\"/lw/135/timeless_decision_theory_problems_i_cant_solve/\">Timeless Decision Theory: Problems I Can t Solve</a>, <a href=\"/lw/164/timeless_decision_theory_and_metacircular/\">Timeless Decision Theory and Meta-Circular Decision Theory</a>. Eliezer s posts describe the main details of his Timeless Decision Theory, some problems for which he doesn t possess decision theories and reply to Gary Drescher s comment describing Meta-Circular Decision Theory. These insights later culminated in <a href=\"http://singularity.org/files/TDT.pdf\">Yudkowsky (2010)</a>.</p>\n</li>\n<li>\n<p><a href=\"/lw/16i/confusion_about_newcomb_is_confusion_about/\">Confusion about Newcomb is confusion about counterfactuals</a>, <a href=\"/lw/174/decision_theory_why_we_need_to_reduce_could_would/\">Why we need to reduce  could ,  would ,  should </a>, <a href=\"/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/\">Decision theory: why Pearl helps reduce  could  and  would , but still leaves us with at least three alternatives</a>.  Anna Salamon s posts use causal Bayes nets to explore the difficulty of interpreting counterfactual reasoning and the related concepts of  should,   could,  and  would.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/148/bayesian_utility_representing_preference_by/\">Bayesian Utility: Representing Preference by Probability Measures</a>. Vladimir Nesov presents a transformation of the standard expected utility formula.</p>\n</li>\n<li>\n<p><a href=\"/lw/fkx/a_definition_of_wireheading/\">A definition of wireheading</a>. Anja attempts to reach a definition of wireheading that encompasses the intuitions about the concept that have emerged from LW discussions.</p>\n</li>\n<li>\n<p><a href=\"/lw/fu1/why_you_must_maximize_expected_utility/\">Why you must maximize expected utility</a>. Benja presents a slight variant on the Von Neumann-Morgenstern approach to the axiomatic justification of the principle of maximizing expected utility.</p>\n</li>\n<li>\n<p><a href=\"/lw/fyr/a_utilitymaximizing_varient_of_aixi/\">A utility-maximizing varient of AIXI</a>. Alex Mennen builds on Anja's specification of a utility-maximizing variant of AIXI.</p>\n</li>\n<li>\n<p><a href=\"/lw/gap/a_fungibility_theorem/\">A fungibility theorem</a>. Nisan s alternative  to the von Neumann-Morgenstern theorem, proposing the maximization of the expectation of a linear aggregation of one s values.</p>\n</li>\n<li>\n<p><a href=\"/lw/ee2/logical_uncertainty_kind_of_a_proposal_at_least/\">Logical uncertainty, kind of. A proposal, at least</a>. Manfred's proposed solution on how to apply the basic laws of belief manipulation to cases where an agent is computationally limited.</p>\n</li>\n<li>\n<p><a href=\"/lw/gex/save_the_princess_a_tale_of_aixi_and_utility/\">Save the princess: A tale of AIXI and utility functions</a>. Anja discusses utility functions, delusion boxes, and cartesian dualism in attempting to improve upon the original AIXI formalism.</p>\n</li>\n<li>\n<p><a href=\"/lw/giu/naturalism_versus_unbounded_or_unmaximisable/\">Naturalism versus unbounded (or unmaximisable) utility options</a>. Stuart Armstrong poses a series of questions regarding unbounded utility functions.</p>\n</li>\n<li>\n<p><a href=\"/lw/f7t/beyond_bayesians_and_frequentists/\">Beyond Bayesians and Frequentists</a>. Jacob Steinhardt compares two approaches to statistics and discusses when to use them.</p>\n</li>\n<li>\n<p><a href=\"/lw/gr6/vnm_agents_and_lotteries_involving_an_infinite/\">VNM agents and lotteries involving an infinite number of possible outcomes</a>. AlexMennen summarizes: The VNM utility theorem only applies to lotteries that involve a finite number of possible outcomes. If an agent maximizes the expected value of a utility function when considering lotteries that involve a potentially infinite number of outcomes as well, then its utility function must be bounded.</p>\n</li>\n<li>\n<p><a href=\"/lw/gxf/a_problem_with_playing_chicken_with_the_universe/\">A Problem with playing chicken with the universe</a>. Karl explains how a model of UDT with a halting oracle might have some problematic elements.</p>\n</li>\n<li>\n<p><a href=\"/lw/h4x/intelligence_metrics_and_decision_theories/\">Intelligence Metrics and Decision Theories</a>, <a href=\"/lw/h93/metatickle_intelligence_metrics_and_friendly/\">Metatickle Intelligence Metrics and Friendly Utility Functions</a>. Squark reviews a few previously proposed mathematical metrics of general intelligence and proposes his own approach.</p>\n</li>\n<li>\n<p><a href=\"/lw/h9k/probabilistic_l\ufffdb_theorem/\">Probabilistic L\ufffdb theorem</a>, <a href=\"/lw/h9l/logic_in_the_language_of_probability/\">Logic in the Language of Probability</a>. Stuart Armstrong looks at whether reflective theories of logical uncertainty still suffer from L\ufffdb's theorem.</p>\n</li>\n<li>\n<p><a href=\"/r/discussion/lw/imz/notes_on_logical_priors_from_the_miri_workshop/\">Notes on logical priors from the MIRI workshop</a>. Vladimir Slepnev summarizes: \"In Counterfactual Mugging with a logical coin, a 'stupid' agent that can't compute the outcome of the coinflip should agree to pay, and a 'smart' agent that considers the coinflip as obvious as 1=1 should refuse to pay. But if a stupid agent is asked to write a smart agent, it will want to write an agent that will agree to pay. Therefore the smart agent who refuses to pay is reflectively inconsistent in some sense. What's the right thing to do in this case?\"</p>\n</li>\n<li>\n<p><a href=\"/lw/inh/cooperating_with_agents_with_different_ideas_of/\">Cooperating with agents with different ideas of fairness, while resisting exploitation</a>. Eliezer Yudkowsky investigates some ideas from the MIRI workshop that he hasn\u2019t seen in informal theories of negotiation.</p>\n</li>\n<li>\n<p><a href=\"/lw/jcq/naturalistic_trust_among_ais_the_parable_of_the/\">Naturalistic trust among AIs: The parable of the thesis advisor\u2019s theorem</a>. Benja discusses Nik Weaver's suggestion for 'naturalistic trust'.</p>\n</li>\n</ul>\n<h4>\n<hr>\n</h4>\n<h4 id=\"Ethics\">Ethics</h4>\n<ul>\n<li>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">The Metaethics Sequence</a>. Eliezer explains his theory of metaethics. Many readers have difficulty grokking his central points, and may find clarifications in the discussion <a href=\"/lw/435/what_is_eliezer_yudkowskys_metaethical_theory/\">here</a>.</p>\n</li>\n<li>\n<p><a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">No-Nonsense Metaethics</a>. Lukeprog begins to outline his theory of metaethics in this unfinished sequence.</p>\n</li>\n<li>\n<p><a href=\"/lw/xy/the_fun_theory_sequence/\">The Fun Theory Sequence</a>. Eliezer develops a new subfield of ethics, \"fun theory.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/778/consequentialism_need_not_be_nearsighted/\">Consequentialism Need Not Be Near-Sighted</a>. Orthonormal's summary: \"If you object to consequentialist ethical theories because you think they endorse horrible or catastrophic decisions, then you may instead be objecting to short-sighted utility functions or poor decision theories.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/d8z/a_small_critique_of_total_utilitarianism/\">A (small) critique of total utilitarianism</a>. Stuart Armstrong analyzes some weaknesses of total utilitarianism.</p>\n</li>\n<li>\n<p><a href=\"/lw/8qv/in_the_pareto_world_liars_prosper/\">In the Pareto world, liars prosper</a>. Stuart Armstrong presents a new picture proof of a previously known result, that \"if there is any decision process that will find a Pareto outcome for two people, it must be that liars will prosper: there are some circumstances where you would come out ahead if you were to lie about your utility function.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/2qq/politics_as_charity/\">Politics as Charity</a>, <a href=\"/lw/2ur/probability_and_politics/\">Probability and Politics</a>. Carl Shulman analyzes the prospects for doing effective charity work by influencing elections.</p>\n</li>\n<li>\n<p><a href=\"/lw/1ns/value_uncertainty_and_the_singleton_scenario/\">Value Uncertainty and the Singleton Scenario</a>. Wei Dai examines the problem of value uncertainty, a special case of moral uncertainty in which consequentialism is assumed.</p>\n</li>\n<li>\n<p><a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's Mugging: Tiny Probabilities of Vast Utilities</a>. Eliezer describes the problem of Pascal's Mugging, later published in <a href=\"http://www.nickbostrom.com/papers/pascal.pdf\">Bostrom (2009)</a>.</p>\n</li>\n<li>\n<p><a href=\"/lw/fyb/ontological_crisis_in_humans/\">Ontological Crisis in Humans</a>. Wei Dai presents the ontological crisis concept applied to human existence and goes over some examples.</p>\n</li>\n<li>\n<p><a href=\"/lw/g35/ideal_advisor_theories_and_personal_cev/\">Ideal Advisor Theories and Personal CEV</a>. Luke Muehlhauser and crazy88 place CEV in the context of mainstream moral philosophy, and use a variant of CEV to address a standard line of objections to ideal advisor theories in ethics..</p>\n</li>\n<li>\n<p><a href=\"/lw/g85/harsanyis_social_aggregation_theorem_and_what_it/\">Harsanyi s Social Aggregation Theorem and what it means for CEV</a>. Alex Mennen describes the relevance of Harsanyi's Social Aggregation Theorem to possible formalizations of CEV.</p>\n</li>\n<li>\n<p><a href=\"/lw/g5k/three_kinds_of_moral_uncertainty/\">Three Kinds of Moral Uncertainty</a>. Kaj Sotala's attempts to explain what it means to be uncertain about moral theories.</p>\n</li>\n<li>\n<p><a href=\"/lw/gln/a_brief_history_of_ethically_concerned_scientists/\">A brief history of ethically concerned scientists</a>. Kaj_Sotala gives historical examples of ethically concerned scientists.</p>\n</li>\n<li>\n<p><a href=\"/lw/ftk/pascals_mugging_for_bounded_utility_functions/\">Pascal s Mugging for bounded utility functions</a>. Benja s post describing the Pascal Mugging problem under truly bounded utility functions.</p>\n</li>\n<li>\n<p><a href=\"/lw/h8k/pascals_muggle_infinitesimal_priors_and_strong/\">Pascal's Muggle: Infinitesimal Priors and Strong Evidence</a>. Eliezer Yudkowsky discusses the role of infinitesimal priors and their decision-theoretic consequences in a \"Pascal's Mugging\"-type situation.</p>\n</li>\n<li>\n<p><a href=\"/lw/i0m/a_solution_to_preference_uncertainty_using_vnm/\">An Attempt at Preference Uncertainty Using VNM</a>. nyan_sandwich tackles the problem of making decisions when you are uncertain about what your object-level preferences should be.</p>\n</li>\n<li>\n<p><a href=\"/lw/hv6/gains_from_trade_slug_versus_galaxy_how_much/\">Gains from trade: Slug versus Galaxy - how much would I give up to control you?</a> and <a href=\"/lw/i20/even_with_default_points_systems_remain/\">Even with default points, systems remain exploitable</a>. Stuart_Armstrong provides a suggestion as to how to split the gains from trade in some situations and how such solutions can be exploitable.</p>\n</li>\n<li>\n<p><a href=\"/lw/isp/on_the_importance_of_taking_limits_infinite/\">On the importance of taking limits: Infinite Spheres of Utility</a>. aspera shows that \"if we want to make a decision based on additive utility, the infinite problem is ill posed; it has no unique solution unless we take on additional assumptions.\"</p>\n</li>\n<li>\n<p><a href=\"/lw/ixs/change_the_labels_undo_infinitely_good/\">Change the labels, undo infinitely good</a>. Stuart_Armstrong talks about a small selection of paradoxes connected with infinite ethics.</p>\n</li>\n</ul>\n<h4>\n<hr>\n</h4>\n<h4 id=\"AI_Risk_Strategy\">AI Risk Strategy</h4>\n<ul>\n<li>\n<p><a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">AI Risk and Opportunity: A Strategic Analysis</a>. The only original work here so far is the two-part history of AI risk thought, including descriptions of works previously unknown to the LW/SI/FHI community (e.g. Good <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Speculations-on-perceptrons-and-other-automata.pdf\">1959</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Some-future-social-repurcussions-of-computers.pdf\">1970</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Ethical-Machines.pdf\">1982</a>; <a href=\"http://www.amazon.com/Other-Worlds-Than-Cecil-Maxwell/dp/0800861256/\">Cade 1966</a>).</p>\n</li>\n<li>\n<p><a href=\"/lw/e36/ai_timeline_predictions_are_we_getting_better/\">AI timeline predictions: Are we getting better?</a>, <a href=\"/lw/e79/ai_timeline_prediction_data/\">AI timeline prediction data</a>. A preview of Stuart Armstrong's and Kaj Sotala's work on AI predictions, later published in <a href=\"http://singularity.org/files/Armstrong-Sotala-How-Were-Predicting-AI-or-Failing-to.pdf\">Armstrong &amp; Sotala (2012)</a>.</p>\n</li>\n<li>\n<p><a href=\"/lw/gta/selfassessment_in_expert_ai_predictions/\">Self-Assessment in Expert Ai Prediction</a>. Stuart_Armstrong suggests that the predictive accuracy of self selected experts might be different from those elicited from less selected groups.</p>\n</li>\n<li>\n<p><a href=\"/lw/diz/kurzweils_predictions_good_accuracy_poor/\">Kurzweil's predictions: good accuracy, poor self-calibration</a>. A new analysis of Kurzweil's predictions, by Stuart Armstrong.</p>\n</li>\n<li>\n<p><a href=\"/lw/cfd/tools_versus_agents/\">Tools versus agents</a>, <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Reply to Holden on Tool AI</a>. Stuart Armstrong and Eliezer examine Holden's proposal for Tool AI.</p>\n</li>\n<li>\n<p><a href=\"/lw/aro/what_is_the_best_compact_formalization_of_the/\">What is the best compact formalization of the argument for AI risk from fast takeoff?</a> A suggestion of a few steps on how to compact and clarify the argument for the Singularity Institute s  Big Scary Idea , by LW user utility monster.</p>\n</li>\n<li>\n<p><a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">The Hanson-Yudkowsky AI-Foom Debate</a>. The 2008 debate between Robin Hanson and Eliezer Yudkowsky, largely used to exemplify the difficulty of resolving disagreements even between expert rationalists. It focus on the likelihood of hard AI takeoff, the need for a theory of Friendliness, the future of AI, brain emulations and recursive improvement.</p>\n</li>\n<li>\n<p><a href=\"/lw/3dw/what_can_you_do_with_an_unfriendly_ai/\">What can you do with an Unfriendly AI?</a> Paul Christiano discusses how we could eventually turn an Unfriendly AI into a useful system by filtering the way it interacts and constraining how its answers are given to us.</p>\n</li>\n<li>\n<p><a href=\"/lw/3cz/cryptographic_boxes_for_unfriendly_ai/\">Cryptographic Boxes for Unfriendly AI</a>. Following the tone of the previous post and AI boxing in general, this post by Paul Christiano explores the possibility of using cryptography as a way to guarantee friendly outputs.</p>\n</li>\n<li>\n<p><a href=\"/lw/ffh/how_can_i_reduce_existential_risk_from_ai/\">How can I reduce existential risk from AI?</a>. A post by Luke Muehlhauser describing the Meta, Strategic and Direct work one could do in order to reduce the risk for humanity stemming from AI.</p>\n</li>\n<li>\n<p><a href=\"/lw/bs3/intelligence_explosion_vs_cooperative_explosion/\">Intelligence explosion vs Co-operative explosion</a>. Kaj Sotala s description of how an intelligence explosion could emerge from the cooperation of individual artificial intelligent systems forming a superorganism.</p>\n</li>\n<li>\n<p><a href=\"/lw/gbi/assessing_kurzweil_the_results/\">Assessing Kurzweil: The Results</a>; <a href=\"/lw/gbh/assessing_kurzweil_the_gory_details/\">Assessing Kurzweil: The Gory Details</a>. Stuart_Armstrong's attempt to evaluate the accuracy of Ray Kurzweil's model of technological intelligence development.</p>\n</li>\n<li>\n<p><a href=\"/lw/gmx/domesticating_reduced_impact_ais/\">Domesticating reduced impact AIs</a>. Stuart Armstrong attempts to give a solid foundation from which one can build a 'reduced impact AI'.</p>\n</li>\n<li>\n<p><a href=\"/lw/gn3/why_ai_may_not_foom/\">Why Ai may not foom</a>. John_Maxwell_IV explains how the intelligence of a self-improving AI may not grow as fast as we might think.</p>\n</li>\n<li>\n<p><a href=\"/lw/hdw/singleton_the_risks_and_benefits_of_one_world/\">Singleton: the risks and benefits of one world governments</a>. Stuart_Armstrong attempts to lay out a reasonable plan for tackling the singleton problem.</p>\n</li>\n<li>\n<p><a href=\"/lw/hoz/do_earths_with_slower_economic_growth_have_a/\">Do Earths with slower economic growth have a better chance at FAI?</a>. Eliezer Yudkowsky argues that GDP growth acceleration may actually decrease our chances of getting FAI.</p>\n</li>\n<li>\n<p><a href=\"/lw/iyx/reduced_impact_ai_no_back_channels/\">Reduced impact AI: no back channels</a>. Stuart_Armstrong presents a further development of the reduced impact AI approach.</p>\n</li>\n<li>\n<p><a href=\"/lw/j9u/international_cooperation_vs_ai_arms_race/\">International cooperation vs. AI arms race</a>. Brian_Tomasik talks about the role of government in a possible AI arms race.</p>\n</li>\n</ul>", "sections": [{"title": "General philosophy", "anchor": "General_philosophy", "level": 1}, {"title": "Decision theory / AI architectures / mathematical logic", "anchor": "Decision_theory___AI_architectures___mathematical_logic", "level": 1}, {"title": "Ethics", "anchor": "Ethics", "level": 1}, {"title": "AI Risk Strategy", "anchor": "AI_Risk_Strategy", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "48 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ASpGaS3HGEQCbJbjS", "g4khHmSckWe5PZYDA", "wXxPmc9W6kPb6i7vj", "z2YwmzuT7nWx62Kfh", "FS6NCWzzP8DHp4aD4", "bhb54ZC72Y2KkgEeG", "jRK5syD9MXGc4jYgv", "rrtGEhFCqYZD4v8cc", "pzxMrvAoDP464SEuJ", "gzQT5AAw8oQdzuwBG", "vw9QAviBxcGodMHfN", "s3PjeZuXmw5jziNDT", "3wYjyQ839MDsZ6E3L", "pi5DAEZWJK3c9NAhW", "gyz2MsHM9GKxX62hj", "YMwf9agAPTJPqgk9h", "iJNK7rE5jphMSJJCa", "QmWNbCRMgRBcMK6RK", "hqET2rKbLeZFPD6uM", "aD7mZMeu9waAfm3au", "XwxHwyLtZkQi8jtgg", "de3xjFaACCAk6imzv", "g8xh9R7RaNitKtkaa", "mg6jDEuQEjBGtibX7", "6bdb4F6Lif5AanRAd", "DDJr5fuR5jeD47k9g", "EqCxSMZoZPmdARCTm", "PgKADaJE4ERjtMtP9", "af9MjBqF2hgu3EN6r", "2JdvZw3CXzafxQugN", "TxDcvtn2teAMobG2Z", "AMwzjjvFxEgxvL7xe", "ShD7EHb4HmPgfveim", "X9vT3o3MmtWoRRKkm", "HT8jwNJ6vH7p9gaTT", "DM87z7xQxPNMY4evK", "z7SuGwxTBnQm8uFq4", "cW45ADhbckFDomAEv", "2qCxguXuZERZNKcNi", "LDbnZDRidKNKQBFyY", "8Nwg7kqAfCM46tuHq", "TA7kDYZGjMcSCH75C", "sC6dyvk7tNHfunBBC", "W2ufY8ihDDWWqJA7h", "Ljy3CSwTFPEpnGLLJ", "tGhz4aKyNzXjvnWhX", "2GebvAXXfRMTjY2g7", "m39dkp73YhN9QKYb9", "ecCLANfPSDQMzyfDf", "dsbjbopyxbQMK93om", "Bj244uWzDBXvE2N2S", "yX9pMZik7r38da7Fc", "AfbY36m8TDYZBjHcu", "GfHdNfqxe3cSCfpHL", "yTHYf5hLoSJNFXcYb", "oatMFQjAzEnuzti7u", "q3mZNmvqBtnG2nQre", "szfxvS8nsxTgJLBHs", "c3wWnvgzdbRhNnNbQ", "fQv85Rd3pw789MHaX", "B7bMmhvaufdtxBtLW", "gxxpK3eiSQ3XG3DW7", "miwf7qQTh2HXNnSuq", "kYgWmKJnqq8QkbjFj", "aMXhaj6zZBgbTrfqA", "F46jPraqp258q67nE", "LGSotF4bQ2pRSbB8a", "oRRpsGkCZHA3pzhvm", "K2YZPnASN88HTWhAN", "RxQE4m9QgNwuq764M", "PpTN7GP2FsPyHfKrs", "o32tEFf5zBiByL2xv", "7wmBH76BGScL7XNct", "9KoyMKHmwCCJdMma4", "vPtMSvnF8B5hM5LdL", "cL3ATdvGZ5kTy8AB4", "oBDTMnEzptBidvmw7", "3atutKpoGAfDe5XFo", "3R2vH2Ar5AbC9m8Qj", "K4aGvLnHvYgX9pZHS", "prb8raC4XGJiRWs5n", "pyTuR4ZbLfqpS2oMh", "vSgaExrundWJJBDmZ", "SgZ2mhvDbneBusFEB", "YafmHeLuxfRNRkgN2", "Qh6bnkxbMFz5SNeFd", "a5JAiTdytou3Jg749", "KLaJjNdENsHhKhG5m", "q9ZSXiiA7wEuRgnkS", "z8afQRsH9wWsB4iMD", "AytzBuJSD9v2cWu3m", "hxaq9MCaSrwWPmooZ", "daaLFCP34Ba9pRyy2", "Ap4KfkHyxjYPDiqh2", "f3JgMn85PB7cf4jcg", "7kvBxG9ZmYb5rDRiq", "gtGjiizupfCYCz7Lo", "i2XoqtYEykc4XWp9B", "47ci9ixyEbGKWENwR", "Q6oWinLaKXmGNWGLy", "KovmjL7fKyCuBtzgd", "kK5rabDsKWMkup7gw", "nAwTGhgrdxE85Bjmg", "sizjfDgCgAsuLJQmm", "AAG7vj5cyNZ2Gxtnk", "SpHYBhkaeDZpZyRvj", "2Wf3R4NZ77CLczLL2", "qARBe3jBodrdPeRE6", "rLzMBxew4S4TevtqB", "kbA6T3xpxtko36GgP", "edd9mAcMByL2BFNJv", "FdcxknHjeNH2MzrTj", "77xLbXs6vYQuhT8hq", "4cD4dywgtX5pdxbHT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-30T00:28:15.767Z", "modifiedAt": null, "url": null, "title": "Incentives to Make Money More Effectively, Should We List Them?", "slug": "incentives-to-make-money-more-effectively-should-we-list", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:28.786Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YQSqrQGRQAGK652Pa/incentives-to-make-money-more-effectively-should-we-list", "pageUrlRelative": "/posts/YQSqrQGRQAGK652Pa/incentives-to-make-money-more-effectively-should-we-list", "linkUrl": "https://www.lesswrong.com/posts/YQSqrQGRQAGK652Pa/incentives-to-make-money-more-effectively-should-we-list", "postedAtFormatted": "Tuesday, October 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Incentives%20to%20Make%20Money%20More%20Effectively%2C%20Should%20We%20List%20Them%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIncentives%20to%20Make%20Money%20More%20Effectively%2C%20Should%20We%20List%20Them%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQSqrQGRQAGK652Pa%2Fincentives-to-make-money-more-effectively-should-we-list%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Incentives%20to%20Make%20Money%20More%20Effectively%2C%20Should%20We%20List%20Them%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQSqrQGRQAGK652Pa%2Fincentives-to-make-money-more-effectively-should-we-list", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQSqrQGRQAGK652Pa%2Fincentives-to-make-money-more-effectively-should-we-list", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 719, "htmlBody": "<p>There is a bit of nice and recent discussion going on about <a title=\"passive income? Real, imaginary, complex\" href=\"/r/discussion/lw/f2q/passive_income_for_dummies/\">money here</a>, \"money\" in the sense of the common peasant \"I want more money\" not in the senses of \"Unit of caring\" \"Utilon buyer\" \"Trade-able entity unavailable for acausal trade\" etc...</p>\n<p>There is also, on the web, and in your local store, hundreds of thousands of advices on how to make money: fast, more, passively, selling your body, or brain, or virtual structures.&nbsp;</p>\n<p>People who have money (entrepreneurs, owners), study money (economists) or focus on money (stockbrokers) have a lot to tell you about something closely related. Incentives. They studied and understand when incentives <a href=\"http://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem\">work</a> and when <a href=\"http://www.ted.com/talks/dan_pink_on_motivation.html\">they don't</a>. Some have become masters of creating the right incentives, for employees, customers, CEOs, everyone.</p>\n<p>I wont lie, usually, the incentive is money. Pure, abstract, trade-able, feeling-of-power causing, money. Sometimes (Citibank, Safra Bank) also travelling.</p>\n<p>What I don't see around a lot, and would like to ask you to help me brainstorm, and list (later I'll edit by adding suggestions and maybe commenting) are the <em>incentives to make money themselves</em> that work, and those that don't.</p>\n<p>There are a lot of LWers, I know, who excel at domain X and are emotionally averse to making money, out of X, or sometimes, at all. Money-making is, in a word, impure! Item 6 will deal with that.</p>\n<p>I'll say some random unconventional things and open the discussion for those with more experience:</p>\n<p>1)Belonging to a society that lacks money and money making altogether is a very powerful incentive for wanting money. The Indians of Brazil (my homeland) frequently engage in juridic battles and bureaucracy to obtain more money from governmental organs, more time than they would dedicate to get medical attention, for instance.</p>\n<p>2)I've heard that having a lot of money is an incentive to make more, by making it easier to do so. <a href=\"http://www.quickmeme.com/Spiteful-Billionaire/popular/18/?upcoming\">Spiteful Billionaire meme</a> makes fun of this hypothesis</p>\n<p>3)Also heard that not having money is a great incentive to make more. This seems more logical. When you see the empty plate in the horizon, or being thrown in the streets, some sort of alarm must trigger. (If you have experience with this, relate it please)</p>\n<p>Yet, I find that even though there is a strong correlation (within the brazilian cultural elite I hang around with) between having a job and not having money at ages 18-28, there is no correlation at all between not having money and efficiency of making money (by either already be making a lot, or in a career with such prospect). It is as if the emergency button can get you to hike a hill, but not to climb a mountain.</p>\n<p>4)Morals against munching on others. Traditional, moral or religious frameworks of mind will make an individual believe (probably truly) that munching on friends, family, couchsurfers, franciscans, altruists and other exploitable folk is a bad/undesirable thing to do. One who doesn't want to munch has a stronger incentive to get his own money.</p>\n<p>5)Pride. Lots of people are very very proud of having money, theirs or familial. I never understood that, but here it is, just a fact, stated.</p>\n<p>6)Nobility times. My mum points out frequently that in our day and age (in Brazil, but also in the states) there is an idealization of making money. Sometimes it is<a href=\"http://paulgraham.com/wealth.html\"> great</a>, sometimes it is nauseating. Her point though is that during the medieval ages when social class was fundamentally determined from birth, working was seen as a lower activity. It was nearly opposite of Self-Madesmanship. Only the needy, with little status shall work. Same view was held by Aristotle, who, in his conception of working, never worked a day in his life. Something like how we see manual tough labour nowadays, but looking down on every worker. Some people, her and me included, were raised somehow in this anomic (mislocated) environment which persists in many European colonies and perhaps among the descendants, within Europe, of past nobles. Worse for them in a changing world.</p>\n<p>7) Your suggestions will be listed here soon...</p>\n<p>I ask you to suggest things which are incentives for, or against making money. And I don't mean \"Good Reasons\" and \"Bad Reasons\" I mean which incentives, in economic jargon, <em>work</em> <em>effectively</em> as an incentive for people to make more money. Then after that you can write about their goodness and badness.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YQSqrQGRQAGK652Pa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -1, "extendedScore": null, "score": 1.0211528811688123e-06, "legacy": true, "legacyId": "19681", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wvW8qdmfMTkmgnvuJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-30T03:40:56.686Z", "modifiedAt": null, "url": null, "title": "An anecdote about names", "slug": "an-anecdote-about-names", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:30.499Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wJY6ifuSjbYd9RK7Z/an-anecdote-about-names", "pageUrlRelative": "/posts/wJY6ifuSjbYd9RK7Z/an-anecdote-about-names", "linkUrl": "https://www.lesswrong.com/posts/wJY6ifuSjbYd9RK7Z/an-anecdote-about-names", "postedAtFormatted": "Tuesday, October 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20anecdote%20about%20names&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20anecdote%20about%20names%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwJY6ifuSjbYd9RK7Z%2Fan-anecdote-about-names%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20anecdote%20about%20names%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwJY6ifuSjbYd9RK7Z%2Fan-anecdote-about-names", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwJY6ifuSjbYd9RK7Z%2Fan-anecdote-about-names", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 305, "htmlBody": "<p>I've always been bad at names. But this semester, as part of my duties as a physics teacher, I tried to learn the names of 100 students. It went alright - two months later and there are still some I have trouble with. At the start, I was a bit worried. What if I've just read to many books and things, and <em>used up</em>&nbsp;most of my abnormally small&nbsp;allotment&nbsp;of memorized names? Then wasting names on students would be a really bad idea.</p>\n<p>But things turned out not to work that way (as might be expected from how big the human brain is, viz people with eidetic memories). I've started remembering peoples' names after just one introduction, sometimes two. And the reasonable culprit just seems to be practice. You practice learning names, you get better at doing so. Before, I made occasional conversational detours if I couldn't remember someone's name. Now, I ask them again, because I'm confident that I'll remember it without tons of further awkwardness. If I'm really having trouble, sometimes I've written down a name with a short description, and that usually cements it. Remembering names isn't the most important social skill I've learned, but it's a surprisingly dramatic one - it makes me feel closer to people, and vice versa. And it only took having to learn the names of 100 people to get started.</p>\n<p>The lesson from this is not necessarily just about learning names, though that might be useful to you. The lesson is about how ordinary people get a lot&nbsp;of practice at doing things like remembering names - if you can't do something that someone else can do, practice may be an <em>effective</em>&nbsp;place to start. &nbsp;What seems like a property of yourself (\"bad at names\") may turn out to be quite mutable with the kind of practice those other people are doing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"x6evH6MyPK3nxsoff": 1, "WqLn4pAWi5hn6McHQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wJY6ifuSjbYd9RK7Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 39, "extendedScore": null, "score": 1.021257925059917e-06, "legacy": true, "legacyId": "19690", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-30T04:07:22.855Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Failure By Analogy", "slug": "seq-rerun-failure-by-analogy", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KSy8xhPYskg5x65Gz/seq-rerun-failure-by-analogy", "pageUrlRelative": "/posts/KSy8xhPYskg5x65Gz/seq-rerun-failure-by-analogy", "linkUrl": "https://www.lesswrong.com/posts/KSy8xhPYskg5x65Gz/seq-rerun-failure-by-analogy", "postedAtFormatted": "Tuesday, October 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Failure%20By%20Analogy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Failure%20By%20Analogy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKSy8xhPYskg5x65Gz%2Fseq-rerun-failure-by-analogy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Failure%20By%20Analogy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKSy8xhPYskg5x65Gz%2Fseq-rerun-failure-by-analogy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKSy8xhPYskg5x65Gz%2Fseq-rerun-failure-by-analogy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<p>Today's post, <a href=\"/lw/vx/failure_by_analogy/\">Failure By Analogy</a> was originally published on 18 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Failure_By_Analogy\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It's very tempting to reason that your invention X will do Y, because it is similar to thing Z, which also does Y. But reality very often ignores this justification for why your new invention will work.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/f69/seq_rerun_logical_or_connectionist_ai/\">Logical or Connectionist AI?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KSy8xhPYskg5x65Gz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0212723386775142e-06, "legacy": true, "legacyId": "19691", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["C4EjbrvG3PvZzizZb", "zC3L4mmmrjoiQ7TN2", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-30T08:56:59.232Z", "modifiedAt": null, "url": null, "title": "[LINK] How rational is the US federal state", "slug": "link-how-rational-is-the-us-federal-state", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:28.444Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cJhoJNiPHAdGv6GMW/link-how-rational-is-the-us-federal-state", "pageUrlRelative": "/posts/cJhoJNiPHAdGv6GMW/link-how-rational-is-the-us-federal-state", "linkUrl": "https://www.lesswrong.com/posts/cJhoJNiPHAdGv6GMW/link-how-rational-is-the-us-federal-state", "postedAtFormatted": "Tuesday, October 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20How%20rational%20is%20the%20US%20federal%20state&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20How%20rational%20is%20the%20US%20federal%20state%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcJhoJNiPHAdGv6GMW%2Flink-how-rational-is-the-us-federal-state%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20How%20rational%20is%20the%20US%20federal%20state%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcJhoJNiPHAdGv6GMW%2Flink-how-rational-is-the-us-federal-state", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcJhoJNiPHAdGv6GMW%2Flink-how-rational-is-the-us-federal-state", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 12, "htmlBody": "<p>http://www.weeklystandard.com/blogs/over-60000-welfare-spentper-household-poverty_657889.html</p>\n<p>&nbsp;</p>\n<p>60000 dollars per year per poor family, if the article is correct.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cJhoJNiPHAdGv6GMW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": -34, "extendedScore": null, "score": 1.0214302625885172e-06, "legacy": true, "legacyId": "19701", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-30T11:07:50.989Z", "modifiedAt": null, "url": null, "title": "Checking Kurzweil's track record ", "slug": "checking-kurzweil-s-track-record", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:03.453Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pimArtGNE2K2A4x58/checking-kurzweil-s-track-record", "pageUrlRelative": "/posts/pimArtGNE2K2A4x58/checking-kurzweil-s-track-record", "linkUrl": "https://www.lesswrong.com/posts/pimArtGNE2K2A4x58/checking-kurzweil-s-track-record", "postedAtFormatted": "Tuesday, October 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Checking%20Kurzweil's%20track%20record%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChecking%20Kurzweil's%20track%20record%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpimArtGNE2K2A4x58%2Fchecking-kurzweil-s-track-record%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Checking%20Kurzweil's%20track%20record%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpimArtGNE2K2A4x58%2Fchecking-kurzweil-s-track-record", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpimArtGNE2K2A4x58%2Fchecking-kurzweil-s-track-record", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 358, "htmlBody": "<p>Predictions are cheap and easy; verification is hard, essential, and rare. For things like AI, we seem to be restricted to nothing but expert predictions - but expert predictions on AI are <a href=\"http://fora.tv/2012/10/14/Stuart_Armstrong_How_Were_Predicting_AI\">not very good</a>, either in <a href=\"/lw/e46/competence_in_experts_summary/\">theory</a> or in <a href=\"/lw/e36/ai_timeline_predictions_are_we_getting_better/\">practice</a>. If we are some experts who stand out, we would really want to identify them - and there is nothing better than a track record for identifying true experts.</p>\n<p>So we're asking for help to verify the predictions of one of the most prominent futurists of this century: Ray Kurzweil, from his book \"The Age of Spiritual Machines\". By examining his predictions for times that have already come and gone, we'll be able to more appropriately weight his predictions for times still to come. By taking part, by lending your time to this, you will be directly helping us understand and predict the future, and will get showered in gratitude and kudos and maybe even karma.</p>\n<p>I've already made <a href=\"/lw/diz/kurzweils_predictions_good_accuracy_poor/\">an attempt</a> at this (if you are interested in taking part in this project, avoid clicking on that link for now!). But you cannot trust a single person's opinions, and that was from a small (albeit random) sample of the predictions. For this project, I've transcribed his predictions into 172 separate (short) statements, and any volunteers would be presented with a random selection among these. The volunteers would then do some Google research (or other) to establish whether the prediction had come to pass, and then indicate their verdict. More details on what exactly will be measured, and how to interpret ambiguous statements, will be given to the volunteers once the project starts.</p>\n<p>If you are interested, please let me know at <a href=\"mailto:stuart.armstrong@philosophy.ox.ac.uk\">stuart.armstrong@philosophy.ox.ac.uk</a>&nbsp;(or in the comment thread here), indicating how many of the 172 questions you would like to attempt. The exercise will probably happen in late November or early December.</p>\n<p>This will be done unblinded, because Kurzweil's predictions are so well known that it would be infeasible to find large numbers of people who are technologically aware but ignorant of them. Please avoid sharing your verdicts with others; it is entirely your own individual assessment that we are interested in having.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pimArtGNE2K2A4x58", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 17, "extendedScore": null, "score": 1.0215016369396048e-06, "legacy": true, "legacyId": "19702", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yrNW4ApXrpn2KMhxr", "47ci9ixyEbGKWENwR", "kK5rabDsKWMkup7gw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-30T13:02:20.077Z", "modifiedAt": "2022-02-24T01:55:59.887Z", "url": null, "title": "Proofs, Implications, and Models", "slug": "proofs-implications-and-models", "viewCount": null, "lastCommentedAt": "2021-07-04T08:48:31.412Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Z2CuyKtkCmWGQtAEh/proofs-implications-and-models", "pageUrlRelative": "/posts/Z2CuyKtkCmWGQtAEh/proofs-implications-and-models", "linkUrl": "https://www.lesswrong.com/posts/Z2CuyKtkCmWGQtAEh/proofs-implications-and-models", "postedAtFormatted": "Tuesday, October 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proofs%2C%20Implications%2C%20and%20Models&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProofs%2C%20Implications%2C%20and%20Models%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ2CuyKtkCmWGQtAEh%2Fproofs-implications-and-models%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proofs%2C%20Implications%2C%20and%20Models%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ2CuyKtkCmWGQtAEh%2Fproofs-implications-and-models", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ2CuyKtkCmWGQtAEh%2Fproofs-implications-and-models", "socialPreviewImageUrl": "http://wiki.lesswrong.com/mediawiki/images/2/20/CatTheoremA.jpg", "question": false, "authorIsUnreviewed": false, "wordCount": 3733, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/f1u/causal_reference/\">Causal Reference</a></p>\n<p>From a <a href=\"http://mathbabe.org/2012/07/11/mathematicians-know-how-to-admit-theyre-wrong/\">math professor's blog</a>:</p>\n<p style=\"padding-left: 30px; \">One thing I discussed with my students here at HCSSiM yesterday is the question of what is a proof.</p>\n<p style=\"padding-left: 30px; \">They\u2019re smart kids, but completely new to proofs, and they often have questions about whether what they\u2019ve written down constitutes a proof. Here\u2019s what I said to them.</p>\n<p style=\"padding-left: 30px; \">A proof is a social construct \u2013 it is what we need it to be in order to be convinced something is true. If you write something down and you want it to count as a proof, the only real issue is whether you\u2019re completely convincing.</p>\n<p>This is not quite the definition I would give of what constitutes \"proof\" in mathematics - perhaps because I am so used to isolating arguments that are convincing, but ought not to be.</p>\n<p>Or here again, from&nbsp;\"<a href=\"http://math.ucsd.edu/~sbuss/ResearchWeb/handbookI/ChapterI.pdf\">An Introduction to Proof Theory</a>\" by Samuel R. Buss:</p>\n<p style=\"padding-left: 30px; \">There are two distinct viewpoints of what a mathematical proof is. The first view is that proofs are social conventions by which mathematicians convince one another of the truth of theorems. That is to say, a proof is expressed in natural language plus possibly symbols and figures, and is sufficient to convince an expert of the correctness of a theorem. Examples of social proofs include the kinds of proofs that are presented in conversations or published in articles. Of course, it is impossible to precisely define what constitutes a valid proof in this social sense; and, the standards for valid proofs may vary with the audience and over time. The second view of proofs is more narrow in scope: in this view, a proof consists of a string of symbols which satisfy some precisely stated set of rules and which prove a theorem, which itself must also be expressed as a string of symbols. According to this view, mathematics can be regarded as a 'game' played with strings of symbols according to some precisely defined rules. Proofs of the latter kind are called \"formal\" proofs to distinguish them from \"social\" proofs.</p>\n<p>In modern mathematics there is a much better answer that could be given to a student who asks, \"What exactly is a proof?\", which does not match <em>either</em> of the above ideas. So:</p>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Meditation\">Meditation</a>: What distinguishes a correct mathematical proof from an incorrect mathematical proof - what does it mean for a mathematical proof to be good? And why, in the real world, would anyone ever be interested in a mathematical proof of this type, or obeying whatever goodness-rule you just set down? How could you use your notion of 'proof' to improve the real-world efficacy of an Artificial Intelligence?<a id=\"more\"></a></p>\n<p>...<br>...<br>...</p>\n<p>Consider the following syllogism:</p>\n<ol>\n<li>All kittens are little;</li>\n<li>Everything little is innocent;</li>\n<li>Therefore all kittens are innocent.</li>\n</ol>\n<p>Here's four mathematical universes, aka \"models\", in which the objects collectively obey or disobey these three rules:</p>\n<table border=\"0\" cellpadding=\"5\">\n<tbody>\n<tr>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/2/20/CatTheoremA.jpg\" alt=\"little innocent kitten, little innocent kitten, big evil dog, little innocent dog\" width=\"300\" height=\"181\"></td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/d/da/CatTheoremB.jpg\" alt=\"little innocent kitten, big evil kitten, big evil dog, little innocent dog\" width=\"300\" height=\"181\"></td>\n</tr>\n<tr>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/d/d8/CatTheoremC.jpg\" alt=\"little innocent kitten, little evil kitten, big innocent dog, little innocent dog\" width=\"300\" height=\"181\"></td>\n<td><img src=\"http://wiki.lesswrong.com/mediawiki/images/4/41/CatTheoremD.jpg\" alt=\"little innocent kitten, big innocent kitten, big evil dog, little evil dog\" width=\"300\" height=\"181\"></td>\n</tr>\n</tbody>\n</table>\n<p>There are some models where not all kittens are little, like models B and D. And there are models where not everything little is innocent, like models C and D. But there are no models where all kittens are little, <em>and</em> everything little is innocent, and yet there exists a guilty kitten. Try as you will, you won't be able to imagine a model like that. Any model containing a guilty kitten has at least one kitten that isn't little, or at least one little entity that isn't innocent - no way around it.</p>\n<p>Thus, the jump from 1 &amp; 2, to 3, is <em>truth-preserving: </em>in any universe where premises (1) and (2) are true to start with, the conclusion (3) is true of the same universe at the end.</p>\n<p>Which is what makes the following implication <em>valid</em>, or, as people would usually just say, \"true\":</p>\n<p>\"If all kittens are little and everything little is innocent, then all kittens are innocent.\"</p>\n<p>The <em>advanced&nbsp;</em>mainstream&nbsp;view of logic and mathematics (i.e., the mainstream view among professional scholars of logic as such, not necessarily among all mathematicians in general) is that when we talk about math, we are talking about <em>which conclusions follow from which premises</em>. The \"truth\" of a mathematical theorem - or to not overload the word 'true' meaning&nbsp;<a href=\"/lw/eva/the_fabric_of_real_things/\">comparison-to-causal-reality</a>, the <em>validity</em> of a mathematical theorem - has nothing to do with the physical truth or falsity of the conclusion in our world, and everything to do with the inevitability of the <em>implication.</em> From the standpoint of <em>validity</em>, it doesn't matter a fig whether or not all kittens are innocent in our <em>own</em> universe, the connected causal fabric within which we are embedded. What matters is whether or not you can prove the implication, starting from the premises; whether or not, if all kittens <em>were</em> little and all little things <em>were</em> innocent, it would follow <em>inevitably</em> that all kittens were innocent.</p>\n<hr>\n<p>To paraphrase Mayor Daley, logic is not there to <em>create</em> truth, logic is there to <em>preserve</em> truth. Let's illustrate this point by assuming the following equation:</p>\n<pre style=\"padding-left: 30px;\">x = y = 1</pre>\n<p>...which is true in at least some cases. &nbsp;E.g. 'x' could be the number of thumbs on my right hand, and 'y' the number of thumbs on my left hand.</p>\n<p>Now, starting from the above, we do a little algebra:</p>\n<table border=\"1\" cellpadding=\"1\">\n<tbody>\n<tr>\n<td>\n<pre><strong>1</strong></pre>\n</td>\n<td>\n<pre>x = y = 1</pre>\n</td>\n<td>\n<pre>starting premise</pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre><strong>2</strong></pre>\n</td>\n<td>\n<pre>x<sup>2</sup> = xy</pre>\n</td>\n<td>\n<pre>multiply both sides by x</pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre><strong>3</strong></pre>\n</td>\n<td>\n<pre>x<sup>2</sup> - y<sup>2</sup> = xy - y<sup>2</sup></pre>\n</td>\n<td>\n<pre>subtract y<sup>2</sup> from both sides</pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre><strong>4</strong></pre>\n</td>\n<td>\n<pre>(x + y)(x - y) = y(x - y)</pre>\n</td>\n<td>\n<pre>factor</pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre><strong>5</strong></pre>\n</td>\n<td>\n<pre>x + y = y</pre>\n</td>\n<td>\n<pre>cancel</pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre><strong>6</strong></pre>\n</td>\n<td>\n<pre>2 = 1</pre>\n</td>\n<td>\n<pre>substitute 1 for x and 1 for y</pre>\n</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p>We have reached the conclusion that in every case where x and y are equal to 1, 2 is equal to 1. This does not seem like it should follow inevitably.</p>\n<p>You could try to find the flaw just by staring at the lines... maybe you'd suspect that the error was between line 3 and line 4, following the heuristic of first mistrusting what looks like the most complicated step... but another way of doing it would be to try <em>evaluating</em> each line to see what it said concretely, for example, multiplying out x<sup>2</sup> = xy in line 2 to get (1<sup>2</sup>) = (1 * 1) or 1 = 1. Let's try doing this for each step, and then afterward mark whether each equation looks <em>true</em> or <em>false:</em></p>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>\n<pre><strong>1</strong></pre>\n</td>\n<td>\n<pre>x = y = 1</pre>\n</td>\n<td>\n<pre>1 = 1</pre>\n</td>\n<td>\n<pre><strong>true</strong></pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre><strong>2</strong></pre>\n</td>\n<td>\n<pre>x<sup>2</sup> = xy</pre>\n</td>\n<td>\n<pre>1 = 1</pre>\n</td>\n<td>\n<pre><strong>true</strong></pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre><strong>3</strong></pre>\n</td>\n<td>\n<pre>x<sup>2</sup> - y<sup>2</sup> = xy - y<sup>2</sup></pre>\n</td>\n<td>\n<pre>0 = 0</pre>\n</td>\n<td>\n<pre><strong>true</strong></pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre><strong>4</strong></pre>\n</td>\n<td>\n<pre>(x + y)(x - y) = y(x - y)</pre>\n</td>\n<td>\n<pre>0 = 0</pre>\n</td>\n<td>\n<pre><strong>true</strong></pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre><strong>5</strong></pre>\n</td>\n<td>\n<pre>x + y = y</pre>\n</td>\n<td>\n<pre>2 = 1</pre>\n</td>\n<td>\n<pre><strong>false</strong></pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre><strong>6</strong></pre>\n</td>\n<td>\n<pre>2 = 1</pre>\n</td>\n<td>\n<pre>2 = 1</pre>\n</td>\n<td>\n<pre><strong>false</strong></pre>\n</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p>Logic is there to preserve truth, not to create truth. Whenever we take a logically valid step, we can't guarantee that the premise is true to start with, but <em>if</em> the premise is true the conclusion should always be true. Since we went from a true equation to a false equation between step 4 and step 5, we must've done something that is <em>in general</em> invalid.</p>\n<p>In <em>particular, </em>we divided both sides of the equation by (x - y).</p>\n<p>Which is invalid, i.e. <em>not universally truth-preserving,</em> because (x - y) might be equal to 0.</p>\n<p>And if you divide both sides by 0, you can get a false statement from a true statement. 3 * 0 = 2 * 0 is a true equation, but 3 = 2 is a false equation, so it is not allowable in general to cancel <em>any</em> factor if the factor might equal zero.</p>\n<p>On the other hand, adding 1 to both sides of an equation is <em>always</em> truth-preserving. We can't guarantee as a matter of logic that x = y to start with - for example, x might be my number of ears and y might be my number of fingers. &nbsp;But <em>if</em> x = y&nbsp;<em>then</em> x + 1 = y + 1, always. Logic is not there to create truth; logic is there to preserve truth. <em>If </em>a scale <em>starts out</em> balanced, then adding the same weight to both sides will result in a scale that is <em>still </em>balanced:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/3/39/Scales.jpg\" alt=\"\" width=\"680\" height=\"340\"></p>\n<p>I will remark, in some horror and exasperation with the modern educational system, that I do not recall any math-book of my youth ever once explaining that the reason why you are always allowed to add 1 to both sides of an equation is that it is a kind of step which always produces true equations from true equations.</p>\n<p>What is a valid proof in algebra? It's a proof where, in each step, we do something that is<em> universally allowed,</em> something which can only produce true equations from true equations, and so the proof gradually transforms the starting equation into a final equation which must be true if the starting equation was true. Each step should also - this is part of what makes proofs <em>useful in reasoning</em> - be <em>locally verifiable</em> as allowed, by looking at only a small number of previous points, not the entire past history of the proof. If in some previous step I believed x<sup>2</sup> - y = 2, I only need to look at that single step to get the conclusion x<sup>2</sup> = 2 + y, because I am always allowed to add y to both sides of the equation; because I am always allowed to add any quantity to both sides of the equation; because if the two sides of an equation are in balance to start with, adding the same quantity to both sides of the balance will preserve that balance. I can know the inevitability of this implication without considering all the surrounding circumstances; it's a step which is <em>locally</em> guaranteed to be <em>valid</em>. (Note the similarity - and the differences - to how we can compute a causal entity <a href=\"/lw/ev3/causal_diagrams_and_causal_models/\">knowing only its immediate parents</a>, and no other ancestors.)</p>\n<hr>\n<p>You may have read - I've certainly read - some philosophy which endeavors to score points for&nbsp;counter-intuitive&nbsp;cynicism by saying that all mathematics is a <em>mere game of tokens; </em>that we start with a meaningless string of symbols like:</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?\\forall x : (K(x) \\rightarrow L(x)) \\land (L(x) \\rightarrow I(x))\" alt=\"\" width=\"285\" height=\"19\"></p>\n<p>...and we follow some symbol-manipulation rules like \"If you have the string 'A \u2227 (A \u2192 B)' you are allowed to go to the string 'B'\", and so finally end up with the string:</p>\n<p><img src=\"https://web.archive.org/web/20201128105020im_/http://www.codecogs.com/png.latex?\\forall x: K(x) \\rightarrow I(x)\" alt=\"\" width=\"136\" height=\"19\"></p>\n<p>...and this activity of string-manipulation is all there is to what mathematicians call \"theorem-proving\" - all there is to the glorious human endeavor of mathematics.</p>\n<p>This, like a lot of other <a href=\"/lw/ym/cynical_about_cynicism/\">cynicism</a>&nbsp;out there, is <em>needlessly</em> <a href=\"/lw/oo/explaining_vs_explaining_away/\">deflationary</a>.</p>\n<p>There's a family of techniques in machine learning known as \"<a href=\"http://en.wikipedia.org/wiki/Monte_Carlo_method\">Monte Carlo methods</a>\" or \"Monte Carlo simulation\", one of which says, roughly, \"To find the probability of a proposition Q given a set of premises P, simulate random models that obey P, and then count how often Q is true.\" Stanislaw Ulam invented the idea after trying for a while to calculate the probability that a random Canfield solitaire layout would be solvable, and finally realizing that he could get better information by trying it a hundred times and counting the number of successful plays. This was during the era when computers were first becoming available, and the thought occurred to Ulam that the same technique might work on a current neutron diffusion problem as well.</p>\n<p>Similarly, to answer a question like, \"What is the probability that a random Canfield solitaire is solvable, given that the top card in the deck is a king?\" you might imagine simulating many 52-card layouts, throwing <em>away</em> all the layouts where the top card in the deck was not a king, using a computer algorithm to solve the remaining layouts, and counting what percentage of those were solvable. (It would be more efficient, in this case, to start by directly placing a king on top and then randomly distributing the other 51 cards; but this is not always efficient in Monte Carlo simulations when the condition to be fulfilled is more complex.)</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/9/98/KingFilter.jpg\" alt=\"\" width=\"635\" height=\"345\"></p>\n<p>Okay, now for a harder problem. Suppose you've wandered through the world a bit, and you've observed the following:</p>\n<p>(1) So far I've seen 20 objects which have been kittens, and on the 6 occasions I've paid a penny to observe the size of something that's a kitten, all 6 kitten-objects have been little.</p>\n<p>(2) So far I've seen 50 objects which have been little, and on the 30 occasions where I've paid a penny to observe the morality of something little, all 30 little objects have been innocent.</p>\n<p>(3) This object happens to be a kitten. I want to know whether it's innocent, but I don't want to pay a cent to find out directly. (E.g., if it's an innocent kitten, I can buy it for a cent, sell it for two cents, and make a one-cent profit. But if I pay a penny to observe directly whether the kitten is innocent, I won't be able to make a profit, since gathering evidence is costly.)</p>\n<p>Your previous experiences have led you to suspect the general rule \"All kittens are little\" and also the rule \"All little things are innocent\", even though you've never before <em>directly</em> checked whether a kitten is innocent.</p>\n<p>Furthermore...</p>\n<p>You've <em>never heard of logic,</em> and you have no idea how to play that 'game of symbols' with K(x), I(x), and L(x) that we were talking about earlier.</p>\n<p>But that's all right. The problem is still solvable by Monte Carlo methods!</p>\n<p>First we'll generate a large set of random universes. Then, for each universe, we'll check whether that universe obeys all the rules we currently suspect or believe to be true, like \"All kittens are little\" and \"All little things are innocent\" and \"The force of gravity goes as the square of the distance between two objects and the product of their masses\". &nbsp;If a universe passes this test, we'll check to see whether the inquiry of interest, \"Is the kitten in front of me innocent?\", also happens to be true in that universe.</p>\n<p>We shall repeat this test <em>a large number of times</em>, and at the end we shall have an approximate estimate of the probability that the kitten in front of you is innocent.</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/d/d2/CatFilter.jpg\" alt=\"\" width=\"643\" height=\"266\"></p>\n<p>On this algorithm, you perform inference by visualizing many possible universes, throwing out universes that disagree with generalizations you already believe in, and then checking what's true (probable) in the universes that remain. This algorithm doesn't tell you the state of the real physical world with certainty. Rather, it gives you a measure of probability - i.e., the probability that the kitten is innocent - <em>given everything else you already believe to be true</em>.</p>\n<p>And if, instead of visualizing many imaginable universes, you checked <em>all possible logical models</em> - which would take something beyond magic, because that would include models containing uncountably large numbers of objects - and the inquiry-of-interest was true in <em>every</em>&nbsp;model matching your previous beliefs, you would have found that the conclusion followed <em>inevitably</em> if the generalizations you already believed were true.</p>\n<p>This might take a whole lot of reasoning, but at least you wouldn't have to pay a cent to observe the kitten's innocence directly.</p>\n<p>But it would also <em>save you some computation</em> if you could play that<em> game of symbols</em> we talked about earlier - a game which does not create truth, but <em>preserves</em> truth. In this game, the steps can be <em>locally</em> pronounced valid by a mere 'syntactic' check that doesn't require us to visualize all possible models. Instead, if the mere&nbsp;<em>syntax</em> of the proof checks out, we know that the conclusion is always true in a model whenever the premises are true in that model.</p>\n<p>And that's a mathematical proof: &nbsp;A conclusion which is true in any model where the axioms are true, which we know because we went through a series of transformations-of-belief, each step being licensed by some rule which guarantees that such steps never generate a false statement from a true statement.</p>\n<p>The way we would say it in standard mathematical logic is as follows:</p>\n<p>A collection of axioms X <em>semantically</em> implies a theorem Y, if Y is true in all models where X are true.&nbsp;We write this as X&nbsp;<span style=\"color: #222222; font-family: arial, sans-serif; line-height: 16px;\">\u22a8</span>&nbsp;Y.</p>\n<p>A collection of axioms X <em>syntactically</em> implies a theorem Y within a system S, if we can get to Y from X using transformation steps allowed within S. We write this as X&nbsp;<span style=\"color: #222222; font-family: arial, sans-serif; line-height: 16px;\">\u22a2</span>&nbsp;Y.</p>\n<p>The point of the system S known as \"classical logic\" is that its syntactic transformations preserve semantic implication, so that any syntactically allowed proof is semantically valid:</p>\n<p>If X&nbsp;<span style=\"color: #222222; font-family: arial, sans-serif; line-height: 16px;\">\u22a2&nbsp;</span>Y, then X&nbsp;<span style=\"color: #222222; font-family: arial, sans-serif; line-height: 16px;\">\u22a8</span>&nbsp;Y.</p>\n<p>If you make this idea be about proof steps in algebra doing things that always preserve the balance of a previously balanced scale, I see no reason why this idea couldn't be presented in eighth grade or earlier.</p>\n<p>I can attest by spot-checking for small N that even most <em>mathematicians</em> have not been exposed to this idea. It's the&nbsp;standard concept in mathematical logic, but for some odd reason, the knowledge seems <em>constrained </em>to the study of \"mathematical logic\" as a separate field, which not all mathematicians are interested in (many just want to do Diophantine analysis or whatever).</p>\n<hr>\n<p>So far as real life is concerned, mathematical logic only tells us the implications of what we already believe or suspect, but this is a computational problem of supreme difficulty and importance. After the first thousand times we observe that objects in Earth gravity accelerate downward at 9.8 m/s<sup>2</sup>, we can suspect that this will be true on the next occasion - which is a matter of probabilistic induction, not valid logic. But then to go from that suspicion, <em>plus</em> the observation that a building is 45 meters tall, to a <em>specific</em> prediction of how long it takes the object to hit the ground, is a matter of logic - what will happen if everything we else we already believe, is actually true. It requires computation to make this conclusion transparent. &nbsp;We are not 'logically omniscient' - the technical term for the impossible dreamlike ability of knowing all the implications of everything you believe.</p>\n<p>The great virtue of logic in <em>argument</em> is not that you can prove things by logic that are absolutely certain. Since logical implications are valid in every possible world, \"observing\" them never tells us <em>anything</em> about <em>which</em> possible world we live in. Logic can't tell you that you won't suddenly float up into the atmosphere. (What if we're in the Matrix, and the Matrix Lords decide to do that to you on a whim as soon as you finish reading this sentence?) Logic can only tell you that, if that <em>does</em> happen, you were wrong in your extremely strong suspicion about gravitation being always and everywhere true in our universe.</p>\n<p>The great virtue of valid logic in <em>argument,</em> rather, is that logical argument exposes premises, so that anyone who disagrees with your conclusion has to (a) point out a premise they disagree with or (b) point out an invalid step in reasoning which is strongly liable to generate false statements from true statements.</p>\n<p>For example: Nick Bostrom put forth the Simulation Argument, which is that <em>you must disagree with either statement (1) or (2) or else agree with statement (3):</em></p>\n<p>(1) Earth-originating intelligent life will, in the future, acquire vastly greater computing resources.</p>\n<p>(2) Some of these computing resources will be used to run many simulations of ancient Earth, aka \"ancestor simulations\".</p>\n<p>(3) We are almost certainly living in a computer simulation.</p>\n<p>...but unfortunately it appears that not only do most respondents decline to say <em>why</em> they disbelieve in (3), most are unable to understand the distinction between the Simulation <em>Hypothesis</em> that we are living in a computer simulation, versus Nick Bostrom's actual support for the Simulation <em> Argument</em> that \"You must either disagree with (1) or (2) or agree with (3)\". &nbsp;They just treat Nick Bostrom as having claimed that we're all living in the Matrix. Really. Look at the media coverage.</p>\n<p>I would seriously generalize that the mainstream media only understands the \"and\" connective, not the \"or\" or \"implies\" connective. I.e., it is impossible for the media to report on a discovery that one of two things must be true, or a discovery that <em>if</em> X is true then Y must be true (when it's not known that X is true). Also, the media only understands the \"not\" prefix when applied to atomic facts; it should go without saying that \"not (A and B)\" cannot be reported-on.</p>\n<p>Robin Hanson sometimes complains that when he tries to argue that conclusion X follows from reasonable-sounding premises Y, his colleagues disagree with X while refusing to say which premise Y they think is false, or else say which step of the reasoning seems like an invalid implication. &nbsp;Such behavior is not only annoying, but&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Logical_rudeness\">logically rude</a>, because someone else went out of their way and put in extra effort to make it <em>as easy as possible</em> for you to explain why you disagreed, and <em>you </em>couldn't be bothered to pick one item off a multiple-choice menu.</p>\n<p>The inspiration of logic for argument is to lay out a modular debate, one which conveniently breaks into smaller pieces that can examined with smaller conversations.&nbsp;&nbsp;At least when it comes to trying to have a real conversation with a respected partner - I wouldn't necessarily advise a teenager to try it on their oblivious parents - that is the great inspiration we can take from the study of mathematical logic: <em>An argument is a succession of statements each allegedly following with high probability from previous statements or shared background knowledge.</em> Rather than, say, snowing someone under with as much fury and as many demands for applause as you can fit into sixty seconds.</p>\n<p>Michael Vassar is fond of claiming that most people don't have the concept of an argument, and that it's pointless to try and teach them anything else until you can convey an intuitive sense for what it means to argue. I <em>think</em> that's what he's talking about.</p>\n<hr>\n<p><strong><a href=\"#7ors\">Meditation</a></strong>: It has been claimed that logic and mathematics is the study of which conclusions follow from which premises. But when we say that 2 + 2 = 4, are we really just <em>assuming</em> that? It seems like 2 + 2 = 4 was true well before anyone was around to assume it, that two apples&nbsp;equaled&nbsp;two apples before there was anyone to count them, and that we couldn't make it 5 just by assuming differently.</p>\n<hr>\n<p><strong><a href=\"#7ort\">Mainstream status.</a></strong></p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/f4e/logical_pinpointing/\">Logical Pinpointing</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/f1u/causal_reference/\">Causal Reference</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1, "EnFKSZYiDHqMJuvJL": 1, "wzgcQCrwKfETcBpR9": 1, "FJ3MGb684F88BoN2o": 7}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Z2CuyKtkCmWGQtAEh", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 81, "baseScore": 112, "extendedScore": null, "score": 0.000245, "legacy": true, "legacyId": "19587", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "http://wiki.lesswrong.com/mediawiki/images/2/20/CatTheoremA.jpg", "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "logical-pinpointing", "canonicalPrevPostSlug": "causal-universes", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 112, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 218, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["PXoWk554FZ4Gpfvah", "h6fzC6wFYFxxKDm8u", "hzuSDMx7pd2uxFc5w", "R2crzhnqrytPycc6C", "cphoF8naigLhRf3tu", "3FoMuCLqZggTxoC3S"]}, "moderationGuidelinesVersion": "1.1.0", "customHighlightVersion": "1.1.0", "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-30T18:01:56.568Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 10/29/12", "slug": "group-rationality-diary-10-29-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:32.445Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xeYzNEvNRagBQPQBY/group-rationality-diary-10-29-12", "pageUrlRelative": "/posts/xeYzNEvNRagBQPQBY/group-rationality-diary-10-29-12", "linkUrl": "https://www.lesswrong.com/posts/xeYzNEvNRagBQPQBY/group-rationality-diary-10-29-12", "postedAtFormatted": "Tuesday, October 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%2010%2F29%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%2010%2F29%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxeYzNEvNRagBQPQBY%2Fgroup-rationality-diary-10-29-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%2010%2F29%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxeYzNEvNRagBQPQBY%2Fgroup-rationality-diary-10-29-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxeYzNEvNRagBQPQBY%2Fgroup-rationality-diary-10-29-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">T<span style=\"color: #333333;\">his is the public group instrumental rationality diary for the week of October 29th. It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<div id=\"entry_t3_drj\" class=\"content clear\" style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; font-size: 12px; line-height: 18px;\">\n<div class=\"md\">\n<ul style=\"padding: 0px; line-height: 19px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Thanks to everyone who contributes!</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\"><a style=\"color: #8a8a8b;\" href=\"/lw/eys/group_rationality_diary_101512/\">Previous diary</a>;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">archive of prior diaries</a>.</p>\n</div>\n</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xeYzNEvNRagBQPQBY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 1.0217275472891317e-06, "legacy": true, "legacyId": "19706", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ekdyBuErNG9MDKYeg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-31T02:35:56.354Z", "modifiedAt": null, "url": null, "title": "Question about application of Bayes", "slug": "question-about-application-of-bayes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:57.356Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bCozRPjExJd845gwN/question-about-application-of-bayes", "pageUrlRelative": "/posts/bCozRPjExJd845gwN/question-about-application-of-bayes", "linkUrl": "https://www.lesswrong.com/posts/bCozRPjExJd845gwN/question-about-application-of-bayes", "postedAtFormatted": "Wednesday, October 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Question%20about%20application%20of%20Bayes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestion%20about%20application%20of%20Bayes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbCozRPjExJd845gwN%2Fquestion-about-application-of-bayes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Question%20about%20application%20of%20Bayes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbCozRPjExJd845gwN%2Fquestion-about-application-of-bayes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbCozRPjExJd845gwN%2Fquestion-about-application-of-bayes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 196, "htmlBody": "<p>I have successfully confused myself about probability again.&nbsp;</p>\n<p>I am debugging an intermittent crash; it doesn't happen every time I run the program. After much confusion I believe I have traced the problem to a specific line (activating my debug logger, as it happens; irony...) I have tested my program with and without this line commented out. I find that, when the line is active, I get two crashes on seven runs. Without the line, I get no crashes on ten runs. Intuitively this seems like evidence in favour of the hypothesis that the line is causing the crash. But I'm confused on how to set up the equations. Do I need a probability distribution over crash frequencies? That was the solution the last time I was confused over Bayes, but I don't understand what it means to say \"The probability of having the line, given crash frequency f\", which it seems I need to know to calculate a new probability distribution.&nbsp;</p>\n<p>I'm going to go with my intuition and code on the assumption that the debug logger should be activated much later in the program to avoid a race condition, but I'd like to understand this math.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bCozRPjExJd845gwN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 3, "extendedScore": null, "score": 1.0220080835202903e-06, "legacy": true, "legacyId": "19709", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-31T04:53:10.551Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Failure By Affective Analogy", "slug": "seq-rerun-failure-by-affective-analogy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:32.406Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/59GR5czLoZFdoqpAN/seq-rerun-failure-by-affective-analogy", "pageUrlRelative": "/posts/59GR5czLoZFdoqpAN/seq-rerun-failure-by-affective-analogy", "linkUrl": "https://www.lesswrong.com/posts/59GR5czLoZFdoqpAN/seq-rerun-failure-by-affective-analogy", "postedAtFormatted": "Wednesday, October 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Failure%20By%20Affective%20Analogy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Failure%20By%20Affective%20Analogy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F59GR5czLoZFdoqpAN%2Fseq-rerun-failure-by-affective-analogy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Failure%20By%20Affective%20Analogy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F59GR5czLoZFdoqpAN%2Fseq-rerun-failure-by-affective-analogy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F59GR5czLoZFdoqpAN%2Fseq-rerun-failure-by-affective-analogy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<p>Today's post, <a href=\"/lw/vy/failure_by_affective_analogy/\">Failure By Affective Analogy</a> was originally published on 18 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Failure_By_Affective_Analogy\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Making analogies to things that have positive or negative connotations is an even better way to make sure you fail.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/f6z/seq_rerun_failure_by_analogy/\">Failure By Analogy</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "59GR5czLoZFdoqpAN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0220830093471853e-06, "legacy": true, "legacyId": "19718", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["o8Ju8pTGvAfkkg6xZ", "KSy8xhPYskg5x65Gz", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-31T05:09:00.208Z", "modifiedAt": null, "url": null, "title": "Things philosophers have debated", "slug": "things-philosophers-have-debated", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:33.906Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sbFKPrKHjgAQSE65L/things-philosophers-have-debated", "pageUrlRelative": "/posts/sbFKPrKHjgAQSE65L/things-philosophers-have-debated", "linkUrl": "https://www.lesswrong.com/posts/sbFKPrKHjgAQSE65L/things-philosophers-have-debated", "postedAtFormatted": "Wednesday, October 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Things%20philosophers%20have%20debated&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThings%20philosophers%20have%20debated%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsbFKPrKHjgAQSE65L%2Fthings-philosophers-have-debated%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Things%20philosophers%20have%20debated%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsbFKPrKHjgAQSE65L%2Fthings-philosophers-have-debated", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsbFKPrKHjgAQSE65L%2Fthings-philosophers-have-debated", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 290, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/Trivialism\">Straight from Wikipedia</a>.</p>\n<p>I just had to stare at this a while. &nbsp;We can have papers published about this, we really ought to be able to get papers published about Friendly AI subproblems.</p>\n<p>My favorite part is at the very end.</p>\n<p>\n<hr />\n</p>\n<p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.200000762939453px; font-family: sans-serif; font-size: 13px;\"><strong>Trivialism</strong>&nbsp;is the&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Theory\" href=\"http://en.wikipedia.org/wiki/Theory\">theory</a>&nbsp;that every&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Proposition\" href=\"http://en.wikipedia.org/wiki/Proposition\">proposition</a>&nbsp;is true. A&nbsp;<a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Logical consequence\" href=\"http://en.wikipedia.org/wiki/Logical_consequence\">consequence</a>&nbsp;of trivialism is that all statements, including all&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Contradiction\" href=\"http://en.wikipedia.org/wiki/Contradiction\">contradictions</a>&nbsp;of the form \"p and not p\" (that something both 'is' and 'isn't' at the same time), are true.<sup id=\"cite_ref-GabbayWoods2007_0-0\" class=\"reference\" style=\"line-height: 1em;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"http://en.wikipedia.org/wiki/Trivialism#cite_note-GabbayWoods2007-0\">[1]</a></sup></p>\n<h2 style=\"background-image: none; font-weight: normal; margin: 0px 0px 0.6em; overflow: hidden; padding-top: 0.5em; padding-bottom: 0.17em; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #aaaaaa; font-size: 19px; font-family: sans-serif; line-height: 19.200000762939453px;\"><span class=\"editsection\" style=\"-webkit-user-select: none; float: right; font-size: 13px; margin-left: 5px;\">[<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Edit section: See also\" href=\"http://en.wikipedia.org/w/index.php?title=Trivialism&amp;action=edit&amp;section=1\">edit</a>]</span><span id=\"See_also\" class=\"mw-headline\">See also</span></h2>\n<ul style=\"line-height: 19.200000762939453px; list-style-type: square; margin: 0.3em 0px 0px 1.6em; padding: 0px; list-style-image: url(data; font-family: sans-serif; font-size: 13px;\">\n<li style=\"margin-bottom: 0.1em;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Dialetheism\" href=\"http://en.wikipedia.org/wiki/Dialetheism\">Dialetheism</a></li>\n<li style=\"margin-bottom: 0.1em;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Kevala Jnana\" href=\"http://en.wikipedia.org/wiki/Kevala_Jnana\">Kevala Jnana</a></li>\n<li style=\"margin-bottom: 0.1em;\"><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Paraconsistency\" href=\"http://en.wikipedia.org/wiki/Paraconsistency\">Paraconsistency</a></li>\n<li style=\"margin-bottom: 0.1em;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Principle of explosion\" href=\"http://en.wikipedia.org/wiki/Principle_of_explosion\">Principle of explosion</a></li>\n</ul>\n<h2 style=\"background-image: none; font-weight: normal; margin: 0px 0px 0.6em; overflow: hidden; padding-top: 0.5em; padding-bottom: 0.17em; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #aaaaaa; font-size: 19px; font-family: sans-serif; line-height: 19.200000762939453px;\"><span class=\"editsection\" style=\"-webkit-user-select: none; float: right; font-size: 13px; margin-left: 5px;\">[<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Edit section: References\" href=\"http://en.wikipedia.org/w/index.php?title=Trivialism&amp;action=edit&amp;section=2\">edit</a>]</span><span id=\"References\" class=\"mw-headline\">References</span></h2>\n<div class=\"reflist\" style=\"font-size: 12px; margin-bottom: 0.5em; font-family: sans-serif; line-height: 19.200000762939453px; list-style-type: decimal;\"><ol class=\"references\" style=\"line-height: 1.5em; margin: 0.3em 0px 0.5em 3.2em; padding: 0px; list-style-image: none; list-style-type: inherit;\">\n<li id=\"cite_note-GabbayWoods2007-0\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"http://en.wikipedia.org/wiki/Trivialism#cite_ref-GabbayWoods2007_0-0\">^</a></strong></span>&nbsp;<span class=\"reference-text\"><span class=\"citation book\" style=\"word-wrap: break-word;\">Graham Priest; John Woods (2007).&nbsp;<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-right: 13px; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"http://books.google.com/books?id=3TNj1ZkP3qEC&amp;pg=PA131\">\"Paraconsistency and Dialetheism\"</a>.&nbsp;<em>The Many Valued and Nonmonotonic Turn in Logic</em>. Elsevier. p.&nbsp;131.&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"International Standard Book Number\" href=\"http://en.wikipedia.org/wiki/International_Standard_Book_Number\">ISBN</a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Special:BookSources/978-0-444-51623-7\" href=\"http://en.wikipedia.org/wiki/Special:BookSources/978-0-444-51623-7\">978-0-444-51623-7</a>.</span></span></li>\n</ol></div>\n<h2 style=\"background-image: none; font-weight: normal; margin: 0px 0px 0.6em; overflow: hidden; padding-top: 0.5em; padding-bottom: 0.17em; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #aaaaaa; font-size: 19px; font-family: sans-serif; line-height: 19.200000762939453px;\"><span class=\"editsection\" style=\"-webkit-user-select: none; float: right; font-size: 13px; margin-left: 5px;\">[<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Edit section: Further reading\" href=\"http://en.wikipedia.org/w/index.php?title=Trivialism&amp;action=edit&amp;section=3\">edit</a>]</span><span id=\"Further_reading\" class=\"mw-headline\">Further reading</span></h2>\n<ul style=\"line-height: 19.200000762939453px; list-style-type: square; margin: 0.3em 0px 0px 1.6em; padding: 0px; list-style-image: url(data; font-family: sans-serif; font-size: 13px;\">\n<li style=\"margin-bottom: 0.1em;\">Paul Kabay (2008). \"<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-right: 13px; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"http://repository.unimelb.edu.au/10187/5734\">A defense of trivialism</a>\". PhD thesis, School of Philosophy, Anthropology, and Social Inquiry, The University of Melbourne.</li>\n<li style=\"margin-bottom: 0.1em;\"><span class=\"citation book\" style=\"word-wrap: break-word;\">Paul Kabay (2010).&nbsp;<em>On the Plenitude of Truth. A Defense of Trivialism</em>. Lambert Academic Publishing.&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"International Standard Book Number\" href=\"http://en.wikipedia.org/wiki/International_Standard_Book_Number\">ISBN</a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Special:BookSources/978-3-8383-5102-5\" href=\"http://en.wikipedia.org/wiki/Special:BookSources/978-3-8383-5102-5\">978-3-8383-5102-5</a>.</span></li>\n<li style=\"margin-bottom: 0.1em;\">Luis Estrada-Gonz&aacute;lez (2012) \"<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(http://upload.wikimedia.org/wikipedia/commons/2/23/Icons-mini-file_acrobat.gif); padding-right: 18px; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"http://www.logika.umk.pl/llp/212/4-212zw.pdf\">Models of Possibilism and Trivialism</a>\", Logic and Logical Philosophy, Volume 21, 175&ndash;205</li>\n<li style=\"margin-bottom: 0.1em;\"><span class=\"citation book\" style=\"word-wrap: break-word;\">Frederick Kroon (2004). \"Realism and Dialetheism\". In Graham Priest, J. C. Beall, and Bradley Armour-Garb.&nbsp;<em>The Law of Non-Contradiction: New Philosophical Essays</em>. Oxford University Press.&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"International Standard Book Number\" href=\"http://en.wikipedia.org/wiki/International_Standard_Book_Number\">ISBN</a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Special:BookSources/978-0-19-926517-6\" href=\"http://en.wikipedia.org/wiki/Special:BookSources/978-0-19-926517-6\">978-0-19-926517-6</a>.</span></li>\n<li style=\"margin-bottom: 0.1em;\">Paul Kabay (2010).&nbsp;<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-right: 13px; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"https://docs.google.com/viewer?a=v&amp;pid=explorer&amp;chrome=true&amp;srcid=114v_6Is4P48MZ58phTkHo9vJccTeJaDCEGU2rU7gzoFxdOg7OQGJ-ER8o4jk&amp;hl=en&amp;authkey=CNuH1OIP&amp;pli=1\">Interpreting the divyadhvani: On Why the Digambara Sect Is Right about the Nature of the Kevalin</a>. The Australasian Philosophy of Religion Association Conference</li>\n<li style=\"margin-bottom: 0.1em;\"><span class=\"citation Journal\" style=\"word-wrap: break-word;\">Bueno, O. V. (2007). \"Troubles with Trivialism\".&nbsp;<em>Inquiry</em>&nbsp;<strong>50</strong>&nbsp;(6): 655&ndash;667.&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Digital object identifier\" href=\"http://en.wikipedia.org/wiki/Digital_object_identifier\">doi</a>:<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-right: 13px; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"http://dx.doi.org/10.1080%2F00201740701698670\">10.1080/00201740701698670</a>.</span>&nbsp;<span class=\"plainlinks noprint\" style=\"font-size: smaller;\"><a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-top: 0px !important; padding-right: 13px; padding-bottom: 0px !important; padding-left: 0px !important; background-position: 0% 0%; background-repeat: no-repeat no-repeat;\" href=\"http://en.wikipedia.org/w/index.php?title=Template:Cite_doi/10.1080.2F00201740701698670&amp;action=edit&amp;editintro=Template:Cite_doi/editintro2\">edit</a></span></li>\n<li style=\"margin-bottom: 0.1em;\"><span class=\"citation Journal\" style=\"word-wrap: break-word;\">Priest, G. (2000). \"Could everything be true?\".&nbsp;<em>Australasian Journal of Philosophy</em>&nbsp;<strong>78</strong>&nbsp;(2): 189&ndash;195.&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Digital object identifier\" href=\"http://en.wikipedia.org/wiki/Digital_object_identifier\">doi</a>:<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-right: 13px; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"http://dx.doi.org/10.1080%2F00048400012349471\">10.1080/00048400012349471</a>.</span>&nbsp;<span class=\"plainlinks noprint\" style=\"font-size: smaller;\"><a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-top: 0px !important; padding-right: 13px; padding-bottom: 0px !important; padding-left: 0px !important; background-position: 0% 0%; background-repeat: no-repeat no-repeat;\" href=\"http://en.wikipedia.org/w/index.php?title=Template:Cite_doi/10.1080.2F00048400012349471&amp;action=edit&amp;editintro=Template:Cite_doi/editintro2\">edit</a></span></li>\n</ul>\n<table class=\"metadata plainlinks stub\" style=\"font-size: 13px; color: #000000; font-family: sans-serif; line-height: 19.200000762939453px; text-align: start; background-color: transparent;\" border=\"0\">\n<tbody>\n<tr>\n<td><a class=\"image\" style=\"text-decoration: none; color: #0b0080; background-image: none !important; padding: 0px !important; background-position: initial initial !important; background-repeat: initial initial !important;\" href=\"http://en.wikipedia.org/w/index.php?title=File:Logic.svg&amp;page=1\"><img style=\"border-style: none; vertical-align: middle;\" src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Logic.svg/40px-Logic.svg.png\" alt=\"Stub icon\" width=\"40\" height=\"13\" /></a></td>\n<td><em>This&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none !important; padding: 0px !important; background-position: initial initial !important; background-repeat: initial initial !important;\" title=\"Logic\" href=\"http://en.wikipedia.org/wiki/Logic\">logic</a>-related article is a&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none !important; padding: 0px !important; background-position: initial initial !important; background-repeat: initial initial !important;\" title=\"Wikipedia:Stub\" href=\"http://en.wikipedia.org/wiki/Wikipedia:Stub\">stub</a>. You can help Wikipedia by&nbsp;<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-top: 0px !important; padding-right: 13px; padding-bottom: 0px !important; padding-left: 0px !important; background-position: 0% 0%; background-repeat: no-repeat no-repeat;\" href=\"http://en.wikipedia.org/w/index.php?title=Trivialism&amp;action=edit\">expanding&nbsp;</a>it.</em></td>\n</tr>\n</tbody>\n</table>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sbFKPrKHjgAQSE65L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 11, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "19719", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://en.wikipedia.org/wiki/Trivialism\">Straight from Wikipedia</a>.</p>\n<p>I just had to stare at this a while. &nbsp;We can have papers published about this, we really ought to be able to get papers published about Friendly AI subproblems.</p>\n<p>My favorite part is at the very end.</p>\n<p>\n</p><hr>\n<p></p>\n<p>\n</p><p style=\"margin: 0.4em 0px 0.5em; line-height: 19.200000762939453px; font-family: sans-serif; font-size: 13px;\"><strong>Trivialism</strong>&nbsp;is the&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Theory\" href=\"http://en.wikipedia.org/wiki/Theory\">theory</a>&nbsp;that every&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Proposition\" href=\"http://en.wikipedia.org/wiki/Proposition\">proposition</a>&nbsp;is true. A&nbsp;<a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Logical consequence\" href=\"http://en.wikipedia.org/wiki/Logical_consequence\">consequence</a>&nbsp;of trivialism is that all statements, including all&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Contradiction\" href=\"http://en.wikipedia.org/wiki/Contradiction\">contradictions</a>&nbsp;of the form \"p and not p\" (that something both 'is' and 'isn't' at the same time), are true.<sup id=\"cite_ref-GabbayWoods2007_0-0\" class=\"reference\" style=\"line-height: 1em;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"http://en.wikipedia.org/wiki/Trivialism#cite_note-GabbayWoods2007-0\">[1]</a></sup></p>\n<h2 style=\"background-image: none; font-weight: normal; margin: 0px 0px 0.6em; overflow: hidden; padding-top: 0.5em; padding-bottom: 0.17em; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #aaaaaa; font-size: 19px; font-family: sans-serif; line-height: 19.200000762939453px;\" id=\"_edit_See_also\"><span class=\"editsection\" style=\"-webkit-user-select: none; float: right; font-size: 13px; margin-left: 5px;\">[<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Edit section: See also\" href=\"http://en.wikipedia.org/w/index.php?title=Trivialism&amp;action=edit&amp;section=1\">edit</a>]</span><span id=\"See_also\" class=\"mw-headline\">See also</span></h2>\n<ul style=\"line-height: 19.200000762939453px; list-style-type: square; margin: 0.3em 0px 0px 1.6em; padding: 0px; list-style-image: url(data; font-family: sans-serif; font-size: 13px;\">\n<li style=\"margin-bottom: 0.1em;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Dialetheism\" href=\"http://en.wikipedia.org/wiki/Dialetheism\">Dialetheism</a></li>\n<li style=\"margin-bottom: 0.1em;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Kevala Jnana\" href=\"http://en.wikipedia.org/wiki/Kevala_Jnana\">Kevala Jnana</a></li>\n<li style=\"margin-bottom: 0.1em;\"><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Paraconsistency\" href=\"http://en.wikipedia.org/wiki/Paraconsistency\">Paraconsistency</a></li>\n<li style=\"margin-bottom: 0.1em;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Principle of explosion\" href=\"http://en.wikipedia.org/wiki/Principle_of_explosion\">Principle of explosion</a></li>\n</ul>\n<h2 style=\"background-image: none; font-weight: normal; margin: 0px 0px 0.6em; overflow: hidden; padding-top: 0.5em; padding-bottom: 0.17em; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #aaaaaa; font-size: 19px; font-family: sans-serif; line-height: 19.200000762939453px;\" id=\"_edit_References\"><span class=\"editsection\" style=\"-webkit-user-select: none; float: right; font-size: 13px; margin-left: 5px;\">[<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Edit section: References\" href=\"http://en.wikipedia.org/w/index.php?title=Trivialism&amp;action=edit&amp;section=2\">edit</a>]</span><span id=\"References\" class=\"mw-headline\">References</span></h2>\n<div class=\"reflist\" style=\"font-size: 12px; margin-bottom: 0.5em; font-family: sans-serif; line-height: 19.200000762939453px; list-style-type: decimal;\"><ol class=\"references\" style=\"line-height: 1.5em; margin: 0.3em 0px 0.5em 3.2em; padding: 0px; list-style-image: none; list-style-type: inherit;\">\n<li id=\"cite_note-GabbayWoods2007-0\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"http://en.wikipedia.org/wiki/Trivialism#cite_ref-GabbayWoods2007_0-0\">^</a></strong></span>&nbsp;<span class=\"reference-text\"><span class=\"citation book\" style=\"word-wrap: break-word;\">Graham Priest; John Woods (2007).&nbsp;<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-right: 13px; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"http://books.google.com/books?id=3TNj1ZkP3qEC&amp;pg=PA131\">\"Paraconsistency and Dialetheism\"</a>.&nbsp;<em>The Many Valued and Nonmonotonic Turn in Logic</em>. Elsevier. p.&nbsp;131.&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"International Standard Book Number\" href=\"http://en.wikipedia.org/wiki/International_Standard_Book_Number\">ISBN</a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Special:BookSources/978-0-444-51623-7\" href=\"http://en.wikipedia.org/wiki/Special:BookSources/978-0-444-51623-7\">978-0-444-51623-7</a>.</span></span></li>\n</ol></div>\n<h2 style=\"background-image: none; font-weight: normal; margin: 0px 0px 0.6em; overflow: hidden; padding-top: 0.5em; padding-bottom: 0.17em; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #aaaaaa; font-size: 19px; font-family: sans-serif; line-height: 19.200000762939453px;\" id=\"_edit_Further_reading\"><span class=\"editsection\" style=\"-webkit-user-select: none; float: right; font-size: 13px; margin-left: 5px;\">[<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Edit section: Further reading\" href=\"http://en.wikipedia.org/w/index.php?title=Trivialism&amp;action=edit&amp;section=3\">edit</a>]</span><span id=\"Further_reading\" class=\"mw-headline\">Further reading</span></h2>\n<ul style=\"line-height: 19.200000762939453px; list-style-type: square; margin: 0.3em 0px 0px 1.6em; padding: 0px; list-style-image: url(data; font-family: sans-serif; font-size: 13px;\">\n<li style=\"margin-bottom: 0.1em;\">Paul Kabay (2008). \"<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-right: 13px; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"http://repository.unimelb.edu.au/10187/5734\">A defense of trivialism</a>\". PhD thesis, School of Philosophy, Anthropology, and Social Inquiry, The University of Melbourne.</li>\n<li style=\"margin-bottom: 0.1em;\"><span class=\"citation book\" style=\"word-wrap: break-word;\">Paul Kabay (2010).&nbsp;<em>On the Plenitude of Truth. A Defense of Trivialism</em>. Lambert Academic Publishing.&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"International Standard Book Number\" href=\"http://en.wikipedia.org/wiki/International_Standard_Book_Number\">ISBN</a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Special:BookSources/978-3-8383-5102-5\" href=\"http://en.wikipedia.org/wiki/Special:BookSources/978-3-8383-5102-5\">978-3-8383-5102-5</a>.</span></li>\n<li style=\"margin-bottom: 0.1em;\">Luis Estrada-Gonz\u00e1lez (2012) \"<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(http://upload.wikimedia.org/wikipedia/commons/2/23/Icons-mini-file_acrobat.gif); padding-right: 18px; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"http://www.logika.umk.pl/llp/212/4-212zw.pdf\">Models of Possibilism and Trivialism</a>\", Logic and Logical Philosophy, Volume 21, 175\u2013205</li>\n<li style=\"margin-bottom: 0.1em;\"><span class=\"citation book\" style=\"word-wrap: break-word;\">Frederick Kroon (2004). \"Realism and Dialetheism\". In Graham Priest, J. C. Beall, and Bradley Armour-Garb.&nbsp;<em>The Law of Non-Contradiction: New Philosophical Essays</em>. Oxford University Press.&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"International Standard Book Number\" href=\"http://en.wikipedia.org/wiki/International_Standard_Book_Number\">ISBN</a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Special:BookSources/978-0-19-926517-6\" href=\"http://en.wikipedia.org/wiki/Special:BookSources/978-0-19-926517-6\">978-0-19-926517-6</a>.</span></li>\n<li style=\"margin-bottom: 0.1em;\">Paul Kabay (2010).&nbsp;<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-right: 13px; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"https://docs.google.com/viewer?a=v&amp;pid=explorer&amp;chrome=true&amp;srcid=114v_6Is4P48MZ58phTkHo9vJccTeJaDCEGU2rU7gzoFxdOg7OQGJ-ER8o4jk&amp;hl=en&amp;authkey=CNuH1OIP&amp;pli=1\">Interpreting the divyadhvani: On Why the Digambara Sect Is Right about the Nature of the Kevalin</a>. The Australasian Philosophy of Religion Association Conference</li>\n<li style=\"margin-bottom: 0.1em;\"><span class=\"citation Journal\" style=\"word-wrap: break-word;\">Bueno, O. V. (2007). \"Troubles with Trivialism\".&nbsp;<em>Inquiry</em>&nbsp;<strong>50</strong>&nbsp;(6): 655\u2013667.&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Digital object identifier\" href=\"http://en.wikipedia.org/wiki/Digital_object_identifier\">doi</a>:<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-right: 13px; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"http://dx.doi.org/10.1080%2F00201740701698670\">10.1080/00201740701698670</a>.</span>&nbsp;<span class=\"plainlinks noprint\" style=\"font-size: smaller;\"><a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-top: 0px !important; padding-right: 13px; padding-bottom: 0px !important; padding-left: 0px !important; background-position: 0% 0%; background-repeat: no-repeat no-repeat;\" href=\"http://en.wikipedia.org/w/index.php?title=Template:Cite_doi/10.1080.2F00201740701698670&amp;action=edit&amp;editintro=Template:Cite_doi/editintro2\">edit</a></span></li>\n<li style=\"margin-bottom: 0.1em;\"><span class=\"citation Journal\" style=\"word-wrap: break-word;\">Priest, G. (2000). \"Could everything be true?\".&nbsp;<em>Australasian Journal of Philosophy</em>&nbsp;<strong>78</strong>&nbsp;(2): 189\u2013195.&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Digital object identifier\" href=\"http://en.wikipedia.org/wiki/Digital_object_identifier\">doi</a>:<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-right: 13px; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"http://dx.doi.org/10.1080%2F00048400012349471\">10.1080/00048400012349471</a>.</span>&nbsp;<span class=\"plainlinks noprint\" style=\"font-size: smaller;\"><a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-top: 0px !important; padding-right: 13px; padding-bottom: 0px !important; padding-left: 0px !important; background-position: 0% 0%; background-repeat: no-repeat no-repeat;\" href=\"http://en.wikipedia.org/w/index.php?title=Template:Cite_doi/10.1080.2F00048400012349471&amp;action=edit&amp;editintro=Template:Cite_doi/editintro2\">edit</a></span></li>\n</ul>\n<table class=\"metadata plainlinks stub\" style=\"font-size: 13px; color: #000000; font-family: sans-serif; line-height: 19.200000762939453px; text-align: start; background-color: transparent;\" border=\"0\">\n<tbody>\n<tr>\n<td><a class=\"image\" style=\"text-decoration: none; color: #0b0080; background-image: none !important; padding: 0px !important; background-position: initial initial !important; background-repeat: initial initial !important;\" href=\"http://en.wikipedia.org/w/index.php?title=File:Logic.svg&amp;page=1\"><img style=\"border-style: none; vertical-align: middle;\" src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Logic.svg/40px-Logic.svg.png\" alt=\"Stub icon\" width=\"40\" height=\"13\"></a></td>\n<td><em>This&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none !important; padding: 0px !important; background-position: initial initial !important; background-repeat: initial initial !important;\" title=\"Logic\" href=\"http://en.wikipedia.org/wiki/Logic\">logic</a>-related article is a&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none !important; padding: 0px !important; background-position: initial initial !important; background-repeat: initial initial !important;\" title=\"Wikipedia:Stub\" href=\"http://en.wikipedia.org/wiki/Wikipedia:Stub\">stub</a>. You can help Wikipedia by&nbsp;<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(data; padding-top: 0px !important; padding-right: 13px; padding-bottom: 0px !important; padding-left: 0px !important; background-position: 0% 0%; background-repeat: no-repeat no-repeat;\" href=\"http://en.wikipedia.org/w/index.php?title=Trivialism&amp;action=edit\">expanding&nbsp;</a>it.</em></td>\n</tr>\n</tbody>\n</table>\n<p></p>", "sections": [{"title": "[edit]See also", "anchor": "_edit_See_also", "level": 1}, {"title": "[edit]References", "anchor": "_edit_References", "level": 1}, {"title": "[edit]Further reading", "anchor": "_edit_Further_reading", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "78 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-31T07:03:00.818Z", "modifiedAt": null, "url": null, "title": "Beyond Bayesians and Frequentists", "slug": "beyond-bayesians-and-frequentists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:01.337Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsteinhardt", "createdAt": "2010-08-05T03:07:27.568Z", "isAdmin": false, "displayName": "jsteinhardt"}, "userId": "EF8W65G6RaXxZjLBX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o32tEFf5zBiByL2xv/beyond-bayesians-and-frequentists", "pageUrlRelative": "/posts/o32tEFf5zBiByL2xv/beyond-bayesians-and-frequentists", "linkUrl": "https://www.lesswrong.com/posts/o32tEFf5zBiByL2xv/beyond-bayesians-and-frequentists", "postedAtFormatted": "Wednesday, October 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Beyond%20Bayesians%20and%20Frequentists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeyond%20Bayesians%20and%20Frequentists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo32tEFf5zBiByL2xv%2Fbeyond-bayesians-and-frequentists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Beyond%20Bayesians%20and%20Frequentists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo32tEFf5zBiByL2xv%2Fbeyond-bayesians-and-frequentists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo32tEFf5zBiByL2xv%2Fbeyond-bayesians-and-frequentists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3347, "htmlBody": "<p>(Note: this is cross-posted from my <a href=\"http://jsteinhardt.wordpress.com/2012/10/31/beyond-bayesians-and-frequentists/\">blog</a>&nbsp;and also available in pdf <a href=\"http://web.mit.edu/jsteinha/www/stats-essay.pdf\">here</a>.)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">If you are a newly initiated student into the field of machine learning, it won't be long before you start hearing the words \"Bayesian\" and \"frequentist\" thrown around. Many people around you probably have strong opinions on which is the \"right\" way to do statistics, and within a year you've probably developed your own strong opinions (which are suspiciously similar to those of the people around you, despite there being a much greater variance of opinion between different labs). In fact, now that the year is 2012 the majority of new graduate students are being raised as Bayesians (at least in the U.S.) with frequentists thought of as stodgy emeritus professors stuck in their ways.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">If you are like me, the preceding set of facts will make you very uneasy. They will make you uneasy because simple pattern-matching -- the strength of people's opinions, the reliability with which these opinions split along age boundaries and lab boundaries, and the ridicule that each side levels at the other camp &ndash; makes the \"Bayesians vs. frequentists\" debate look far more like politics than like scholarly discourse. Of course, that alone does not necessarily prove anything; these disconcerting similarities could just be coincidences that I happened to cherry-pick.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">My next point, then, is that we are right to be uneasy, because such debate makes us less likely to evaluate the strengths and weaknesses of both approaches in good faith. This essay is a push against that --- I summarize the justifications for Bayesian methods and where they fall short, show how frequentist approaches can fill in some of their shortcomings, and then present my personal (though probably woefully under-informed) guidelines for choosing which type of approach to use.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Before doing any of this, though, a bit of background is in order...</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong>1. Background on Bayesians and Frequentists</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong>1.1. Three Levels of Argument</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">As Andrew Critch [6] insightfully points out, the Bayesians vs. frequentists debate is really three debates at once, centering around one or more of the following arguments:</p>\n<ol style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">\n<li>Whether to interpret subjective beliefs as probabilities</li>\n<li>Whether to interpret probabilities as subjective beliefs (as opposed to asymptotic frequencies)</li>\n<li>Whether a Bayesian or frequentist algorithm is better suited to solving a particular problem.</li>\n</ol>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Given my own research interests, I will add a fourth argument:</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">4. Whether Bayesian or frequentist techniques are better suited to engineering an artificial intelligence.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Andrew Gelman [9] has his own well-written essay on the subject, where he expands on these distinctions and presents his own more nuanced view.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Why are these arguments so commonly conflated? I'm not entirely sure; I would guess it is for historical reasons but I have so far been unable to find said historical reasons. Whatever the reasons, what this boils down to in the present day is that people often form opinions on 1. and 2., which then influence their answers to 3. and 4. This is&nbsp;<em>not good</em>, since 1. and 2. are philosophical in nature and difficult to resolve correctly, whereas 3. and 4. are often much easier to resolve and extremely important to resolve correctly in practice. Let me re-iterate:&nbsp;<em>the Bayes vs. frequentist discussion should center on the practical employment of the two methods, or, if epistemology must be discussed, it should be clearly separated from the day-to-day practical decisions</em>. Aside from the difficulties with correctly deciding epistemology, the relationship between generic epistemology and specific practices in cutting-edge statistical research is only via a long causal chain, and it should be completely unsurprising if Bayesian epistemology leads to the employment of frequentist tools or vice versa.<a id=\"more\"></a></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">For this reason and for reasons of space, I will spend the remainder of the essay focusing on&nbsp;<em>statistical algorithms</em>&nbsp;rather than on&nbsp;<em>interpretations of probability</em>. For those who really want to discuss interpretations of probability, I will address that in a later essay.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong>1.2. Recap of Bayesian Decision Theory</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">(What follows will be review for many.) In Bayesian decision theory, we assume that there is some underlying world state &theta;&nbsp;and a&nbsp;<em>likelihood function</em>&nbsp;p(X1,...,Xn |&nbsp;&theta;)&nbsp; over possible observations. (A&nbsp;<em>likelihood function</em>&nbsp;is just a conditional probability distribution where the parameter conditioned on can vary.) We also have a space A of possible actions and a utility function U(&theta;; a) that gives the utility of performing action a if the underlying world state is&nbsp;&theta;. We can incorporate notions like planning and value of information by defining U(&theta;; a) recursively in terms of an identical agent to ourselves who has seen one additional observation (or, if we are planning against an adversary, in terms of the adversary). For a more detailed overview of this material, see the tutorial by North [11].</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">What distinguishes the Bayesian approach in particular is one additional assumption, a&nbsp;<em>prior distribution</em>&nbsp;p(&theta;) over possible world states. To make a decision with respect to a given prior, we compute the posterior distribution p<sub>posterior</sub>(&theta; | X1,...,Xn)&nbsp;using Bayes' theorem, then take the action a that maximizes <img src=\"http://www.codecogs.com/png.latex?\\mathbb{E}_{p_{\\mathrm{posterior}}}[U(\\theta; a)]\" alt=\"\" />.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">In practice,&nbsp;p<sub>posterior</sub>(&theta; | X1,...,Xn)&nbsp;can be quite difficult to compute, and so we often attempt to approximate it. Such attempts are known as&nbsp;<em>approximate inference algorithms</em>.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong>1.3. Steel-manning Frequentists</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">There are many different ideas that fall under the broad umbrella of frequentist techniques. While it would be impossible to adequately summarize all of them even if I attempted to, there are three in particular that I would like to describe, and which I will call&nbsp;<em>frequentist decision theory</em>,&nbsp;<em>frequentist guarantees</em>, and&nbsp;<em>frequentist analysis tools</em>.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Frequentist decision theory has a very similar setup to Bayesian decision theory, with a few key differences. These are discussed in detail and contrasted with Bayesian decision theory in [10], although we summarize the differences here. There is still a likelihood function p(X1,...,Xn |&nbsp;&theta;) and a utility function U(&theta;; a). However, we do not assume the existence of a prior on&nbsp;&theta;, and instead choose the decision rule a(X1,...,Xn) that maximizes</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\displaystyle \\min\\limits_{\\theta} \\mathbb{E}[U(a(X_1,\\ldots,X_n); \\theta) \\mid \\theta]. \\ \\ \\ \\ \\ (1)\" alt=\"\" /></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">In other words, we ask for a worst case guarantee rather than an average case guarantee. As an example of how these would differ, imagine a scenario where we have no data to observe, an unknown&nbsp;&theta; in&nbsp;{1,...,N}, and we choose an action a in {0,...,N}. Furthermore, U(0;&nbsp;&theta;) = 0 for all&nbsp;&theta;, U(a;&nbsp;&theta;) = -1 if a =&nbsp;&theta;, and U(a;&theta;) = 1 if a&nbsp;&ne;&nbsp;0 and a&nbsp;&ne;&nbsp;&theta;. Then a frequentist will always choose a = 0 because any other action gets -1 utility in the worst case; a Bayesian, on the other hand, will happily choose any non-zero value of a since such an action gains (N-2)/N utility in expectation. (I am purposely ignoring more complex ideas like mixed strategies for the purpose of illustration.).</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Note that the frequentist optimization problem is more complicated than in the Bayesian case, since the value of (1) depends on the joint behavior of a(X1,...,Xn), whereas with Bayes we can optimize a(X1,...,Xn) for each set of observations separately.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">As a result of this more complex optimization problem, it is often not actually possible to maximize (1), so many frequentist techniques instead develop tools to lower-bound (1)&nbsp;for a given decision procedure, and then try to construct a decision procedure that is reasonably close to the optimum. Support vector machines [2], which try to pick separating hyperplanes that minimize generalization error, are one example of this where the algorithm is explicitly trying to maximize worst-case utility. Another example of a frequentist decision procedure is L1-regularized least squares for sparse recovery [3], where the procedure itself does not look like it is explicitly maximizing any utility function, but a separate analysis shows that it is close to the optimal procedure anyways.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">The second sort of frequentist approach to statistics is what I call a&nbsp;<em>frequentist guarantee</em>. A frequentist guarantee on an algorithm is a guarantee that, with high probability with respect to how the data was generated, the output of the algorithm will satisfy a given property. The most familiar example of this is any algorithm that generates a frequentist confidence interval: to generate a 95% frequentist confidence interval for a parameter&nbsp;&theta;&nbsp;is to run an algorithm that outputs an interval, such that with probability at least 95%&nbsp;&theta;&nbsp;lies within the interval. An important fact about most such algorithms is that the size of the interval only grows logarithmically with the amount of confidence we require, so getting a 99.9999% confidence interval is only slightly harder than getting a 95% confidence interval (and we should probably be asking for the former whenever possible).</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">If we use such algorithms to test hypotheses or to test discrete properties of&nbsp;&theta;, then we can obtain algorithms that take in probabilistically generated data and produce an output that with high probability depends&nbsp;<em>only on how the data was generated</em>, not on the specific random samples that were given. For instance, we can create an algorithm that takes in samples from two distributions, and is guaranteed to output 1 whenever they are the same, 0 whenever they differ by at least &epsilon;&nbsp;in total variational distance, and could have arbitrary output if they are different but the total variational distance is less than &epsilon;. This is an amazing property --- it takes in random input and produces an essentially deterministic answer.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Finally, a third type of frequentist approach seeks to construct&nbsp;<em>analysis tools</em>&nbsp;for understanding the behavior of random variables. Metric entropy, the Chernoff and Azuma-Hoeffding bounds [12], and Doob's optional stopping theorem are representative examples of this sort of approach. Arguably, everyone with the time to spare should master these techniques, since being able to analyze random variables is important no matter what approach to statistics you take. Indeed, frequentist analysis tools have no conflict at all with Bayesian methods --- they simply provide techniques for understanding the behavior of the Bayesian model.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong>2. Bayes vs. Other Methods</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong>2.1. Justification for Bayes</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">We presented Bayesian decision theory above, but are there any reasons why we should actually use it? One commonly-given reason is that Bayesian statistics is merely the application of Bayes' Theorem, which, being a theorem, describes the only correct way to update beliefs in response to new evidence; anything else can only be justified to the extent that it provides a good approximation to Bayesian updating. This may be true, but Bayes' Theorem only applies if we already have a prior, and if we accept probability as the correct framework for expressing uncertain beliefs. We might want to avoid one or both of these assumptions. Bayes' theorem also doesn't explain why we care about expected utility as opposed to some other statistic of the distribution over utilities (although note that frequentist decision theory also tries to maximize expected utility).</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">One compelling answer to this is&nbsp;<strong>dutch-booking</strong>, which shows that any agent must implicitly be using a probability model to make decisions, or else&nbsp;there is a series of bets that they would be willing to make that causes them to lose money with certainty. Another answer is the&nbsp;<strong>complete class theorem</strong>, which shows that any non-Bayesian decision procedure is&nbsp;<em>strictly dominated</em>&nbsp;by a Bayesian decision procedure --- meaning that the Bayesian procedure performs at least as well as the non-Bayesian procedure in all cases with certainty. In other words, if you are doing anything non-Bayesian, then either it is secretly a Bayesian procedure or there is another procedure that does strictly better than it. Finally, the&nbsp;<strong>VNM Utility Theorem</strong>&nbsp;states that any agent with consistent preferences over distributions of outcomes must be implicitly maximizing the expected value of some scalar-valued function, which we can then use as our choice of utility function U. These theorems, however, ignore the issue of computation --- while the best decision procedure may be Bayesian, the best computationally-efficient decision procedure could easily be non-Bayesian.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Another justification for Bayes is that, in contrast to ad hoc frequentist techniques, it actually provides a general theory for constructing statistical algorithms, as well as for incorporating side information such as expert knowledge. Indeed, when trying to model complex and highly structured situations it is difficult to obtain any sort of frequentist guarantees (although analysis tools can still often be applied to gain intuition about parts of the model). A prior lets us write down the sorts of models that would allow us to capture structured situations (for instance, when trying to do language modeling or transfer learning). Non-Bayesian methods exist for these situations, but they are often ad hoc and in many cases ends up looking like an approximation to Bayes. One example of this is Kneser-Ney smoothing for n-gram models, an ad hoc algorithm that ended up being very similar to an approximate inference algorithm for the hierarchical Pitman-Yor process [15, 14, 17, 8]. This raises another important point&nbsp;<em>against</em>&nbsp;Bayes, which is that the proper Bayesian interpretation may be very mathematically complex. Pitman-Yor processes are on the cutting-edge of Bayesian nonparametric statistics, which is itself one of the more technical subfields of statistical machine learning, so it was probably much easier to come up with Kneser-Ney smoothing than to find the interpretation in terms of Pitman-Yor processes.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong>2.2. When the Justifications Fail</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">The first and most common objection to Bayes is that a Bayesian method is only as good as its prior. While for simple models the performance of Bayes is relatively independent of the prior, such models can only capture data where frequentist techniques would also perform very well. For more complex (especially nonparametric) Bayesian models, the performance can depend strongly on the prior, and designing good priors is still an open problem. As one example I point to my own research on hierarchical nonparametric models, where the most straightforward attempts to build a hierarchical model lead to severe pathologies [13].</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Even if a Bayesian model does have a good prior, it may be computationally intractable to perform posterior inference. For instance, structure learning in Bayesian networks is NP-hard [4], as is topic inference in the popular latent Dirichlet allocation model (and this continues to hold even if we only want to perform approximate inference). Similar stories probably hold for other common models, although a theoretical survey has yet to be made; suffice to say that in practice approximate inference remains a difficult and unsolved problem, with many models not even considered because of the apparent hopelessness of performing inference in them.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Because frequentist methods often come with an analysis of the specific algorithm being employed, they can sometimes overcome these computational issues. One example of this mentioned already is L1 regularized least squares [3]. The problem setup is that we have a linear regression task Ax = b+v where A and b are known, v is a noise vector, and x is believed to be sparse (typically x has many more rows than b, so without the sparsity assumption x would be underdetermined). Let us suppose that x has n rows and k non-zero rows --- then the number of possible sparsity patterns is <img src=\"http://www.codecogs.com/png.latex?\\binom{n}{k}\" alt=\"\" />&nbsp;--- large enough that a brute force consideration of all possible sparsity patterns is intractable. However, we can show that solving a certain semidefinite program will with high probability yield the appropriate sparsity pattern, after which recovering x reduces to a simple least squares problem. (A&nbsp;<em>semidefinite program</em>&nbsp;is a certain type of optimization problem that can be solved efficiently [16].)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Finally, Bayes has no good way of dealing with adversaries or with cases where the data was generated in a complicated way that could make it highly biased (for instance, as the output of an optimization procedure). A toy example of an adversary would be playing rock-paper-scissors --- how should a Bayesian play such a game? The straightforward answer is to build up a model of the opponent based on their plays so far, and then to make the play that maximizes the expected score (probability of winning minus probability of losing). However, such a strategy fares poorly against any opponent with access to the model being used, as they can then just run the model themselves to predict the Bayesian's plays in advance, thereby winning every single time. In contrast, there is a frequentist strategy called the&nbsp;<strong>multiplicative weights update method</strong>&nbsp;that fairs well against an arbitrary opponent (even one with superior computational resources and access to our agent's source code). The multiplicative weights method does far more than winning at rock-paper-scissors --- it is also a key component of the fastest algorithm for solving many important optimization problems (including the network flow algorithm), and it forms the theoretical basis for the widely used AdaBoost algorithm [1, 5, 7].</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong>2.3. When To Use Each Method</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">The essential difference between Bayesian and frequentist decision theory is that Bayes makes the additional assumption of a prior over &theta;, and optimizes for average-case performance rather than worst-case performance.&nbsp;<em>It follows, then, that Bayes is the superior method whenever we can obtain a good prior and when good average-case performance is sufficient.</em>&nbsp;However, if we have no way of obtaining a good prior, or when we need guaranteed performance, frequentist methods are the way to go. For instance, if we are trying to build a software package that should be widely deployable, we might want to use a frequentist method because users can be sure that the software will work as long as some number of easily-checkable assumptions are met.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">A nice middle-ground between purely Bayesian and purely frequentist methods is to use a Bayesian model coupled with frequentist model-checking techniques; this gives us the freedom in modeling afforded by a prior but also gives us some degree of confidence that our model is correct. This approach is suggested by both Gelman [9] and Jordan [10].</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong>3. Conclusion</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">When the assumptions of Bayes' Theorem hold, and when Bayesian updating can be performed computationally efficiently, then it is indeed tautological that Bayes is the optimal approach. Even when some of these assumptions fail, Bayes can still be a fruitful approach. However, by working under weaker (sometimes even adversarial) assumptions, frequentist approaches can perform well in very complicated domains even with fairly simple models; this is because, with fewer assumptions being made at the outset, less work has to be done to ensure that those assumptions are met.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">From a research perspective, we should be far from satisfied with either approach --- Bayesian methods make stronger assumptions than may be warranted, and frequentists methods provide little in the way of a coherent framework for constructing models, and ask for worst-case guarantees, which probably cannot be obtained in general. We should seek to develop a statistical modeling framework that, unlike Bayes, can deal with unknown priors, adversaries, and limited computational resources.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong>4. Acknowledgements</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Thanks to Emma Pierson, Vladimir Slepnev, and Wei Dai for reading preliminary versions of this work and providing many helpful comments.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong>5. References</strong><strong>&nbsp;</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[1] Sanjeev Arora, Elad Hazan, and Satyen Kale.&nbsp;The multiplicative weights update method: a meta algorithm and applications.&nbsp;<em>Working Paper</em>, 2005.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[2] Christopher J.C. Burges. A tutorial on support vector machines for pattern recognition.&nbsp;<em>Data Mining and Knowledge Discovery</em>, 2:121--167, 1998.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[3] Emmanuel J. Candes. Compressive sampling.&nbsp;In&nbsp;<em>Proceedings of the International Congress of Mathematicians</em>. European Mathematical Society, 2006.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[4] D.M. Chickering. Learning bayesian networks is NP-complete.&nbsp;<em>LECTURE NOTES IN STATISTICS-NEW YORK-SPRINGER VERLAG-</em>, pages 121--130, 1996.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[5] Paul Christiano, Jonathan A. Kelner, Aleksander Madry, Daniel Spielman, and Shang-Hua Teng. Electrical flows, laplacian systems, and faster approximation of maximum flow in undirected graphs.&nbsp;In&nbsp;<em>Proceedings of the 43rd ACM Symposium on Theory of Computing</em>, 2011.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[6] Andrew Critch. Frequentist vs. bayesian breakdown: Interpretation vs. inference.&nbsp;http://lesswrong.com/lw/7ck/frequentist_vs_bayesian_breakdown_interpretation/.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[7] Yoav Freund and Robert E. Schapire. A short introduction to boosting.&nbsp;<em>Journal of Japanese Society for Artificial Intelligence</em>, 14(5):771--780, Sep. 1999.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[8] J. Gasthaus and Y.W. Teh. Improvements to the sequence memoizer.&nbsp;In&nbsp;<em>Advances in Neural Information Processing Systems</em>, 2011.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[9] Andrew Gelman. Induction and deduction in bayesian data analysis.&nbsp;<em>RMM</em>, 2:67--78, 2011.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[10] Michael I. Jordan. Are you a bayesian or a frequentist?&nbsp;Machine Learning Summer School 2009 (video lecture at http://videolectures.net/mlss09uk_jordan_bfway/).</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[11] D. Warner North. A tutorial introduction to decision theory.&nbsp;<em>IEEE Transactions on Systems Science and Cybernetics</em>, SSC-4(3):200--210, Sep. 1968.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[12] Igal Sason. On refined versions of the Azuma-Hoeffding inequality with applications in information theory.&nbsp;<em>CoRR</em>, abs/1111.1977, 2011.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[13] Jacob Steinhardt and Zoubin Ghahramani. Pathological properties of deep bayesian hierarchies.&nbsp;In&nbsp;<em>NIPS Workshop on Bayesian Nonparametrics</em>, 2011.&nbsp;Extended Abstract.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[14] Y.W. Teh. A bayesian interpretation of interpolated Kneser-Ney.&nbsp;Technical Report TRA2/06, School of Computing, NUS, 2006.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[15] Y.W. Teh. A hierarchical bayesian language model based on pitman-yor processes.&nbsp;<em>Coling/ACL</em>, 2006.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[16] Lieven Vandenberghe and Stephen Boyd. Semidefinite programming.&nbsp;<em>SIAM Review</em>, 38(1):49--95, Mar. 1996.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[17] F.~Wood, C.~Archambeau, J.~Gasthaus, L.~James, and Y.W. Teh. A stochastic memoizer for sequence data.&nbsp;In&nbsp;<em>Proceedings of the 26th International Conference on Machine Learning</em>, pages 1129--1136, 2009.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 2, "bh7uxTTqmsQ8jZJdB": 2, "5f5c37ee1b5cdee568cfb294": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o32tEFf5zBiByL2xv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 53, "extendedScore": null, "score": 0.000550405687542528, "legacy": true, "legacyId": "19721", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>(Note: this is cross-posted from my <a href=\"http://jsteinhardt.wordpress.com/2012/10/31/beyond-bayesians-and-frequentists/\">blog</a>&nbsp;and also available in pdf <a href=\"http://web.mit.edu/jsteinha/www/stats-essay.pdf\">here</a>.)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">If you are a newly initiated student into the field of machine learning, it won't be long before you start hearing the words \"Bayesian\" and \"frequentist\" thrown around. Many people around you probably have strong opinions on which is the \"right\" way to do statistics, and within a year you've probably developed your own strong opinions (which are suspiciously similar to those of the people around you, despite there being a much greater variance of opinion between different labs). In fact, now that the year is 2012 the majority of new graduate students are being raised as Bayesians (at least in the U.S.) with frequentists thought of as stodgy emeritus professors stuck in their ways.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">If you are like me, the preceding set of facts will make you very uneasy. They will make you uneasy because simple pattern-matching -- the strength of people's opinions, the reliability with which these opinions split along age boundaries and lab boundaries, and the ridicule that each side levels at the other camp \u2013 makes the \"Bayesians vs. frequentists\" debate look far more like politics than like scholarly discourse. Of course, that alone does not necessarily prove anything; these disconcerting similarities could just be coincidences that I happened to cherry-pick.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">My next point, then, is that we are right to be uneasy, because such debate makes us less likely to evaluate the strengths and weaknesses of both approaches in good faith. This essay is a push against that --- I summarize the justifications for Bayesian methods and where they fall short, show how frequentist approaches can fill in some of their shortcomings, and then present my personal (though probably woefully under-informed) guidelines for choosing which type of approach to use.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Before doing any of this, though, a bit of background is in order...</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong id=\"1__Background_on_Bayesians_and_Frequentists\">1. Background on Bayesians and Frequentists</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong id=\"1_1__Three_Levels_of_Argument\">1.1. Three Levels of Argument</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">As Andrew Critch [6] insightfully points out, the Bayesians vs. frequentists debate is really three debates at once, centering around one or more of the following arguments:</p>\n<ol style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">\n<li>Whether to interpret subjective beliefs as probabilities</li>\n<li>Whether to interpret probabilities as subjective beliefs (as opposed to asymptotic frequencies)</li>\n<li>Whether a Bayesian or frequentist algorithm is better suited to solving a particular problem.</li>\n</ol>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Given my own research interests, I will add a fourth argument:</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">4. Whether Bayesian or frequentist techniques are better suited to engineering an artificial intelligence.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Andrew Gelman [9] has his own well-written essay on the subject, where he expands on these distinctions and presents his own more nuanced view.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Why are these arguments so commonly conflated? I'm not entirely sure; I would guess it is for historical reasons but I have so far been unable to find said historical reasons. Whatever the reasons, what this boils down to in the present day is that people often form opinions on 1. and 2., which then influence their answers to 3. and 4. This is&nbsp;<em>not good</em>, since 1. and 2. are philosophical in nature and difficult to resolve correctly, whereas 3. and 4. are often much easier to resolve and extremely important to resolve correctly in practice. Let me re-iterate:&nbsp;<em>the Bayes vs. frequentist discussion should center on the practical employment of the two methods, or, if epistemology must be discussed, it should be clearly separated from the day-to-day practical decisions</em>. Aside from the difficulties with correctly deciding epistemology, the relationship between generic epistemology and specific practices in cutting-edge statistical research is only via a long causal chain, and it should be completely unsurprising if Bayesian epistemology leads to the employment of frequentist tools or vice versa.<a id=\"more\"></a></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">For this reason and for reasons of space, I will spend the remainder of the essay focusing on&nbsp;<em>statistical algorithms</em>&nbsp;rather than on&nbsp;<em>interpretations of probability</em>. For those who really want to discuss interpretations of probability, I will address that in a later essay.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong id=\"1_2__Recap_of_Bayesian_Decision_Theory\">1.2. Recap of Bayesian Decision Theory</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">(What follows will be review for many.) In Bayesian decision theory, we assume that there is some underlying world state \u03b8&nbsp;and a&nbsp;<em>likelihood function</em>&nbsp;p(X1,...,Xn |&nbsp;\u03b8)&nbsp; over possible observations. (A&nbsp;<em>likelihood function</em>&nbsp;is just a conditional probability distribution where the parameter conditioned on can vary.) We also have a space A of possible actions and a utility function U(\u03b8; a) that gives the utility of performing action a if the underlying world state is&nbsp;\u03b8. We can incorporate notions like planning and value of information by defining U(\u03b8; a) recursively in terms of an identical agent to ourselves who has seen one additional observation (or, if we are planning against an adversary, in terms of the adversary). For a more detailed overview of this material, see the tutorial by North [11].</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">What distinguishes the Bayesian approach in particular is one additional assumption, a&nbsp;<em>prior distribution</em>&nbsp;p(\u03b8) over possible world states. To make a decision with respect to a given prior, we compute the posterior distribution p<sub>posterior</sub>(\u03b8 | X1,...,Xn)&nbsp;using Bayes' theorem, then take the action a that maximizes <img src=\"http://www.codecogs.com/png.latex?\\mathbb{E}_{p_{\\mathrm{posterior}}}[U(\\theta; a)]\" alt=\"\">.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">In practice,&nbsp;p<sub>posterior</sub>(\u03b8 | X1,...,Xn)&nbsp;can be quite difficult to compute, and so we often attempt to approximate it. Such attempts are known as&nbsp;<em>approximate inference algorithms</em>.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong id=\"1_3__Steel_manning_Frequentists\">1.3. Steel-manning Frequentists</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">There are many different ideas that fall under the broad umbrella of frequentist techniques. While it would be impossible to adequately summarize all of them even if I attempted to, there are three in particular that I would like to describe, and which I will call&nbsp;<em>frequentist decision theory</em>,&nbsp;<em>frequentist guarantees</em>, and&nbsp;<em>frequentist analysis tools</em>.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Frequentist decision theory has a very similar setup to Bayesian decision theory, with a few key differences. These are discussed in detail and contrasted with Bayesian decision theory in [10], although we summarize the differences here. There is still a likelihood function p(X1,...,Xn |&nbsp;\u03b8) and a utility function U(\u03b8; a). However, we do not assume the existence of a prior on&nbsp;\u03b8, and instead choose the decision rule a(X1,...,Xn) that maximizes</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\displaystyle \\min\\limits_{\\theta} \\mathbb{E}[U(a(X_1,\\ldots,X_n); \\theta) \\mid \\theta]. \\ \\ \\ \\ \\ (1)\" alt=\"\"></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">In other words, we ask for a worst case guarantee rather than an average case guarantee. As an example of how these would differ, imagine a scenario where we have no data to observe, an unknown&nbsp;\u03b8 in&nbsp;{1,...,N}, and we choose an action a in {0,...,N}. Furthermore, U(0;&nbsp;\u03b8) = 0 for all&nbsp;\u03b8, U(a;&nbsp;\u03b8) = -1 if a =&nbsp;\u03b8, and U(a;\u03b8) = 1 if a&nbsp;\u2260&nbsp;0 and a&nbsp;\u2260&nbsp;\u03b8. Then a frequentist will always choose a = 0 because any other action gets -1 utility in the worst case; a Bayesian, on the other hand, will happily choose any non-zero value of a since such an action gains (N-2)/N utility in expectation. (I am purposely ignoring more complex ideas like mixed strategies for the purpose of illustration.).</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Note that the frequentist optimization problem is more complicated than in the Bayesian case, since the value of (1) depends on the joint behavior of a(X1,...,Xn), whereas with Bayes we can optimize a(X1,...,Xn) for each set of observations separately.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">As a result of this more complex optimization problem, it is often not actually possible to maximize (1), so many frequentist techniques instead develop tools to lower-bound (1)&nbsp;for a given decision procedure, and then try to construct a decision procedure that is reasonably close to the optimum. Support vector machines [2], which try to pick separating hyperplanes that minimize generalization error, are one example of this where the algorithm is explicitly trying to maximize worst-case utility. Another example of a frequentist decision procedure is L1-regularized least squares for sparse recovery [3], where the procedure itself does not look like it is explicitly maximizing any utility function, but a separate analysis shows that it is close to the optimal procedure anyways.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">The second sort of frequentist approach to statistics is what I call a&nbsp;<em>frequentist guarantee</em>. A frequentist guarantee on an algorithm is a guarantee that, with high probability with respect to how the data was generated, the output of the algorithm will satisfy a given property. The most familiar example of this is any algorithm that generates a frequentist confidence interval: to generate a 95% frequentist confidence interval for a parameter&nbsp;\u03b8&nbsp;is to run an algorithm that outputs an interval, such that with probability at least 95%&nbsp;\u03b8&nbsp;lies within the interval. An important fact about most such algorithms is that the size of the interval only grows logarithmically with the amount of confidence we require, so getting a 99.9999% confidence interval is only slightly harder than getting a 95% confidence interval (and we should probably be asking for the former whenever possible).</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">If we use such algorithms to test hypotheses or to test discrete properties of&nbsp;\u03b8, then we can obtain algorithms that take in probabilistically generated data and produce an output that with high probability depends&nbsp;<em>only on how the data was generated</em>, not on the specific random samples that were given. For instance, we can create an algorithm that takes in samples from two distributions, and is guaranteed to output 1 whenever they are the same, 0 whenever they differ by at least \u03b5&nbsp;in total variational distance, and could have arbitrary output if they are different but the total variational distance is less than \u03b5. This is an amazing property --- it takes in random input and produces an essentially deterministic answer.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Finally, a third type of frequentist approach seeks to construct&nbsp;<em>analysis tools</em>&nbsp;for understanding the behavior of random variables. Metric entropy, the Chernoff and Azuma-Hoeffding bounds [12], and Doob's optional stopping theorem are representative examples of this sort of approach. Arguably, everyone with the time to spare should master these techniques, since being able to analyze random variables is important no matter what approach to statistics you take. Indeed, frequentist analysis tools have no conflict at all with Bayesian methods --- they simply provide techniques for understanding the behavior of the Bayesian model.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong id=\"2__Bayes_vs__Other_Methods\">2. Bayes vs. Other Methods</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong id=\"2_1__Justification_for_Bayes\">2.1. Justification for Bayes</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">We presented Bayesian decision theory above, but are there any reasons why we should actually use it? One commonly-given reason is that Bayesian statistics is merely the application of Bayes' Theorem, which, being a theorem, describes the only correct way to update beliefs in response to new evidence; anything else can only be justified to the extent that it provides a good approximation to Bayesian updating. This may be true, but Bayes' Theorem only applies if we already have a prior, and if we accept probability as the correct framework for expressing uncertain beliefs. We might want to avoid one or both of these assumptions. Bayes' theorem also doesn't explain why we care about expected utility as opposed to some other statistic of the distribution over utilities (although note that frequentist decision theory also tries to maximize expected utility).</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">One compelling answer to this is&nbsp;<strong>dutch-booking</strong>, which shows that any agent must implicitly be using a probability model to make decisions, or else&nbsp;there is a series of bets that they would be willing to make that causes them to lose money with certainty. Another answer is the&nbsp;<strong>complete class theorem</strong>, which shows that any non-Bayesian decision procedure is&nbsp;<em>strictly dominated</em>&nbsp;by a Bayesian decision procedure --- meaning that the Bayesian procedure performs at least as well as the non-Bayesian procedure in all cases with certainty. In other words, if you are doing anything non-Bayesian, then either it is secretly a Bayesian procedure or there is another procedure that does strictly better than it. Finally, the&nbsp;<strong>VNM Utility Theorem</strong>&nbsp;states that any agent with consistent preferences over distributions of outcomes must be implicitly maximizing the expected value of some scalar-valued function, which we can then use as our choice of utility function U. These theorems, however, ignore the issue of computation --- while the best decision procedure may be Bayesian, the best computationally-efficient decision procedure could easily be non-Bayesian.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Another justification for Bayes is that, in contrast to ad hoc frequentist techniques, it actually provides a general theory for constructing statistical algorithms, as well as for incorporating side information such as expert knowledge. Indeed, when trying to model complex and highly structured situations it is difficult to obtain any sort of frequentist guarantees (although analysis tools can still often be applied to gain intuition about parts of the model). A prior lets us write down the sorts of models that would allow us to capture structured situations (for instance, when trying to do language modeling or transfer learning). Non-Bayesian methods exist for these situations, but they are often ad hoc and in many cases ends up looking like an approximation to Bayes. One example of this is Kneser-Ney smoothing for n-gram models, an ad hoc algorithm that ended up being very similar to an approximate inference algorithm for the hierarchical Pitman-Yor process [15, 14, 17, 8]. This raises another important point&nbsp;<em>against</em>&nbsp;Bayes, which is that the proper Bayesian interpretation may be very mathematically complex. Pitman-Yor processes are on the cutting-edge of Bayesian nonparametric statistics, which is itself one of the more technical subfields of statistical machine learning, so it was probably much easier to come up with Kneser-Ney smoothing than to find the interpretation in terms of Pitman-Yor processes.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong id=\"2_2__When_the_Justifications_Fail\">2.2. When the Justifications Fail</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">The first and most common objection to Bayes is that a Bayesian method is only as good as its prior. While for simple models the performance of Bayes is relatively independent of the prior, such models can only capture data where frequentist techniques would also perform very well. For more complex (especially nonparametric) Bayesian models, the performance can depend strongly on the prior, and designing good priors is still an open problem. As one example I point to my own research on hierarchical nonparametric models, where the most straightforward attempts to build a hierarchical model lead to severe pathologies [13].</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Even if a Bayesian model does have a good prior, it may be computationally intractable to perform posterior inference. For instance, structure learning in Bayesian networks is NP-hard [4], as is topic inference in the popular latent Dirichlet allocation model (and this continues to hold even if we only want to perform approximate inference). Similar stories probably hold for other common models, although a theoretical survey has yet to be made; suffice to say that in practice approximate inference remains a difficult and unsolved problem, with many models not even considered because of the apparent hopelessness of performing inference in them.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Because frequentist methods often come with an analysis of the specific algorithm being employed, they can sometimes overcome these computational issues. One example of this mentioned already is L1 regularized least squares [3]. The problem setup is that we have a linear regression task Ax = b+v where A and b are known, v is a noise vector, and x is believed to be sparse (typically x has many more rows than b, so without the sparsity assumption x would be underdetermined). Let us suppose that x has n rows and k non-zero rows --- then the number of possible sparsity patterns is <img src=\"http://www.codecogs.com/png.latex?\\binom{n}{k}\" alt=\"\">&nbsp;--- large enough that a brute force consideration of all possible sparsity patterns is intractable. However, we can show that solving a certain semidefinite program will with high probability yield the appropriate sparsity pattern, after which recovering x reduces to a simple least squares problem. (A&nbsp;<em>semidefinite program</em>&nbsp;is a certain type of optimization problem that can be solved efficiently [16].)</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Finally, Bayes has no good way of dealing with adversaries or with cases where the data was generated in a complicated way that could make it highly biased (for instance, as the output of an optimization procedure). A toy example of an adversary would be playing rock-paper-scissors --- how should a Bayesian play such a game? The straightforward answer is to build up a model of the opponent based on their plays so far, and then to make the play that maximizes the expected score (probability of winning minus probability of losing). However, such a strategy fares poorly against any opponent with access to the model being used, as they can then just run the model themselves to predict the Bayesian's plays in advance, thereby winning every single time. In contrast, there is a frequentist strategy called the&nbsp;<strong>multiplicative weights update method</strong>&nbsp;that fairs well against an arbitrary opponent (even one with superior computational resources and access to our agent's source code). The multiplicative weights method does far more than winning at rock-paper-scissors --- it is also a key component of the fastest algorithm for solving many important optimization problems (including the network flow algorithm), and it forms the theoretical basis for the widely used AdaBoost algorithm [1, 5, 7].</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong id=\"2_3__When_To_Use_Each_Method\">2.3. When To Use Each Method</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">The essential difference between Bayesian and frequentist decision theory is that Bayes makes the additional assumption of a prior over \u03b8, and optimizes for average-case performance rather than worst-case performance.&nbsp;<em>It follows, then, that Bayes is the superior method whenever we can obtain a good prior and when good average-case performance is sufficient.</em>&nbsp;However, if we have no way of obtaining a good prior, or when we need guaranteed performance, frequentist methods are the way to go. For instance, if we are trying to build a software package that should be widely deployable, we might want to use a frequentist method because users can be sure that the software will work as long as some number of easily-checkable assumptions are met.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">A nice middle-ground between purely Bayesian and purely frequentist methods is to use a Bayesian model coupled with frequentist model-checking techniques; this gives us the freedom in modeling afforded by a prior but also gives us some degree of confidence that our model is correct. This approach is suggested by both Gelman [9] and Jordan [10].</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong id=\"3__Conclusion\">3. Conclusion</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">When the assumptions of Bayes' Theorem hold, and when Bayesian updating can be performed computationally efficiently, then it is indeed tautological that Bayes is the optimal approach. Even when some of these assumptions fail, Bayes can still be a fruitful approach. However, by working under weaker (sometimes even adversarial) assumptions, frequentist approaches can perform well in very complicated domains even with fairly simple models; this is because, with fewer assumptions being made at the outset, less work has to be done to ensure that those assumptions are met.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">From a research perspective, we should be far from satisfied with either approach --- Bayesian methods make stronger assumptions than may be warranted, and frequentists methods provide little in the way of a coherent framework for constructing models, and ask for worst-case guarantees, which probably cannot be obtained in general. We should seek to develop a statistical modeling framework that, unlike Bayes, can deal with unknown priors, adversaries, and limited computational resources.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong id=\"4__Acknowledgements\">4. Acknowledgements</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">Thanks to Emma Pierson, Vladimir Slepnev, and Wei Dai for reading preliminary versions of this work and providing many helpful comments.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\"><strong>5. References</strong><strong>&nbsp;</strong></p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[1] Sanjeev Arora, Elad Hazan, and Satyen Kale.&nbsp;The multiplicative weights update method: a meta algorithm and applications.&nbsp;<em>Working Paper</em>, 2005.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[2] Christopher J.C. Burges. A tutorial on support vector machines for pattern recognition.&nbsp;<em>Data Mining and Knowledge Discovery</em>, 2:121--167, 1998.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[3] Emmanuel J. Candes. Compressive sampling.&nbsp;In&nbsp;<em>Proceedings of the International Congress of Mathematicians</em>. European Mathematical Society, 2006.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[4] D.M. Chickering. Learning bayesian networks is NP-complete.&nbsp;<em>LECTURE NOTES IN STATISTICS-NEW YORK-SPRINGER VERLAG-</em>, pages 121--130, 1996.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[5] Paul Christiano, Jonathan A. Kelner, Aleksander Madry, Daniel Spielman, and Shang-Hua Teng. Electrical flows, laplacian systems, and faster approximation of maximum flow in undirected graphs.&nbsp;In&nbsp;<em>Proceedings of the 43rd ACM Symposium on Theory of Computing</em>, 2011.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[6] Andrew Critch. Frequentist vs. bayesian breakdown: Interpretation vs. inference.&nbsp;http://lesswrong.com/lw/7ck/frequentist_vs_bayesian_breakdown_interpretation/.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[7] Yoav Freund and Robert E. Schapire. A short introduction to boosting.&nbsp;<em>Journal of Japanese Society for Artificial Intelligence</em>, 14(5):771--780, Sep. 1999.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[8] J. Gasthaus and Y.W. Teh. Improvements to the sequence memoizer.&nbsp;In&nbsp;<em>Advances in Neural Information Processing Systems</em>, 2011.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[9] Andrew Gelman. Induction and deduction in bayesian data analysis.&nbsp;<em>RMM</em>, 2:67--78, 2011.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[10] Michael I. Jordan. Are you a bayesian or a frequentist?&nbsp;Machine Learning Summer School 2009 (video lecture at http://videolectures.net/mlss09uk_jordan_bfway/).</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[11] D. Warner North. A tutorial introduction to decision theory.&nbsp;<em>IEEE Transactions on Systems Science and Cybernetics</em>, SSC-4(3):200--210, Sep. 1968.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[12] Igal Sason. On refined versions of the Azuma-Hoeffding inequality with applications in information theory.&nbsp;<em>CoRR</em>, abs/1111.1977, 2011.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[13] Jacob Steinhardt and Zoubin Ghahramani. Pathological properties of deep bayesian hierarchies.&nbsp;In&nbsp;<em>NIPS Workshop on Bayesian Nonparametrics</em>, 2011.&nbsp;Extended Abstract.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[14] Y.W. Teh. A bayesian interpretation of interpolated Kneser-Ney.&nbsp;Technical Report TRA2/06, School of Computing, NUS, 2006.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[15] Y.W. Teh. A hierarchical bayesian language model based on pitman-yor processes.&nbsp;<em>Coling/ACL</em>, 2006.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[16] Lieven Vandenberghe and Stephen Boyd. Semidefinite programming.&nbsp;<em>SIAM Review</em>, 38(1):49--95, Mar. 1996.</p>\n<p style=\"color: #333333; font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-size: 13px; line-height: 19px;\">[17] F.~Wood, C.~Archambeau, J.~Gasthaus, L.~James, and Y.W. Teh. A stochastic memoizer for sequence data.&nbsp;In&nbsp;<em>Proceedings of the 26th International Conference on Machine Learning</em>, pages 1129--1136, 2009.</p>\n<p>&nbsp;</p>", "sections": [{"title": "1. Background on Bayesians and Frequentists", "anchor": "1__Background_on_Bayesians_and_Frequentists", "level": 1}, {"title": "1.1. Three Levels of Argument", "anchor": "1_1__Three_Levels_of_Argument", "level": 1}, {"title": "1.2. Recap of Bayesian Decision Theory", "anchor": "1_2__Recap_of_Bayesian_Decision_Theory", "level": 1}, {"title": "1.3. Steel-manning Frequentists", "anchor": "1_3__Steel_manning_Frequentists", "level": 1}, {"title": "2. Bayes vs. Other Methods", "anchor": "2__Bayes_vs__Other_Methods", "level": 1}, {"title": "2.1. Justification for Bayes", "anchor": "2_1__Justification_for_Bayes", "level": 1}, {"title": "2.2. When the Justifications Fail", "anchor": "2_2__When_the_Justifications_Fail", "level": 1}, {"title": "2.3. When To Use Each Method", "anchor": "2_3__When_To_Use_Each_Method", "level": 1}, {"title": "3. Conclusion", "anchor": "3__Conclusion", "level": 1}, {"title": "4. Acknowledgements", "anchor": "4__Acknowledgements", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "51 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-31T08:25:29.503Z", "modifiedAt": null, "url": null, "title": "Guessing game -how low can you go?", "slug": "guessing-game-how-low-can-you-go", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:29.631Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sundar", "createdAt": "2011-05-02T04:18:12.542Z", "isAdmin": false, "displayName": "sundar"}, "userId": "pSmMKn5pZhmj6bDSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6qZy3RXA6AGhrZ8ob/guessing-game-how-low-can-you-go", "pageUrlRelative": "/posts/6qZy3RXA6AGhrZ8ob/guessing-game-how-low-can-you-go", "linkUrl": "https://www.lesswrong.com/posts/6qZy3RXA6AGhrZ8ob/guessing-game-how-low-can-you-go", "postedAtFormatted": "Wednesday, October 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Guessing%20game%20-how%20low%20can%20you%20go%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGuessing%20game%20-how%20low%20can%20you%20go%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6qZy3RXA6AGhrZ8ob%2Fguessing-game-how-low-can-you-go%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Guessing%20game%20-how%20low%20can%20you%20go%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6qZy3RXA6AGhrZ8ob%2Fguessing-game-how-low-can-you-go", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6qZy3RXA6AGhrZ8ob%2Fguessing-game-how-low-can-you-go", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 32, "htmlBody": "<p>A game similar to Guess 2/3 of the average,</p>\n<p><strong style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.499999046325684px;\">Choose a number below 1000. </strong></p>\n<p><strong style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.499999046325684px;\">Unique number closest to it wins.&nbsp;</strong><strong style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.499999046325684px;\">(People with same answer are eliminated)</strong></p>\n<p>What is your pick and the reason for your choice?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6qZy3RXA6AGhrZ8ob", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -11, "extendedScore": null, "score": 1.022198944789572e-06, "legacy": true, "legacyId": "19726", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-31T15:49:43.656Z", "modifiedAt": null, "url": null, "title": "[Link] Which results from cognitive psychology are robust & real?", "slug": "link-which-results-from-cognitive-psychology-are-robust-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.939Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KfhkjPGkf4fjaaZui/link-which-results-from-cognitive-psychology-are-robust-and", "pageUrlRelative": "/posts/KfhkjPGkf4fjaaZui/link-which-results-from-cognitive-psychology-are-robust-and", "linkUrl": "https://www.lesswrong.com/posts/KfhkjPGkf4fjaaZui/link-which-results-from-cognitive-psychology-are-robust-and", "postedAtFormatted": "Wednesday, October 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Which%20results%20from%20cognitive%20psychology%20are%20robust%20%26%20real%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Which%20results%20from%20cognitive%20psychology%20are%20robust%20%26%20real%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKfhkjPGkf4fjaaZui%2Flink-which-results-from-cognitive-psychology-are-robust-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Which%20results%20from%20cognitive%20psychology%20are%20robust%20%26%20real%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKfhkjPGkf4fjaaZui%2Flink-which-results-from-cognitive-psychology-are-robust-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKfhkjPGkf4fjaaZui%2Flink-which-results-from-cognitive-psychology-are-robust-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 505, "htmlBody": "<p><a href=\"http://blogs.discovermagazine.com/gnxp/2012/10/which-results-from-cognitive-psychology-are-robust-real/\">Link.</a></p>\n<blockquote>\n<p>A paper on the psychology of religious belief, <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/acp.2874/abstract;jsessionid=A86958DB79B551F9D68CE59206597137.d01t03\">Paranormal and Religious Believers Are More Prone to Illusory Face Perception than Skeptics and Non-believers</a>, came onto my radar recently. I used to talk a lot about the <strong>theory</strong> of religious cognitive psychology years ago, but the interest kind of faded when it seemed that empirical results were relatively thin in relation to the system building (<a href=\"http://www2.psych.ubc.ca/%7Eara/research.htm\">Ara Norenzayan&rsquo;s</a> work being an exception to this generality). The theory is rather straightforward: <strong>religious <em>belief</em> is a naturally evoked consequence of the general architecture of our minds.</strong> For example, gods are simply extensions of persons, and make natural sense in light of our tendency to anthromorphize the world around us (this may have had evolutionary benefit, in that false positives for detection of other agents was far less costly than false negatives; think an ambush by a rival clan).*</p>\n<p>&nbsp;</p>\n<p>But enough theory. <strong>Are religious people cognitively different from those who are atheists?</strong>&nbsp;I suspect so. I speak as someone who never ever really believed in God, despite being inculcated in religious ideas from childhood. By the time I was seven years of age I realized that I was an atheist, and that my prior &ldquo;beliefs&rdquo; about God were basically analogous to Spinozan Deism. I had simply never believed in a personal God, but for many of earliest years it was less a matter of disbelief, than that did not even comprehend or cogently in my mind elaborate the idea of this entity, which others took for granted as self-evidently obvious. From talking to many other atheists I have come to the conclusion that <a href=\"http://blogs.discovermagazine.com/gnxp/2011/09/atheism-as-mental-deviance/\">Atheism is a mental deviance</a>. This does not mean that mental peculiarities are necessary or sufficient for atheism, but they increase the odds.</p>\n<p>And yet after reading the above paper my confidence in that theory is reduced. The authors used ~50 individuals, and attempted to correct demographic confounds. Additionally, the results were statistically significant. But to me the above theory should make powerful predictions in terms of effect size. The differences between non-believers, the religious, and those who accepted the paranormal, were just not striking enough for me.</p>\n<p>Because of theoretical commitments my prejudiced impulse was to accept these findings. But looking deeply within they just aren&rsquo;t persuasive in light of my prior expectations. This a fundamental problem in much of social science. Statistical significance is powerful when you have a preference for the hypothesis forwarded. In contrast, the knives of skepticism come out when research is published which goes against your preconceptions.</p>\n<p>So a question for psychologists: <strong>which results are robust and real, to the point where you would be willing to make a serious monetary bet on it being the orthodoxy in 10 years</strong>? My primary interest is cognitive psychology, but I am curious about other fields too.</p>\n<p>* <a href=\"http://www.amazon.com/exec/obidos/ASIN/0195149300/geneexpressio-20\">In Gods We Trust</a> and <a href=\"http://www.amazon.com/exec/obidos/ASIN/0465006965/geneexpressio-20\">Religion Explained</a> are good introductions to this area of research.</p>\n</blockquote>\n<p>Considering the communities heavy reliance on such results I think we should answer the question as well.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KfhkjPGkf4fjaaZui", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "19729", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-31T16:24:41.431Z", "modifiedAt": null, "url": null, "title": "Meetup : 18/11  London Meetup", "slug": "meetup-18-11-london-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:31.526Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tenoke", "createdAt": "2012-04-10T21:37:29.739Z", "isAdmin": false, "displayName": "Tenoke"}, "userId": "CRSZPEg9dHyMspTzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MDqYgF7n2QAiXTmux/meetup-18-11-london-meetup", "pageUrlRelative": "/posts/MDqYgF7n2QAiXTmux/meetup-18-11-london-meetup", "linkUrl": "https://www.lesswrong.com/posts/MDqYgF7n2QAiXTmux/meetup-18-11-london-meetup", "postedAtFormatted": "Wednesday, October 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%2018%2F11%20%20London%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%2018%2F11%20%20London%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMDqYgF7n2QAiXTmux%2Fmeetup-18-11-london-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%2018%2F11%20%20London%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMDqYgF7n2QAiXTmux%2Fmeetup-18-11-london-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMDqYgF7n2QAiXTmux%2Fmeetup-18-11-london-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fc'>18/11  London Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 November 2012 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A meetup at 2 pm on the 18th of November at the Shakespeares's head pub by holborn tube station. Everyone is welcome to attend.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fc'>18/11  London Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MDqYgF7n2QAiXTmux", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.022460697884815e-06, "legacy": true, "legacyId": "19730", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___18_11__London_Meetup\">Discussion article for the meetup : <a href=\"/meetups/fc\">18/11  London Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 November 2012 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A meetup at 2 pm on the 18th of November at the Shakespeares's head pub by holborn tube station. Everyone is welcome to attend.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___18_11__London_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/fc\">18/11  London Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : 18/11  London Meetup", "anchor": "Discussion_article_for_the_meetup___18_11__London_Meetup", "level": 1}, {"title": "Discussion article for the meetup : 18/11  London Meetup", "anchor": "Discussion_article_for_the_meetup___18_11__London_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-31T16:36:49.726Z", "modifiedAt": null, "url": null, "title": "[Optimal Philanthropy] Laptops without instructions", "slug": "optimal-philanthropy-laptops-without-instructions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:37.178Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gJTWuSqBwREGi7fba/optimal-philanthropy-laptops-without-instructions", "pageUrlRelative": "/posts/gJTWuSqBwREGi7fba/optimal-philanthropy-laptops-without-instructions", "linkUrl": "https://www.lesswrong.com/posts/gJTWuSqBwREGi7fba/optimal-philanthropy-laptops-without-instructions", "postedAtFormatted": "Wednesday, October 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BOptimal%20Philanthropy%5D%20Laptops%20without%20instructions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BOptimal%20Philanthropy%5D%20Laptops%20without%20instructions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgJTWuSqBwREGi7fba%2Foptimal-philanthropy-laptops-without-instructions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BOptimal%20Philanthropy%5D%20Laptops%20without%20instructions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgJTWuSqBwREGi7fba%2Foptimal-philanthropy-laptops-without-instructions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgJTWuSqBwREGi7fba%2Foptimal-philanthropy-laptops-without-instructions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 533, "htmlBody": "<p>Just read this article, which describes a splashy, interesting narrative which jives nicely with my worldview. Which makes me suspicious.</p>\n<p>http://dvice.com/archives/2012/10/ethiopian-kids.php</p>\n<blockquote>\n<p><a name=\"more\"></a></p>\n<p>The One Laptop Per Child project started as a way of delivering  technology and resources to schools in countries with little or no  education infrastructure, using inexpensive computers to improve  traditional curricula. What the OLPC Project has realized over the last  five or six years, though, is that teaching kids <em>stuff</em> is  really not that valuable. Yes, knowing all your state capitols how to  spell \"neighborhood\" properly and whatnot isn't a bad thing, but  memorizing facts and procedures isn't going to inspire kids to go out  and learn by teaching themselves, which is the key to a good education.  Instead, OLPC is trying to figure out a way to teach kids to <em>learn</em>, which is what this experiment is all about.</p>\n<p>Rather than give out laptops (they're actually Motorola Zoom tablets  plus solar chargers running custom software) to kids in schools with  teachers, the OLPC Project decided to try something completely  different: it delivered some boxes of tablets to two villages in  Ethiopia, taped shut, with no instructions whatsoever. Just like, \"hey  kids, here's this box, you can open it if you want, see ya!\"</p>\n<p>Just to give you a sense of what these villages in Ethiopia are like, the kids (and most of the adults) there <strong>have never seen a word</strong>.  No books, no newspapers, no street signs, no labels on packaged foods  or goods. Nothing. And these villages aren't unique in that respect;  there are many of them in Africa where the literacy rate is close to  zero. So you might think that if you're going to give out fancy tablet  computers, it would be helpful to have someone along to show these  people how to use them, right?</p>\n<p>But that's not what OLPC did. They just left the boxes there, sealed  up, containing one tablet for every kid in each of the villages (nearly a  thousand tablets in total), pre-loaded with a custom English-language  operating system and SD cards with tracking software on them to record  how the tablets were used. Here's how it went down, as related by OLPC  founder Nicholas Negroponte at MIT Technology Review's EmTech conference  last week:</p>\n<p><em></em></p>\n</blockquote>\n<blockquote><em>\"We left the boxes in the village.  Closed. Taped shut. No instruction, no human being. I thought, the kids  will play with the boxes! Within four minutes, one kid not only opened  the box, but found the on/off switch. He'd never <em>seen</em> an on/off  switch. He powered it up. Within five days, they were using 47 apps per  child per day. Within two weeks, they were singing ABC songs [in  English] in the village. And within five months, they had hacked  Android. Some idiot in our organization or in the Media Lab had disabled  the camera! And they figured out it had a camera, and they hacked  Android.</em></blockquote>\n<p>So this sounds really inspiring and stuff, even subtracting some  obviously sensational stuff (I assume \"hacked Android\" means \"opened up  the preferences dialog and flicked a switch\"). I've poked around a bit and found similarly fluffy pop-philanthropy articles. Anyone know if there's more reliable information about this out there?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gJTWuSqBwREGi7fba", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "19731", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-31T18:13:57.163Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham HPMoR discussion, ch 12-14", "slug": "meetup-durham-hpmor-discussion-ch-12-14", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Hribo33Y3K2LTW8u9/meetup-durham-hpmor-discussion-ch-12-14", "pageUrlRelative": "/posts/Hribo33Y3K2LTW8u9/meetup-durham-hpmor-discussion-ch-12-14", "linkUrl": "https://www.lesswrong.com/posts/Hribo33Y3K2LTW8u9/meetup-durham-hpmor-discussion-ch-12-14", "postedAtFormatted": "Wednesday, October 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20HPMoR%20discussion%2C%20ch%2012-14&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20HPMoR%20discussion%2C%20ch%2012-14%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHribo33Y3K2LTW8u9%2Fmeetup-durham-hpmor-discussion-ch-12-14%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20HPMoR%20discussion%2C%20ch%2012-14%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHribo33Y3K2LTW8u9%2Fmeetup-durham-hpmor-discussion-ch-12-14", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHribo33Y3K2LTW8u9%2Fmeetup-durham-hpmor-discussion-ch-12-14", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fd'>Durham HPMoR discussion, ch 12-14</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 November 2012 11:00:00AM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Guglhupf: 2706 Durham-Chapel Hill Boulevard, Durham NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be discussing Harry Potter and the Methods of Rationality, chapters 12-14.</p>\n\n<p>We encourage you to do some or all of the reading, but will be summarizing the chapters as we go for the benefit of those who haven't done the reading or haven't done it recently.</p>\n\n<p>Please feel free to come even if you haven't read any of the chapters!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fd'>Durham HPMoR discussion, ch 12-14</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Hribo33Y3K2LTW8u9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0225203968534736e-06, "legacy": true, "legacyId": "19732", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_discussion__ch_12_14\">Discussion article for the meetup : <a href=\"/meetups/fd\">Durham HPMoR discussion, ch 12-14</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 November 2012 11:00:00AM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Guglhupf: 2706 Durham-Chapel Hill Boulevard, Durham NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be discussing Harry Potter and the Methods of Rationality, chapters 12-14.</p>\n\n<p>We encourage you to do some or all of the reading, but will be summarizing the chapters as we go for the benefit of those who haven't done the reading or haven't done it recently.</p>\n\n<p>Please feel free to come even if you haven't read any of the chapters!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_discussion__ch_12_141\">Discussion article for the meetup : <a href=\"/meetups/fd\">Durham HPMoR discussion, ch 12-14</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham HPMoR discussion, ch 12-14", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_discussion__ch_12_14", "level": 1}, {"title": "Discussion article for the meetup : Durham HPMoR discussion, ch 12-14", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_discussion__ch_12_141", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-31T18:19:46.577Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham LW: Technical explanation, meta", "slug": "meetup-durham-lw-technical-explanation-meta", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EJg8GRErvAndx78KA/meetup-durham-lw-technical-explanation-meta", "pageUrlRelative": "/posts/EJg8GRErvAndx78KA/meetup-durham-lw-technical-explanation-meta", "linkUrl": "https://www.lesswrong.com/posts/EJg8GRErvAndx78KA/meetup-durham-lw-technical-explanation-meta", "postedAtFormatted": "Wednesday, October 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20LW%3A%20Technical%20explanation%2C%20meta&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20LW%3A%20Technical%20explanation%2C%20meta%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEJg8GRErvAndx78KA%2Fmeetup-durham-lw-technical-explanation-meta%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20LW%3A%20Technical%20explanation%2C%20meta%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEJg8GRErvAndx78KA%2Fmeetup-durham-lw-technical-explanation-meta", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEJg8GRErvAndx78KA%2Fmeetup-durham-lw-technical-explanation-meta", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fe'>Durham LW: Technical explanation, meta</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 November 2012 08:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Francesca's, 706 9th Street, Durham, NC 27705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet to Discuss Technical Explanation and meta topics.</p>\n\n<p>Optional reading: <a href=\"http://yudkowsky.net/rational/technical\" rel=\"nofollow\">http://yudkowsky.net/rational/technical</a> (Evan will summarize for those that haven't read it)</p>\n\n<p>Meta topics will be things like how to keep meetups interesting, what to cover in future meetups, organizational roles, etc. Suggested reading: <a href=\"http://lesswrong.com/lw/crs/how_to_run_a_successful_less_wrong_meetup/\" rel=\"nofollow\">http://lesswrong.com/lw/crs/how_to_run_a_successful_less_wrong_meetup/</a></p>\n\n<p>If you're only going to read one of these, I suggest reading the meetup guide. If you don't want to bother reading all 33 pages, David suggests the following sections:</p>\n\n<p>How To Build Your Team of Heroes\nLong-term Meetup Group Maintenance\nMeetup Content: Discussions and Presentations</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fe'>Durham LW: Technical explanation, meta</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EJg8GRErvAndx78KA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0225235788959255e-06, "legacy": true, "legacyId": "19733", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_LW__Technical_explanation__meta\">Discussion article for the meetup : <a href=\"/meetups/fe\">Durham LW: Technical explanation, meta</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 November 2012 08:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Francesca's, 706 9th Street, Durham, NC 27705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet to Discuss Technical Explanation and meta topics.</p>\n\n<p>Optional reading: <a href=\"http://yudkowsky.net/rational/technical\" rel=\"nofollow\">http://yudkowsky.net/rational/technical</a> (Evan will summarize for those that haven't read it)</p>\n\n<p>Meta topics will be things like how to keep meetups interesting, what to cover in future meetups, organizational roles, etc. Suggested reading: <a href=\"http://lesswrong.com/lw/crs/how_to_run_a_successful_less_wrong_meetup/\" rel=\"nofollow\">http://lesswrong.com/lw/crs/how_to_run_a_successful_less_wrong_meetup/</a></p>\n\n<p>If you're only going to read one of these, I suggest reading the meetup guide. If you don't want to bother reading all 33 pages, David suggests the following sections:</p>\n\n<p>How To Build Your Team of Heroes\nLong-term Meetup Group Maintenance\nMeetup Content: Discussions and Presentations</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_LW__Technical_explanation__meta1\">Discussion article for the meetup : <a href=\"/meetups/fe\">Durham LW: Technical explanation, meta</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham LW: Technical explanation, meta", "anchor": "Discussion_article_for_the_meetup___Durham_LW__Technical_explanation__meta", "level": 1}, {"title": "Discussion article for the meetup : Durham LW: Technical explanation, meta", "anchor": "Discussion_article_for_the_meetup___Durham_LW__Technical_explanation__meta1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qMuAazqwJvkvo8teR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-31T19:25:07.964Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Social Meetup", "slug": "meetup-washington-dc-social-meetup-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:31.221Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oGqYtPhuzPp2Fk2z9/meetup-washington-dc-social-meetup-0", "pageUrlRelative": "/posts/oGqYtPhuzPp2Fk2z9/meetup-washington-dc-social-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/oGqYtPhuzPp2Fk2z9/meetup-washington-dc-social-meetup-0", "postedAtFormatted": "Wednesday, October 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoGqYtPhuzPp2Fk2z9%2Fmeetup-washington-dc-social-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoGqYtPhuzPp2Fk2z9%2Fmeetup-washington-dc-social-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoGqYtPhuzPp2Fk2z9%2Fmeetup-washington-dc-social-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 75, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/ff\">Washington DC Social Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">04 November 2012 03:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">National Portrait Gallery Plaza, Washington, DC 20001, USA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Not too many people showed up last meetup (shockingly, hurricanes reduce attendance), so we didn't discuses what to do this meetup. Given that, I think a sensible default is games; I'll bring Zendo, if others bring other things than we'll have more to do.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/ff\">Washington DC Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oGqYtPhuzPp2Fk2z9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.022559291662306e-06, "legacy": true, "legacyId": "19734", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/ff\">Washington DC Social Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">04 November 2012 03:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">National Portrait Gallery Plaza, Washington, DC 20001, USA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Not too many people showed up last meetup (shockingly, hurricanes reduce attendance), so we didn't discuses what to do this meetup. Given that, I think a sensible default is games; I'll bring Zendo, if others bring other things than we'll have more to do.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/ff\">Washington DC Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Social Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Social Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-31T19:45:22.255Z", "modifiedAt": null, "url": null, "title": "Is insider information worth looking for? [LINK]", "slug": "is-insider-information-worth-looking-for-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:29.576Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/363WF7qW4FW2Phuyv/is-insider-information-worth-looking-for-link", "pageUrlRelative": "/posts/363WF7qW4FW2Phuyv/is-insider-information-worth-looking-for-link", "linkUrl": "https://www.lesswrong.com/posts/363WF7qW4FW2Phuyv/is-insider-information-worth-looking-for-link", "postedAtFormatted": "Wednesday, October 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20insider%20information%20worth%20looking%20for%3F%20%5BLINK%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20insider%20information%20worth%20looking%20for%3F%20%5BLINK%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F363WF7qW4FW2Phuyv%2Fis-insider-information-worth-looking-for-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20insider%20information%20worth%20looking%20for%3F%20%5BLINK%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F363WF7qW4FW2Phuyv%2Fis-insider-information-worth-looking-for-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F363WF7qW4FW2Phuyv%2Fis-insider-information-worth-looking-for-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 23, "htmlBody": "<p><a href=\"http://crookedtimber.org/2012/10/30/insider-knowledge/#comments\">A claim</a> that the valuable information just about entirely found by intelligently filtering public sources, in particular about the field of international relations.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "363WF7qW4FW2Phuyv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.022570350851443e-06, "legacy": true, "legacyId": "19735", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-31T20:29:26.156Z", "modifiedAt": null, "url": null, "title": "[LINK] Rhonda Cornum: How to Create a Strong Army and Society", "slug": "link-rhonda-cornum-how-to-create-a-strong-army-and-society", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:34.820Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "loup-vaillant", "createdAt": "2011-03-23T10:39:25.887Z", "isAdmin": false, "displayName": "loup-vaillant"}, "userId": "wdoZti3BcPbJXsZ66", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5Nr5D68h8AkqdPcoQ/link-rhonda-cornum-how-to-create-a-strong-army-and-society", "pageUrlRelative": "/posts/5Nr5D68h8AkqdPcoQ/link-rhonda-cornum-how-to-create-a-strong-army-and-society", "linkUrl": "https://www.lesswrong.com/posts/5Nr5D68h8AkqdPcoQ/link-rhonda-cornum-how-to-create-a-strong-army-and-society", "postedAtFormatted": "Wednesday, October 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Rhonda%20Cornum%3A%20How%20to%20Create%20a%20Strong%20Army%20and%20Society&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Rhonda%20Cornum%3A%20How%20to%20Create%20a%20Strong%20Army%20and%20Society%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5Nr5D68h8AkqdPcoQ%2Flink-rhonda-cornum-how-to-create-a-strong-army-and-society%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Rhonda%20Cornum%3A%20How%20to%20Create%20a%20Strong%20Army%20and%20Society%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5Nr5D68h8AkqdPcoQ%2Flink-rhonda-cornum-how-to-create-a-strong-army-and-society", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5Nr5D68h8AkqdPcoQ%2Flink-rhonda-cornum-how-to-create-a-strong-army-and-society", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<p>On <a href=\"http://fora.tv/2012/10/16/Rhonda_Cornum_How_to_Create_a_Strong_Army_and_Society\">fora.tv</a>.</p>\n<p>This discussion is about methods to overcome adverse events and challenges so that (1) you don't panic when they hit you, and (2) you're not traumatized afterwards.</p>\n<p>There's little explanation about how to do it, but it was enough for me to pattern-match it with \"rationality sub-skill\". It looks like a training for <em>not</em> losing your rationalist powers when you most need them. Or at least remember to \"stick them with the pointy end\".</p>\n<p>That said, maybe CFAR already teaches some of that?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5Nr5D68h8AkqdPcoQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 6, "extendedScore": null, "score": 1.0225944310358864e-06, "legacy": true, "legacyId": "19736", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-10-31T23:36:28.873Z", "modifiedAt": null, "url": null, "title": "My experience with dieting and exercise", "slug": "my-experience-with-dieting-and-exercise", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:28.905Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Apprentice", "createdAt": "2009-05-20T13:06:35.976Z", "isAdmin": false, "displayName": "Apprentice"}, "userId": "X5f7X9PkRFwGyxWsb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zQrFZGE2ZNv78NMwr/my-experience-with-dieting-and-exercise", "pageUrlRelative": "/posts/zQrFZGE2ZNv78NMwr/my-experience-with-dieting-and-exercise", "linkUrl": "https://www.lesswrong.com/posts/zQrFZGE2ZNv78NMwr/my-experience-with-dieting-and-exercise", "postedAtFormatted": "Wednesday, October 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20experience%20with%20dieting%20and%20exercise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20experience%20with%20dieting%20and%20exercise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzQrFZGE2ZNv78NMwr%2Fmy-experience-with-dieting-and-exercise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20experience%20with%20dieting%20and%20exercise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzQrFZGE2ZNv78NMwr%2Fmy-experience-with-dieting-and-exercise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzQrFZGE2ZNv78NMwr%2Fmy-experience-with-dieting-and-exercise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 875, "htmlBody": "<p>Tomorrow I will start on a diet plan to bring my weight down. I'm aiming for a reduction by 5 kg in 6 weeks. Many plans like this fail but I am confident (let's say 80% confident) that this one will succeed. Why? Because I've done this several times before and succeeded. I seem to be reasonably good at dieting.<br /><br />The obvious problem is that I start regaining weight once a diet is over, necessitating another diet down the line. Now, some people say that cycles of weight-gain and dieting are a horrible thing where each gaining period brings the weight higher than before and each dieting period is harder than before and makes you lose muscle and screw up your body. I haven't personally experienced this. I am 183 cm tall and during my adulthood, my body weight has fluctuated between 80 and 100 kg. I don't have extensive records but I was about 85-88 kg at age 18. At age 21 I was about 83 kg. On my wedding day (age 24) I was 100 kg but that was exceptional and fairly quickly went down again. A couple of years ago I was 80 kg. Now (age 32) I am 90 kg. You get the idea.<br /><br />After a successful diet I think to myself: \"Well, if I can lose weight with amount X of willpower and organizational skills I ought to be able to maintain my weight with amount Y where Y &lt;&lt; X. Since I am capable of exerting X for weeks at a time, I should be capable of exerting Y indefinitely.\" Clearly, this fails to work for me and I think the reason is that Y &lt;&lt; X just isn't true. The organizational effort to eat a maintenance diet is not substantially less than the effort to eat a weight-loss diet. And the willpower side of thing doesn't look too good either. It's a bit easier to resist tasty treats when I'm not really hungry - but it's not that much easier. Also, losing weight is a very motivating goal and I get to see positive results as things progress. Maintaining weight is a boring goal which I find it hard to get worked up about.<br /><br />Now, upon reaching this conclusion I've thought things like: \"Maybe effort X is necessary to maintain my weight. But I can't spend effort X on this stuff all the time! I'm not a health nut, I can't obsess over my body year in year out.\" Well, now I've decided that maybe I can bite this bullet and make it work for me. I'm going to try to keep the effort high on an ongoing basis. But maintaining weight is still a super-boring goal so I need to spice things up a bit. That's where exercise comes in.<br /><br />My most efficient diets haven't incorporated any exercise. Doing workouts throws additional variables into the mix and from a simple weight-loss perspective the additional management effort doesn't seem worth it. But I do actually like exercise and the good thing about it is that it can provide me with fun and motivating body goals even when I'm not trying to lose weight. So, I think spending effort X on my body is going to be more sustainable in the long term if X involves a lot of exercise.<br /><br />I started doing regular exercise in September, focusing on increasing my strength. I'm happy with the results so far. I'm measurably stronger than when I started and look slightly more muscular. Tomorrow I'll shift emphasis to weight loss while trying to maintain my modest strength gains. After the weight loss phase I'll work on something else - haven't decided what yet.<br /><br />I'm all for self-improvement and optimization and I'd like my exercises to be effective. To that end, I've been trying to do some reading. Lots of people like Starting Strength and I did pick up some useful things from that book. But it happens to be the case that barbell exercises are not an option for me. My constraints are as follows: My wife and I have an 8 month old child and a 3 year old autistic child. They need a lot of attention. If one of the adults is away from home in the evening it puts a lot of strain on the other one. This makes a regular gym habit impractical for me. Going out for a run isn't much better.<br /><br />So I do my workouts at home and it turns out that this works just fine. There are plenty of bodyweight exercises to do and I've got a couple of adjustable dumbbells to spice things up a bit. Books that have helped me include You Are Your Own Gym by Mark Lauren, Never Gymless by Ross Enamait and The Naked Warrior by Pavel Tsatsouline.<br /><br />See also <a href=\"/lw/d7h/minimum_viable_workout_routine/\">Minimum viable workout routine</a> which is a strength-training program that means to minimize time and mental effort. What I mostly need to minimize is time away from home. Perhaps people differ substantially in the obstacles they need to remove to achieve their fitness goals.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zQrFZGE2ZNv78NMwr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -7, "extendedScore": null, "score": -5e-06, "legacy": true, "legacyId": "19737", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xAiMk4e7neP6Ah7FG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-01T04:12:51.457Z", "modifiedAt": null, "url": null, "title": "[POLL] AI-FOOM Debate in Sequence Reruns?", "slug": "poll-ai-foom-debate-in-sequence-reruns", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:30.610Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WAj4A7b3dj2ugDTfg/poll-ai-foom-debate-in-sequence-reruns", "pageUrlRelative": "/posts/WAj4A7b3dj2ugDTfg/poll-ai-foom-debate-in-sequence-reruns", "linkUrl": "https://www.lesswrong.com/posts/WAj4A7b3dj2ugDTfg/poll-ai-foom-debate-in-sequence-reruns", "postedAtFormatted": "Thursday, November 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BPOLL%5D%20AI-FOOM%20Debate%20in%20Sequence%20Reruns%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BPOLL%5D%20AI-FOOM%20Debate%20in%20Sequence%20Reruns%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWAj4A7b3dj2ugDTfg%2Fpoll-ai-foom-debate-in-sequence-reruns%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BPOLL%5D%20AI-FOOM%20Debate%20in%20Sequence%20Reruns%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWAj4A7b3dj2ugDTfg%2Fpoll-ai-foom-debate-in-sequence-reruns", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWAj4A7b3dj2ugDTfg%2Fpoll-ai-foom-debate-in-sequence-reruns", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 198, "htmlBody": "<p>We're now at the point in the sequences when the <a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">AI-FOOM Debate</a>&nbsp;took place between Eliezer Yudkowsky and Robin Hanson. Do people want me to include them in the sequence rerun posts, and if so, how? Should I make one post a day? Should I post all of the posts that were made in one day back in 2008, so that we would possibly get one Yudkowsky and one Hanson post in a single day of reruns? If I'm rerunning two posts a day, should I make one rerun post or two? When rerunning a Hanson post, how should the <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">standard rerun template</a> be adjusted?&nbsp;</p>\n<p>I titled this as a poll, but that's not really what I want to do, since I'm not sure I have come up with all of the relevant options. I've got my own thoughts on all of the questions I just asked, but I'm going to hold off on mentioning them, in the interests of sparking discussion. Whatever I do, I will need to decide fairly quickly (as in, next three days at the latest).</p>\n<p>I'm sorry that this is short term; I've been busy and wasn't really looking that far ahead.</p>\n<p>How should I do this?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WAj4A7b3dj2ugDTfg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 18, "extendedScore": null, "score": 1.0228477374371135e-06, "legacy": true, "legacyId": "19748", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-01T04:18:52.272Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Weak Inside View", "slug": "seq-rerun-the-weak-inside-view", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RySGY6CFzQG56PC5s/seq-rerun-the-weak-inside-view", "pageUrlRelative": "/posts/RySGY6CFzQG56PC5s/seq-rerun-the-weak-inside-view", "linkUrl": "https://www.lesswrong.com/posts/RySGY6CFzQG56PC5s/seq-rerun-the-weak-inside-view", "postedAtFormatted": "Thursday, November 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Weak%20Inside%20View&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Weak%20Inside%20View%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRySGY6CFzQG56PC5s%2Fseq-rerun-the-weak-inside-view%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Weak%20Inside%20View%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRySGY6CFzQG56PC5s%2Fseq-rerun-the-weak-inside-view", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRySGY6CFzQG56PC5s%2Fseq-rerun-the-weak-inside-view", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 187, "htmlBody": "<p>Today's post, <a href=\"/lw/vz/the_weak_inside_view/\">The Weak Inside View</a> was originally published on 18 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_Weak_Inside_View\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>On cases where the causal factors creating a circumstance are changing, the outside view may be misleading. In that case, the best you can do may just be to take the inside view, but not try to assign predictions that were too precise.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/f7q/seq_rerun_failure_by_affective_analogy/\">Failure By Affective Analogy</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RySGY6CFzQG56PC5s", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.022851025195985e-06, "legacy": true, "legacyId": "19749", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["w9KWNWFTXivjJ7rjF", "59GR5czLoZFdoqpAN", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-01T13:45:42.687Z", "modifiedAt": null, "url": null, "title": "[LINK] Temporal Binding", "slug": "link-temporal-binding", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:32.462Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "twanvl", "createdAt": "2009-07-23T14:08:02.633Z", "isAdmin": false, "displayName": "twanvl"}, "userId": "tSciHLMSqaRcuX4ym", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2YmQPmKBPyjBZTSfs/link-temporal-binding", "pageUrlRelative": "/posts/2YmQPmKBPyjBZTSfs/link-temporal-binding", "linkUrl": "https://www.lesswrong.com/posts/2YmQPmKBPyjBZTSfs/link-temporal-binding", "postedAtFormatted": "Thursday, November 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Temporal%20Binding&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Temporal%20Binding%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2YmQPmKBPyjBZTSfs%2Flink-temporal-binding%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Temporal%20Binding%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2YmQPmKBPyjBZTSfs%2Flink-temporal-binding", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2YmQPmKBPyjBZTSfs%2Flink-temporal-binding", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<p>I just read an <a href=\"http://theness.com/neurologicablog/index.php/temporal-binding/\">article on Steven Novella's NeurologicaBlog</a> on temporal binding, a cognitive bias I hadn't seen before:</p>\n<blockquote>\n<p>Temporal binding is a phenomenon that reinforces that assumption of  cause and effect once we have linked two events causally in our minds.  The effect biases our memory so that we remember the apparent cause and  effect occurring closer together in time. In experiments we tend to  remember the cause as happening later and the effect happening earlier.</p>\n</blockquote>\n<p>Temporal binding is like the reverse of \"post hoc ergo propter hoc\", and you could perhaps perhaps also call it \"propter hoc ergo post hoc\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2YmQPmKBPyjBZTSfs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.0231610131570638e-06, "legacy": true, "legacyId": "19753", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-01T19:47:50.632Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge UK Weekly Meetup", "slug": "meetup-cambridge-uk-weekly-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sohum", "createdAt": "2012-02-06T06:28:59.691Z", "isAdmin": false, "displayName": "Sohum"}, "userId": "DP4jNdSvbeAofhxkb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ogCyujwRLLfzvKW6A/meetup-cambridge-uk-weekly-meetup", "pageUrlRelative": "/posts/ogCyujwRLLfzvKW6A/meetup-cambridge-uk-weekly-meetup", "linkUrl": "https://www.lesswrong.com/posts/ogCyujwRLLfzvKW6A/meetup-cambridge-uk-weekly-meetup", "postedAtFormatted": "Thursday, November 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%20UK%20Weekly%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%20UK%20Weekly%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FogCyujwRLLfzvKW6A%2Fmeetup-cambridge-uk-weekly-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%20UK%20Weekly%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FogCyujwRLLfzvKW6A%2Fmeetup-cambridge-uk-weekly-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FogCyujwRLLfzvKW6A%2Fmeetup-cambridge-uk-weekly-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fg'>Cambridge UK Weekly Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 November 2012 07:47:31PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">JCR, Trinity College, Cambridge, CB2 1TQ, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>See: <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK</a>\n(It is 11 am, by the local time)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fg'>Cambridge UK Weekly Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ogCyujwRLLfzvKW6A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.023359139570417e-06, "legacy": true, "legacyId": "19756", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge_UK_Weekly_Meetup\">Discussion article for the meetup : <a href=\"/meetups/fg\">Cambridge UK Weekly Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 November 2012 07:47:31PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">JCR, Trinity College, Cambridge, CB2 1TQ, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>See: <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK</a>\n(It is 11 am, by the local time)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge_UK_Weekly_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/fg\">Cambridge UK Weekly Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge UK Weekly Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge_UK_Weekly_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge UK Weekly Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge_UK_Weekly_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-01T23:57:07.894Z", "modifiedAt": null, "url": null, "title": "Meetup : Champaign, IL meetup", "slug": "meetup-champaign-il-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:30.691Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aspera", "createdAt": "2012-03-20T21:28:32.004Z", "isAdmin": false, "displayName": "aspera"}, "userId": "W58nKSarCzqXj8TPS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i2MBoZbnd26Lvnc3W/meetup-champaign-il-meetup", "pageUrlRelative": "/posts/i2MBoZbnd26Lvnc3W/meetup-champaign-il-meetup", "linkUrl": "https://www.lesswrong.com/posts/i2MBoZbnd26Lvnc3W/meetup-champaign-il-meetup", "postedAtFormatted": "Thursday, November 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Champaign%2C%20IL%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Champaign%2C%20IL%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi2MBoZbnd26Lvnc3W%2Fmeetup-champaign-il-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Champaign%2C%20IL%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi2MBoZbnd26Lvnc3W%2Fmeetup-champaign-il-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi2MBoZbnd26Lvnc3W%2Fmeetup-champaign-il-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fh'>Champaign, IL meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 November 2012 09:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Paradiso Cafe, Nevada Street, Champaign, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi all, \nWe're meeting at Paradiso Cafe at 8pm.  I'll try to have a little sign. We'd like to talk about\n1) How to get the word out and expand the group\n2) What kind of structure we'd like to see for the meetings\n3) What topics we want to cover in detail\n4) When we should regularly meet.</p>\n\n<p>See you then.</p>\n\n<p>Brad</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fh'>Champaign, IL meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i2MBoZbnd26Lvnc3W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0234955669370298e-06, "legacy": true, "legacyId": "19758", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Champaign__IL_meetup\">Discussion article for the meetup : <a href=\"/meetups/fh\">Champaign, IL meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 November 2012 09:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Paradiso Cafe, Nevada Street, Champaign, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi all, \nWe're meeting at Paradiso Cafe at 8pm.  I'll try to have a little sign. We'd like to talk about\n1) How to get the word out and expand the group\n2) What kind of structure we'd like to see for the meetings\n3) What topics we want to cover in detail\n4) When we should regularly meet.</p>\n\n<p>See you then.</p>\n\n<p>Brad</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Champaign__IL_meetup1\">Discussion article for the meetup : <a href=\"/meetups/fh\">Champaign, IL meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Champaign, IL meetup", "anchor": "Discussion_article_for_the_meetup___Champaign__IL_meetup", "level": 1}, {"title": "Discussion article for the meetup : Champaign, IL meetup", "anchor": "Discussion_article_for_the_meetup___Champaign__IL_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-02T01:08:20.318Z", "modifiedAt": null, "url": null, "title": "The Emergence of Math", "slug": "the-emergence-of-math", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:31.695Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnotherIdiot", "createdAt": "2012-06-24T20:16:20.645Z", "isAdmin": false, "displayName": "AnotherIdiot"}, "userId": "2wSSd9wbDwAkYTzq6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8Wq69DN7zkaAoizxg/the-emergence-of-math", "pageUrlRelative": "/posts/8Wq69DN7zkaAoizxg/the-emergence-of-math", "linkUrl": "https://www.lesswrong.com/posts/8Wq69DN7zkaAoizxg/the-emergence-of-math", "postedAtFormatted": "Friday, November 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Emergence%20of%20Math&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Emergence%20of%20Math%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Wq69DN7zkaAoizxg%2Fthe-emergence-of-math%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Emergence%20of%20Math%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Wq69DN7zkaAoizxg%2Fthe-emergence-of-math", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Wq69DN7zkaAoizxg%2Fthe-emergence-of-math", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1169, "htmlBody": "<p>In his latest post, <a href=\"/lw/f4e/logical_pinpointing/\">Logical Pinpointing</a>, Eliezer talks about the nature of math. I have my own views on the subject, but when writing about them, it became too long for a comment, so I'm posting it here in discussion.</p>\n<p>I think it's important to clarify that I am not posting this under the guise that I am correct. I'm just posting my current view of things, which may or may not be correct, and which has not been assessed by others yet. So though I have pushed myself to write in a confident (and clear) tone, I am not actually confident in my views ... well, okay, I'm lying. I <em>am </em>confident in my views, but I know I <em>shouldn't</em> be, so I'm <em>trying</em> to pretend to myself that I'm posting this to have my mind changed, when in reality I'm posting it with the stupid expectation that you'll all magically be convinced. Thankfully, despite all that, I don't have a tendency to cling to beliefs which I have seen proven wrong, so<em> </em>I <em>will</em> change my mind when someone kicks my belief's ass. I won't go down without a fight, but once I'm dead, I'm dead.</p>\n<p>Before I can share my views, I need to clarify a few concepts, and then I'll go about showing what I believe and why.</p>\n<p><strong>Abstractions</strong></p>\n<p>Abstract models are models in which some information is ignored. Take, for example, the abstract concept of the ball. I don't care if the ball is made of rubber or steel, nor do I care if it has a radius of 6.7cm or a metre, a ball is a ball. I couldn't care less about the underlying configuration of quarks in the ball, as long as it's a sphere.</p>\n<p>Numbers are also abstractions. If I've got some apples, when I abstract this into a number, I don't care that they are apples. All I care about is how many there are. I can conveniently forget about the fact that it's apples, and about the concept of time, and the hole in my bag through which the apples might fall.</p>\n<p><strong>Axiomatic Systems (for example, Peano Arithmetic)<br /></strong></p>\n<p>Edit: clarified this paragraph a bit.</p>\n<p>I don't have much to say about these for now, except that they can all be reduced to physics. Given that mathematicians store axiomatic systems in their minds, and use them to prove things, they cannot help but be reducible to physical things, unless the mind itself is not physical. I think most LessWrongers, being reductionists, believe this, so I won't go into much more detail. I'll just say that this will be important later on.</p>\n<p><strong>I Can Prove Things About Numbers Using Apples</strong></p>\n<p>Let's say I have 2 apples. Then someone gives me 2 more. I now have 4. I have just shown that 2+2=4, assuming that apples behave like natural numbers (which they do).</p>\n<p>But let's continue this hypothetical. As I put my 2 delicious new apples into my bag, one falls out through a hole in my bag. So if I count how many I have, I see 3. I had 2 to begin with, and 2 more were given to me. It seems I have just shown 2+2=3, if I can prove things about numbers using apples.</p>\n<p>The problem lies in the information that I abstracted away during the conversion from apples to numbers. Because my conversion from apples to numbers failed to include information about the hole, my abstract model gave incorrect results. Like my predictions about balls might end up being incorrect if I don't take into account every quark that composes it. Upon observing that the apple had fallen through the hole, I would realize that an event which rendered my model erroneous had occurred, so I would abstract this new event into -1, which would fix the error: 2+2-1=3.</p>\n<p>To summarize this section, I can \"prove\" things about numbers using apples, but because apples are not simple numbers (they have many properties which numbers don't), when I fail to take into account certain apple properties which will affect the number of apples I have, I will get incorrect results about the numbers.</p>\n<p><strong>Apples vs Peano Arithmetic</strong></p>\n<p>We know that Peano Arithmetic describes numbers very well. Numbers emerge in PA; we designed PA to do so. If PA described unicorns instead, it wouldn't be very useful. And if PA emerges from the laws of physics (we can see PA emerge in mathematicians' minds, and even on pieces of paper in the form of writing), then the numbers which emerge from PA emerge from the laws of physics. So there is nothing magical about PA. It's just a system of \"rules\" (physical processes) from which numbers emerge, like apples (I patched up the hole in my bag ;) ).</p>\n<p>Of course, PA is much more convenient for proving things about numbers than apples. But they are inherently just physical processes from which I have decided to ignore most details, to focus only on the numbers. In my bag of 3 apples, if I ignore that it's apples there, I get the number 3. In SSS0, if I forget about the whole physical process giving emergence to PA, I am just left with 3.</p>\n<p>So I can go from 3 apples to the number 3 by removing details, and from the number 3 to PA by adding in a couple of details. I can likewise go from PA to numbers, and then to apples.</p>\n<p><strong>To Conclude, Predictions are \"Proofs\"</strong></p>\n<p>From all this, I conclude that numbers<strong> </strong>are simply an abstraction which emerges in many places thanks to our uniform laws of physics, much like the abstract concept \"ball\". I also conclude that what we classify as a \"prediction\" is in fact a \"proof\". It's simply using the rules to find other truths about the object. If I predict the trajectory of a ball, I am using the rules behind balls to get more information about the ball. If I use PA or apples to prove something about numbers, I am using the rules behind PA (or apples) to prove something about the numbers which emerge from PA (or apples). Of course, the proof with PA (or apples) is much more general than the \"proof\" about the ball's trajectory, because numbers are much more abstract than balls, and so they emerge in more places.</p>\n<p>So my response to this part of Eliezer's post:</p>\n<blockquote>\n<p>Never mind the question of why the laws of physics are stable - why is logic stable?</p>\n</blockquote>\n<p>Logic is stable for the same reasons the laws of physics are stable. Logic emerges from the laws of physics, and the laws of physics themselves are stable (or so it seems). In this way, I dissolve the question and mix it with the question why the laws of physics are stable -- a question which I don't know enough to attempt to answer.</p>\n<p>&nbsp;</p>\n<p>Edit: I'm going to retract this and try to write a clearer post. I still have not seen arguments which have fully convinced me I am wrong, though I still have a bit to digest.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8Wq69DN7zkaAoizxg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 0, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "19757", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In his latest post, <a href=\"/lw/f4e/logical_pinpointing/\">Logical Pinpointing</a>, Eliezer talks about the nature of math. I have my own views on the subject, but when writing about them, it became too long for a comment, so I'm posting it here in discussion.</p>\n<p>I think it's important to clarify that I am not posting this under the guise that I am correct. I'm just posting my current view of things, which may or may not be correct, and which has not been assessed by others yet. So though I have pushed myself to write in a confident (and clear) tone, I am not actually confident in my views ... well, okay, I'm lying. I <em>am </em>confident in my views, but I know I <em>shouldn't</em> be, so I'm <em>trying</em> to pretend to myself that I'm posting this to have my mind changed, when in reality I'm posting it with the stupid expectation that you'll all magically be convinced. Thankfully, despite all that, I don't have a tendency to cling to beliefs which I have seen proven wrong, so<em> </em>I <em>will</em> change my mind when someone kicks my belief's ass. I won't go down without a fight, but once I'm dead, I'm dead.</p>\n<p>Before I can share my views, I need to clarify a few concepts, and then I'll go about showing what I believe and why.</p>\n<p><strong id=\"Abstractions\">Abstractions</strong></p>\n<p>Abstract models are models in which some information is ignored. Take, for example, the abstract concept of the ball. I don't care if the ball is made of rubber or steel, nor do I care if it has a radius of 6.7cm or a metre, a ball is a ball. I couldn't care less about the underlying configuration of quarks in the ball, as long as it's a sphere.</p>\n<p>Numbers are also abstractions. If I've got some apples, when I abstract this into a number, I don't care that they are apples. All I care about is how many there are. I can conveniently forget about the fact that it's apples, and about the concept of time, and the hole in my bag through which the apples might fall.</p>\n<p><strong id=\"Axiomatic_Systems__for_example__Peano_Arithmetic_\">Axiomatic Systems (for example, Peano Arithmetic)<br></strong></p>\n<p>Edit: clarified this paragraph a bit.</p>\n<p>I don't have much to say about these for now, except that they can all be reduced to physics. Given that mathematicians store axiomatic systems in their minds, and use them to prove things, they cannot help but be reducible to physical things, unless the mind itself is not physical. I think most LessWrongers, being reductionists, believe this, so I won't go into much more detail. I'll just say that this will be important later on.</p>\n<p><strong id=\"I_Can_Prove_Things_About_Numbers_Using_Apples\">I Can Prove Things About Numbers Using Apples</strong></p>\n<p>Let's say I have 2 apples. Then someone gives me 2 more. I now have 4. I have just shown that 2+2=4, assuming that apples behave like natural numbers (which they do).</p>\n<p>But let's continue this hypothetical. As I put my 2 delicious new apples into my bag, one falls out through a hole in my bag. So if I count how many I have, I see 3. I had 2 to begin with, and 2 more were given to me. It seems I have just shown 2+2=3, if I can prove things about numbers using apples.</p>\n<p>The problem lies in the information that I abstracted away during the conversion from apples to numbers. Because my conversion from apples to numbers failed to include information about the hole, my abstract model gave incorrect results. Like my predictions about balls might end up being incorrect if I don't take into account every quark that composes it. Upon observing that the apple had fallen through the hole, I would realize that an event which rendered my model erroneous had occurred, so I would abstract this new event into -1, which would fix the error: 2+2-1=3.</p>\n<p>To summarize this section, I can \"prove\" things about numbers using apples, but because apples are not simple numbers (they have many properties which numbers don't), when I fail to take into account certain apple properties which will affect the number of apples I have, I will get incorrect results about the numbers.</p>\n<p><strong id=\"Apples_vs_Peano_Arithmetic\">Apples vs Peano Arithmetic</strong></p>\n<p>We know that Peano Arithmetic describes numbers very well. Numbers emerge in PA; we designed PA to do so. If PA described unicorns instead, it wouldn't be very useful. And if PA emerges from the laws of physics (we can see PA emerge in mathematicians' minds, and even on pieces of paper in the form of writing), then the numbers which emerge from PA emerge from the laws of physics. So there is nothing magical about PA. It's just a system of \"rules\" (physical processes) from which numbers emerge, like apples (I patched up the hole in my bag ;) ).</p>\n<p>Of course, PA is much more convenient for proving things about numbers than apples. But they are inherently just physical processes from which I have decided to ignore most details, to focus only on the numbers. In my bag of 3 apples, if I ignore that it's apples there, I get the number 3. In SSS0, if I forget about the whole physical process giving emergence to PA, I am just left with 3.</p>\n<p>So I can go from 3 apples to the number 3 by removing details, and from the number 3 to PA by adding in a couple of details. I can likewise go from PA to numbers, and then to apples.</p>\n<p><strong id=\"To_Conclude__Predictions_are__Proofs_\">To Conclude, Predictions are \"Proofs\"</strong></p>\n<p>From all this, I conclude that numbers<strong> </strong>are simply an abstraction which emerges in many places thanks to our uniform laws of physics, much like the abstract concept \"ball\". I also conclude that what we classify as a \"prediction\" is in fact a \"proof\". It's simply using the rules to find other truths about the object. If I predict the trajectory of a ball, I am using the rules behind balls to get more information about the ball. If I use PA or apples to prove something about numbers, I am using the rules behind PA (or apples) to prove something about the numbers which emerge from PA (or apples). Of course, the proof with PA (or apples) is much more general than the \"proof\" about the ball's trajectory, because numbers are much more abstract than balls, and so they emerge in more places.</p>\n<p>So my response to this part of Eliezer's post:</p>\n<blockquote>\n<p>Never mind the question of why the laws of physics are stable - why is logic stable?</p>\n</blockquote>\n<p>Logic is stable for the same reasons the laws of physics are stable. Logic emerges from the laws of physics, and the laws of physics themselves are stable (or so it seems). In this way, I dissolve the question and mix it with the question why the laws of physics are stable -- a question which I don't know enough to attempt to answer.</p>\n<p>&nbsp;</p>\n<p>Edit: I'm going to retract this and try to write a clearer post. I still have not seen arguments which have fully convinced me I am wrong, though I still have a bit to digest.</p>", "sections": [{"title": "Abstractions", "anchor": "Abstractions", "level": 1}, {"title": "Axiomatic Systems (for example, Peano Arithmetic)", "anchor": "Axiomatic_Systems__for_example__Peano_Arithmetic_", "level": 1}, {"title": "I Can Prove Things About Numbers Using Apples", "anchor": "I_Can_Prove_Things_About_Numbers_Using_Apples", "level": 1}, {"title": "Apples vs Peano Arithmetic", "anchor": "Apples_vs_Peano_Arithmetic", "level": 1}, {"title": "To Conclude, Predictions are \"Proofs\"", "anchor": "To_Conclude__Predictions_are__Proofs_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "44 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3FoMuCLqZggTxoC3S"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-02T02:11:24.517Z", "modifiedAt": null, "url": null, "title": "Open Thread, November 1-15, 2012", "slug": "open-thread-november-1-15-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:38.359Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AeBfaMZt7e7ye38Hw/open-thread-november-1-15-2012", "pageUrlRelative": "/posts/AeBfaMZt7e7ye38Hw/open-thread-november-1-15-2012", "linkUrl": "https://www.lesswrong.com/posts/AeBfaMZt7e7ye38Hw/open-thread-november-1-15-2012", "postedAtFormatted": "Friday, November 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20November%201-15%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20November%201-15%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAeBfaMZt7e7ye38Hw%2Fopen-thread-november-1-15-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20November%201-15%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAeBfaMZt7e7ye38Hw%2Fopen-thread-november-1-15-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAeBfaMZt7e7ye38Hw%2Fopen-thread-november-1-15-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>If it's worth saying, but not worth its own post, even in Discussion, it goes here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AeBfaMZt7e7ye38Hw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.0235685986465373e-06, "legacy": true, "legacyId": "19762", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 377, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-02T04:29:26.247Z", "modifiedAt": null, "url": null, "title": "Quote on Nate Silver, and how to think about probabilities", "slug": "quote-on-nate-silver-and-how-to-think-about-probabilities", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:33.374Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FNK5Cw7d2i54kjpDT/quote-on-nate-silver-and-how-to-think-about-probabilities", "pageUrlRelative": "/posts/FNK5Cw7d2i54kjpDT/quote-on-nate-silver-and-how-to-think-about-probabilities", "linkUrl": "https://www.lesswrong.com/posts/FNK5Cw7d2i54kjpDT/quote-on-nate-silver-and-how-to-think-about-probabilities", "postedAtFormatted": "Friday, November 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Quote%20on%20Nate%20Silver%2C%20and%20how%20to%20think%20about%20probabilities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuote%20on%20Nate%20Silver%2C%20and%20how%20to%20think%20about%20probabilities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFNK5Cw7d2i54kjpDT%2Fquote-on-nate-silver-and-how-to-think-about-probabilities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Quote%20on%20Nate%20Silver%2C%20and%20how%20to%20think%20about%20probabilities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFNK5Cw7d2i54kjpDT%2Fquote-on-nate-silver-and-how-to-think-about-probabilities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFNK5Cw7d2i54kjpDT%2Fquote-on-nate-silver-and-how-to-think-about-probabilities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 116, "htmlBody": "<p>From <a href=\"http://www.washingtonpost.com/blogs/ezra-klein/wp/2012/10/30/the-nate-silver-backlash/\">Ezra Klein:</a></p>\n<blockquote>\n<p>If Mitt Romney wins on election day, it doesn&rsquo;t mean Silver&rsquo;s model was wrong. After all, the model has been fluctuating between giving Romney a 25 percent and 40 percent chance of winning the election. That&rsquo;s a pretty good chance! If you told me I had a 35 percent chance of winning a million dollars tomorrow, I&rsquo;d be excited. And if I won the money, I wouldn&rsquo;t turn around and tell you your information was wrong. I&rsquo;d still have no evidence I&rsquo;d ever had anything more than a 35 percent chance.</p>\n</blockquote>\n<p>Okay, technically, winning the money would be <em>very weak</em> Bayesian evidence that the initial probability estimate was wrong. Still a very good quote.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FNK5Cw7d2i54kjpDT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "19770", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-02T05:22:12.144Z", "modifiedAt": null, "url": null, "title": "POLL : Aging research booster", "slug": "poll-aging-research-booster", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:33.870Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blogospheroid", "createdAt": "2009-03-17T08:11:01.816Z", "isAdmin": false, "displayName": "blogospheroid"}, "userId": "dgscyYwrDh3u7dE7h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SJC5q8GCNbigybs9X/poll-aging-research-booster", "pageUrlRelative": "/posts/SJC5q8GCNbigybs9X/poll-aging-research-booster", "linkUrl": "https://www.lesswrong.com/posts/SJC5q8GCNbigybs9X/poll-aging-research-booster", "postedAtFormatted": "Friday, November 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20POLL%20%3A%20Aging%20research%20booster&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APOLL%20%3A%20Aging%20research%20booster%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSJC5q8GCNbigybs9X%2Fpoll-aging-research-booster%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=POLL%20%3A%20Aging%20research%20booster%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSJC5q8GCNbigybs9X%2Fpoll-aging-research-booster", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSJC5q8GCNbigybs9X%2Fpoll-aging-research-booster", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 326, "htmlBody": "<p>I wanted to take a poll among LW folk since this question I had didn't seem to have a straight forward utilitarian answer. Please correct my impression, if wrong.</p>\n<p><strong> Situation </strong></p>\n<p>A government has a limited budget of USD $10 Billion, which it will award in 10 years with which it plans to encourage aging stoppage/reversal. It plans to do this in a relatively fair manner by announcing a competition in which various therapies will be evaluated.</p>\n<p>The methodology is simple. The photographs of people who have undertaken the therapy and a control group are shown to people and they evaluate whether this person looks B years old or B + I years old. Similar checks are done with the blood and various other health checkups. These are evaluated by doctors, who answer the same question. Does this look like the report of one who is B or B+ I years old. All possible blindings are done correctly.</p>\n<p>B is the base year against which the therapy will be evaluated and I is the increment of years for which the therapy should work.</p>\n<p>For eg. the competition announcement can be. The government will offer the prize of $10 Billion to the team that demonstrates stoppage of aging from the age of 30 to 40. (Here, B is 30 and I is 10)</p>\n<p>What do you think are the optimal values of B and I, given the constraints mentioned above (There are actually two variables which I assumed constant for this , the amount of the prize A and the time in which the research has to be done T.)</p>\n<p>The reason I thought this wasn't a straight forward question is that there seem to be tradeoffs between whether to increase the work life time (eg. 30 to 45) or should we cease aging at retirement time (eg. 60 to 75), allowing for greater leisure.</p>\n<p>Assume that this therapy is tested on women.&nbsp;</p>\n<p>[EDIT : Confusion with the poll system. The polls are in the first comment]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SJC5q8GCNbigybs9X", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 0, "extendedScore": null, "score": 1.0236735160627816e-06, "legacy": true, "legacyId": "19773", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-02T05:39:07.817Z", "modifiedAt": null, "url": null, "title": "Desired articles on AI risk?", "slug": "desired-articles-on-ai-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:35.584Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/noYEeN2kuuTNEnvLw/desired-articles-on-ai-risk", "pageUrlRelative": "/posts/noYEeN2kuuTNEnvLw/desired-articles-on-ai-risk", "linkUrl": "https://www.lesswrong.com/posts/noYEeN2kuuTNEnvLw/desired-articles-on-ai-risk", "postedAtFormatted": "Friday, November 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Desired%20articles%20on%20AI%20risk%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADesired%20articles%20on%20AI%20risk%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnoYEeN2kuuTNEnvLw%2Fdesired-articles-on-ai-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Desired%20articles%20on%20AI%20risk%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnoYEeN2kuuTNEnvLw%2Fdesired-articles-on-ai-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnoYEeN2kuuTNEnvLw%2Fdesired-articles-on-ai-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 485, "htmlBody": "<p>I've once again updated my list of <a href=\"http://tinyurl.com/ai-risk-research\">forthcoming and desired articles on AI risk</a>, which currently names 17 forthcoming articles and books about AGI risk, and also names 26 <em>desired</em>&nbsp;articles that I <em>wish</em> researchers were currently writing.</p>\n<p>But I'd like to hear your suggestions, too. Which articles not already on the list as \"forthcoming\" or \"desired\" would you most like to see written, on the subject of AGI risk?</p>\n<p>Book/article titles reproduced below for convenience...</p>\n<p><a id=\"more\"></a></p>\n<p><strong>Forthcoming</strong></p>\n<ul>\n<li><em>Superintelligence: Groundwork for a Strategic Analysis</em> by Nick Bostrom</li>\n<li><em><a href=\"http://www.amazon.com/Singularity-Hypotheses-Scientific-Philosophical-Assessment/dp/3642325599/\">Singularity Hypotheses</a></em>, edited by Amnon Eden et al.</li>\n<li><em>Singularity Hypotheses, Vol. 2</em>, edited by Vic Callaghan</li>\n<li>\"General Purpose Intelligence: Arguing the Orthogonality Thesis\" by Stuart Armstrong</li>\n<li>\"Responses to AGI Risk\" by Kaj Sotala et al.</li>\n<li>\"How we're predicting AI... or failing to\" by Stuart Armstrong &amp; Kaj Sotala</li>\n<li>\"A Comparison of Decision Algorithms on Newcomblike Problems\" by Alex Altair</li>\n<li>\"A Representation Theorem for Decisions about Causal Models\" by Daniel Dewey</li>\n<li>\"Reward Function Integrity in Artificially Intelligent Systems\" by Roman Yampolskiy</li>\n<li>\"Bounding the impact of AGI\" by Andras Kornai</li>\n<li>\"Minimizing Risks in Developing Artificial General Intelligence\" by Ted Goertzel</li>\n<li>\"Limitations and Risks of Machine Ethics\" by Miles Brundage</li>\n<li>\"Universal empathy and ethical bias for artificial general intelligence\" by Alexey Potapov &amp; Sergey Rodiono</li>\n<li>\"Could we use untrustworthy human brain emulations to make trustworthy ones?\" by Carl Shulman</li>\n<li>\"Ethics and Impact of Brain Emulations\" by Anders Sandberg</li>\n<li>\"Envisioning The Economy, and Society, of Whole Brain Emulations\" by Robin Hanson</li>\n<li>\"Autonomous Technology and the Greater Human Good\" by Steve Omohundro</li>\n</ul>\n<p><strong>Desired</strong></p>\n<p>&nbsp;</p>\n<ul>\n<li>\"AI Risk Reduction: Key Strategic Questions\"</li>\n<li>\"Predicting Machine Superintelligence\"</li>\n<li>\"Self-Modification and L&ouml;b's Theorem\"</li>\n<li>\"Solomonoff Induction and Second-Order Logic\"</li>\n<li>\"The Challenge of Preference Extraction\"</li>\n<li>\"Value Extrapolation\"</li>\n<li>\"Losses in Hardscrabble Hell\"</li>\n<li>\"Will Values Converge?\"</li>\n<li>\"AI Takeoff Scenarios\"</li>\n<li>\"AI Will Be Maleficent by Default\"</li>\n<li>\"Biases in AI Research\"</li>\n<li>\"Catastrophic Risks and Existential Risks\"</li>\n<li>\"Uncertainty and Decision Theories\"</li>\n<li>\"Intelligence Explosion: The Proportionality Thesis\"</li>\n<li>\"Hazards from Large Scale Computation\"</li>\n<li>\"Tool Oracles for Safe AI Development\"</li>\n<li>\"Stable Attractors for Technologically Advanced Civilizations\"</li>\n<li>\"AI Risk: Private Projects vs. Government Projects\"</li>\n<li>\"Why AI researchers will fail to hit the narrow target of desirable AI goal systems\"</li>\n<li>\"When will whole brain emulation be possible?\"</li>\n<li>\"Is it desirable to accelerate progress toward whole brain emulation?\"</li>\n<li>\"Awareness of nanotechnology risks: Lessons for AI risk mitigation\"</li>\n<li>\"AI and Physical Effects\"</li>\n<li>\"Moore's Law of Mad Science\"</li>\n<li>\"What Would AIXI Do With Infinite Computing Power and a Halting Oracle?\"</li>\n<li>\"AI Capability vs. AI Safety\"</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "noYEeN2kuuTNEnvLw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 19, "extendedScore": null, "score": 1.0236827841361943e-06, "legacy": true, "legacyId": "19776", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I've once again updated my list of <a href=\"http://tinyurl.com/ai-risk-research\">forthcoming and desired articles on AI risk</a>, which currently names 17 forthcoming articles and books about AGI risk, and also names 26 <em>desired</em>&nbsp;articles that I <em>wish</em> researchers were currently writing.</p>\n<p>But I'd like to hear your suggestions, too. Which articles not already on the list as \"forthcoming\" or \"desired\" would you most like to see written, on the subject of AGI risk?</p>\n<p>Book/article titles reproduced below for convenience...</p>\n<p><a id=\"more\"></a></p>\n<p><strong id=\"Forthcoming\">Forthcoming</strong></p>\n<ul>\n<li><em>Superintelligence: Groundwork for a Strategic Analysis</em> by Nick Bostrom</li>\n<li><em><a href=\"http://www.amazon.com/Singularity-Hypotheses-Scientific-Philosophical-Assessment/dp/3642325599/\">Singularity Hypotheses</a></em>, edited by Amnon Eden et al.</li>\n<li><em>Singularity Hypotheses, Vol. 2</em>, edited by Vic Callaghan</li>\n<li>\"General Purpose Intelligence: Arguing the Orthogonality Thesis\" by Stuart Armstrong</li>\n<li>\"Responses to AGI Risk\" by Kaj Sotala et al.</li>\n<li>\"How we're predicting AI... or failing to\" by Stuart Armstrong &amp; Kaj Sotala</li>\n<li>\"A Comparison of Decision Algorithms on Newcomblike Problems\" by Alex Altair</li>\n<li>\"A Representation Theorem for Decisions about Causal Models\" by Daniel Dewey</li>\n<li>\"Reward Function Integrity in Artificially Intelligent Systems\" by Roman Yampolskiy</li>\n<li>\"Bounding the impact of AGI\" by Andras Kornai</li>\n<li>\"Minimizing Risks in Developing Artificial General Intelligence\" by Ted Goertzel</li>\n<li>\"Limitations and Risks of Machine Ethics\" by Miles Brundage</li>\n<li>\"Universal empathy and ethical bias for artificial general intelligence\" by Alexey Potapov &amp; Sergey Rodiono</li>\n<li>\"Could we use untrustworthy human brain emulations to make trustworthy ones?\" by Carl Shulman</li>\n<li>\"Ethics and Impact of Brain Emulations\" by Anders Sandberg</li>\n<li>\"Envisioning The Economy, and Society, of Whole Brain Emulations\" by Robin Hanson</li>\n<li>\"Autonomous Technology and the Greater Human Good\" by Steve Omohundro</li>\n</ul>\n<p><strong id=\"Desired\">Desired</strong></p>\n<p>&nbsp;</p>\n<ul>\n<li>\"AI Risk Reduction: Key Strategic Questions\"</li>\n<li>\"Predicting Machine Superintelligence\"</li>\n<li>\"Self-Modification and L\u00f6b's Theorem\"</li>\n<li>\"Solomonoff Induction and Second-Order Logic\"</li>\n<li>\"The Challenge of Preference Extraction\"</li>\n<li>\"Value Extrapolation\"</li>\n<li>\"Losses in Hardscrabble Hell\"</li>\n<li>\"Will Values Converge?\"</li>\n<li>\"AI Takeoff Scenarios\"</li>\n<li>\"AI Will Be Maleficent by Default\"</li>\n<li>\"Biases in AI Research\"</li>\n<li>\"Catastrophic Risks and Existential Risks\"</li>\n<li>\"Uncertainty and Decision Theories\"</li>\n<li>\"Intelligence Explosion: The Proportionality Thesis\"</li>\n<li>\"Hazards from Large Scale Computation\"</li>\n<li>\"Tool Oracles for Safe AI Development\"</li>\n<li>\"Stable Attractors for Technologically Advanced Civilizations\"</li>\n<li>\"AI Risk: Private Projects vs. Government Projects\"</li>\n<li>\"Why AI researchers will fail to hit the narrow target of desirable AI goal systems\"</li>\n<li>\"When will whole brain emulation be possible?\"</li>\n<li>\"Is it desirable to accelerate progress toward whole brain emulation?\"</li>\n<li>\"Awareness of nanotechnology risks: Lessons for AI risk mitigation\"</li>\n<li>\"AI and Physical Effects\"</li>\n<li>\"Moore's Law of Mad Science\"</li>\n<li>\"What Would AIXI Do With Infinite Computing Power and a Halting Oracle?\"</li>\n<li>\"AI Capability vs. AI Safety\"</li>\n</ul>\n<p>&nbsp;</p>", "sections": [{"title": "Forthcoming", "anchor": "Forthcoming", "level": 1}, {"title": "Desired", "anchor": "Desired", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "26 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-02T05:47:56.017Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The First World Takeover", "slug": "seq-rerun-the-first-world-takeover", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:32.977Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FfHPcEGLnpwiewjMi/seq-rerun-the-first-world-takeover", "pageUrlRelative": "/posts/FfHPcEGLnpwiewjMi/seq-rerun-the-first-world-takeover", "linkUrl": "https://www.lesswrong.com/posts/FfHPcEGLnpwiewjMi/seq-rerun-the-first-world-takeover", "postedAtFormatted": "Friday, November 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20First%20World%20Takeover&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20First%20World%20Takeover%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFfHPcEGLnpwiewjMi%2Fseq-rerun-the-first-world-takeover%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20First%20World%20Takeover%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFfHPcEGLnpwiewjMi%2Fseq-rerun-the-first-world-takeover", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFfHPcEGLnpwiewjMi%2Fseq-rerun-the-first-world-takeover", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<p>Today's post, <a href=\"/lw/w0/the_first_world_takeover/\">The First World Takeover</a> was originally published on 19 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_First_World_Takeover\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The first replicator was the original black swan. A couple of molecules that, despite not having a particularly good optimization process, could explore new regions of pattern-space. This is an event that would have implications that would have seemed absurd to predict.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/f8l/seq_rerun_the_weak_inside_view/\">The Weak Inside View</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FfHPcEGLnpwiewjMi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0236876040460172e-06, "legacy": true, "legacyId": "19778", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["spKYZgoh3RmhxMqyu", "RySGY6CFzQG56PC5s", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-02T06:13:46.750Z", "modifiedAt": null, "url": null, "title": "November 2012 Media Thread", "slug": "november-2012-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:06.407Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4Wv9LKpNwvJuxvCeb/november-2012-media-thread", "pageUrlRelative": "/posts/4Wv9LKpNwvJuxvCeb/november-2012-media-thread", "linkUrl": "https://www.lesswrong.com/posts/4Wv9LKpNwvJuxvCeb/november-2012-media-thread", "postedAtFormatted": "Friday, November 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20November%202012%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANovember%202012%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Wv9LKpNwvJuxvCeb%2Fnovember-2012-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=November%202012%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Wv9LKpNwvJuxvCeb%2Fnovember-2012-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Wv9LKpNwvJuxvCeb%2Fnovember-2012-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 217, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. I find that exposure to LW ideas makes me less likely to enjoy some entertainment media that is otherwise quite popular, and finding media recommended by LWers is a good way to mitigate this. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the&nbsp;<a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a&nbsp;<a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4Wv9LKpNwvJuxvCeb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.0237017549000187e-06, "legacy": true, "legacyId": "19780", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 92, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-02T11:13:24.131Z", "modifiedAt": null, "url": null, "title": "Teaching English in Shanghai", "slug": "teaching-english-in-shanghai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:04.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ShanghaiTEFLer", "createdAt": "2012-10-30T04:53:51.915Z", "isAdmin": false, "displayName": "ShanghaiTEFLer"}, "userId": "y6mHaejfj9BZPhgqs", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4aaPheZh5eAowZdjW/teaching-english-in-shanghai", "pageUrlRelative": "/posts/4aaPheZh5eAowZdjW/teaching-english-in-shanghai", "linkUrl": "https://www.lesswrong.com/posts/4aaPheZh5eAowZdjW/teaching-english-in-shanghai", "postedAtFormatted": "Friday, November 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Teaching%20English%20in%20Shanghai&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATeaching%20English%20in%20Shanghai%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4aaPheZh5eAowZdjW%2Fteaching-english-in-shanghai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Teaching%20English%20in%20Shanghai%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4aaPheZh5eAowZdjW%2Fteaching-english-in-shanghai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4aaPheZh5eAowZdjW%2Fteaching-english-in-shanghai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1570, "htmlBody": "<p>&nbsp;</p>\n<p>If your English is good enough to follow discussions on this site you can get a job teaching English in Shanghai. If you can participate meaningfully you are frankly overqualified. I am currently saving about 2,000 dollars a month working approximately 9-5 every day.</p>\n<p>The minimum legal requirements to teach are that you be over 23 years of age, have a (relevant) degree and two years of relevant work experience as well as a TEFL qualification that took 120 hours to complete. In theory you should also be from a country where English is a native language to teach but this requirement is often honoured in the breach. In practice you must be over 23 and know someone who can fake up a degree certificate in Photoshop, and be willing to write yourself a fitting resume and sign a form stating that all the documents you are submitting are true. Some companies will do all of the faking and lying for you, some will at least want you to give them the appropriate fake documents so there&rsquo;s some farcical plausible deniability.</p>\n<p>If you are not on a work visa doing work of any kind for pay is illegal. This rule is ignored all the time, including by large Western multinational companies with legal departments because the rules change depending on who&rsquo;s interpreting them and which website you&rsquo;re looking at. But people get Business visas to come over here and do internships regularly and if you are from a First World country the worst that will happen to you for getting caught working on a tourist or student visa is a large fine (5500 yuan). If you overstay your visa by more than about three days you will be deported. This is one of the few things the government care about when it comes to foreigners. Drugs, gambling, prostitution, as long as there are no nationals involved it needs to get quite big for them to care. But you can get unlucky. It's rare but it happens.</p>\n<p>Demand for English teachers in Shanghai is insatiable. If you are being paid by the hour the minimum acceptable rate for someone with no experience, who can&rsquo;t spell, and can&rsquo;t teach is 150 RMB per hour. Never accept less than this for any job. The local public primary schools are legally barred from hiring foreigners so they go through companies who hire them instead. You will not really be teaching at most of these companies, more providing a prop; you, the foreign teacher. There isn&rsquo;t that much you can teach in 35 minutes a week when half of it should be games or the children will dislike you, meaning the parents will dislike you, meaning the school will complain about you to the company. But you can make English fun, and given sufficient planning and practice you can teach something even in such small classes. If you just want a job for a visa there are companies who will provide one for teaching one day a week. You can make 7,000 RMB a month easily doing this four days a week. I used to make 10,000 when I was doing a similar schedule. This is in a city where you can eat well for 50 RMB a day, very well, have a maid come to your apartment three times a week for less than 200 and you can get a nice apartment for 3,000 a month. Ten RMB is worth approximately 1 pound sterling, so at a guess 1.6 dollars. My single greatest living expense is going out. There are \"dive bars\" with relatively low prices but most places that cater to foreigners make you pay for it. One company that is of average incompetence (very) and unusual honesty (except during negotiations about hours, pay per hour and where they have available schools) is Corneil. They will often screw up the recording of your hours but rarely by a significant amount and will give it to you if it's pointed out to them. It's not malice, they're just incompetent. So is everyone else.</p>\n<p>To illustrate how low the standards are here I have met someone who was teaching at a high school who was moved to another at the school&rsquo;s request because he was swearing in class, smoking in school and sharing with the students. They also suspected he was sleeping with one of his students. After they moved him the school requested him back because he was very popular. He also can&rsquo;t spell. He has been teaching for seven years.</p>\n<p>Getting work at the weekend is very, very easy. Again, never accept less than 150 an hour. <a href=\"http://www.reddit.com/r/shanghai/comments/12fm95/shane_english_craigslist_drama/c6uqxhm\">Some people</a> think that's low.There are many, many companies serving this market and all of them are perfectly willing to pay people who do not and can not legally work for them. In theory most of them have standards and demand lesson planning, something resembling professionalism and turning up to work on time. In practice if you arrive on time all the time and ask the co-teacher what pages to teach for the next 45 minutes you will be fine. Then there&rsquo;s a break and you play a game for the remainder of the class. If you are working legally for one of these companies they will usually try to get you to work three evenings a week as well. I wouldn&rsquo;t but if you want to you can. If you&rsquo;re willing to work in the evenings you can just post an ad on one of the local expat magazine websites and you will be able to do private lessons. It&rsquo;s easier to go through an agency that charges introduction fees at first but after you&rsquo;ve been here a while you can just do it all yourself.</p>\n<p>Teaching business or other professional English can be much more lucrative but the standards are higher.</p>\n<p>If you actually have a relevant degree and two years of teaching experience you can probably get a job at an international school. They pay better, have higher minimum standards and as far as I can tell from talking to friends who teach at them they are all quite political places. Being a good or very good teacher will not protect you from politics but being reasonably good at politics will save you from anything but being an abombinably bad teacher. Most of the international schools start hiring for the next school year about now but just before the beginning of the school year is also reasonably good because if they need somebody they need them now. It is possible to go transition from teaching English as a foreign language or teaching a subject you know through English in a Chinese school to working in an international school but it takes a while. You must actually become a reasonable teacher first. If you want to do it for the long term it&rsquo;s a good idea to get a teaching degree at some point. Once you teach in one international school you can travel almost anywhere and teach in others. It has much to recommend it.</p>\n<p>Many of the business people here have a very dismissive attitude to teachers, whether TEFLers or international school teachers. If it bugs you don&rsquo;t hang out with people like that.</p>\n<p>Most single foreigners who have been here a significant length of time end up in Jing&rsquo;an or the Former French Concession. You pay a premium for the central location but the cultural and other amenities make it well worth it. People with families are more common near Hongqiao Road in the Minhang area but that&rsquo;s not relevant to you unless you&rsquo;re homeschooling because even the &ldquo;international&rdquo; schools like Shanghai United International School that accept Chinese children cost very large sums of money. Shanghai American School&rsquo;s yearly tuition is comparable to Harvard&rsquo;s.</p>\n<p>The dating situation here is <a href=\"http://www.reddit.com/r/China/comments/10va8o/china_is_making_me_hate_myself/\">incredibly easy</a> for men and &nbsp;<a href=\"http://www.shanghaiexpat.com/phpbbforum/local-women-vs-western-women-no-competition-t146893.html\">pretty</a> <a href=\"http://www.shanghaiexpat.com/phpbbforum/do-expats-find-girls-only-to-have-fun-t140308.html\">terrible</a> for women, at least for expats. If I was a single woman I wouldn&rsquo;t move over here unless I was very attractive, very outgoing or both. Even TEFL teachers here can easily be in the 90th percentile of income working five days a week. International school teachers get paid better. This, together, with the fact there are lots of women with a thing for either English speaking guys or white or black guys makes it really, really easy to meet women here. If you want a love life here as a woman it will really help to initiate. Also, if you move over with a boyfriend or husband he will get hit on all the time, including by people who know that he&rsquo;s not single. There are a lot of very, very mercenary girls here who will use you for your bank balance but they&rsquo;re not really that hard to avoid if you have some sense. As a teacher the real gold diggers will not care about you at all.</p>\n<p>You will not learn much Chinese here unless you make a serious effort to do so. I have met expats who have been here eight years who know five words of Chinese. Mine is better than average by virtue of having completed the Pimsleur Mandarin series. This does not mean my Mandarin is good, it means everyone else's is crap.If you want to learn Mandarin every single other city in China is better, Beijing included.</p>\n<p>If you have questions ask. If you can think of suitable tags or links to be added feel free to PM me.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4aaPheZh5eAowZdjW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 19, "extendedScore": null, "score": 1.0238658301567335e-06, "legacy": true, "legacyId": "19788", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 69, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-02T15:08:03.789Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Cambridge UK, Melbourne, Moscow, Munich, Pittsburgh, Washington DC", "slug": "weekly-lw-meetups-cambridge-uk-melbourne-moscow-munich", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9ccefQts7PK7fQY57/weekly-lw-meetups-cambridge-uk-melbourne-moscow-munich", "pageUrlRelative": "/posts/9ccefQts7PK7fQY57/weekly-lw-meetups-cambridge-uk-melbourne-moscow-munich", "linkUrl": "https://www.lesswrong.com/posts/9ccefQts7PK7fQY57/weekly-lw-meetups-cambridge-uk-melbourne-moscow-munich", "postedAtFormatted": "Friday, November 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Cambridge%20UK%2C%20Melbourne%2C%20Moscow%2C%20Munich%2C%20Pittsburgh%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Cambridge%20UK%2C%20Melbourne%2C%20Moscow%2C%20Munich%2C%20Pittsburgh%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9ccefQts7PK7fQY57%2Fweekly-lw-meetups-cambridge-uk-melbourne-moscow-munich%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Cambridge%20UK%2C%20Melbourne%2C%20Moscow%2C%20Munich%2C%20Pittsburgh%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9ccefQts7PK7fQY57%2Fweekly-lw-meetups-cambridge-uk-melbourne-moscow-munich", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9ccefQts7PK7fQY57%2Fweekly-lw-meetups-cambridge-uk-melbourne-moscow-munich", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 480, "htmlBody": "<p><strong>This summary was posted to LW main on October 26th. The following summary is <a href=\"/lw/f9r/weekly_lw_meetups_austin_berlin_cambridge_uk/\">here</a>.</strong></p>\n<p>For <strong>LW readers under 20</strong>: Note that <strong>the Thiel Fellowships (20 under 20) are now open</strong> for their next round of applications, and as they put it, \"you have a huge readership of folk who would make great applicants\". More info <a href=\"http://us5.campaign-archive1.com/?u=acaba706ab5c95042f36f6e5c&amp;id=59d5da85a4&amp;e=c872586ed7\">here</a>.</p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/ev\">Moscow: Biases, Applied Rationality, Visual Thinking:&nbsp;<span class=\"date\">27 October 2012 04:00PM</span></a></li>\n<li><a href=\"/meetups/e9\">Munich Meetup, EDIT: October 28th:&nbsp;<span class=\"date\">28 October 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/f2\">Washington DC Ethics meetup:&nbsp;<span class=\"date\">28 October 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/f9\">Pittsburgh: Belief as attire:&nbsp;<span class=\"date\">30 October 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/f3\">Berlin Meetup:&nbsp;<span class=\"date\">03 November 2012 08:30PM</span></a></li>\n<li><a href=\"/meetups/el\">Sofia, Bulgaria Meetup:&nbsp;<span class=\"date\">09 December 2012 05:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/f7\">Cambridge UK Weekly Meeting:&nbsp;<span class=\"date\">28 October 2012 11:00AM</span></a></li>\n<li><a href=\"/meetups/f8\">Melbourne, practical rationality:&nbsp;<span class=\"date\">02 November 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/en\">Winter Solstice Megameetup - NYC:&nbsp;<span class=\"date\">15 December 2012 05:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9ccefQts7PK7fQY57", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.0239943643393345e-06, "legacy": true, "legacyId": "19641", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jhBXeFbxEJhRGg9eR", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-02T15:33:19.634Z", "modifiedAt": null, "url": null, "title": "Logical Pinpointing", "slug": "logical-pinpointing", "viewCount": null, "lastCommentedAt": "2018-01-23T10:33:15.206Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3FoMuCLqZggTxoC3S/logical-pinpointing", "pageUrlRelative": "/posts/3FoMuCLqZggTxoC3S/logical-pinpointing", "linkUrl": "https://www.lesswrong.com/posts/3FoMuCLqZggTxoC3S/logical-pinpointing", "postedAtFormatted": "Friday, November 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Logical%20Pinpointing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALogical%20Pinpointing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3FoMuCLqZggTxoC3S%2Flogical-pinpointing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Logical%20Pinpointing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3FoMuCLqZggTxoC3S%2Flogical-pinpointing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3FoMuCLqZggTxoC3S%2Flogical-pinpointing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2969, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/f1u/causal_reference/\">Causal Reference</a>, <a href=\"/lw/f43/proofs_implications_and_models/\">Proofs, Implications and Models</a></p>\n<p style=\"padding-left: 30px;\">The fact that one apple added to one apple invariably gives two apples helps in the teaching of arithmetic, but has no bearing on the truth of the proposition that 1 + 1 = 2.</p>\n<p style=\"padding-left: 30px;\">-- James R. Newman, The World of Mathematics</p>\n<p><em><a href=\"/lw/f1u/causal_reference/#7nyc\">Previous meditation 1:</a></em>&nbsp;If we can only meaningfully talk about parts of the universe that can be pinned down by chains of cause and effect, where do we find the fact that 2 + 2 = 4? Or did I just make a meaningless noise, there? Or if you claim that \"2 + 2 = 4\"<em>isn't</em>&nbsp;meaningful or true, then what alternate property does the sentence \"2 + 2 = 4\" have which makes it so much more useful than the sentence \"2 + 2 = 3\"?</p>\n<p><a href=\"/lw/f43/proofs_implications_and_models/#7ors\"><em>Previous&nbsp;</em><em>meditation</em><em>&nbsp;2:</em></a>&nbsp;It has been claimed that logic and mathematics is the study of which conclusions follow from which premises. But when we say that 2 + 2 = 4, are we really just&nbsp;<em>assuming</em>&nbsp;that? It seems like 2 + 2 = 4 was true well before anyone was around to assume it, that two apples equalled two apples before there was anyone to count them, and that we couldn't make it 5 just by assuming differently.</p>\n<p>Speaking conventional English, we'd say the sentence 2 + 2 = 4 is \"true\", and anyone who put down \"false\" instead on a math-test would be marked wrong by the schoolteacher (and not without justice).</p>\n<p>But what can&nbsp;<em>make</em>&nbsp;such a belief true, what is the belief&nbsp;<em>about,</em>&nbsp;what is the truth-condition of the belief which can make it true or alternatively false? The sentence '2 + 2 = 4' is true if and only if... what?</p>\n<p>In the previous post I asserted that the study of logic is the study of which conclusions follow from which premises; and that although this sort of inevitable implication is sometimes called \"true\", it could more specifically be called \"valid\", since checking for inevitability seems quite different from comparing a belief to our own universe. And you could claim, accordingly, that \"2 + 2 = 4\" is 'valid' because it is an inevitable implication of the axioms of Peano Arithmetic.</p>\n<p>And yet thinking about 2 + 2 = 4 doesn't really&nbsp;<em>feel</em>&nbsp;that way. Figuring out facts about the natural numbers doesn't&nbsp;feel&nbsp;like the operation of making up assumptions and then deducing conclusions from them. It feels like the numbers are just&nbsp;<em>out</em>&nbsp;there, and the only point of making up the axioms of Peano Arithmetic was to&nbsp;<em>allow</em>&nbsp;mathematicians to talk about them. The Peano axioms might have been convenient for&nbsp;<em>deducing</em>&nbsp;a set of theorems like 2 + 2 = 4, but really all of those theorems were true&nbsp;<em>about&nbsp;</em>numbers to begin with. Just like \"The sky is blue\" is true about the sky, regardless of whether it follows from any particular assumptions.</p>\n<p>So comparison-to-a-standard does seem to be at work, just as with&nbsp;<em>physical</em>&nbsp;truth... and yet this notion of 2 + 2 = 4 seems different from \"<a href=\"/lw/ezu/stuff_that_makes_stuff_happen/\">stuff that makes stuff happen</a>\". Numbers don't occupy space or time, they don't arrive in any order of cause and effect, there are no&nbsp;<em>events</em>&nbsp;in numberland.</p>\n<p><strong><a href=\"http://wiki.lesswrong.com/wiki/Meditation\">Meditation</a>:&nbsp;</strong>What are we talking&nbsp;<em>about</em>&nbsp;when we talk about numbers? We can't navigate to them by following causal connections - so how do we get there from here?<a id=\"more\"></a></p>\n<p>...<br />...<br />...</p>\n<p>\"Well,\" says the mathematical logician, \"that's indeed a very important and interesting question - where are the numbers - but first, I have a question for you.&nbsp;<em>What</em>&nbsp;are these 'numbers' that you're talking about? I don't believe I've heard that word before.\"</p>\n<p>Yes you have.</p>\n<p>\"No, I haven't. I'm not a typical mathematical logician; I was just created five minutes ago for the purposes of this conversation. So I genuinely don't know what numbers are.\"</p>\n<p>But... you know, 0, 1, 2, 3...</p>\n<p>\"I don't recognize that 0 thingy - what is it? I'm not asking you to give an exact definition, I'm just trying to figure out what the heck you're talking about in the first place.\"</p>\n<p>Um... okay... look, can I start by asking you to just take on faith that there are these thingies called 'numbers' and 0 is one of them?</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/f/f3/1.jpg\" alt=\"\" width=\"366\" height=\"336\" /></p>\n<p>\"Of course! 0 is a number. I'm happy to believe that. Just to check that I understand correctly, that does mean there exists a number, right?\"</p>\n<p>Um, yes. And then I'll ask you to believe that we can take the successor of any number. So we can talk about the successor of 0, the successor of the successor of 0, and so on. Now 1 is the successor of 0, 2 is the successor of 1, 3 is the successor of 2, and so on indefinitely, because we can take the successor of any number -</p>\n<p>\"In other words, the successor of any number is also a number.\"</p>\n<p>Exactly.</p>\n<p>\"And in a simple case - I'm just trying to visualize how things might work - we would have 2 equal to 0.\"</p>\n<p>What? No, why would that be -</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/1/15/2.jpg\" alt=\"\" width=\"366\" height=\"336\" /></p>\n<p>\"I was visualizing a case where there were two numbers that were the successors of each other, so SS0 = 0. I mean, I could've visualized one number that was the successor of itself, but I didn't want to make things&nbsp;<em>too</em>&nbsp;trivial -\"</p>\n<p>No! That model you just drew - that's&nbsp;<em>not</em>&nbsp;a model of the numbers.</p>\n<p>\"Why not? I mean, what property do the numbers have that this model doesn't?\"</p>\n<p>Because, um... zero is not the successor of&nbsp;<em>any</em>&nbsp;number. Your model has a successor link from 1 to 0, and that's not allowed.</p>\n<p>\"I see! So we can't have SS0=0. But we could still have SSS0=S0.\"</p>\n<p>What? How -</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/79/3.jpg\" alt=\"\" width=\"366\" height=\"336\" /></p>\n<p>No! Because -</p>\n<p><em>(consults textbook)</em></p>\n<p>- if two numbers have the same successor, they are the same number, that's why! You can't have 2 and 0&nbsp;<em>both</em>&nbsp;having 1 as a successor unless they're the same number, and if 2 was the same number as 0, then 1's successor would be 0, and that's not allowed! &nbsp;Because 0 is not the successor of any number!</p>\n<p>\"I see. Oh, wow, there's an awful lot of numbers, then. The first chain goes on&nbsp;<em>forever</em>.\"</p>\n<p>It sounds like you're starting to get what I - wait. Hold on. What do you mean, the&nbsp;<em>first</em>&nbsp;chain -</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/d/d0/4.jpg\" alt=\"\" width=\"450\" height=\"256\" /></p>\n<p>\"I mean, you said that there was at least one start of an infinite chain, called 0, but -\"</p>\n<p>I misspoke. Zero is the&nbsp;<em>only</em>&nbsp;number which is not the successor of any number.</p>\n<p>\"I see, so any other chains would either have to loop or go on forever in&nbsp;<em>both</em>&nbsp;directions.\"</p>\n<p>Wha?</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/0/03/5.jpg\" alt=\"\" width=\"322\" height=\"335\" /></p>\n<p>\"You said that zero is the only number which is not the successor of any number, that the successor of every number is a number, and that if two numbers have the same successor they are the same number. So, following those rules, any successor-chains besides the one that start at 0 have to loop or go on forever in both directions -\"</p>\n<p>There&nbsp;<em>aren't supposed to be any chains</em>&nbsp;besides the one that starts at 0! Argh! And now you're going to ask me how to say that there shouldn't be any other chains, and I'm not a mathematician so I can't figure out exactly how to -</p>\n<p>\"Hold on! Calm down.&nbsp;<em>I'm</em>&nbsp;a mathematician, after all, so I can help you out. Like I said, I'm not trying to torment you here, just understand what you&nbsp;<em>mean</em>. You're right that it's not trivial to formalize your statement that there's only one successor-chain in the model. In fact, you can't say that&nbsp;<em>at all</em>&nbsp;inside what's called&nbsp;<em>first-order logic.</em>&nbsp;You have to jump to something called&nbsp;<em>second-order logic</em>&nbsp;that has some remarkably different properties (ha ha!) and make the statement there.\"</p>\n<p>What the heck is second-order logic?</p>\n<p>\"It's the logic of properties! First-order logic lets you quantify over&nbsp;<em>all objects</em>&nbsp;- you can say that all objects are red, or all objects are blue, or '<span style=\"color: #222222; font-family: arial, sans-serif; line-height: 16px; \">&forall;</span>x: red(x)&rarr;&not;blue(x)', and so on. Now, that 'red' and 'blue' we were just talking about - those are&nbsp;<em>properties,</em>&nbsp;functions which, applied to any object, yield either 'true' or 'false'. A property divides all objects into two classes, a class inside the property and a complementary class outside the property. So everything in the universe is either blue or not-blue, red or not-red, and so on. And then second-order logic lets you quantify over properties - instead of looking at particular objects and asking whether they're blue or red, we can talk&nbsp;<em>about</em>&nbsp;properties in general - quantify over&nbsp;<em>all possible&nbsp;</em>ways of sorting the objects in the universe into classes. We can say, 'For all properties P', not just, 'For all objects X'.\"</p>\n<p>&nbsp;</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/78/FirstOrder.jpg\" alt=\"\" width=\"263\" height=\"322\" />&nbsp;<img src=\"http://wiki.lesswrong.com/mediawiki/images/8/81/2ndOrder.jpg\" alt=\"\" width=\"390\" height=\"317\" /></p>\n<p>&nbsp;</p>\n<p>Okay, but what does that have to do with saying that there's only one chain of successors?</p>\n<p>\"To say that there's only one chain, you have to make the jump to second-order logic, and say that&nbsp;<em>for all properties P</em>, if P being true of a number implies P being true of the successor of that number,&nbsp;<em>and</em>&nbsp;P is true of 0,&nbsp;<em>then</em>&nbsp;P is true of all numbers.\"</p>\n<p>Um... huh. That does sound reminiscent of something I remember hearing about Peano Arithmetic. But how does that solve the problem with chains of successors?</p>\n<p>\"Because if you had another&nbsp;<em>separated</em>&nbsp;chain, you could have a property P that was true all along the 0-chain, but false along the separated chain. And then P would be true of 0, true of the successor of any number of which it was true, and&nbsp;<em>not</em>&nbsp;true of all numbers.\"</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/5/54/SecondOrderLogic.jpg\" alt=\"\" width=\"310\" height=\"323\" /></p>\n<p>I... huh. That's pretty neat, actually. You thought of that pretty fast, for somebody who's never heard of numbers.</p>\n<p>\"Thank you! I'm an imaginary fictionalized representation of a very&nbsp;<em>fast</em>&nbsp;mathematical reasoner.\"</p>\n<p>Anyway, the next thing I want to talk about is addition. First, suppose that for every x, x + 0 = x. &nbsp;Next suppose that if x + y = z, then x + Sy = Sz -</p>\n<p>\"There's no need for that. We're done.\"</p>\n<p>What do you mean, we're done?</p>\n<p>\"Every number has a successor. If two numbers have the same successor, they are the same number. There's a number 0, which is the only number that is not the successor of any other number. And every property true at 0, and for which P(Sx) is true whenever P(x) is true, is true of all numbers. In combination, those premises narrow down a&nbsp;<em>single</em>&nbsp;model in mathematical space, up to isomorphism. If you show me two models matching these requirements, I can perfectly map the objects and successor relations in them. You can't add any new object to the model, or subtract an object, without violating the axioms you've already given me. It's a uniquely identified mathematical collection, the objects and their structure&nbsp;<em>completely pinned down</em>. Ergo, there's no point in adding any more requirements. Any meaningful statement you can make about these 'numbers', as you've defined them, is&nbsp;<em>already true</em>&nbsp;or&nbsp;<em>already false</em>&nbsp;within that pinpointed model - its truth-value is already semantically implied by the axioms you used to talk about 'numbers' as opposed to something else. If the new axiom is already true, adding it won't change what the previous axioms&nbsp;<em>semantically</em>&nbsp;imply.\"</p>\n<p>Whoa. But don't I have to define the + operation before I can talk about it?</p>\n<p>\"Not in second-order logic, which can quantify over relations as well as properties. You just say: 'For every relation R that works exactly like addition, the following statement Q is true about that relation.' It would look like, '<span style=\"color: #222222; font-family: arial, sans-serif; line-height: 16px; \">&forall;</span>&nbsp;relations R: (&forall;x&forall;y&forall;z: (R(x, 0, z)&harr;(x=z)) &and; (R(x, Sy, z)&harr;R(Sx, y, z)))&nbsp;&rarr;&nbsp;Q)', where Q says whatever you meant to say about +, using the token R. Oh, sure, it's more convenient to add + to the language, but that's a mere&nbsp;<em>convenience</em>&nbsp;- it doesn't change which facts you can prove. Or to say it outside the system: So long as I&nbsp;<em>know</em>&nbsp;what numbers are, you can just explain to me how to add them; that doesn't change which mathematical structure we're already talking about.\"</p>\n<p>...Gosh. I think I see the idea now. It's not that 'axioms' are mathematicians asking for you to just assume some things about numbers that seem obvious but can't be proven. Rather, axioms&nbsp;<em>pin down that we're talking about numbers as opposed to something else.</em></p>\n<p>\"Exactly. That's why the&nbsp;<em>mathematical</em>&nbsp;study of numbers is&nbsp;<em>equivalent</em>&nbsp;to the&nbsp;<em>logical</em>&nbsp;study of which conclusions follow inevitably from the number-axioms. When you formalize logic into syntax, and prove theorems like '2 + 2 = 4' by syntactically deriving new sentences from the axioms, you can safely infer that 2 + 2 = 4 is semantically implied within&nbsp;the mathematical universe that the axioms pin down. And there's no way to try to 'just study the numbers without assuming any axioms', because those axioms are how you can talk about&nbsp;<em>numbers</em>&nbsp;as opposed to something else. You can't take for granted that just because your mouth makes a sound 'NUM-burz', it's a meaningful sound. The axioms aren't things you're arbitrarily making up, or assuming for convenience-of-proof, about some pre-existent thing called numbers. You need axioms to pin down a mathematical universe before you can talk&nbsp;<em>about</em>&nbsp;it in the first place. The axioms are pinning down what the heck this 'NUM-burz' sound means in the first place - that your mouth is talking about 0, 1, 2, 3, and so on.\"</p>\n<p>Could you also talk about unicorns that way?</p>\n<p>\"I suppose. Unicorns don't exist in reality - there's nothing in the world that behaves like that - but they could nonetheless be described using a consistent set of axioms, so that it would be&nbsp;<em>valid</em>&nbsp;if not quite&nbsp;<em>true</em>&nbsp;to say that if a unicorn would be attracted to Bob, then Bob must be a virgin. Some people might dispute whether unicorns&nbsp;<em>must</em>&nbsp;be attracted to virgins, but since unicorns aren't real - since we aren't locating them within our universe using a causal reference - they'd just be talking about different models, rather than arguing about the properties of a known, fixed mathematical model. The 'axioms' aren't making questionable guesses about some real physical unicorn, or even a mathematical unicorn-model that's already been pinpointed; they're just fictional premises that make the word 'unicorn' talk about something&nbsp;inside a story.\"</p>\n<p>But when I put two apples into a bowl, and then put in another two apples, I get four apples back out, regardless of anything I assume or don't assume. I don't need any axioms at all to get four apples back out.</p>\n<p>\"Well, you do need axioms to talk about&nbsp;<em>four,&nbsp;</em>SSSS0, when you say that you got 'four' apples back out. That said, indeed your experienced outcome - what your eyes see - doesn't depend on what axioms you assume. But that's because the apples are behaving like numbers whether you believe in numbers or not!\"</p>\n<p>The apples are behaving like numbers? What do you mean? I thought numbers were this ethereal mathematical model that got pinpointed by axioms, not by looking at the real world.</p>\n<p>\"Whenever a part of reality behaves in a way that conforms to the number-axioms - for example, if putting apples into a bowl obeys rules, like no apple spontaneously appearing or vanishing, which yields the high-level behavior of numbers - then all the mathematical theorems we proved valid in the universe of numbers can be imported back into reality. The conclusion isn't absolutely certain, because it's not absolutely certain that nobody will sneak in and steal an apple and change the physical bowl's behavior so that it doesn't match the axioms any more. But so long as the premises are true, the conclusions are true; the conclusion can't fail unless a premise also failed. You get four apples in reality, because those apples&nbsp;<em>behaving numerically</em>&nbsp;isn't something you&nbsp;<em>assume,</em>&nbsp;it's something that's&nbsp;<em>physically true.</em>&nbsp;When two clouds collide and form a bigger cloud, on the other hand, they aren't behaving like integers, whether you assume they are or not.\"</p>\n<p>But if the awesome hidden power of mathematical reasoning is to be imported into parts of reality that behave like math, why not reason about apples in the first place instead of these ethereal 'numbers'?</p>\n<p>\"Because you can prove once and for all that&nbsp;<em>in any process which behaves like integers,</em>&nbsp;2 thingies + 2 thingies = 4 thingies. You can store this general fact, and recall the resulting prediction, for&nbsp;<em>many</em>&nbsp;different places inside reality where physical things behave in accordance with the number-axioms. Moreover, so long as we believe that a calculator behaves like numbers, pressing '2 + 2' on a calculator and getting '4' tells us that 2 + 2 = 4 is true of numbers and then to expect four apples in the bowl. It's not like anything fundamentally different from that is going on when we try to add 2 + 2 inside our own&nbsp;<em>brains</em>&nbsp;- all the information we get about these 'logical models' is coming from the observation of physical things that allegedly behave like their axioms, whether it's our neurally-patterned thought processes, or a calculator, or apples in a bowl.\"</p>\n<p>I... think I need to consider this for a while.</p>\n<p>\"Be my guest! Oh, and if you run out of things to think about from what I've said already -\"</p>\n<p>Hold on.</p>\n<p>\"- try pondering this one. Why does 2 + 2 come out the same way each time? Never mind the question of why the&nbsp;<em>laws of physics</em>&nbsp;are stable - why is&nbsp;<em>logic</em>&nbsp;stable? Of course I can't&nbsp;<em>imagine</em>&nbsp;it being any other way, but that's not an explanation.\"</p>\n<p>Are you sure you didn't just degenerate into talking bloody nonsense?</p>\n<p>\"Of&nbsp;<em>course</em>&nbsp;it's bloody nonsense. If I knew a way to think about the question that wasn't bloody nonsense, I would already know the answer.\"</p>\n<hr />\n<p><a href=\"#7os2\">Meditation for next time</a>:</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-variant: small-caps;\">Humans need fantasy to be human.</span></p>\n<p style=\"padding-left: 30px;\">\"Tooth fairies? Hogfathers? Little&mdash;\"</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-variant: small-caps;\">Yes. As practice. You have to start out learning to believe the&nbsp;<em>little</em>&nbsp;lies.</span></p>\n<p style=\"padding-left: 30px;\">\"So we can believe the big ones?\"</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-variant: small-caps;\">Yes. Justice. Mercy. Duty. That sort of thing.</span></p>\n<p style=\"padding-left: 30px;\">\"They're not the same at all!\"</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-variant: small-caps;\">You think so? Then take the universe and grind it down to the finest powder and sieve it through the finest sieve and then&nbsp;<em>show</em>&nbsp;me one atom of justice, one molecule of mercy.</span></p>\n<p style=\"padding-left: 60px;\">- Susan and Death, in <em>Hogfather</em> by Terry Pratchett</p>\n<p>So far we've talked about two kinds of meaningfulness and two ways that sentences can refer; a way of <a href=\"/lw/eqn/the_useful_idea_of_truth/\">comparing to physical things</a> found by <a href=\"/lw/f1u/causal_reference/\">following pinned-down causal links</a>, and logical reference by comparison to models pinned-down by axioms. Is there anything else that can be meaningfully talked about? Where would you find justice, or mercy?</p>\n<hr />\n<p><strong><a href=\"#7os1\">Mainstream status</a>.</strong></p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/fok/causal_universes/\">Causal Universes</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/f43/proofs_implications_and_models/\">Proofs, Implications, and Models</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LnEEs8xGooYmQ8iLA": 1, "6nS8oYmSMuFMaiowF": 3, "yXNtYNHJB54T3bGm3": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3FoMuCLqZggTxoC3S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 75, "baseScore": 104, "extendedScore": null, "score": 0.000228, "legacy": true, "legacyId": "19598", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "standard-and-nonstandard-numbers", "canonicalPrevPostSlug": "proofs-implications-and-models", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 104, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 344, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PXoWk554FZ4Gpfvah", "Z2CuyKtkCmWGQtAEh", "NhQju3htS9W6p6wE6", "XqvnWFtRD2keJdwjX", "o5F2p3krzT4JgzqQc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-02T17:08:17.011Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge, MA Sunday meetup", "slug": "meetup-cambridge-ma-sunday-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HattQ8LFkDdBZwB8s/meetup-cambridge-ma-sunday-meetup", "pageUrlRelative": "/posts/HattQ8LFkDdBZwB8s/meetup-cambridge-ma-sunday-meetup", "linkUrl": "https://www.lesswrong.com/posts/HattQ8LFkDdBZwB8s/meetup-cambridge-ma-sunday-meetup", "postedAtFormatted": "Friday, November 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%2C%20MA%20Sunday%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%2C%20MA%20Sunday%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHattQ8LFkDdBZwB8s%2Fmeetup-cambridge-ma-sunday-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%2C%20MA%20Sunday%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHattQ8LFkDdBZwB8s%2Fmeetup-cambridge-ma-sunday-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHattQ8LFkDdBZwB8s%2Fmeetup-cambridge-ma-sunday-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fi'>Cambridge, MA Sunday meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 November 2012 02:07:17PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups recur on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66].</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fi'>Cambridge, MA Sunday meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HattQ8LFkDdBZwB8s", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0240602254172996e-06, "legacy": true, "legacyId": "19793", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_Sunday_meetup\">Discussion article for the meetup : <a href=\"/meetups/fi\">Cambridge, MA Sunday meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 November 2012 02:07:17PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups recur on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66].</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_Sunday_meetup1\">Discussion article for the meetup : <a href=\"/meetups/fi\">Cambridge, MA Sunday meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge, MA Sunday meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_Sunday_meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge, MA Sunday meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_Sunday_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-02T19:02:40.723Z", "modifiedAt": null, "url": null, "title": "Conformity", "slug": "conformity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:30.527Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hJ8AnX7Csse4XRD6f/conformity", "pageUrlRelative": "/posts/hJ8AnX7Csse4XRD6f/conformity", "linkUrl": "https://www.lesswrong.com/posts/hJ8AnX7Csse4XRD6f/conformity", "postedAtFormatted": "Friday, November 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Conformity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConformity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhJ8AnX7Csse4XRD6f%2Fconformity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Conformity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhJ8AnX7Csse4XRD6f%2Fconformity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhJ8AnX7Csse4XRD6f%2Fconformity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 191, "htmlBody": "<p>A rather good 10 minute YouTube video presenting the results of several papers relevant to how conformity affects our thinking:</p>\n<p><a href=\"http://www.youtube.com/watch?v=TrNIuFrso8I\">http://www.youtube.com/watch?v=TrNIuFrso8I</a></p>\n<p>&nbsp;</p>\n<p>The papers mentioned are:</p>\n<p>Sherif, M. (1935). A study of some social factors in perception. Archives of Psychology, 27(187), pp.17-22.<br /><br />Asch,  S.E. (1951). Effects of group pressure upon the modification and  distortion of judgment. In H. Guetzkow (ed.) Groups, leadership and men.  Pittsburgh, PA: Carnegie Press.<br />Asch, S.E. (1955). Opinions and social pressure. Scientific American, 193(5), pp.31-35.<br /><br />Berns,  G.S., Chappelow, J., Zink, C.F., Pagnoni, G., Martin-Skurski, M.E., and  Richards, J. (2005) 'Neurobiological Correlates of Social Conformity  and Independence During Mental Rotation' Biological Psychiatry, 58(3),  pp.245-253.<br /><br />Weaver, K., Garcia, S.M., Schwarz, N., &amp; Miller,  D.T. (2007) Inferring the popularity of an opinion from its familiarity:  A repetitive voice can sound like a chorus. Journal of Personality and  Social Psychology, 92(5), 821-833.</p>\n<p>&nbsp;</p>\n<p><span style=\"color: #0000ff;\">What techniques do other posters, here on LessWrong, use to monitor and counter these effects in their lives?</span></p>\n<p><span style=\"color: #0000ff;\">The video also lists some of the advantages to a society of having a certain amount of this effect in place.&nbsp;&nbsp; Does anyone here conform too little?</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb124": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hJ8AnX7Csse4XRD6f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 14, "extendedScore": null, "score": 1.0241229021970054e-06, "legacy": true, "legacyId": "19794", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-03T04:59:25.612Z", "modifiedAt": null, "url": null, "title": "Less Wrong Parents", "slug": "less-wrong-parents", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:07.879Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "saliency", "createdAt": "2009-10-25T03:59:58.587Z", "isAdmin": false, "displayName": "saliency"}, "userId": "RNx6ydjKM2J3Heae3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oRr547QkAwsKj5AmP/less-wrong-parents", "pageUrlRelative": "/posts/oRr547QkAwsKj5AmP/less-wrong-parents", "linkUrl": "https://www.lesswrong.com/posts/oRr547QkAwsKj5AmP/less-wrong-parents", "postedAtFormatted": "Saturday, November 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20Parents&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20Parents%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoRr547QkAwsKj5AmP%2Fless-wrong-parents%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20Parents%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoRr547QkAwsKj5AmP%2Fless-wrong-parents", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoRr547QkAwsKj5AmP%2Fless-wrong-parents", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p>Less Wrong Parents <br /> <a href=\"https://groups.google.com/forum/?hl=en&amp;fromgroups=#!forum/less-wrong-parents\">https://groups.google.com/forum/?hl=en&amp;fromgroups=#!forum/less-wrong-parents</a></p>\n<p>Recently the NYC LW/OB community had two babies and is expecting a third.<br /> I created a google group as a way of sharing information,&nbsp;primarily&nbsp;thinking of the NYC community.</p>\n<p>I posted my pre-baby purchase list and William Eden posted an extensive list of books on early parenting.</p>\n<p>William suggested opening up the group so as to get insight from the larger LW community on parenting.<br /> I think this is *probably* a good idea. &nbsp;Google groups are simple to set up but have limits.<br />For this reason I request that if you are going to have an extensive debate on a subject you create a new thread (aka: get a room)</p>\n<p>The primary objective is to lower the cost of obtaining information on parenting.<br /> I&nbsp;believe&nbsp;this overall goal to be more&nbsp;important&nbsp;then any&nbsp;particular&nbsp;\"truth\".</p>\n<p>My hope is that this will&nbsp;primarily&nbsp;serve as a place for people to ask parenting question and post guides.<br /> Perhaps if enough guides are posted they can&nbsp;eventually&nbsp;be consolidated into a wiki.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oRr547QkAwsKj5AmP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 19, "extendedScore": null, "score": 1.0244499683364865e-06, "legacy": true, "legacyId": "19792", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-03T07:31:36.972Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne social meetup", "slug": "meetup-melbourne-social-meetup-4", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:37.794Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "toner", "createdAt": "2009-02-27T05:14:52.530Z", "isAdmin": false, "displayName": "toner"}, "userId": "XaYyFkpvhiiMscJJZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4WAyhqowAGnK9WkKN/meetup-melbourne-social-meetup-4", "pageUrlRelative": "/posts/4WAyhqowAGnK9WkKN/meetup-melbourne-social-meetup-4", "linkUrl": "https://www.lesswrong.com/posts/4WAyhqowAGnK9WkKN/meetup-melbourne-social-meetup-4", "postedAtFormatted": "Saturday, November 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4WAyhqowAGnK9WkKN%2Fmeetup-melbourne-social-meetup-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4WAyhqowAGnK9WkKN%2Fmeetup-melbourne-social-meetup-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4WAyhqowAGnK9WkKN%2Fmeetup-melbourne-social-meetup-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fj'>Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 November 2012 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">see mailing list, Carlton VIC 3053 Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next social meetup is on Friday 16 November, 6:30 for 7pm, at my house. If you have any trouble working out the location or getting in, you can call me on 0412 996 288.</p>\n\n<p>We'll get some snacks and organise some form of take-away for dinner. BYO drinks and games.</p>\n\n<p>We always look forward to meeting new people!</p>\n\n<p>This meetup is usually on the third Friday of the month. We also have a regular meetup about practical rationality on the first Friday of each month.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fj'>Melbourne social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4WAyhqowAGnK9WkKN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.024533409777484e-06, "legacy": true, "legacyId": "19813", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/fj\">Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 November 2012 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">see mailing list, Carlton VIC 3053 Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next social meetup is on Friday 16 November, 6:30 for 7pm, at my house. If you have any trouble working out the location or getting in, you can call me on 0412 996 288.</p>\n\n<p>We'll get some snacks and organise some form of take-away for dinner. BYO drinks and games.</p>\n\n<p>We always look forward to meeting new people!</p>\n\n<p>This meetup is usually on the third Friday of the month. We also have a regular meetup about practical rationality on the first Friday of each month.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/fj\">Melbourne social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-03T22:38:38.971Z", "modifiedAt": null, "url": null, "title": "Sign up to be notified about new LW meetups in your area", "slug": "sign-up-to-be-notified-about-new-lw-meetups-in-your-area", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:31.952Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/97WbQTb4Etch9mDuT/sign-up-to-be-notified-about-new-lw-meetups-in-your-area", "pageUrlRelative": "/posts/97WbQTb4Etch9mDuT/sign-up-to-be-notified-about-new-lw-meetups-in-your-area", "linkUrl": "https://www.lesswrong.com/posts/97WbQTb4Etch9mDuT/sign-up-to-be-notified-about-new-lw-meetups-in-your-area", "postedAtFormatted": "Saturday, November 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sign%20up%20to%20be%20notified%20about%20new%20LW%20meetups%20in%20your%20area&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASign%20up%20to%20be%20notified%20about%20new%20LW%20meetups%20in%20your%20area%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F97WbQTb4Etch9mDuT%2Fsign-up-to-be-notified-about-new-lw-meetups-in-your-area%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sign%20up%20to%20be%20notified%20about%20new%20LW%20meetups%20in%20your%20area%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F97WbQTb4Etch9mDuT%2Fsign-up-to-be-notified-about-new-lw-meetups-in-your-area", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F97WbQTb4Etch9mDuT%2Fsign-up-to-be-notified-about-new-lw-meetups-in-your-area", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<p>LessWrong has rolled out a new feature on the <a href=\"/prefs/\">user preference pages</a> under \"Location\":</p>\n<p>&nbsp;</p>\n<p><img src=\"http://louie.divide0.net/lwshot.png\" alt=\"\" /></p>\n<p>&nbsp;</p>\n<p>This is UNCHECKED by default. &nbsp;Also, we don't know by default where you live.</p>\n<p>If you think you might want to meet up with other LWers, please&nbsp;input your location and check this box. Once enough people have signed up in a new area, it will make starting a LW meetup there that much more straightforward for the future organizer. &nbsp;(You'll just get that email; they won't see your email address.) &nbsp;In general, it seems like being able to know something about where LWers live would be helpful, so please consider entering your approximate location even if you don't check the box.</p>\n<p>Thanks to <a href=\"/user/wmoore/\">Wesley Moore</a> for deploying this upgrade to the LW backend.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "T57Qd9J3AfxmwhQtY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "97WbQTb4Etch9mDuT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 48, "extendedScore": null, "score": 1.0250309639562876e-06, "legacy": true, "legacyId": "19789", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-03T23:00:19.891Z", "modifiedAt": null, "url": null, "title": "2012 Less Wrong Census/Survey", "slug": "2012-less-wrong-census-survey", "viewCount": null, "lastCommentedAt": "2012-12-07T23:06:08.308Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bJiyYJeCyh4HcKHub/2012-less-wrong-census-survey", "pageUrlRelative": "/posts/bJiyYJeCyh4HcKHub/2012-less-wrong-census-survey", "linkUrl": "https://www.lesswrong.com/posts/bJiyYJeCyh4HcKHub/2012-less-wrong-census-survey", "postedAtFormatted": "Saturday, November 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202012%20Less%20Wrong%20Census%2FSurvey&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2012%20Less%20Wrong%20Census%2FSurvey%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbJiyYJeCyh4HcKHub%2F2012-less-wrong-census-survey%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2012%20Less%20Wrong%20Census%2FSurvey%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbJiyYJeCyh4HcKHub%2F2012-less-wrong-census-survey", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbJiyYJeCyh4HcKHub%2F2012-less-wrong-census-survey", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 405, "htmlBody": "<p><strong>11/26: The survey is now closed. Please do not take the survey. Your results will not be counted.<br /></strong></p>\n<p>It's that time of year again.</p>\n<p>If you are reading this post, and have not been sent here by some sort of conspiracy trying to throw off the survey results, then you are the target population for the Less Wrong Census/Survey. Please take it. Doesn't matter if you don't post much. Doesn't matter if you're a lurker. Take the survey.</p>\n<p>This year's census contains a \"main survey\" that should take about ten or fifteen minutes, as well as a bunch of \"extra credit questions\". You may do the extra credit questions if you want. <strong>You may skip all the extra credit questions if you want.</strong> They're pretty long and not all of them are very interesting. But it is very important that you not put off doing the survey or not do the survey at all because you're intimidated by the extra credit questions.<br /><br />The survey will probably remain open for a month or so, but once again <strong>do not delay</strong> taking the survey just for the sake of the extra credit questions.</p>\n<p>Please make things easier for my computer and by extension me by reading all the instructions and by answering any text questions in the most obvious possible way. For example, if it asks you \"What language do you speak?\" please answer \"English\" instead of \"I speak English\" or \"It's English\" or \"English since I live in Canada\" or \"English (US)\" or anything else. This will help me sort responses quickly and easily. Likewise, if a question asks for a number, please answer with a number such as \"4\", rather than \"four\".</p>\n<p>Okay! Enough nitpicky rules! Time to take the...</p>\n<p><a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dG1pTzlrTnJ4eks3aE13Ni1lbV8yUkE6MQ#gid=0\"><strong>2012 Less Wrong Census/Survey</strong></a></p>\n<p>Thanks to everyone who suggested questions and ideas for the 2012 Less Wrong Census Survey. I regret I was unable to take all of your suggestions into account, because some of them were contradictory, others were vague, and others would have required me to provide two dozen answers and a thesis paper worth of explanatory text for every question anyone might conceivably misunderstand. But I did make about twenty changes based on the feedback, and *most* of the suggested questions have found their way into the text.</p>\n<p>By ancient tradition, if you take the survey you may comment saying you have done so here, and people will upvote you and you will get karma.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"kJrjorSx3hXa7q7CJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bJiyYJeCyh4HcKHub", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 66, "baseScore": 94, "extendedScore": null, "score": 0.00021733215455337925, "legacy": true, "legacyId": "19785", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 65, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 737, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-11-03T23:00:19.891Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-03T23:47:36.580Z", "modifiedAt": null, "url": null, "title": "Analyzing FF.net reviews of 'Harry Potter and the Methods of Rationality'", "slug": "analyzing-ff-net-reviews-of-harry-potter-and-the-methods-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:34.322Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X92BMJ6jLHnyJJHxF/analyzing-ff-net-reviews-of-harry-potter-and-the-methods-of", "pageUrlRelative": "/posts/X92BMJ6jLHnyJJHxF/analyzing-ff-net-reviews-of-harry-potter-and-the-methods-of", "linkUrl": "https://www.lesswrong.com/posts/X92BMJ6jLHnyJJHxF/analyzing-ff-net-reviews-of-harry-potter-and-the-methods-of", "postedAtFormatted": "Saturday, November 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Analyzing%20FF.net%20reviews%20of%20'Harry%20Potter%20and%20the%20Methods%20of%20Rationality'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnalyzing%20FF.net%20reviews%20of%20'Harry%20Potter%20and%20the%20Methods%20of%20Rationality'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX92BMJ6jLHnyJJHxF%2Fanalyzing-ff-net-reviews-of-harry-potter-and-the-methods-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Analyzing%20FF.net%20reviews%20of%20'Harry%20Potter%20and%20the%20Methods%20of%20Rationality'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX92BMJ6jLHnyJJHxF%2Fanalyzing-ff-net-reviews-of-harry-potter-and-the-methods-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX92BMJ6jLHnyJJHxF%2Fanalyzing-ff-net-reviews-of-harry-potter-and-the-methods-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<blockquote>\n<p>The unprecedented gap in <em>Methods of Rationality</em> updates prompts musing about whether readership is increasing enough &amp; what statistics one would use; I write code to download FF.net reviews, clean it, parse it, load into R, summarize the data &amp; depict it graphically, run linear regression on a subset &amp; all reviews, note the poor fit, develop a quadratic fit instead, and use it to predict future review quantities.</p>\n<p>Then, I run a similar analysis on a competing fanfiction to find out when they will have equal total review-counts. A try at logarithmic fits fails; fitting a linear model to the previous 100 days of _MoR_ and the competitor works much better, and they predict a convergence in &lt;5 years.</p>\n</blockquote>\n<p>Master version: <a href=\"http://www.gwern.net/hpmor#analysis\">http://www.gwern.net/hpmor#analysis</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X92BMJ6jLHnyJJHxF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 40, "extendedScore": null, "score": 1.0250683417990258e-06, "legacy": true, "legacyId": "19816", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-04T04:26:17.343Z", "modifiedAt": null, "url": null, "title": "In Defense of Moral Investigation", "slug": "in-defense-of-moral-investigation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:31.843Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MTGandP", "createdAt": "2012-09-06T18:23:32.966Z", "isAdmin": false, "displayName": "MTGandP"}, "userId": "7k2bePi5NxYxr34jv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fsm6WRZJPADEKEr2R/in-defense-of-moral-investigation", "pageUrlRelative": "/posts/fsm6WRZJPADEKEr2R/in-defense-of-moral-investigation", "linkUrl": "https://www.lesswrong.com/posts/fsm6WRZJPADEKEr2R/in-defense-of-moral-investigation", "postedAtFormatted": "Sunday, November 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20Defense%20of%20Moral%20Investigation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20Defense%20of%20Moral%20Investigation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffsm6WRZJPADEKEr2R%2Fin-defense-of-moral-investigation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20Defense%20of%20Moral%20Investigation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffsm6WRZJPADEKEr2R%2Fin-defense-of-moral-investigation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffsm6WRZJPADEKEr2R%2Fin-defense-of-moral-investigation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1530, "htmlBody": "<p><em>Cross-posted from <a href=\"http://mtgap.wordpress.com/2012/10/31/in-defense-of-moral-investigation/\">my blog</a></em>.</p>\n<p>Some argue that certain claims about the nature of reality could cause people to become more immoral. Examples of such suppositions include:</p>\n<p>1. People should follow Christianity because we will be more moral if we have to avoid eternal damnation.<br /> 2. The theory of evolution says that since people evolved from bacteria and have no immortal souls, human lives are worthless. Therefore, we can rape and kill each other and there&rsquo;s nothing wrong with that.<br /> 3. The theory of evolution says that people should act selfishly all the time.<br /> 4. If free will doesn&rsquo;t exist, people will be free to hurt and kill each other and won&rsquo;t be held responsible.</p>\n<p>Such arguments are bogus. Any new information about reality, <em>if properly understood</em> (that part is important), can only cause people to become <em>more</em> ethical. Morality is contingent upon the nature of the universe; the better we understand the universe, the better we understand morality.</p>\n<p>Some people fear that if we investigate reality, we will discover truths that cause us to behave unethically. In some cases, people even wish to discount discoveries that already have been made&mdash;such as natural selection or the nonexistence of free will <a href=\"#1\">[1]</a>&mdash;on the basis that these discoveries may lead to immoral behavior.</p>\n<p>Someone may take the theory of evolution and use that as evidence that it is morally justified to behave selfishly at the expense of others. However, such a person would be misinterpreting evolution. Nowhere does the theory of evolution say that we <em>should</em> attempt to propagate our genes at the expense of every other living being; it merely explains that beings that <em>do</em> do that tend to survive and reproduce. Evolution tells us nothing about what we ought to value.</p>\n<p>On the other hand, evolution (and, in fact, all branches of science) does tell us something about how to achieve what we do value. Once we understand how the world works, we can take it into account and more effectively work towards our goals. (This is Sam Harris&rsquo; thesis in <a href=\"http://mtgap.wordpress.com/2011/12/02/book-review-the-moral-landscape/\"><em>The Moral Landscape</em></a>.) For example, positive psychology provides insights into how best to make ourselves happy; and biology tells us which animals can feel pain and therefore <a href=\"http://mtgap.wordpress.com/2012/06/18/animal-suffering/\">deserve moral consideration</a>.</p>\n<p>It is possible to make a discovery that changes our conceptions about what is or is not moral. If such a discovery is made, what was previously thought to be immoral may be found to be moral, or vice versa. Some Europeans justified slavery by claiming that Africans were stupid or unable to take care of themselves, and that having a master was good for them; when science proved such claims to be false, it was impossible to scientifically support slavery.</p>\n<p><strong>Race and Intelligence</strong></p>\n<p>When anthropologist Samuel Morton found that Africans had smaller craniums than Europeans, he stirred up considerable controversy. Evolutionary biologist Stephen Jay Gould argued that Morton&rsquo;s findings were the result of bias, but <a href=\"http://www.nytimes.com/2011/06/14/science/14skull.html\">later studies</a> affirmed Morton&rsquo;s results and concluded that Gould had in fact been biased by his desire to affirm racial equality.</p>\n<p>As modern neuroscience has shown, there is no correlation between cranial size and intelligence (at least between individuals of the same species). But imagine that it were discovered that Africans and those of African descent are indeed less intelligent on average than Europeans. What would that say about how we should treat them?</p>\n<p>It certainly would not justify slavery: a person&rsquo;s moral worth has nothing to do with her intelligence. If people took an enlightened perspective about this new discovery, it could only serve to improve the world. There would be no question that people of African descent could still be happy and contribute to society. If brains truly functioned differently for different races, a strong understanding of those differences could empower us to improve the education system by teaching in different and more appropriate learning styles. An outcome where a particular race becomes less happy could only arise because the science was not properly understood.</p>\n<p>As I write this, I feel some stigma attached to discussing the possibility that people of African descent are less intelligent. I see three main reasons for this. The first is that, not so long ago, African-Americans were considered unintelligent by the large portion of Western society and currently it is taboo even to raise a hypothetical scenario in which they are less intelligent. The second reason is that they are probably not. Races tend to score differently on IQ tests (with Asians scoring the highest), but (a) this could be the result of environmental influences and biases (including <a href=\"http://en.wikipedia.org/wiki/Stereotype_threat\">stereotype threat</a>) and (b) IQ is a very limited measure of intelligence (I find myself continually surprised when news articles use the terms &ldquo;IQ&rdquo; and &ldquo;intelligence&rdquo; interchangeably). No robust evidence has ever demonstrated that one race is more or less intelligent than another. If we treated black people as though they were less intelligent than other races, that would clearly be a problem. The third reason is that even if some races are more or less intelligent <em>on average</em>, there would still be a large amount of overlap. Africans tend to be taller than Asians, for example, but there are plenty of tall Asians and plenty of short Africans. Therefore, it would be unfair to treat all Asians as though they are short and all Africans as though they are tall. (Obviously this example is a bit silly since one can immediately assess how tall a person is, but it is meant only as an illustrative analogy.)</p>\n<p>But when people are truly different, treating them differently is not a bad thing. Consider dyslexia. People with dyslexia generally perform more poorly on certain tasks than people without dyslexia. However, they are not stigmatized or oppressed (for the most part, at least); instead, they are given specialized education programs designed to help them learn more effectively. Such programs help dyslexics more easily perform certain tasks that they would otherwise have difficulty performing. And although dyslexics are treated differently, it would not make sense to create separate bathrooms for them or require them to sit at the back of the bus. People with dyslexia are normal in every way, and where they are not, society does not stigmatize but helps them (for the most part, anyway). And where society does fail to help them, it is not because we know too much about them; indeed, it is often because we know too little.</p>\n<p><strong>Irrationality</strong></p>\n<p>Sometimes knowing the truth makes things worse, but only if one holds irrational beliefs. For example, one may believe that the theory of evolution dictates that people should act selfishly all the time. If one held such a belief, it may be better to ignore the evidence in favor of evolution. Of course, such a belief has no rational basis.</p>\n<p>Unfortunately, even mostly-rational people may have difficulty avoiding <a href=\"/\">irrational</a> emotional reactions to facts <a href=\"#2\">[2]</a>. A rational person can sometimes override an emotional response, but even the best of us cannot behave completely rationally. Given what we <a href=\"http://youarenotsosmart.com/\">know</a> about human irrationality, how should we adjust our behavior?</p>\n<p>Even if we acknowledge that humans behave in <a href=\"http://danariely.com/\">predictably irrational</a> ways, we should still err on the side of investigating truth too much rather than too little. Knowing the truth rarely hurts; when it does, it is because we are behaving irrationally; when we are, we can often overcome our irrationality. Indeed, uncovering the truth may actually <em>help us</em> overcome our irrationality.</p>\n<p>A particular truth can only hurt someone if he holds a false belief. For example, if he believes that <em>if</em> African-Americans are less intelligent <em>then</em> slavery is justified, it is better for him to believe that black people are not less intelligent. However, the best solution is to rectify the false premise: even if African-Americans are less intelligent, slavery is <em>not</em> justified.</p>\n<p>As Eliezer Yudkowsky put it, &ldquo;Doing worse with more knowledge means you are doing something very wrong.&rdquo;</p>\n<p>To close, here is a quote by Richard Feynman:</p>\n<blockquote>\n<p>Poets say science takes away from the beauty of the stars&mdash;mere globs of gas atoms. Nothing is &lsquo;mere&rsquo;. I too can see the stars on a desert night, and feel them. But do I see less or more? The vastness of the heavens stretches my imagination&mdash;stuck on this carousel my little eye can catch one-million-year-old light. A vast pattern&mdash;of which I am a part&hellip; What is the pattern, or the meaning, or the why? It does not do harm to the mystery to know a little about it. For far more marvelous is the truth than any artists of the past imagined it. Why do the poets of the present not speak of it? What men are poets who can speak of Jupiter if he were a man, but if he is an immense spinning sphere of methane and ammonia must be silent?</p>\n</blockquote>\n<p><strong>Notes</strong></p>\n<p><a name=\"1\">[1]</a> Free will is a persistent illusion, and many readers may doubt me when I claim that it does not exist. Sam Harris offers an eloquent and accessible explanation of free will, found <a href=\"http://www.samharris.org/blog/item/free-will-why-you-still-dont-have-it/\">here</a> and continued <a href=\"http://www.samharris.org/blog/item/you-do-not-choose-what-you-choose/\">here</a>. I have also <a href=\"http://mtgap.wordpress.com/category/philosophy/free-will/\">written</a> on the subject.</p>\n<p><a name=\"2\">[2]</a> This is not to say that emotions are always irrational, or that rationality is opposed to emotion. Rather, some particular emotional responses can arise for irrational reasons. See Eliezer Yudkowsky, <a href=\"/lw/hp/feeling_rational/\">&ldquo;Feeling Rational&rdquo;</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fsm6WRZJPADEKEr2R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": -9, "extendedScore": null, "score": -2.3e-05, "legacy": true, "legacyId": "19755", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Cross-posted from <a href=\"http://mtgap.wordpress.com/2012/10/31/in-defense-of-moral-investigation/\">my blog</a></em>.</p>\n<p>Some argue that certain claims about the nature of reality could cause people to become more immoral. Examples of such suppositions include:</p>\n<p>1. People should follow Christianity because we will be more moral if we have to avoid eternal damnation.<br> 2. The theory of evolution says that since people evolved from bacteria and have no immortal souls, human lives are worthless. Therefore, we can rape and kill each other and there\u2019s nothing wrong with that.<br> 3. The theory of evolution says that people should act selfishly all the time.<br> 4. If free will doesn\u2019t exist, people will be free to hurt and kill each other and won\u2019t be held responsible.</p>\n<p>Such arguments are bogus. Any new information about reality, <em>if properly understood</em> (that part is important), can only cause people to become <em>more</em> ethical. Morality is contingent upon the nature of the universe; the better we understand the universe, the better we understand morality.</p>\n<p>Some people fear that if we investigate reality, we will discover truths that cause us to behave unethically. In some cases, people even wish to discount discoveries that already have been made\u2014such as natural selection or the nonexistence of free will <a href=\"#1\">[1]</a>\u2014on the basis that these discoveries may lead to immoral behavior.</p>\n<p>Someone may take the theory of evolution and use that as evidence that it is morally justified to behave selfishly at the expense of others. However, such a person would be misinterpreting evolution. Nowhere does the theory of evolution say that we <em>should</em> attempt to propagate our genes at the expense of every other living being; it merely explains that beings that <em>do</em> do that tend to survive and reproduce. Evolution tells us nothing about what we ought to value.</p>\n<p>On the other hand, evolution (and, in fact, all branches of science) does tell us something about how to achieve what we do value. Once we understand how the world works, we can take it into account and more effectively work towards our goals. (This is Sam Harris\u2019 thesis in <a href=\"http://mtgap.wordpress.com/2011/12/02/book-review-the-moral-landscape/\"><em>The Moral Landscape</em></a>.) For example, positive psychology provides insights into how best to make ourselves happy; and biology tells us which animals can feel pain and therefore <a href=\"http://mtgap.wordpress.com/2012/06/18/animal-suffering/\">deserve moral consideration</a>.</p>\n<p>It is possible to make a discovery that changes our conceptions about what is or is not moral. If such a discovery is made, what was previously thought to be immoral may be found to be moral, or vice versa. Some Europeans justified slavery by claiming that Africans were stupid or unable to take care of themselves, and that having a master was good for them; when science proved such claims to be false, it was impossible to scientifically support slavery.</p>\n<p><strong id=\"Race_and_Intelligence\">Race and Intelligence</strong></p>\n<p>When anthropologist Samuel Morton found that Africans had smaller craniums than Europeans, he stirred up considerable controversy. Evolutionary biologist Stephen Jay Gould argued that Morton\u2019s findings were the result of bias, but <a href=\"http://www.nytimes.com/2011/06/14/science/14skull.html\">later studies</a> affirmed Morton\u2019s results and concluded that Gould had in fact been biased by his desire to affirm racial equality.</p>\n<p>As modern neuroscience has shown, there is no correlation between cranial size and intelligence (at least between individuals of the same species). But imagine that it were discovered that Africans and those of African descent are indeed less intelligent on average than Europeans. What would that say about how we should treat them?</p>\n<p>It certainly would not justify slavery: a person\u2019s moral worth has nothing to do with her intelligence. If people took an enlightened perspective about this new discovery, it could only serve to improve the world. There would be no question that people of African descent could still be happy and contribute to society. If brains truly functioned differently for different races, a strong understanding of those differences could empower us to improve the education system by teaching in different and more appropriate learning styles. An outcome where a particular race becomes less happy could only arise because the science was not properly understood.</p>\n<p>As I write this, I feel some stigma attached to discussing the possibility that people of African descent are less intelligent. I see three main reasons for this. The first is that, not so long ago, African-Americans were considered unintelligent by the large portion of Western society and currently it is taboo even to raise a hypothetical scenario in which they are less intelligent. The second reason is that they are probably not. Races tend to score differently on IQ tests (with Asians scoring the highest), but (a) this could be the result of environmental influences and biases (including <a href=\"http://en.wikipedia.org/wiki/Stereotype_threat\">stereotype threat</a>) and (b) IQ is a very limited measure of intelligence (I find myself continually surprised when news articles use the terms \u201cIQ\u201d and \u201cintelligence\u201d interchangeably). No robust evidence has ever demonstrated that one race is more or less intelligent than another. If we treated black people as though they were less intelligent than other races, that would clearly be a problem. The third reason is that even if some races are more or less intelligent <em>on average</em>, there would still be a large amount of overlap. Africans tend to be taller than Asians, for example, but there are plenty of tall Asians and plenty of short Africans. Therefore, it would be unfair to treat all Asians as though they are short and all Africans as though they are tall. (Obviously this example is a bit silly since one can immediately assess how tall a person is, but it is meant only as an illustrative analogy.)</p>\n<p>But when people are truly different, treating them differently is not a bad thing. Consider dyslexia. People with dyslexia generally perform more poorly on certain tasks than people without dyslexia. However, they are not stigmatized or oppressed (for the most part, at least); instead, they are given specialized education programs designed to help them learn more effectively. Such programs help dyslexics more easily perform certain tasks that they would otherwise have difficulty performing. And although dyslexics are treated differently, it would not make sense to create separate bathrooms for them or require them to sit at the back of the bus. People with dyslexia are normal in every way, and where they are not, society does not stigmatize but helps them (for the most part, anyway). And where society does fail to help them, it is not because we know too much about them; indeed, it is often because we know too little.</p>\n<p><strong id=\"Irrationality\">Irrationality</strong></p>\n<p>Sometimes knowing the truth makes things worse, but only if one holds irrational beliefs. For example, one may believe that the theory of evolution dictates that people should act selfishly all the time. If one held such a belief, it may be better to ignore the evidence in favor of evolution. Of course, such a belief has no rational basis.</p>\n<p>Unfortunately, even mostly-rational people may have difficulty avoiding <a href=\"/\">irrational</a> emotional reactions to facts <a href=\"#2\">[2]</a>. A rational person can sometimes override an emotional response, but even the best of us cannot behave completely rationally. Given what we <a href=\"http://youarenotsosmart.com/\">know</a> about human irrationality, how should we adjust our behavior?</p>\n<p>Even if we acknowledge that humans behave in <a href=\"http://danariely.com/\">predictably irrational</a> ways, we should still err on the side of investigating truth too much rather than too little. Knowing the truth rarely hurts; when it does, it is because we are behaving irrationally; when we are, we can often overcome our irrationality. Indeed, uncovering the truth may actually <em>help us</em> overcome our irrationality.</p>\n<p>A particular truth can only hurt someone if he holds a false belief. For example, if he believes that <em>if</em> African-Americans are less intelligent <em>then</em> slavery is justified, it is better for him to believe that black people are not less intelligent. However, the best solution is to rectify the false premise: even if African-Americans are less intelligent, slavery is <em>not</em> justified.</p>\n<p>As Eliezer Yudkowsky put it, \u201cDoing worse with more knowledge means you are doing something very wrong.\u201d</p>\n<p>To close, here is a quote by Richard Feynman:</p>\n<blockquote>\n<p>Poets say science takes away from the beauty of the stars\u2014mere globs of gas atoms. Nothing is \u2018mere\u2019. I too can see the stars on a desert night, and feel them. But do I see less or more? The vastness of the heavens stretches my imagination\u2014stuck on this carousel my little eye can catch one-million-year-old light. A vast pattern\u2014of which I am a part\u2026 What is the pattern, or the meaning, or the why? It does not do harm to the mystery to know a little about it. For far more marvelous is the truth than any artists of the past imagined it. Why do the poets of the present not speak of it? What men are poets who can speak of Jupiter if he were a man, but if he is an immense spinning sphere of methane and ammonia must be silent?</p>\n</blockquote>\n<p><strong id=\"Notes\">Notes</strong></p>\n<p><a name=\"1\">[1]</a> Free will is a persistent illusion, and many readers may doubt me when I claim that it does not exist. Sam Harris offers an eloquent and accessible explanation of free will, found <a href=\"http://www.samharris.org/blog/item/free-will-why-you-still-dont-have-it/\">here</a> and continued <a href=\"http://www.samharris.org/blog/item/you-do-not-choose-what-you-choose/\">here</a>. I have also <a href=\"http://mtgap.wordpress.com/category/philosophy/free-will/\">written</a> on the subject.</p>\n<p><a name=\"2\">[2]</a> This is not to say that emotions are always irrational, or that rationality is opposed to emotion. Rather, some particular emotional responses can arise for irrational reasons. See Eliezer Yudkowsky, <a href=\"/lw/hp/feeling_rational/\">\u201cFeeling Rational\u201d</a>.</p>", "sections": [{"title": "Race and Intelligence", "anchor": "Race_and_Intelligence", "level": 1}, {"title": "Irrationality", "anchor": "Irrationality", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "78 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SqF8cHjJv43mvJJzx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-04T11:59:04.640Z", "modifiedAt": null, "url": null, "title": "[LINK] Q&A with Larry Wasserman on risks from AI", "slug": "link-q-and-a-with-larry-wasserman-on-risks-from-ai", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hb2C3ftDXhaXZCAZk/link-q-and-a-with-larry-wasserman-on-risks-from-ai", "pageUrlRelative": "/posts/hb2C3ftDXhaXZCAZk/link-q-and-a-with-larry-wasserman-on-risks-from-ai", "linkUrl": "https://www.lesswrong.com/posts/hb2C3ftDXhaXZCAZk/link-q-and-a-with-larry-wasserman-on-risks-from-ai", "postedAtFormatted": "Sunday, November 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Q%26A%20with%20Larry%20Wasserman%20on%20risks%20from%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Q%26A%20with%20Larry%20Wasserman%20on%20risks%20from%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhb2C3ftDXhaXZCAZk%2Flink-q-and-a-with-larry-wasserman-on-risks-from-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Q%26A%20with%20Larry%20Wasserman%20on%20risks%20from%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhb2C3ftDXhaXZCAZk%2Flink-q-and-a-with-larry-wasserman-on-risks-from-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhb2C3ftDXhaXZCAZk%2Flink-q-and-a-with-larry-wasserman-on-risks-from-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<p>I believe XiXiDu isn't posting on LW any more, but those following his interview series may be interested in the latest installment, an <a href=\"http://kruel.co/2012/11/04/qa-with-larry-wasserman-on-risks-from-ai/\">interview</a> with Larry Wasserman of the Department of Statistics and the Machine Learning Department at&nbsp;Carnegie Mellon University.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hb2C3ftDXhaXZCAZk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 7, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "19820", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-04T15:09:30.066Z", "modifiedAt": null, "url": null, "title": "[LINK] Q&A with John E. Laird and Kristinn R. Thorisson on risks from AI", "slug": "link-q-and-a-with-john-e-laird-and-kristinn-r-thorisson-on", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZF9XwXa4rYRX3oobu/link-q-and-a-with-john-e-laird-and-kristinn-r-thorisson-on", "pageUrlRelative": "/posts/ZF9XwXa4rYRX3oobu/link-q-and-a-with-john-e-laird-and-kristinn-r-thorisson-on", "linkUrl": "https://www.lesswrong.com/posts/ZF9XwXa4rYRX3oobu/link-q-and-a-with-john-e-laird-and-kristinn-r-thorisson-on", "postedAtFormatted": "Sunday, November 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Q%26A%20with%20John%20E.%20Laird%20and%20Kristinn%20R.%20Thorisson%20on%20risks%20from%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Q%26A%20with%20John%20E.%20Laird%20and%20Kristinn%20R.%20Thorisson%20on%20risks%20from%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZF9XwXa4rYRX3oobu%2Flink-q-and-a-with-john-e-laird-and-kristinn-r-thorisson-on%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Q%26A%20with%20John%20E.%20Laird%20and%20Kristinn%20R.%20Thorisson%20on%20risks%20from%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZF9XwXa4rYRX3oobu%2Flink-q-and-a-with-john-e-laird-and-kristinn-r-thorisson-on", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZF9XwXa4rYRX3oobu%2Flink-q-and-a-with-john-e-laird-and-kristinn-r-thorisson-on", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 137, "htmlBody": "<p>(Not added to the previous post so as not to foul up your RSS feeds.)</p>\n<p>The <a href=\"http://kruel.co/2012/08/15/qa-with-experts-on-risks-from-ai-5/\">previous interview</a> in XiXiDu's series, not posted here but on his site. They're all <a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">indexed</a> on the wiki.</p>\n<p style=\"padding-left: 30px;\">Professor&nbsp;<strong>John E. Laird</strong> is the founder of&nbsp;Soar Technology, an Ann Arbor company specializing in creating autonomous AI entities. His&nbsp;major research interest is in creating human-level artificial intelligent entities, with an emphasis on the underlying cognitive architecture.</p>\n<p style=\"padding-left: 30px;\">Dr. <strong>Kristinn R. Thorisson</strong> has been developing A.I. systems and technologies for over two decades. He is the Coordinator / Principal Investigator of the HUMANOBS FP7 project and co-author of the AERA architecture, with Eric Nivel, which targets artificial general intelligence. A key driving force behind the project is Thorisson&rsquo;s new Constructivist Methodology which lays out principles for why and how AI architectures must be given introspective and self-programming capabilities.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZF9XwXa4rYRX3oobu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "19821", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}